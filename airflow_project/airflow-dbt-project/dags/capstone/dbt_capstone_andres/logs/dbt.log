[0m14:43:54.139051 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111eeff10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111edc490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111e9b650>]}


============================== 14:43:54.144291 | 7e17404f-318d-4f44-b914-ac94d3a3735d ==============================
[0m14:43:54.144291 [info ] [MainThread]: Running with dbt=1.9.0-b3
[0m14:43:54.144974 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'profiles_dir': '.', 'log_path': 'logs', 'debug': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'introspect': 'True', 'invocation_command': 'dbt compile -s stg_fiscal_data', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m14:43:54.148597 [error] [MainThread]: Encountered an error:
Runtime Error
  Could not find profile named 'andres_capstone_dbt'
[0m14:43:54.150975 [debug] [MainThread]: Resource report: {"command_name": "compile", "command_wall_clock_time": 0.08340391, "process_user_time": 1.282514, "process_kernel_time": 0.262941, "process_mem_max_rss": "92803072", "command_success": false, "process_in_blocks": "0", "process_out_blocks": "0"}
[0m14:43:54.151899 [debug] [MainThread]: Command `dbt compile` failed at 14:43:54.151732 after 0.08 seconds
[0m14:43:54.152458 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111ea4e90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111ef1e90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e24cd50>]}
[0m14:43:54.152958 [debug] [MainThread]: Flushing usage events
[0m14:43:54.358380 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:44:34.588631 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109717810>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1096c0b90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1096c0990>]}


============================== 14:44:34.594433 | ce6e3862-e583-4f0b-9012-fa8ca3c390df ==============================
[0m14:44:34.594433 [info ] [MainThread]: Running with dbt=1.9.0-b3
[0m14:44:34.595182 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'fail_fast': 'False', 'log_path': 'logs', 'debug': 'False', 'profiles_dir': '.', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt compile -s stg_fiscal_data', 'introspect': 'True', 'log_format': 'default', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m14:44:36.200645 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'ce6e3862-e583-4f0b-9012-fa8ca3c390df', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d9a8550>]}
[0m14:44:36.264802 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'ce6e3862-e583-4f0b-9012-fa8ca3c390df', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108977ed0>]}
[0m14:44:36.266245 [info ] [MainThread]: Registered adapter: snowflake=1.8.4
[0m14:44:36.473124 [debug] [MainThread]: checksum: 5f4fde221ca8627174a0341efc73099aaec05b8546408ceed6f59b1ec0755115, vars: {}, profile: , target: , version: 1.9.0b3
[0m14:44:36.474426 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m14:44:36.475025 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'ce6e3862-e583-4f0b-9012-fa8ca3c390df', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1096fa590>]}
[0m14:44:37.785901 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.andres_capstone_dbt.example
[0m14:44:37.796571 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'ce6e3862-e583-4f0b-9012-fa8ca3c390df', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10fe01490>]}
[0m14:44:37.903439 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'ce6e3862-e583-4f0b-9012-fa8ca3c390df', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x113bdb0d0>]}
[0m14:44:37.903978 [info ] [MainThread]: Found 3 models, 4 data tests, 2 sources, 459 macros
[0m14:44:37.904359 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ce6e3862-e583-4f0b-9012-fa8ca3c390df', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10fae11d0>]}
[0m14:44:37.905820 [info ] [MainThread]: 
[0m14:44:37.906192 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:44:37.906526 [info ] [MainThread]: 
[0m14:44:37.907004 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m14:44:37.911569 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_DATAEXPERT_STUDENT_andres'
[0m14:44:37.927188 [debug] [ThreadPool]: Using snowflake connection "list_DATAEXPERT_STUDENT_andres"
[0m14:44:37.927639 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "connection_name": "list_DATAEXPERT_STUDENT_andres"} */
show objects in DATAEXPERT_STUDENT.andres limit 10000
[0m14:44:37.927973 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:44:39.237046 [debug] [ThreadPool]: SQL status: SUCCESS 18 in 1.309 seconds
[0m14:44:39.239640 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: Close
[0m14:44:39.461480 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ce6e3862-e583-4f0b-9012-fa8ca3c390df', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110018890>]}
[0m14:44:39.464451 [debug] [Thread-1 (]: Began running node model.dbt_capstone_andres.stg_fiscal_data
[0m14:44:39.464923 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_DATAEXPERT_STUDENT_andres, now model.dbt_capstone_andres.stg_fiscal_data)
[0m14:44:39.465326 [debug] [Thread-1 (]: Began compiling node model.dbt_capstone_andres.stg_fiscal_data
[0m14:44:39.472169 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_capstone_andres.stg_fiscal_data"
[0m14:44:39.473215 [debug] [Thread-1 (]: Began executing node model.dbt_capstone_andres.stg_fiscal_data
[0m14:44:39.473934 [debug] [Thread-1 (]: Finished running node model.dbt_capstone_andres.stg_fiscal_data
[0m14:44:39.474822 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:44:39.475151 [debug] [MainThread]: Connection 'model.dbt_capstone_andres.stg_fiscal_data' was properly closed.
[0m14:44:39.475828 [debug] [MainThread]: Command end result
[0m14:44:39.515436 [info ] [MainThread]: Compiled node 'stg_fiscal_data' is:
select * from source('treasury','fiscal_data')
[0m14:44:39.520143 [debug] [MainThread]: Resource report: {"command_name": "compile", "command_success": true, "command_wall_clock_time": 5.007403, "process_user_time": 4.017456, "process_kernel_time": 0.72072, "process_mem_max_rss": "192000000", "process_in_blocks": "0", "process_out_blocks": "0"}
[0m14:44:39.521131 [debug] [MainThread]: Command `dbt compile` succeeded at 14:44:39.520920 after 5.01 seconds
[0m14:44:39.522216 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109736310>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109718fd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105a3a9d0>]}
[0m14:44:39.523097 [debug] [MainThread]: Flushing usage events
[0m14:44:39.716992 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:45:01.432817 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e70ba10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e5229d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10dad2410>]}


============================== 14:45:01.438165 | 6652900f-1b51-4846-878f-55d3f68334db ==============================
[0m14:45:01.438165 [info ] [MainThread]: Running with dbt=1.9.0-b3
[0m14:45:01.438878 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'debug': 'False', 'warn_error': 'None', 'log_path': 'logs', 'fail_fast': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt compile -s stg_fiscal_data', 'log_format': 'default', 'introspect': 'True', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m14:45:02.466152 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '6652900f-1b51-4846-878f-55d3f68334db', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e75d090>]}
[0m14:45:02.522412 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '6652900f-1b51-4846-878f-55d3f68334db', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114a86fd0>]}
[0m14:45:02.523320 [info ] [MainThread]: Registered adapter: snowflake=1.8.4
[0m14:45:02.687327 [debug] [MainThread]: checksum: 5f4fde221ca8627174a0341efc73099aaec05b8546408ceed6f59b1ec0755115, vars: {}, profile: , target: , version: 1.9.0b3
[0m14:45:02.822858 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m14:45:02.823558 [debug] [MainThread]: Partial parsing: updated file: dbt_capstone_andres://models/staging/stg_fiscal_data.sql
[0m14:45:03.076275 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.andres_capstone_dbt.example
[0m14:45:03.094809 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '6652900f-1b51-4846-878f-55d3f68334db', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114dd29d0>]}
[0m14:45:03.203136 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '6652900f-1b51-4846-878f-55d3f68334db', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x115037cd0>]}
[0m14:45:03.203732 [info ] [MainThread]: Found 3 models, 4 data tests, 2 sources, 459 macros
[0m14:45:03.204124 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6652900f-1b51-4846-878f-55d3f68334db', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114f19b10>]}
[0m14:45:03.205724 [info ] [MainThread]: 
[0m14:45:03.206135 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:45:03.206489 [info ] [MainThread]: 
[0m14:45:03.207043 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m14:45:03.211941 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_DATAEXPERT_STUDENT_andres'
[0m14:45:03.227247 [debug] [ThreadPool]: Using snowflake connection "list_DATAEXPERT_STUDENT_andres"
[0m14:45:03.227743 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "connection_name": "list_DATAEXPERT_STUDENT_andres"} */
show objects in DATAEXPERT_STUDENT.andres limit 10000
[0m14:45:03.228115 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:45:04.107678 [debug] [ThreadPool]: SQL status: SUCCESS 18 in 0.879 seconds
[0m14:45:04.111192 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: Close
[0m14:45:04.344594 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6652900f-1b51-4846-878f-55d3f68334db', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114c7ba90>]}
[0m14:45:04.346887 [debug] [Thread-1 (]: Began running node model.dbt_capstone_andres.stg_fiscal_data
[0m14:45:04.347452 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_DATAEXPERT_STUDENT_andres, now model.dbt_capstone_andres.stg_fiscal_data)
[0m14:45:04.347960 [debug] [Thread-1 (]: Began compiling node model.dbt_capstone_andres.stg_fiscal_data
[0m14:45:04.357334 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_capstone_andres.stg_fiscal_data"
[0m14:45:04.358142 [debug] [Thread-1 (]: Began executing node model.dbt_capstone_andres.stg_fiscal_data
[0m14:45:04.359001 [debug] [Thread-1 (]: Finished running node model.dbt_capstone_andres.stg_fiscal_data
[0m14:45:04.360088 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:45:04.360497 [debug] [MainThread]: Connection 'model.dbt_capstone_andres.stg_fiscal_data' was properly closed.
[0m14:45:04.361250 [debug] [MainThread]: Command end result
[0m14:45:04.393911 [info ] [MainThread]: Compiled node 'stg_fiscal_data' is:
select * from dataexpert_student.andres.fiscal_data
[0m14:45:04.395806 [debug] [MainThread]: Resource report: {"command_name": "compile", "command_success": true, "command_wall_clock_time": 3.0353937, "process_user_time": 3.082767, "process_kernel_time": 0.579248, "process_mem_max_rss": "188612608", "process_in_blocks": "0", "process_out_blocks": "0"}
[0m14:45:04.396451 [debug] [MainThread]: Command `dbt compile` succeeded at 14:45:04.396336 after 3.04 seconds
[0m14:45:04.396885 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10aa14d50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10aa90d10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10aa90c50>]}
[0m14:45:04.397329 [debug] [MainThread]: Flushing usage events
[0m14:45:04.571998 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m16:04:10.293709 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e5c2ed0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e6e7450>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e6b7b90>]}


============================== 16:04:10.298734 | 5e39be53-1cbd-4326-9cc7-429814a566b1 ==============================
[0m16:04:10.298734 [info ] [MainThread]: Running with dbt=1.9.0-b3
[0m16:04:10.299388 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'version_check': 'True', 'fail_fast': 'False', 'log_path': 'logs', 'debug': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'introspect': 'True', 'invocation_command': 'dbt deps', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m16:04:10.443303 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '5e39be53-1cbd-4326-9cc7-429814a566b1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e6e6150>]}
[0m16:04:10.470846 [debug] [MainThread]: Set downloads directory='/var/folders/46/95_hj1f16xsdppwfjwv9_cx80000gn/T/dbt-downloads-39odo6wf'
[0m16:04:10.471387 [debug] [MainThread]: Making package index registry request: GET https://hub.getdbt.com/api/v1/index.json
[0m16:04:10.658142 [debug] [MainThread]: Response from registry index: GET https://hub.getdbt.com/api/v1/index.json 200
[0m16:04:10.659206 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json
[0m16:04:10.783822 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json 200
[0m16:04:10.792542 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/calogica/dbt_expectations.json
[0m16:04:10.850988 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/calogica/dbt_expectations.json 200
[0m16:04:10.856753 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/dbt-labs/codegen.json
[0m16:04:10.923217 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/dbt-labs/codegen.json 200
[0m16:04:10.926791 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/calogica/dbt_date.json
[0m16:04:10.988660 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/calogica/dbt_date.json 200
[0m16:04:11.013270 [info ] [MainThread]: Updating lock file in file path: /Users/thedre/Desktop/bootcamp/airflow-dbt-project/dags/capstone/dbt_capstone_andres/package-lock.yml
[0m16:04:11.017689 [debug] [MainThread]: Set downloads directory='/var/folders/46/95_hj1f16xsdppwfjwv9_cx80000gn/T/dbt-downloads-weow520_'
[0m16:04:11.030097 [info ] [MainThread]: Installing dbt-labs/dbt_utils
[0m16:04:11.421587 [info ] [MainThread]: Installed from version 1.1.1
[0m16:04:11.422076 [info ] [MainThread]: Updated version available: 1.3.0
[0m16:04:11.422594 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'package', 'label': '5e39be53-1cbd-4326-9cc7-429814a566b1', 'property_': 'install', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e7532d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e7f4ed0>]}
[0m16:04:11.423208 [info ] [MainThread]: Installing calogica/dbt_expectations
[0m16:04:12.484623 [info ] [MainThread]: Installed from version 0.10.3
[0m16:04:12.485061 [info ] [MainThread]: Updated version available: 0.10.4
[0m16:04:12.485531 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'package', 'label': '5e39be53-1cbd-4326-9cc7-429814a566b1', 'property_': 'install', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e7b88d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e804510>]}
[0m16:04:12.486131 [info ] [MainThread]: Installing dbt-labs/codegen
[0m16:04:12.691248 [info ] [MainThread]: Installed from version 0.12.1
[0m16:04:12.692112 [info ] [MainThread]: Up to date!
[0m16:04:12.692928 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'package', 'label': '5e39be53-1cbd-4326-9cc7-429814a566b1', 'property_': 'install', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e92a490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e90fd50>]}
[0m16:04:12.694136 [info ] [MainThread]: Installing calogica/dbt_date
[0m16:04:12.942705 [info ] [MainThread]: Installed from version 0.10.1
[0m16:04:12.943470 [info ] [MainThread]: Up to date!
[0m16:04:12.944304 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'package', 'label': '5e39be53-1cbd-4326-9cc7-429814a566b1', 'property_': 'install', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e90fd90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e90fdd0>]}
[0m16:04:12.944932 [info ] [MainThread]: 
[0m16:04:12.945345 [info ] [MainThread]: Updates available for packages: ['dbt-labs/dbt_utils', 'calogica/dbt_expectations']                 
Update your versions in packages.yml, then run dbt deps
[0m16:04:12.955735 [debug] [MainThread]: Resource report: {"command_name": "deps", "command_success": true, "command_wall_clock_time": 2.7323108, "process_user_time": 1.587568, "process_kernel_time": 0.492451, "process_mem_max_rss": "105537536", "process_in_blocks": "0", "process_out_blocks": "0"}
[0m16:04:12.957037 [debug] [MainThread]: Command `dbt deps` succeeded at 16:04:12.956771 after 2.73 seconds
[0m16:04:12.957731 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e761690>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ab04990>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e3c7dd0>]}
[0m16:04:12.958507 [debug] [MainThread]: Flushing usage events
[0m16:04:13.188596 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m16:05:56.240094 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b2e7810>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b2e7890>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b0bac50>]}


============================== 16:05:56.247710 | cfdf9367-c3d5-4991-8baa-9dc5fcf4c5ca ==============================
[0m16:05:56.247710 [info ] [MainThread]: Running with dbt=1.9.0-b3
[0m16:05:56.248501 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'fail_fast': 'False', 'version_check': 'True', 'log_path': 'logs', 'debug': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run-operation generate_base_model --args {"source_name": "treasury", "table_name": "fiscal_data"}', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m16:05:57.821725 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'cfdf9367-c3d5-4991-8baa-9dc5fcf4c5ca', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ff89e90>]}
[0m16:05:57.881850 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'cfdf9367-c3d5-4991-8baa-9dc5fcf4c5ca', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109ebce10>]}
[0m16:05:57.883513 [info ] [MainThread]: Registered adapter: snowflake=1.8.4
[0m16:05:58.114477 [debug] [MainThread]: checksum: 5f4fde221ca8627174a0341efc73099aaec05b8546408ceed6f59b1ec0755115, vars: {}, profile: , target: , version: 1.9.0b3
[0m16:05:58.225752 [info ] [MainThread]: Unable to do partial parsing because a project dependency has been added
[0m16:05:58.227052 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'cfdf9367-c3d5-4991-8baa-9dc5fcf4c5ca', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110863950>]}
[0m16:06:00.182549 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.andres_capstone_dbt.example
[0m16:06:00.193529 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'cfdf9367-c3d5-4991-8baa-9dc5fcf4c5ca', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111bb40d0>]}
[0m16:06:00.312530 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'cfdf9367-c3d5-4991-8baa-9dc5fcf4c5ca', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11b975650>]}
[0m16:06:00.313080 [info ] [MainThread]: Found 4 models, 4 data tests, 2 sources, 862 macros
[0m16:06:00.313468 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'cfdf9367-c3d5-4991-8baa-9dc5fcf4c5ca', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111b03450>]}
[0m16:06:00.313990 [debug] [MainThread]: Acquiring new snowflake connection 'macro_generate_base_model'
[0m16:06:00.338419 [debug] [MainThread]: Using snowflake connection "macro_generate_base_model"
[0m16:06:00.338860 [debug] [MainThread]: On macro_generate_base_model: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "connection_name": "macro_generate_base_model"} */
describe table dataexpert_student.andres.fiscal_data
[0m16:06:00.339194 [debug] [MainThread]: Opening a new connection, currently in state init
[0m16:06:01.137853 [debug] [MainThread]: SQL status: SUCCESS 4 in 0.798 seconds
[0m16:06:01.140098 [info ] [MainThread]: 

with source as (

    select * from {{ source('treasury', 'fiscal_data') }}

),

renamed as (

    select
        record_date,
        channel_type_desc,
        tax_category_desc,
        net_collections_amt

    from source

)

select * from renamed

[0m16:06:01.140667 [debug] [MainThread]: On macro_generate_base_model: Close
[0m16:06:01.463762 [debug] [MainThread]: Resource report: {"command_name": "run-operation", "command_success": true, "command_wall_clock_time": 5.2908206, "process_user_time": 4.590752, "process_kernel_time": 0.682326, "process_mem_max_rss": "195432448", "process_in_blocks": "0", "process_out_blocks": "0"}
[0m16:06:01.464574 [debug] [MainThread]: Command `dbt run-operation` succeeded at 16:06:01.464439 after 5.29 seconds
[0m16:06:01.465052 [debug] [MainThread]: Connection 'macro_generate_base_model' was properly closed.
[0m16:06:01.465522 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b30a3d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b2ecbd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10768cb50>]}
[0m16:06:01.466047 [debug] [MainThread]: Flushing usage events
[0m16:06:01.641666 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m16:06:34.175852 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112bd8b90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1128dbd50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112c2d290>]}


============================== 16:06:34.180286 | 8ffc34c1-6eae-4f7a-952f-4fa48048c011 ==============================
[0m16:06:34.180286 [info ] [MainThread]: Running with dbt=1.9.0-b3
[0m16:06:34.180991 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'debug': 'False', 'log_path': 'logs', 'version_check': 'True', 'profiles_dir': '.', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'introspect': 'True', 'invocation_command': 'dbt run-operation generate_base_model --args {"source_name": "polygon", "table_name": "daily_bars"}', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m16:06:35.182115 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '8ffc34c1-6eae-4f7a-952f-4fa48048c011', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x115334450>]}
[0m16:06:35.239835 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '8ffc34c1-6eae-4f7a-952f-4fa48048c011', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112ae07d0>]}
[0m16:06:35.240718 [info ] [MainThread]: Registered adapter: snowflake=1.8.4
[0m16:06:35.418919 [debug] [MainThread]: checksum: 5f4fde221ca8627174a0341efc73099aaec05b8546408ceed6f59b1ec0755115, vars: {}, profile: , target: , version: 1.9.0b3
[0m16:06:35.674706 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m16:06:35.675299 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m16:06:35.681909 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.andres_capstone_dbt.example
[0m16:06:35.730011 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '8ffc34c1-6eae-4f7a-952f-4fa48048c011', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11938f490>]}
[0m16:06:35.840781 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '8ffc34c1-6eae-4f7a-952f-4fa48048c011', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11920d690>]}
[0m16:06:35.841348 [info ] [MainThread]: Found 4 models, 4 data tests, 2 sources, 862 macros
[0m16:06:35.841746 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '8ffc34c1-6eae-4f7a-952f-4fa48048c011', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1191f7b90>]}
[0m16:06:35.842378 [debug] [MainThread]: Acquiring new snowflake connection 'macro_generate_base_model'
[0m16:06:35.869662 [debug] [MainThread]: Using snowflake connection "macro_generate_base_model"
[0m16:06:35.870124 [debug] [MainThread]: On macro_generate_base_model: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "connection_name": "macro_generate_base_model"} */
describe table dataexpert_student.andres.daily_bars
[0m16:06:35.870480 [debug] [MainThread]: Opening a new connection, currently in state init
[0m16:06:36.666542 [debug] [MainThread]: SQL status: SUCCESS 9 in 0.796 seconds
[0m16:06:36.668854 [info ] [MainThread]: 

with source as (

    select * from {{ source('polygon', 'daily_bars') }}

),

renamed as (

    select
        ticker,
        v,
        vw,
        o,
        c,
        h,
        l,
        timestamp,
        n

    from source

)

select * from renamed

[0m16:06:36.669370 [debug] [MainThread]: On macro_generate_base_model: Close
[0m16:06:36.893025 [debug] [MainThread]: Resource report: {"command_name": "run-operation", "command_success": true, "command_wall_clock_time": 2.7857397, "process_user_time": 2.813401, "process_kernel_time": 0.510043, "process_mem_max_rss": "187236352", "process_in_blocks": "0", "process_out_blocks": "0"}
[0m16:06:36.893678 [debug] [MainThread]: Command `dbt run-operation` succeeded at 16:06:36.893580 after 2.79 seconds
[0m16:06:36.894023 [debug] [MainThread]: Connection 'macro_generate_base_model' was properly closed.
[0m16:06:36.894363 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112c4a1d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112c4a890>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110b71450>]}
[0m16:06:36.894733 [debug] [MainThread]: Flushing usage events
[0m16:06:37.059791 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m16:14:53.599651 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1105bf810>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1105bf390>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1105c1810>]}


============================== 16:14:53.605552 | b5f7640e-14e8-470c-912b-678325baa26c ==============================
[0m16:14:53.605552 [info ] [MainThread]: Running with dbt=1.9.0-b3
[0m16:14:53.606422 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'fail_fast': 'False', 'debug': 'False', 'log_path': 'logs', 'warn_error': 'None', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'log_format': 'default', 'introspect': 'True', 'target_path': 'None', 'invocation_command': 'dbt run-operation generate_source --args {"source_name": "polygon", "table_name": "daily_bars"}', 'send_anonymous_usage_stats': 'True'}
[0m16:14:55.149613 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'b5f7640e-14e8-470c-912b-678325baa26c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1106091d0>]}
[0m16:14:55.206282 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'b5f7640e-14e8-470c-912b-678325baa26c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10f155050>]}
[0m16:14:55.207450 [info ] [MainThread]: Registered adapter: snowflake=1.8.4
[0m16:14:55.422029 [debug] [MainThread]: checksum: 5f4fde221ca8627174a0341efc73099aaec05b8546408ceed6f59b1ec0755115, vars: {}, profile: , target: , version: 1.9.0b3
[0m16:14:55.656109 [debug] [MainThread]: Partial parsing enabled: 1 files deleted, 1 files added, 1 files changed.
[0m16:14:55.657119 [debug] [MainThread]: Partial parsing: added file: dbt_capstone_andres://models/staging/stg_treasury_revenue_collections.sql
[0m16:14:55.657506 [debug] [MainThread]: Partial parsing: deleted file: dbt_capstone_andres://models/staging/stg_fiscal_data.sql
[0m16:14:55.657958 [debug] [MainThread]: Partial parsing: updated file: dbt_capstone_andres://models/staging/stg_daily_bars.sql
[0m16:14:55.893795 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.andres_capstone_dbt.example
[0m16:14:55.985668 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'b5f7640e-14e8-470c-912b-678325baa26c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x116e0b3d0>]}
[0m16:14:56.123224 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'b5f7640e-14e8-470c-912b-678325baa26c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x116aea350>]}
[0m16:14:56.123814 [info ] [MainThread]: Found 4 models, 4 data tests, 2 sources, 862 macros
[0m16:14:56.124207 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b5f7640e-14e8-470c-912b-678325baa26c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x116bbbc50>]}
[0m16:14:56.124770 [debug] [MainThread]: Acquiring new snowflake connection 'macro_generate_source'
[0m16:14:56.129486 [debug] [MainThread]: Snowflake adapter: Error running SQL: macro generate_source
[0m16:14:56.129846 [debug] [MainThread]: Snowflake adapter: Rolling back transaction.
[0m16:14:56.130185 [debug] [MainThread]: Opening a new connection, currently in state init
[0m16:14:56.820746 [debug] [MainThread]: On macro_generate_source: Close
[0m16:14:57.056182 [error] [MainThread]: Encountered an error while running operation: Compilation Error
  macro 'dbt_macro__generate_source' takes no keyword argument 'source_name'
[0m16:14:57.061962 [debug] [MainThread]: Traceback (most recent call last):
  File "/Users/thedre/Desktop/bootcamp/airflow-dbt-project/venv_new/lib/python3.11/site-packages/dbt_common/clients/jinja.py", line 345, in exception_handler
    yield
  File "/Users/thedre/Desktop/bootcamp/airflow-dbt-project/venv_new/lib/python3.11/site-packages/dbt_common/clients/jinja.py", line 322, in call_macro
    return macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/thedre/Desktop/bootcamp/airflow-dbt-project/venv_new/lib/python3.11/site-packages/jinja2/runtime.py", line 757, in __call__
    raise TypeError(
TypeError: macro 'dbt_macro__generate_source' takes no keyword argument 'source_name'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/thedre/Desktop/bootcamp/airflow-dbt-project/venv_new/lib/python3.11/site-packages/dbt/task/run_operation.py", line 66, in run
    self._run_unsafe(package_name, macro_name)
  File "/Users/thedre/Desktop/bootcamp/airflow-dbt-project/venv_new/lib/python3.11/site-packages/dbt/task/run_operation.py", line 47, in _run_unsafe
    res = adapter.execute_macro(
          ^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/thedre/Desktop/bootcamp/airflow-dbt-project/venv_new/lib/python3.11/site-packages/dbt/adapters/base/impl.py", line 1162, in execute_macro
    result = macro_function(**kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/thedre/Desktop/bootcamp/airflow-dbt-project/venv_new/lib/python3.11/site-packages/dbt_common/clients/jinja.py", line 354, in __call__
    return self.call_macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/thedre/Desktop/bootcamp/airflow-dbt-project/venv_new/lib/python3.11/site-packages/dbt_common/clients/jinja.py", line 320, in call_macro
    with self.exception_handler():
  File "/usr/local/Cellar/python@3.11/3.11.10/Frameworks/Python.framework/Versions/3.11/lib/python3.11/contextlib.py", line 158, in __exit__
    self.gen.throw(typ, value, traceback)
  File "/Users/thedre/Desktop/bootcamp/airflow-dbt-project/venv_new/lib/python3.11/site-packages/dbt_common/clients/jinja.py", line 347, in exception_handler
    raise CaughtMacroErrorWithNodeError(exc=e, node=self.macro)
dbt_common.exceptions.macros.CaughtMacroErrorWithNodeError: Compilation Error
  macro 'dbt_macro__generate_source' takes no keyword argument 'source_name'

[0m16:14:57.073647 [debug] [MainThread]: Resource report: {"command_name": "run-operation", "command_wall_clock_time": 3.5527341, "process_user_time": 3.05847, "process_kernel_time": 0.672572, "process_mem_max_rss": "189693952", "command_success": false, "process_in_blocks": "0", "process_out_blocks": "0"}
[0m16:14:57.074719 [debug] [MainThread]: Command `dbt run-operation` failed at 16:14:57.074539 after 3.55 seconds
[0m16:14:57.075192 [debug] [MainThread]: Connection 'macro_generate_source' was properly closed.
[0m16:14:57.075641 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110574a90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1105c0850>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x116aeafd0>]}
[0m16:14:57.076186 [debug] [MainThread]: Flushing usage events
[0m16:14:57.291798 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m16:17:25.613383 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1122ef810>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1120c3110>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1122a0810>]}


============================== 16:17:25.619118 | a81261a5-1ba8-4a62-b45e-cd138b2f1fd0 ==============================
[0m16:17:25.619118 [info ] [MainThread]: Running with dbt=1.9.0-b3
[0m16:17:25.619820 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'version_check': 'True', 'fail_fast': 'False', 'log_path': 'logs', 'debug': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'invocation_command': 'dbt run-operation generate_model_yaml --args {"model_names": ["stg_daily_bars","stg_treasury_revenue_collections"]}', 'introspect': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m16:17:27.300945 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'a81261a5-1ba8-4a62-b45e-cd138b2f1fd0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1122abb90>]}
[0m16:17:27.367394 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'a81261a5-1ba8-4a62-b45e-cd138b2f1fd0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110ec5090>]}
[0m16:17:27.368669 [info ] [MainThread]: Registered adapter: snowflake=1.8.4
[0m16:17:27.573617 [debug] [MainThread]: checksum: 5f4fde221ca8627174a0341efc73099aaec05b8546408ceed6f59b1ec0755115, vars: {}, profile: , target: , version: 1.9.0b3
[0m16:17:27.825322 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m16:17:27.825818 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m16:17:27.831456 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.andres_capstone_dbt.example
[0m16:17:27.877204 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'a81261a5-1ba8-4a62-b45e-cd138b2f1fd0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x118ae7a10>]}
[0m16:17:27.999936 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'a81261a5-1ba8-4a62-b45e-cd138b2f1fd0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x118927e90>]}
[0m16:17:28.000504 [info ] [MainThread]: Found 4 models, 4 data tests, 2 sources, 862 macros
[0m16:17:28.000880 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a81261a5-1ba8-4a62-b45e-cd138b2f1fd0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11889ca50>]}
[0m16:17:28.001466 [debug] [MainThread]: Acquiring new snowflake connection 'macro_generate_model_yaml'
[0m16:17:28.028907 [debug] [MainThread]: Using snowflake connection "macro_generate_model_yaml"
[0m16:17:28.029379 [debug] [MainThread]: On macro_generate_model_yaml: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "connection_name": "macro_generate_model_yaml"} */
describe table DATAEXPERT_STUDENT.andres.stg_daily_bars
[0m16:17:28.029718 [debug] [MainThread]: Opening a new connection, currently in state init
[0m16:17:28.887026 [debug] [MainThread]: Snowflake adapter: Snowflake query id: 01b8761d-0004-3428-0000-33eb0017f9aa
[0m16:17:28.887708 [debug] [MainThread]: Snowflake adapter: Snowflake error: 002003 (42S02): SQL compilation error:
Table 'DATAEXPERT_STUDENT.ANDRES.STG_DAILY_BARS' does not exist or not authorized.
[0m16:17:28.888412 [debug] [MainThread]: Snowflake adapter: Error running SQL: macro get_columns_in_relation
[0m16:17:28.888925 [debug] [MainThread]: Snowflake adapter: Rolling back transaction.
[0m16:17:28.892714 [debug] [MainThread]: Using snowflake connection "macro_generate_model_yaml"
[0m16:17:28.893211 [debug] [MainThread]: On macro_generate_model_yaml: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "connection_name": "macro_generate_model_yaml"} */
describe table DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections
[0m16:17:29.045836 [debug] [MainThread]: Snowflake adapter: Snowflake query id: 01b8761d-0004-32d9-0000-33eb0017eb52
[0m16:17:29.046591 [debug] [MainThread]: Snowflake adapter: Snowflake error: 002003 (42S02): SQL compilation error:
Table 'DATAEXPERT_STUDENT.ANDRES.STG_TREASURY_REVENUE_COLLECTIONS' does not exist or not authorized.
[0m16:17:29.047346 [debug] [MainThread]: Snowflake adapter: Error running SQL: macro get_columns_in_relation
[0m16:17:29.047850 [debug] [MainThread]: Snowflake adapter: Rolling back transaction.
[0m16:17:29.048406 [info ] [MainThread]: version: 2

models:
  - name: stg_daily_bars
    description: ""
    columns:
  - name: stg_treasury_revenue_collections
    description: ""
    columns:
[0m16:17:29.049055 [debug] [MainThread]: On macro_generate_model_yaml: Close
[0m16:17:29.272614 [debug] [MainThread]: Resource report: {"command_name": "run-operation", "command_success": true, "command_wall_clock_time": 3.728877, "process_user_time": 2.913653, "process_kernel_time": 0.691635, "process_mem_max_rss": "186515456", "process_in_blocks": "0", "process_out_blocks": "0"}
[0m16:17:29.273218 [debug] [MainThread]: Command `dbt run-operation` succeeded at 16:17:29.273119 after 3.73 seconds
[0m16:17:29.273569 [debug] [MainThread]: Connection 'macro_generate_model_yaml' was properly closed.
[0m16:17:29.273917 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1189275d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e618ad0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e694ad0>]}
[0m16:17:29.274336 [debug] [MainThread]: Flushing usage events
[0m16:17:29.469599 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m16:18:26.656571 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10960ed50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b5d6910>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b5d6f10>]}


============================== 16:18:26.663141 | af573152-d690-4005-968b-d94322ac1563 ==============================
[0m16:18:26.663141 [info ] [MainThread]: Running with dbt=1.9.0-b3
[0m16:18:26.664077 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'profiles_dir': '.', 'log_path': 'logs', 'fail_fast': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'log_format': 'default', 'invocation_command': 'dbt build', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m16:18:28.267730 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'af573152-d690-4005-968b-d94322ac1563', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c0225d0>]}
[0m16:18:28.328478 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'af573152-d690-4005-968b-d94322ac1563', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a254f90>]}
[0m16:18:28.329669 [info ] [MainThread]: Registered adapter: snowflake=1.8.4
[0m16:18:28.513381 [debug] [MainThread]: checksum: 5f4fde221ca8627174a0341efc73099aaec05b8546408ceed6f59b1ec0755115, vars: {}, profile: , target: , version: 1.9.0b3
[0m16:18:28.749951 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m16:18:28.750377 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m16:18:28.755230 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.andres_capstone_dbt.example
[0m16:18:28.794348 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'af573152-d690-4005-968b-d94322ac1563', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111d33f10>]}
[0m16:18:28.946931 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'af573152-d690-4005-968b-d94322ac1563', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111be34d0>]}
[0m16:18:28.947712 [info ] [MainThread]: Found 4 models, 4 data tests, 2 sources, 862 macros
[0m16:18:28.951112 [info ] [MainThread]: 
[0m16:18:28.951868 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m16:18:28.952350 [info ] [MainThread]: 
[0m16:18:28.953755 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m16:18:28.962940 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_DATAEXPERT_STUDENT'
[0m16:18:29.073033 [debug] [ThreadPool]: Using snowflake connection "list_DATAEXPERT_STUDENT"
[0m16:18:29.073720 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "connection_name": "list_DATAEXPERT_STUDENT"} */
show terse schemas in database DATAEXPERT_STUDENT
    limit 10000
[0m16:18:29.074238 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:18:29.922896 [debug] [ThreadPool]: SQL status: SUCCESS 68 in 0.849 seconds
[0m16:18:29.925712 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT: Close
[0m16:18:30.156072 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_DATAEXPERT_STUDENT, now list_DATAEXPERT_STUDENT_andres)
[0m16:18:30.165229 [debug] [ThreadPool]: Using snowflake connection "list_DATAEXPERT_STUDENT_andres"
[0m16:18:30.165643 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "connection_name": "list_DATAEXPERT_STUDENT_andres"} */
show objects in DATAEXPERT_STUDENT.andres limit 10000
[0m16:18:30.165977 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m16:18:30.877512 [debug] [ThreadPool]: SQL status: SUCCESS 18 in 0.711 seconds
[0m16:18:30.879855 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: Close
[0m16:18:31.112766 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'af573152-d690-4005-968b-d94322ac1563', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b46d310>]}
[0m16:18:31.117031 [debug] [Thread-1 (]: Began running node model.dbt_capstone_andres.my_first_dbt_model
[0m16:18:31.117792 [info ] [Thread-1 (]: 1 of 8 START sql table model andres.my_first_dbt_model ......................... [RUN]
[0m16:18:31.118448 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_DATAEXPERT_STUDENT_andres, now model.dbt_capstone_andres.my_first_dbt_model)
[0m16:18:31.118960 [debug] [Thread-1 (]: Began compiling node model.dbt_capstone_andres.my_first_dbt_model
[0m16:18:31.131415 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_capstone_andres.my_first_dbt_model"
[0m16:18:31.132996 [debug] [Thread-1 (]: Began executing node model.dbt_capstone_andres.my_first_dbt_model
[0m16:18:31.174727 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_capstone_andres.my_first_dbt_model"
[0m16:18:31.176843 [debug] [Thread-1 (]: Using snowflake connection "model.dbt_capstone_andres.my_first_dbt_model"
[0m16:18:31.177457 [debug] [Thread-1 (]: On model.dbt_capstone_andres.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "model.dbt_capstone_andres.my_first_dbt_model"} */
create or replace transient table DATAEXPERT_STUDENT.andres.my_first_dbt_model
         as
        (/*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
        );
[0m16:18:31.177987 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m16:18:32.478342 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 1.300 seconds
[0m16:18:32.498850 [debug] [Thread-1 (]: On model.dbt_capstone_andres.my_first_dbt_model: Close
[0m16:18:32.724495 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'af573152-d690-4005-968b-d94322ac1563', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x115cbbf90>]}
[0m16:18:32.725388 [info ] [Thread-1 (]: 1 of 8 OK created sql table model andres.my_first_dbt_model .................... [[32mSUCCESS 1[0m in 1.60s]
[0m16:18:32.726192 [debug] [Thread-1 (]: Finished running node model.dbt_capstone_andres.my_first_dbt_model
[0m16:18:32.726748 [debug] [Thread-1 (]: Began running node model.dbt_capstone_andres.stg_daily_bars
[0m16:18:32.727654 [info ] [Thread-1 (]: 2 of 8 START sql view model andres.stg_daily_bars .............................. [RUN]
[0m16:18:32.728306 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_capstone_andres.my_first_dbt_model, now model.dbt_capstone_andres.stg_daily_bars)
[0m16:18:32.728808 [debug] [Thread-1 (]: Began compiling node model.dbt_capstone_andres.stg_daily_bars
[0m16:18:32.733664 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_capstone_andres.stg_daily_bars"
[0m16:18:32.734686 [debug] [Thread-1 (]: Began executing node model.dbt_capstone_andres.stg_daily_bars
[0m16:18:32.763536 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_capstone_andres.stg_daily_bars"
[0m16:18:32.765369 [debug] [Thread-1 (]: Using snowflake connection "model.dbt_capstone_andres.stg_daily_bars"
[0m16:18:32.765919 [debug] [Thread-1 (]: On model.dbt_capstone_andres.stg_daily_bars: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "model.dbt_capstone_andres.stg_daily_bars"} */
create or replace   view DATAEXPERT_STUDENT.andres.stg_daily_bars
  
   as (
    with source as (

    select * from dataexpert_student.andres.daily_bars

),

renamed as (

    select
        TO_DATE(TO_TIMESTAMP(timestamp / 1000)) as date,
        ticker as ticker,
        v  as trading_volume,
        n  as number_of_trades,
        vw as volume_weighted_avg_price,
        o  as open_price,
        c  as close_price,
        h  as high_price,
        l  as low_price
    from source

)

select * from renamed
  );
[0m16:18:32.766435 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m16:18:33.806018 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 1.039 seconds
[0m16:18:33.809457 [debug] [Thread-1 (]: On model.dbt_capstone_andres.stg_daily_bars: Close
[0m16:18:34.063812 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'af573152-d690-4005-968b-d94322ac1563', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x115bb9950>]}
[0m16:18:34.064844 [info ] [Thread-1 (]: 2 of 8 OK created sql view model andres.stg_daily_bars ......................... [[32mSUCCESS 1[0m in 1.34s]
[0m16:18:34.065681 [debug] [Thread-1 (]: Finished running node model.dbt_capstone_andres.stg_daily_bars
[0m16:18:34.066217 [debug] [Thread-1 (]: Began running node model.dbt_capstone_andres.stg_treasury_revenue_collections
[0m16:18:34.066856 [info ] [Thread-1 (]: 3 of 8 START sql view model andres.stg_treasury_revenue_collections ............ [RUN]
[0m16:18:34.067432 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_capstone_andres.stg_daily_bars, now model.dbt_capstone_andres.stg_treasury_revenue_collections)
[0m16:18:34.067909 [debug] [Thread-1 (]: Began compiling node model.dbt_capstone_andres.stg_treasury_revenue_collections
[0m16:18:34.072142 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_capstone_andres.stg_treasury_revenue_collections"
[0m16:18:34.073035 [debug] [Thread-1 (]: Began executing node model.dbt_capstone_andres.stg_treasury_revenue_collections
[0m16:18:34.079781 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_capstone_andres.stg_treasury_revenue_collections"
[0m16:18:34.081038 [debug] [Thread-1 (]: Using snowflake connection "model.dbt_capstone_andres.stg_treasury_revenue_collections"
[0m16:18:34.081550 [debug] [Thread-1 (]: On model.dbt_capstone_andres.stg_treasury_revenue_collections: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "model.dbt_capstone_andres.stg_treasury_revenue_collections"} */
create or replace   view DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections
  
   as (
    select * from dataexpert_student.andres.fiscal_data
  );
[0m16:18:34.082040 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m16:18:35.158240 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 1.076 seconds
[0m16:18:35.161517 [debug] [Thread-1 (]: On model.dbt_capstone_andres.stg_treasury_revenue_collections: Close
[0m16:18:35.375161 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'af573152-d690-4005-968b-d94322ac1563', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x115d17990>]}
[0m16:18:35.376192 [info ] [Thread-1 (]: 3 of 8 OK created sql view model andres.stg_treasury_revenue_collections ....... [[32mSUCCESS 1[0m in 1.31s]
[0m16:18:35.377095 [debug] [Thread-1 (]: Finished running node model.dbt_capstone_andres.stg_treasury_revenue_collections
[0m16:18:35.377662 [debug] [Thread-1 (]: Began running node test.dbt_capstone_andres.not_null_my_first_dbt_model_id.5fb22c2710
[0m16:18:35.378263 [info ] [Thread-1 (]: 4 of 8 START test not_null_my_first_dbt_model_id ............................... [RUN]
[0m16:18:35.378880 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_capstone_andres.stg_treasury_revenue_collections, now test.dbt_capstone_andres.not_null_my_first_dbt_model_id.5fb22c2710)
[0m16:18:35.379378 [debug] [Thread-1 (]: Began compiling node test.dbt_capstone_andres.not_null_my_first_dbt_model_id.5fb22c2710
[0m16:18:35.394065 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_capstone_andres.not_null_my_first_dbt_model_id.5fb22c2710"
[0m16:18:35.395154 [debug] [Thread-1 (]: Began executing node test.dbt_capstone_andres.not_null_my_first_dbt_model_id.5fb22c2710
[0m16:18:35.414220 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_capstone_andres.not_null_my_first_dbt_model_id.5fb22c2710"
[0m16:18:35.415824 [debug] [Thread-1 (]: Using snowflake connection "test.dbt_capstone_andres.not_null_my_first_dbt_model_id.5fb22c2710"
[0m16:18:35.416378 [debug] [Thread-1 (]: On test.dbt_capstone_andres.not_null_my_first_dbt_model_id.5fb22c2710: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "test.dbt_capstone_andres.not_null_my_first_dbt_model_id.5fb22c2710"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from DATAEXPERT_STUDENT.andres.my_first_dbt_model
where id is null



      
    ) dbt_internal_test
[0m16:18:35.416875 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m16:18:36.132766 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 0.716 seconds
[0m16:18:36.138018 [debug] [Thread-1 (]: On test.dbt_capstone_andres.not_null_my_first_dbt_model_id.5fb22c2710: Close
[0m16:18:36.365474 [error] [Thread-1 (]: 4 of 8 FAIL 1 not_null_my_first_dbt_model_id ................................... [[31mFAIL 1[0m in 0.99s]
[0m16:18:36.366308 [debug] [Thread-1 (]: Finished running node test.dbt_capstone_andres.not_null_my_first_dbt_model_id.5fb22c2710
[0m16:18:36.366811 [debug] [Thread-1 (]: Began running node test.dbt_capstone_andres.unique_my_first_dbt_model_id.16e066b321
[0m16:18:36.367428 [debug] [Thread-4 (]: Marking all children of 'test.dbt_capstone_andres.not_null_my_first_dbt_model_id.5fb22c2710' to be skipped because of status 'fail'.  Reason: Got 1 result, configured to fail if != 0.
[0m16:18:36.367871 [info ] [Thread-1 (]: 5 of 8 START test unique_my_first_dbt_model_id ................................. [RUN]
[0m16:18:36.368813 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_capstone_andres.not_null_my_first_dbt_model_id.5fb22c2710, now test.dbt_capstone_andres.unique_my_first_dbt_model_id.16e066b321)
[0m16:18:36.369271 [debug] [Thread-1 (]: Began compiling node test.dbt_capstone_andres.unique_my_first_dbt_model_id.16e066b321
[0m16:18:36.377335 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_capstone_andres.unique_my_first_dbt_model_id.16e066b321"
[0m16:18:36.378240 [debug] [Thread-1 (]: Began executing node test.dbt_capstone_andres.unique_my_first_dbt_model_id.16e066b321
[0m16:18:36.381945 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_capstone_andres.unique_my_first_dbt_model_id.16e066b321"
[0m16:18:36.383551 [debug] [Thread-1 (]: Using snowflake connection "test.dbt_capstone_andres.unique_my_first_dbt_model_id.16e066b321"
[0m16:18:36.384074 [debug] [Thread-1 (]: On test.dbt_capstone_andres.unique_my_first_dbt_model_id.16e066b321: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "test.dbt_capstone_andres.unique_my_first_dbt_model_id.16e066b321"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from DATAEXPERT_STUDENT.andres.my_first_dbt_model
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
[0m16:18:36.384530 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m16:18:37.058311 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 0.674 seconds
[0m16:18:37.061342 [debug] [Thread-1 (]: On test.dbt_capstone_andres.unique_my_first_dbt_model_id.16e066b321: Close
[0m16:18:37.273188 [info ] [Thread-1 (]: 5 of 8 PASS unique_my_first_dbt_model_id ....................................... [[32mPASS[0m in 0.90s]
[0m16:18:37.274088 [debug] [Thread-1 (]: Finished running node test.dbt_capstone_andres.unique_my_first_dbt_model_id.16e066b321
[0m16:18:37.274984 [debug] [Thread-1 (]: Began running node model.dbt_capstone_andres.my_second_dbt_model
[0m16:18:37.275560 [info ] [Thread-1 (]: 6 of 8 SKIP relation andres.my_second_dbt_model ................................ [[33mSKIP[0m]
[0m16:18:37.276105 [debug] [Thread-1 (]: Finished running node model.dbt_capstone_andres.my_second_dbt_model
[0m16:18:37.276603 [debug] [Thread-4 (]: Marking all children of 'model.dbt_capstone_andres.my_second_dbt_model' to be skipped because of status 'skipped'. 
[0m16:18:37.277412 [debug] [Thread-1 (]: Began running node test.dbt_capstone_andres.not_null_my_second_dbt_model_id.151b76d778
[0m16:18:37.277968 [info ] [Thread-1 (]: 7 of 8 SKIP test not_null_my_second_dbt_model_id ............................... [[33mSKIP[0m]
[0m16:18:37.278764 [debug] [Thread-1 (]: Finished running node test.dbt_capstone_andres.not_null_my_second_dbt_model_id.151b76d778
[0m16:18:37.279329 [debug] [Thread-1 (]: Began running node test.dbt_capstone_andres.unique_my_second_dbt_model_id.57a0f8c493
[0m16:18:37.279875 [debug] [Thread-4 (]: Marking all children of 'test.dbt_capstone_andres.not_null_my_second_dbt_model_id.151b76d778' to be skipped because of status 'skipped'. 
[0m16:18:37.280399 [info ] [Thread-1 (]: 8 of 8 SKIP test unique_my_second_dbt_model_id ................................. [[33mSKIP[0m]
[0m16:18:37.281108 [debug] [Thread-1 (]: Finished running node test.dbt_capstone_andres.unique_my_second_dbt_model_id.57a0f8c493
[0m16:18:37.281562 [debug] [Thread-4 (]: Marking all children of 'test.dbt_capstone_andres.unique_my_second_dbt_model_id.57a0f8c493' to be skipped because of status 'skipped'. 
[0m16:18:37.282885 [debug] [MainThread]: Connection 'master' was properly closed.
[0m16:18:37.283286 [debug] [MainThread]: Connection 'test.dbt_capstone_andres.unique_my_first_dbt_model_id.16e066b321' was properly closed.
[0m16:18:37.283754 [info ] [MainThread]: 
[0m16:18:37.284183 [info ] [MainThread]: Finished running 1 table model, 4 data tests, 3 view models in 0 hours 0 minutes and 8.33 seconds (8.33s).
[0m16:18:37.286480 [debug] [MainThread]: Command end result
[0m16:18:37.339011 [info ] [MainThread]: 
[0m16:18:37.339530 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successs, and 0 warnings:[0m
[0m16:18:37.339912 [info ] [MainThread]: 
[0m16:18:37.340351 [error] [MainThread]: [31mFailure in test not_null_my_first_dbt_model_id (models/example/schema.yml)[0m
[0m16:18:37.340776 [error] [MainThread]:   Got 1 result, configured to fail if != 0
[0m16:18:37.341122 [info ] [MainThread]: 
[0m16:18:37.341532 [info ] [MainThread]:   compiled code at target/compiled/dbt_capstone_andres/models/example/schema.yml/not_null_my_first_dbt_model_id.sql
[0m16:18:37.341904 [info ] [MainThread]: 
[0m16:18:37.342402 [info ] [MainThread]: Done. PASS=4 WARN=0 ERROR=1 SKIP=3 TOTAL=8
[0m16:18:37.345248 [debug] [MainThread]: Resource report: {"command_name": "build", "command_wall_clock_time": 10.784916, "process_user_time": 3.55617, "process_kernel_time": 0.71185, "process_mem_max_rss": "193302528", "command_success": false, "process_in_blocks": "0", "process_out_blocks": "0"}
[0m16:18:37.345948 [debug] [MainThread]: Command `dbt build` failed at 16:18:37.345834 after 10.79 seconds
[0m16:18:37.346410 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b56ccd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1079eca90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b563c10>]}
[0m16:18:37.346830 [debug] [MainThread]: Flushing usage events
[0m16:18:37.527927 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m16:22:17.756023 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a49b510>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a50f050>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a50f650>]}


============================== 16:22:17.761706 | 0e52626a-9197-4de9-bf01-ff4660dd8f72 ==============================
[0m16:22:17.761706 [info ] [MainThread]: Running with dbt=1.9.0-b3
[0m16:22:17.762403 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'debug': 'False', 'fail_fast': 'False', 'log_path': 'logs', 'version_check': 'True', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'invocation_command': 'dbt build', 'send_anonymous_usage_stats': 'True'}
[0m16:22:19.461959 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '0e52626a-9197-4de9-bf01-ff4660dd8f72', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a535110>]}
[0m16:22:19.520635 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '0e52626a-9197-4de9-bf01-ff4660dd8f72', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1090c4b90>]}
[0m16:22:19.522431 [info ] [MainThread]: Registered adapter: snowflake=1.8.4
[0m16:22:19.717176 [debug] [MainThread]: checksum: 5f4fde221ca8627174a0341efc73099aaec05b8546408ceed6f59b1ec0755115, vars: {}, profile: , target: , version: 1.9.0b3
[0m16:22:19.952770 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m16:22:19.953455 [debug] [MainThread]: Partial parsing: updated file: dbt_capstone_andres://models/staging/stg_daily_bars.sql
[0m16:22:20.193729 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.andres_capstone_dbt.example
[0m16:22:20.292226 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '0e52626a-9197-4de9-bf01-ff4660dd8f72', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e4f6610>]}
[0m16:22:20.447126 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '0e52626a-9197-4de9-bf01-ff4660dd8f72', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108656b50>]}
[0m16:22:20.447718 [info ] [MainThread]: Found 4 models, 4 data tests, 2 sources, 862 macros
[0m16:22:20.449778 [info ] [MainThread]: 
[0m16:22:20.450138 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m16:22:20.450502 [info ] [MainThread]: 
[0m16:22:20.451003 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m16:22:20.455003 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_DATAEXPERT_STUDENT'
[0m16:22:20.467727 [debug] [ThreadPool]: Using snowflake connection "list_DATAEXPERT_STUDENT"
[0m16:22:20.468118 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "connection_name": "list_DATAEXPERT_STUDENT"} */
show terse schemas in database DATAEXPERT_STUDENT
    limit 10000
[0m16:22:20.468422 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:22:21.324832 [debug] [ThreadPool]: SQL status: SUCCESS 68 in 0.856 seconds
[0m16:22:21.328835 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT: Close
[0m16:22:21.554706 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_DATAEXPERT_STUDENT, now list_DATAEXPERT_STUDENT_andres)
[0m16:22:21.566899 [debug] [ThreadPool]: Using snowflake connection "list_DATAEXPERT_STUDENT_andres"
[0m16:22:21.567369 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "connection_name": "list_DATAEXPERT_STUDENT_andres"} */
show objects in DATAEXPERT_STUDENT.andres limit 10000
[0m16:22:21.567778 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m16:22:22.292469 [debug] [ThreadPool]: SQL status: SUCCESS 21 in 0.725 seconds
[0m16:22:22.295080 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: Close
[0m16:22:22.517519 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '0e52626a-9197-4de9-bf01-ff4660dd8f72', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109e715d0>]}
[0m16:22:22.520565 [debug] [Thread-1 (]: Began running node model.dbt_capstone_andres.my_first_dbt_model
[0m16:22:22.521073 [info ] [Thread-1 (]: 1 of 8 START sql table model andres.my_first_dbt_model ......................... [RUN]
[0m16:22:22.521574 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_DATAEXPERT_STUDENT_andres, now model.dbt_capstone_andres.my_first_dbt_model)
[0m16:22:22.521936 [debug] [Thread-1 (]: Began compiling node model.dbt_capstone_andres.my_first_dbt_model
[0m16:22:22.529738 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_capstone_andres.my_first_dbt_model"
[0m16:22:22.531369 [debug] [Thread-1 (]: Began executing node model.dbt_capstone_andres.my_first_dbt_model
[0m16:22:22.561287 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_capstone_andres.my_first_dbt_model"
[0m16:22:22.562584 [debug] [Thread-1 (]: Using snowflake connection "model.dbt_capstone_andres.my_first_dbt_model"
[0m16:22:22.563041 [debug] [Thread-1 (]: On model.dbt_capstone_andres.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "model.dbt_capstone_andres.my_first_dbt_model"} */
create or replace transient table DATAEXPERT_STUDENT.andres.my_first_dbt_model
         as
        (/*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
        );
[0m16:22:22.563472 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m16:22:23.936381 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 1.373 seconds
[0m16:22:23.960736 [debug] [Thread-1 (]: On model.dbt_capstone_andres.my_first_dbt_model: Close
[0m16:22:24.200665 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0e52626a-9197-4de9-bf01-ff4660dd8f72', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110b20c50>]}
[0m16:22:24.201600 [info ] [Thread-1 (]: 1 of 8 OK created sql table model andres.my_first_dbt_model .................... [[32mSUCCESS 1[0m in 1.68s]
[0m16:22:24.202413 [debug] [Thread-1 (]: Finished running node model.dbt_capstone_andres.my_first_dbt_model
[0m16:22:24.202959 [debug] [Thread-1 (]: Began running node model.dbt_capstone_andres.stg_daily_bars
[0m16:22:24.203790 [info ] [Thread-1 (]: 2 of 8 START sql view model andres.stg_daily_bars .............................. [RUN]
[0m16:22:24.204473 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_capstone_andres.my_first_dbt_model, now model.dbt_capstone_andres.stg_daily_bars)
[0m16:22:24.204985 [debug] [Thread-1 (]: Began compiling node model.dbt_capstone_andres.stg_daily_bars
[0m16:22:24.209847 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_capstone_andres.stg_daily_bars"
[0m16:22:24.210875 [debug] [Thread-1 (]: Began executing node model.dbt_capstone_andres.stg_daily_bars
[0m16:22:24.238003 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_capstone_andres.stg_daily_bars"
[0m16:22:24.239539 [debug] [Thread-1 (]: Using snowflake connection "model.dbt_capstone_andres.stg_daily_bars"
[0m16:22:24.240015 [debug] [Thread-1 (]: On model.dbt_capstone_andres.stg_daily_bars: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "model.dbt_capstone_andres.stg_daily_bars"} */
create or replace   view DATAEXPERT_STUDENT.andres.stg_daily_bars
  
   as (
    with source as (

    select * from dataexpert_student.andres.daily_bars

),

renamed as (

    select
        TO_DATE(TO_TIMESTAMP(timestamp / 1000)) as date,
        ticker as ticker,
        v/1000000000  as trading_volume_in_billions,
        n/1000000  as number_of_trades_in_millions,
        vw as volume_weighted_avg_price,
        o  as open_price,
        c  as close_price,
        h  as high_price,
        l  as low_price
    from source

)

select * from renamed
  );
[0m16:22:24.240490 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m16:22:25.270947 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 1.030 seconds
[0m16:22:25.273283 [debug] [Thread-1 (]: On model.dbt_capstone_andres.stg_daily_bars: Close
[0m16:22:25.494984 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0e52626a-9197-4de9-bf01-ff4660dd8f72', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114ddcbd0>]}
[0m16:22:25.496048 [info ] [Thread-1 (]: 2 of 8 OK created sql view model andres.stg_daily_bars ......................... [[32mSUCCESS 1[0m in 1.29s]
[0m16:22:25.496942 [debug] [Thread-1 (]: Finished running node model.dbt_capstone_andres.stg_daily_bars
[0m16:22:25.497556 [debug] [Thread-1 (]: Began running node model.dbt_capstone_andres.stg_treasury_revenue_collections
[0m16:22:25.498152 [info ] [Thread-1 (]: 3 of 8 START sql view model andres.stg_treasury_revenue_collections ............ [RUN]
[0m16:22:25.498777 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_capstone_andres.stg_daily_bars, now model.dbt_capstone_andres.stg_treasury_revenue_collections)
[0m16:22:25.499277 [debug] [Thread-1 (]: Began compiling node model.dbt_capstone_andres.stg_treasury_revenue_collections
[0m16:22:25.503464 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_capstone_andres.stg_treasury_revenue_collections"
[0m16:22:25.505117 [debug] [Thread-1 (]: Began executing node model.dbt_capstone_andres.stg_treasury_revenue_collections
[0m16:22:25.510029 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_capstone_andres.stg_treasury_revenue_collections"
[0m16:22:25.511336 [debug] [Thread-1 (]: Using snowflake connection "model.dbt_capstone_andres.stg_treasury_revenue_collections"
[0m16:22:25.511864 [debug] [Thread-1 (]: On model.dbt_capstone_andres.stg_treasury_revenue_collections: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "model.dbt_capstone_andres.stg_treasury_revenue_collections"} */
create or replace   view DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections
  
   as (
    select * from dataexpert_student.andres.fiscal_data
  );
[0m16:22:25.512363 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m16:22:26.556724 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 1.044 seconds
[0m16:22:26.560299 [debug] [Thread-1 (]: On model.dbt_capstone_andres.stg_treasury_revenue_collections: Close
[0m16:22:26.781470 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0e52626a-9197-4de9-bf01-ff4660dd8f72', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114d0f110>]}
[0m16:22:26.782990 [info ] [Thread-1 (]: 3 of 8 OK created sql view model andres.stg_treasury_revenue_collections ....... [[32mSUCCESS 1[0m in 1.28s]
[0m16:22:26.784681 [debug] [Thread-1 (]: Finished running node model.dbt_capstone_andres.stg_treasury_revenue_collections
[0m16:22:26.785788 [debug] [Thread-1 (]: Began running node test.dbt_capstone_andres.not_null_my_first_dbt_model_id.5fb22c2710
[0m16:22:26.787118 [info ] [Thread-1 (]: 4 of 8 START test not_null_my_first_dbt_model_id ............................... [RUN]
[0m16:22:26.789429 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_capstone_andres.stg_treasury_revenue_collections, now test.dbt_capstone_andres.not_null_my_first_dbt_model_id.5fb22c2710)
[0m16:22:26.791189 [debug] [Thread-1 (]: Began compiling node test.dbt_capstone_andres.not_null_my_first_dbt_model_id.5fb22c2710
[0m16:22:26.820514 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_capstone_andres.not_null_my_first_dbt_model_id.5fb22c2710"
[0m16:22:26.821750 [debug] [Thread-1 (]: Began executing node test.dbt_capstone_andres.not_null_my_first_dbt_model_id.5fb22c2710
[0m16:22:26.857532 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_capstone_andres.not_null_my_first_dbt_model_id.5fb22c2710"
[0m16:22:26.859891 [debug] [Thread-1 (]: Using snowflake connection "test.dbt_capstone_andres.not_null_my_first_dbt_model_id.5fb22c2710"
[0m16:22:26.860967 [debug] [Thread-1 (]: On test.dbt_capstone_andres.not_null_my_first_dbt_model_id.5fb22c2710: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "test.dbt_capstone_andres.not_null_my_first_dbt_model_id.5fb22c2710"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from DATAEXPERT_STUDENT.andres.my_first_dbt_model
where id is null



      
    ) dbt_internal_test
[0m16:22:26.862094 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m16:22:27.653916 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 0.792 seconds
[0m16:22:27.657114 [debug] [Thread-1 (]: On test.dbt_capstone_andres.not_null_my_first_dbt_model_id.5fb22c2710: Close
[0m16:22:27.888363 [error] [Thread-1 (]: 4 of 8 FAIL 1 not_null_my_first_dbt_model_id ................................... [[31mFAIL 1[0m in 1.10s]
[0m16:22:27.889201 [debug] [Thread-1 (]: Finished running node test.dbt_capstone_andres.not_null_my_first_dbt_model_id.5fb22c2710
[0m16:22:27.889707 [debug] [Thread-1 (]: Began running node test.dbt_capstone_andres.unique_my_first_dbt_model_id.16e066b321
[0m16:22:27.890493 [debug] [Thread-4 (]: Marking all children of 'test.dbt_capstone_andres.not_null_my_first_dbt_model_id.5fb22c2710' to be skipped because of status 'fail'.  Reason: Got 1 result, configured to fail if != 0.
[0m16:22:27.891019 [info ] [Thread-1 (]: 5 of 8 START test unique_my_first_dbt_model_id ................................. [RUN]
[0m16:22:27.892003 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_capstone_andres.not_null_my_first_dbt_model_id.5fb22c2710, now test.dbt_capstone_andres.unique_my_first_dbt_model_id.16e066b321)
[0m16:22:27.892485 [debug] [Thread-1 (]: Began compiling node test.dbt_capstone_andres.unique_my_first_dbt_model_id.16e066b321
[0m16:22:27.902182 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_capstone_andres.unique_my_first_dbt_model_id.16e066b321"
[0m16:22:27.903080 [debug] [Thread-1 (]: Began executing node test.dbt_capstone_andres.unique_my_first_dbt_model_id.16e066b321
[0m16:22:27.908515 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_capstone_andres.unique_my_first_dbt_model_id.16e066b321"
[0m16:22:27.909895 [debug] [Thread-1 (]: Using snowflake connection "test.dbt_capstone_andres.unique_my_first_dbt_model_id.16e066b321"
[0m16:22:27.910422 [debug] [Thread-1 (]: On test.dbt_capstone_andres.unique_my_first_dbt_model_id.16e066b321: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "test.dbt_capstone_andres.unique_my_first_dbt_model_id.16e066b321"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from DATAEXPERT_STUDENT.andres.my_first_dbt_model
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
[0m16:22:27.910878 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m16:22:28.562211 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 0.651 seconds
[0m16:22:28.565236 [debug] [Thread-1 (]: On test.dbt_capstone_andres.unique_my_first_dbt_model_id.16e066b321: Close
[0m16:22:28.782323 [info ] [Thread-1 (]: 5 of 8 PASS unique_my_first_dbt_model_id ....................................... [[32mPASS[0m in 0.89s]
[0m16:22:28.783740 [debug] [Thread-1 (]: Finished running node test.dbt_capstone_andres.unique_my_first_dbt_model_id.16e066b321
[0m16:22:28.784750 [debug] [Thread-1 (]: Began running node model.dbt_capstone_andres.my_second_dbt_model
[0m16:22:28.785504 [info ] [Thread-1 (]: 6 of 8 SKIP relation andres.my_second_dbt_model ................................ [[33mSKIP[0m]
[0m16:22:28.786671 [debug] [Thread-1 (]: Finished running node model.dbt_capstone_andres.my_second_dbt_model
[0m16:22:28.787589 [debug] [Thread-4 (]: Marking all children of 'model.dbt_capstone_andres.my_second_dbt_model' to be skipped because of status 'skipped'. 
[0m16:22:28.789633 [debug] [Thread-1 (]: Began running node test.dbt_capstone_andres.not_null_my_second_dbt_model_id.151b76d778
[0m16:22:28.790656 [info ] [Thread-1 (]: 7 of 8 SKIP test not_null_my_second_dbt_model_id ............................... [[33mSKIP[0m]
[0m16:22:28.792131 [debug] [Thread-1 (]: Finished running node test.dbt_capstone_andres.not_null_my_second_dbt_model_id.151b76d778
[0m16:22:28.795631 [debug] [Thread-1 (]: Began running node test.dbt_capstone_andres.unique_my_second_dbt_model_id.57a0f8c493
[0m16:22:28.796359 [debug] [Thread-4 (]: Marking all children of 'test.dbt_capstone_andres.not_null_my_second_dbt_model_id.151b76d778' to be skipped because of status 'skipped'. 
[0m16:22:28.799338 [info ] [Thread-1 (]: 8 of 8 SKIP test unique_my_second_dbt_model_id ................................. [[33mSKIP[0m]
[0m16:22:28.801593 [debug] [Thread-1 (]: Finished running node test.dbt_capstone_andres.unique_my_second_dbt_model_id.57a0f8c493
[0m16:22:28.802773 [debug] [Thread-4 (]: Marking all children of 'test.dbt_capstone_andres.unique_my_second_dbt_model_id.57a0f8c493' to be skipped because of status 'skipped'. 
[0m16:22:28.809734 [debug] [MainThread]: Connection 'master' was properly closed.
[0m16:22:28.811553 [debug] [MainThread]: Connection 'test.dbt_capstone_andres.unique_my_first_dbt_model_id.16e066b321' was properly closed.
[0m16:22:28.812538 [info ] [MainThread]: 
[0m16:22:28.813504 [info ] [MainThread]: Finished running 1 table model, 4 data tests, 3 view models in 0 hours 0 minutes and 8.36 seconds (8.36s).
[0m16:22:28.815891 [debug] [MainThread]: Command end result
[0m16:22:28.910077 [info ] [MainThread]: 
[0m16:22:28.910758 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successs, and 0 warnings:[0m
[0m16:22:28.911514 [info ] [MainThread]: 
[0m16:22:28.912315 [error] [MainThread]: [31mFailure in test not_null_my_first_dbt_model_id (models/example/schema.yml)[0m
[0m16:22:28.912851 [error] [MainThread]:   Got 1 result, configured to fail if != 0
[0m16:22:28.913314 [info ] [MainThread]: 
[0m16:22:28.913828 [info ] [MainThread]:   compiled code at target/compiled/dbt_capstone_andres/models/example/schema.yml/not_null_my_first_dbt_model_id.sql
[0m16:22:28.914266 [info ] [MainThread]: 
[0m16:22:28.914714 [info ] [MainThread]: Done. PASS=4 WARN=0 ERROR=1 SKIP=3 TOTAL=8
[0m16:22:28.918374 [debug] [MainThread]: Resource report: {"command_name": "build", "command_wall_clock_time": 11.232176, "process_user_time": 3.816601, "process_kernel_time": 0.773677, "process_mem_max_rss": "197545984", "command_success": false, "process_in_blocks": "0", "process_out_blocks": "0"}
[0m16:22:28.919140 [debug] [MainThread]: Command `dbt build` failed at 16:22:28.919017 after 11.23 seconds
[0m16:22:28.919691 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a50ec50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a2bef50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106843510>]}
[0m16:22:28.920286 [debug] [MainThread]: Flushing usage events
[0m16:22:29.094468 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m16:25:42.158248 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110bff810>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110c01810>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110bbcf90>]}


============================== 16:25:42.163459 | 54086b19-4bc9-45c2-a318-53fc9393384a ==============================
[0m16:25:42.163459 [info ] [MainThread]: Running with dbt=1.9.0-b3
[0m16:25:42.164142 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'debug': 'False', 'fail_fast': 'False', 'log_path': 'logs', 'warn_error': 'None', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'introspect': 'True', 'log_format': 'default', 'target_path': 'None', 'invocation_command': 'dbt run -s stg_treasury_revenue_collections', 'send_anonymous_usage_stats': 'True'}
[0m16:25:43.599676 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '54086b19-4bc9-45c2-a318-53fc9393384a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1132e7bd0>]}
[0m16:25:43.664924 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '54086b19-4bc9-45c2-a318-53fc9393384a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10f850cd0>]}
[0m16:25:43.666741 [info ] [MainThread]: Registered adapter: snowflake=1.8.4
[0m16:25:43.889022 [debug] [MainThread]: checksum: 5f4fde221ca8627174a0341efc73099aaec05b8546408ceed6f59b1ec0755115, vars: {}, profile: , target: , version: 1.9.0b3
[0m16:25:44.099116 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m16:25:44.099815 [debug] [MainThread]: Partial parsing: updated file: dbt_capstone_andres://models/staging/stg_treasury_revenue_collections.sql
[0m16:25:44.337692 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.andres_capstone_dbt.example
[0m16:25:44.414091 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '54086b19-4bc9-45c2-a318-53fc9393384a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ee83110>]}
[0m16:25:44.533992 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '54086b19-4bc9-45c2-a318-53fc9393384a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x117193310>]}
[0m16:25:44.534565 [info ] [MainThread]: Found 4 models, 4 data tests, 2 sources, 862 macros
[0m16:25:44.534939 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '54086b19-4bc9-45c2-a318-53fc9393384a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10f70a250>]}
[0m16:25:44.536289 [info ] [MainThread]: 
[0m16:25:44.536757 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m16:25:44.537107 [info ] [MainThread]: 
[0m16:25:44.537642 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m16:25:44.538384 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_DATAEXPERT_STUDENT'
[0m16:25:44.553817 [debug] [ThreadPool]: Using snowflake connection "list_DATAEXPERT_STUDENT"
[0m16:25:44.554277 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "connection_name": "list_DATAEXPERT_STUDENT"} */
show terse schemas in database DATAEXPERT_STUDENT
    limit 10000
[0m16:25:44.554634 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:25:45.324005 [debug] [ThreadPool]: SQL status: SUCCESS 68 in 0.769 seconds
[0m16:25:45.326568 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT: Close
[0m16:25:45.567689 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_DATAEXPERT_STUDENT, now list_DATAEXPERT_STUDENT_andres)
[0m16:25:45.586139 [debug] [ThreadPool]: Using snowflake connection "list_DATAEXPERT_STUDENT_andres"
[0m16:25:45.586719 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "connection_name": "list_DATAEXPERT_STUDENT_andres"} */
show objects in DATAEXPERT_STUDENT.andres limit 10000
[0m16:25:45.587159 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m16:25:46.355645 [debug] [ThreadPool]: SQL status: SUCCESS 21 in 0.768 seconds
[0m16:25:46.358297 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: Close
[0m16:25:46.598201 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '54086b19-4bc9-45c2-a318-53fc9393384a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x116f0f3d0>]}
[0m16:25:46.600762 [debug] [Thread-1 (]: Began running node model.dbt_capstone_andres.stg_treasury_revenue_collections
[0m16:25:46.601374 [info ] [Thread-1 (]: 1 of 1 START sql view model andres.stg_treasury_revenue_collections ............ [RUN]
[0m16:25:46.601917 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_DATAEXPERT_STUDENT_andres, now model.dbt_capstone_andres.stg_treasury_revenue_collections)
[0m16:25:46.602341 [debug] [Thread-1 (]: Began compiling node model.dbt_capstone_andres.stg_treasury_revenue_collections
[0m16:25:46.612131 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_capstone_andres.stg_treasury_revenue_collections"
[0m16:25:46.613757 [debug] [Thread-1 (]: Began executing node model.dbt_capstone_andres.stg_treasury_revenue_collections
[0m16:25:46.649342 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_capstone_andres.stg_treasury_revenue_collections"
[0m16:25:46.650739 [debug] [Thread-1 (]: Using snowflake connection "model.dbt_capstone_andres.stg_treasury_revenue_collections"
[0m16:25:46.651212 [debug] [Thread-1 (]: On model.dbt_capstone_andres.stg_treasury_revenue_collections: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "model.dbt_capstone_andres.stg_treasury_revenue_collections"} */
create or replace   view DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections
  
   as (
    with source as (

    select * from dataexpert_student.andres.fiscal_data

),

renamed as (

    select
        record_date as date,
        channel_type_desc,
        tax_category_desc,
        net_collections_amt

    from source

)

select * from renamed
  );
[0m16:25:46.651654 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m16:25:47.448320 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 0.797 seconds
[0m16:25:47.470869 [debug] [Thread-1 (]: On model.dbt_capstone_andres.stg_treasury_revenue_collections: Close
[0m16:25:47.705169 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '54086b19-4bc9-45c2-a318-53fc9393384a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1174802d0>]}
[0m16:25:47.705940 [info ] [Thread-1 (]: 1 of 1 OK created sql view model andres.stg_treasury_revenue_collections ....... [[32mSUCCESS 1[0m in 1.10s]
[0m16:25:47.706599 [debug] [Thread-1 (]: Finished running node model.dbt_capstone_andres.stg_treasury_revenue_collections
[0m16:25:47.707777 [debug] [MainThread]: Connection 'master' was properly closed.
[0m16:25:47.708132 [debug] [MainThread]: Connection 'model.dbt_capstone_andres.stg_treasury_revenue_collections' was properly closed.
[0m16:25:47.708492 [info ] [MainThread]: 
[0m16:25:47.708875 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 3.17 seconds (3.17s).
[0m16:25:47.709568 [debug] [MainThread]: Command end result
[0m16:25:47.757166 [info ] [MainThread]: 
[0m16:25:47.757706 [info ] [MainThread]: [32mCompleted successfully[0m
[0m16:25:47.758079 [info ] [MainThread]: 
[0m16:25:47.758461 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m16:25:47.761342 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 5.665323, "process_user_time": 3.193513, "process_kernel_time": 0.64842, "process_mem_max_rss": "192925696", "process_in_blocks": "0", "process_out_blocks": "0"}
[0m16:25:47.762321 [debug] [MainThread]: Command `dbt run` succeeded at 16:25:47.761976 after 5.67 seconds
[0m16:25:47.762853 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110c3e6d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ce81090>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10cf54c90>]}
[0m16:25:47.763285 [debug] [MainThread]: Flushing usage events
[0m16:25:47.983672 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m16:26:12.917917 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ddaecd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ddac3d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ddaf410>]}


============================== 16:26:12.922269 | 5fda8f88-3b63-4a63-aa03-764691183150 ==============================
[0m16:26:12.922269 [info ] [MainThread]: Running with dbt=1.9.0-b3
[0m16:26:12.922934 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'version_check': 'True', 'fail_fast': 'False', 'log_path': 'logs', 'debug': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'log_format': 'default', 'invocation_command': 'dbt run -s stg_treasury_revenue_collections', 'static_parser': 'True', 'introspect': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m16:26:13.834078 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '5fda8f88-3b63-4a63-aa03-764691183150', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e4c8890>]}
[0m16:26:13.898599 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '5fda8f88-3b63-4a63-aa03-764691183150', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d03c090>]}
[0m16:26:13.899732 [info ] [MainThread]: Registered adapter: snowflake=1.8.4
[0m16:26:14.115678 [debug] [MainThread]: checksum: 5f4fde221ca8627174a0341efc73099aaec05b8546408ceed6f59b1ec0755115, vars: {}, profile: , target: , version: 1.9.0b3
[0m16:26:14.349671 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m16:26:14.350402 [debug] [MainThread]: Partial parsing: updated file: dbt_capstone_andres://models/staging/stg_treasury_revenue_collections.sql
[0m16:26:14.706082 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.andres_capstone_dbt.example
[0m16:26:14.795602 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '5fda8f88-3b63-4a63-aa03-764691183150', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10bbaa150>]}
[0m16:26:14.939044 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '5fda8f88-3b63-4a63-aa03-764691183150', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114335fd0>]}
[0m16:26:14.939764 [info ] [MainThread]: Found 4 models, 4 data tests, 2 sources, 862 macros
[0m16:26:14.940209 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '5fda8f88-3b63-4a63-aa03-764691183150', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1144885d0>]}
[0m16:26:14.941970 [info ] [MainThread]: 
[0m16:26:14.942438 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m16:26:14.942899 [info ] [MainThread]: 
[0m16:26:14.943528 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m16:26:14.944527 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_DATAEXPERT_STUDENT'
[0m16:26:14.962515 [debug] [ThreadPool]: Using snowflake connection "list_DATAEXPERT_STUDENT"
[0m16:26:14.963133 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "connection_name": "list_DATAEXPERT_STUDENT"} */
show terse schemas in database DATAEXPERT_STUDENT
    limit 10000
[0m16:26:14.963571 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:26:15.688461 [debug] [ThreadPool]: SQL status: SUCCESS 68 in 0.725 seconds
[0m16:26:15.691432 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT: Close
[0m16:26:15.905826 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_DATAEXPERT_STUDENT, now list_DATAEXPERT_STUDENT_andres)
[0m16:26:15.915347 [debug] [ThreadPool]: Using snowflake connection "list_DATAEXPERT_STUDENT_andres"
[0m16:26:15.915765 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "connection_name": "list_DATAEXPERT_STUDENT_andres"} */
show objects in DATAEXPERT_STUDENT.andres limit 10000
[0m16:26:15.916125 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m16:26:16.594091 [debug] [ThreadPool]: SQL status: SUCCESS 21 in 0.678 seconds
[0m16:26:16.596598 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: Close
[0m16:26:16.829289 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '5fda8f88-3b63-4a63-aa03-764691183150', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1140f51d0>]}
[0m16:26:16.831753 [debug] [Thread-1 (]: Began running node model.dbt_capstone_andres.stg_treasury_revenue_collections
[0m16:26:16.832434 [info ] [Thread-1 (]: 1 of 1 START sql view model andres.stg_treasury_revenue_collections ............ [RUN]
[0m16:26:16.833049 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_DATAEXPERT_STUDENT_andres, now model.dbt_capstone_andres.stg_treasury_revenue_collections)
[0m16:26:16.833554 [debug] [Thread-1 (]: Began compiling node model.dbt_capstone_andres.stg_treasury_revenue_collections
[0m16:26:16.843583 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_capstone_andres.stg_treasury_revenue_collections"
[0m16:26:16.844605 [debug] [Thread-1 (]: Began executing node model.dbt_capstone_andres.stg_treasury_revenue_collections
[0m16:26:16.878347 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_capstone_andres.stg_treasury_revenue_collections"
[0m16:26:16.879782 [debug] [Thread-1 (]: Using snowflake connection "model.dbt_capstone_andres.stg_treasury_revenue_collections"
[0m16:26:16.880304 [debug] [Thread-1 (]: On model.dbt_capstone_andres.stg_treasury_revenue_collections: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "model.dbt_capstone_andres.stg_treasury_revenue_collections"} */
create or replace   view DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections
  
   as (
    with source as (

    select * from dataexpert_student.andres.fiscal_data

),

renamed as (

    select
        record_date as collection_date,
        channel_type_desc,
        tax_category_desc,
        net_collections_amt

    from source

)

select * from renamed
  );
[0m16:26:16.880755 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m16:26:17.860881 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 0.980 seconds
[0m16:26:17.882265 [debug] [Thread-1 (]: On model.dbt_capstone_andres.stg_treasury_revenue_collections: Close
[0m16:26:18.088287 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5fda8f88-3b63-4a63-aa03-764691183150', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114134110>]}
[0m16:26:18.088967 [info ] [Thread-1 (]: 1 of 1 OK created sql view model andres.stg_treasury_revenue_collections ....... [[32mSUCCESS 1[0m in 1.25s]
[0m16:26:18.089589 [debug] [Thread-1 (]: Finished running node model.dbt_capstone_andres.stg_treasury_revenue_collections
[0m16:26:18.090889 [debug] [MainThread]: Connection 'master' was properly closed.
[0m16:26:18.091197 [debug] [MainThread]: Connection 'model.dbt_capstone_andres.stg_treasury_revenue_collections' was properly closed.
[0m16:26:18.091510 [info ] [MainThread]: 
[0m16:26:18.091889 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 3.15 seconds (3.15s).
[0m16:26:18.092505 [debug] [MainThread]: Command end result
[0m16:26:18.141061 [info ] [MainThread]: 
[0m16:26:18.141801 [info ] [MainThread]: [32mCompleted successfully[0m
[0m16:26:18.142306 [info ] [MainThread]: 
[0m16:26:18.142681 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m16:26:18.144739 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 5.2887273, "process_user_time": 3.239942, "process_kernel_time": 0.535698, "process_mem_max_rss": "192884736", "process_in_blocks": "0", "process_out_blocks": "0"}
[0m16:26:18.145494 [debug] [MainThread]: Command `dbt run` succeeded at 16:26:18.145373 after 5.29 seconds
[0m16:26:18.146132 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ddbd290>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10dbcb790>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a0c0b90>]}
[0m16:26:18.146862 [debug] [MainThread]: Flushing usage events
[0m16:26:18.354013 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m16:27:00.361476 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1111c2b10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1111c0910>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1111c1790>]}


============================== 16:27:00.367008 | 60cdb7f2-d70e-4d0d-b81a-60ddee72d236 ==============================
[0m16:27:00.367008 [info ] [MainThread]: Running with dbt=1.9.0-b3
[0m16:27:00.367705 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'version_check': 'True', 'log_path': 'logs', 'debug': 'False', 'profiles_dir': '.', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'static_parser': 'True', 'log_format': 'default', 'target_path': 'None', 'invocation_command': 'dbt run -s stg_treasury_revenue_collections', 'send_anonymous_usage_stats': 'True'}
[0m16:27:02.429460 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '60cdb7f2-d70e-4d0d-b81a-60ddee72d236', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1111fd850>]}
[0m16:27:02.486040 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '60cdb7f2-d70e-4d0d-b81a-60ddee72d236', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110480090>]}
[0m16:27:02.487276 [info ] [MainThread]: Registered adapter: snowflake=1.8.4
[0m16:27:02.683322 [debug] [MainThread]: checksum: 5f4fde221ca8627174a0341efc73099aaec05b8546408ceed6f59b1ec0755115, vars: {}, profile: , target: , version: 1.9.0b3
[0m16:27:02.948409 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m16:27:02.949134 [debug] [MainThread]: Partial parsing: updated file: dbt_capstone_andres://models/staging/stg_treasury_revenue_collections.sql
[0m16:27:03.205779 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.andres_capstone_dbt.example
[0m16:27:03.301913 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '60cdb7f2-d70e-4d0d-b81a-60ddee72d236', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1173bf450>]}
[0m16:27:03.444674 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '60cdb7f2-d70e-4d0d-b81a-60ddee72d236', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x117803f10>]}
[0m16:27:03.445346 [info ] [MainThread]: Found 4 models, 4 data tests, 2 sources, 862 macros
[0m16:27:03.445787 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '60cdb7f2-d70e-4d0d-b81a-60ddee72d236', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x117817a90>]}
[0m16:27:03.447657 [info ] [MainThread]: 
[0m16:27:03.448169 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m16:27:03.448660 [info ] [MainThread]: 
[0m16:27:03.449236 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m16:27:03.450200 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_DATAEXPERT_STUDENT'
[0m16:27:03.466149 [debug] [ThreadPool]: Using snowflake connection "list_DATAEXPERT_STUDENT"
[0m16:27:03.466691 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "connection_name": "list_DATAEXPERT_STUDENT"} */
show terse schemas in database DATAEXPERT_STUDENT
    limit 10000
[0m16:27:03.467094 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:27:04.342560 [debug] [ThreadPool]: SQL status: SUCCESS 68 in 0.875 seconds
[0m16:27:04.345574 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT: Close
[0m16:27:04.594791 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_DATAEXPERT_STUDENT, now list_DATAEXPERT_STUDENT_andres)
[0m16:27:04.605301 [debug] [ThreadPool]: Using snowflake connection "list_DATAEXPERT_STUDENT_andres"
[0m16:27:04.605784 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "connection_name": "list_DATAEXPERT_STUDENT_andres"} */
show objects in DATAEXPERT_STUDENT.andres limit 10000
[0m16:27:04.606172 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m16:27:05.278924 [debug] [ThreadPool]: SQL status: SUCCESS 21 in 0.673 seconds
[0m16:27:05.281621 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: Close
[0m16:27:05.512182 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '60cdb7f2-d70e-4d0d-b81a-60ddee72d236', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11782a250>]}
[0m16:27:05.516129 [debug] [Thread-1 (]: Began running node model.dbt_capstone_andres.stg_treasury_revenue_collections
[0m16:27:05.516889 [info ] [Thread-1 (]: 1 of 1 START sql view model andres.stg_treasury_revenue_collections ............ [RUN]
[0m16:27:05.517521 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_DATAEXPERT_STUDENT_andres, now model.dbt_capstone_andres.stg_treasury_revenue_collections)
[0m16:27:05.518034 [debug] [Thread-1 (]: Began compiling node model.dbt_capstone_andres.stg_treasury_revenue_collections
[0m16:27:05.529367 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_capstone_andres.stg_treasury_revenue_collections"
[0m16:27:05.530323 [debug] [Thread-1 (]: Began executing node model.dbt_capstone_andres.stg_treasury_revenue_collections
[0m16:27:05.567098 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_capstone_andres.stg_treasury_revenue_collections"
[0m16:27:05.568528 [debug] [Thread-1 (]: Using snowflake connection "model.dbt_capstone_andres.stg_treasury_revenue_collections"
[0m16:27:05.568994 [debug] [Thread-1 (]: On model.dbt_capstone_andres.stg_treasury_revenue_collections: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "model.dbt_capstone_andres.stg_treasury_revenue_collections"} */
create or replace   view DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections
  
   as (
    with source as (

    select * from dataexpert_student.andres.fiscal_data

),

renamed as (

    select
        record_date as collection_date,
        channel_type_desc as channel_type,
        tax_category_desc as tax_category,
        net_collections_amt as amount

    from source

)

select * from renamed
  );
[0m16:27:05.569433 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m16:27:06.373127 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 0.804 seconds
[0m16:27:06.400540 [debug] [Thread-1 (]: On model.dbt_capstone_andres.stg_treasury_revenue_collections: Close
[0m16:27:06.622712 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '60cdb7f2-d70e-4d0d-b81a-60ddee72d236', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1175f1710>]}
[0m16:27:06.623595 [info ] [Thread-1 (]: 1 of 1 OK created sql view model andres.stg_treasury_revenue_collections ....... [[32mSUCCESS 1[0m in 1.10s]
[0m16:27:06.624373 [debug] [Thread-1 (]: Finished running node model.dbt_capstone_andres.stg_treasury_revenue_collections
[0m16:27:06.625829 [debug] [MainThread]: Connection 'master' was properly closed.
[0m16:27:06.626270 [debug] [MainThread]: Connection 'model.dbt_capstone_andres.stg_treasury_revenue_collections' was properly closed.
[0m16:27:06.626694 [info ] [MainThread]: 
[0m16:27:06.627152 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 3.18 seconds (3.18s).
[0m16:27:06.627971 [debug] [MainThread]: Command end result
[0m16:27:06.677676 [info ] [MainThread]: 
[0m16:27:06.678216 [info ] [MainThread]: [32mCompleted successfully[0m
[0m16:27:06.678582 [info ] [MainThread]: 
[0m16:27:06.678954 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m16:27:06.681275 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 6.3843, "process_user_time": 3.470136, "process_kernel_time": 0.757496, "process_mem_max_rss": "192700416", "process_in_blocks": "0", "process_out_blocks": "0"}
[0m16:27:06.682133 [debug] [MainThread]: Command `dbt run` succeeded at 16:27:06.682011 after 6.39 seconds
[0m16:27:06.682609 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d590a90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1111b7e50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110fe2d90>]}
[0m16:27:06.683071 [debug] [MainThread]: Flushing usage events
[0m16:27:06.876120 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m16:29:37.755136 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1137bfd90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11380d890>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1137bcad0>]}


============================== 16:29:37.761640 | 9e5e562c-c485-45a0-9c0e-13f28291cfea ==============================
[0m16:29:37.761640 [info ] [MainThread]: Running with dbt=1.9.0-b3
[0m16:29:37.762451 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'debug': 'False', 'version_check': 'True', 'log_path': 'logs', 'fail_fast': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'introspect': 'True', 'log_format': 'default', 'target_path': 'None', 'invocation_command': 'dbt run -s stg_treasury_revenue_collections', 'send_anonymous_usage_stats': 'True'}
[0m16:29:39.603234 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '9e5e562c-c485-45a0-9c0e-13f28291cfea', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x113f8b810>]}
[0m16:29:39.706918 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '9e5e562c-c485-45a0-9c0e-13f28291cfea', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112a480d0>]}
[0m16:29:39.708933 [info ] [MainThread]: Registered adapter: snowflake=1.8.4
[0m16:29:39.936529 [debug] [MainThread]: checksum: 5f4fde221ca8627174a0341efc73099aaec05b8546408ceed6f59b1ec0755115, vars: {}, profile: , target: , version: 1.9.0b3
[0m16:29:40.198215 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m16:29:40.198867 [debug] [MainThread]: Partial parsing: updated file: dbt_capstone_andres://models/staging/stg_treasury_revenue_collections.sql
[0m16:29:40.496920 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.andres_capstone_dbt.example
[0m16:29:40.593102 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '9e5e562c-c485-45a0-9c0e-13f28291cfea', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1195ea350>]}
[0m16:29:40.734404 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '9e5e562c-c485-45a0-9c0e-13f28291cfea', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119dfd790>]}
[0m16:29:40.734977 [info ] [MainThread]: Found 4 models, 4 data tests, 2 sources, 862 macros
[0m16:29:40.735361 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9e5e562c-c485-45a0-9c0e-13f28291cfea', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119e0c110>]}
[0m16:29:40.736819 [info ] [MainThread]: 
[0m16:29:40.737216 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m16:29:40.737573 [info ] [MainThread]: 
[0m16:29:40.738078 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m16:29:40.738888 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_DATAEXPERT_STUDENT'
[0m16:29:40.753493 [debug] [ThreadPool]: Using snowflake connection "list_DATAEXPERT_STUDENT"
[0m16:29:40.753984 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "connection_name": "list_DATAEXPERT_STUDENT"} */
show terse schemas in database DATAEXPERT_STUDENT
    limit 10000
[0m16:29:40.754342 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:29:41.557321 [debug] [ThreadPool]: SQL status: SUCCESS 68 in 0.803 seconds
[0m16:29:41.559848 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT: Close
[0m16:29:41.841837 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_DATAEXPERT_STUDENT, now list_DATAEXPERT_STUDENT_andres)
[0m16:29:41.856752 [debug] [ThreadPool]: Using snowflake connection "list_DATAEXPERT_STUDENT_andres"
[0m16:29:41.857259 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "connection_name": "list_DATAEXPERT_STUDENT_andres"} */
show objects in DATAEXPERT_STUDENT.andres limit 10000
[0m16:29:41.857663 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m16:29:42.604188 [debug] [ThreadPool]: SQL status: SUCCESS 21 in 0.746 seconds
[0m16:29:42.607852 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: Close
[0m16:29:42.841923 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9e5e562c-c485-45a0-9c0e-13f28291cfea', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11a080050>]}
[0m16:29:42.845043 [debug] [Thread-1 (]: Began running node model.dbt_capstone_andres.stg_treasury_revenue_collections
[0m16:29:42.845689 [info ] [Thread-1 (]: 1 of 1 START sql view model andres.stg_treasury_revenue_collections ............ [RUN]
[0m16:29:42.846236 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_DATAEXPERT_STUDENT_andres, now model.dbt_capstone_andres.stg_treasury_revenue_collections)
[0m16:29:42.846674 [debug] [Thread-1 (]: Began compiling node model.dbt_capstone_andres.stg_treasury_revenue_collections
[0m16:29:42.855462 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_capstone_andres.stg_treasury_revenue_collections"
[0m16:29:42.856341 [debug] [Thread-1 (]: Began executing node model.dbt_capstone_andres.stg_treasury_revenue_collections
[0m16:29:42.888241 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_capstone_andres.stg_treasury_revenue_collections"
[0m16:29:42.889706 [debug] [Thread-1 (]: Using snowflake connection "model.dbt_capstone_andres.stg_treasury_revenue_collections"
[0m16:29:42.890168 [debug] [Thread-1 (]: On model.dbt_capstone_andres.stg_treasury_revenue_collections: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "model.dbt_capstone_andres.stg_treasury_revenue_collections"} */
create or replace   view DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections
  
   as (
    with source as (

    select * from dataexpert_student.andres.fiscal_data

),

renamed as (

    select
        record_date as collection_date,
        channel_type_desc as channel_type,
        tax_category_desc as tax_category,
        net_collections_amt/1000000000 as amount_in_billions

    from source

)

select * from renamed
  );
[0m16:29:42.890603 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m16:29:43.779951 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 0.889 seconds
[0m16:29:43.803607 [debug] [Thread-1 (]: On model.dbt_capstone_andres.stg_treasury_revenue_collections: Close
[0m16:29:44.023027 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9e5e562c-c485-45a0-9c0e-13f28291cfea', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11ddcd390>]}
[0m16:29:44.023890 [info ] [Thread-1 (]: 1 of 1 OK created sql view model andres.stg_treasury_revenue_collections ....... [[32mSUCCESS 1[0m in 1.18s]
[0m16:29:44.024631 [debug] [Thread-1 (]: Finished running node model.dbt_capstone_andres.stg_treasury_revenue_collections
[0m16:29:44.025993 [debug] [MainThread]: Connection 'master' was properly closed.
[0m16:29:44.026429 [debug] [MainThread]: Connection 'model.dbt_capstone_andres.stg_treasury_revenue_collections' was properly closed.
[0m16:29:44.026855 [info ] [MainThread]: 
[0m16:29:44.027313 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 3.29 seconds (3.29s).
[0m16:29:44.028129 [debug] [MainThread]: Command end result
[0m16:29:44.085803 [info ] [MainThread]: 
[0m16:29:44.086450 [info ] [MainThread]: [32mCompleted successfully[0m
[0m16:29:44.086898 [info ] [MainThread]: 
[0m16:29:44.087346 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m16:29:44.091014 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 6.414764, "process_user_time": 3.646642, "process_kernel_time": 0.748767, "process_mem_max_rss": "192237568", "process_in_blocks": "0", "process_out_blocks": "0"}
[0m16:29:44.091793 [debug] [MainThread]: Command `dbt run` succeeded at 16:29:44.091664 after 6.42 seconds
[0m16:29:44.092457 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11380d990>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1136c3050>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10fa88fd0>]}
[0m16:29:44.092996 [debug] [MainThread]: Flushing usage events
[0m16:29:44.273781 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m20:36:18.861456 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10de0ab10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10de089d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10de08a10>]}


============================== 20:36:18.867312 | 1379f160-52a9-462c-9a61-030b2b512ec8 ==============================
[0m20:36:18.867312 [info ] [MainThread]: Running with dbt=1.9.0-b3
[0m20:36:18.868038 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'version_check': 'True', 'warn_error': 'None', 'log_path': 'logs', 'debug': 'False', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'invocation_command': 'dbt run -s dim_date', 'log_format': 'default', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m20:36:20.565060 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '1379f160-52a9-462c-9a61-030b2b512ec8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10de57150>]}
[0m20:36:20.621695 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '1379f160-52a9-462c-9a61-030b2b512ec8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10dd0c610>]}
[0m20:36:20.623292 [info ] [MainThread]: Registered adapter: snowflake=1.8.4
[0m20:36:20.821610 [debug] [MainThread]: checksum: 5f4fde221ca8627174a0341efc73099aaec05b8546408ceed6f59b1ec0755115, vars: {}, profile: , target: , version: 1.9.0b3
[0m20:36:21.059877 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 2 files added, 0 files changed.
[0m20:36:21.060492 [debug] [MainThread]: Partial parsing: added file: dbt_capstone_andres://models/marts/dim_date.sql
[0m20:36:21.060857 [debug] [MainThread]: Partial parsing: added file: dbt_capstone_andres://models/marts/daily_metrics.sql
[0m20:36:21.515361 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.andres_capstone_dbt.example
[0m20:36:21.532439 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '1379f160-52a9-462c-9a61-030b2b512ec8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1144344d0>]}
[0m20:36:21.707382 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '1379f160-52a9-462c-9a61-030b2b512ec8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1143ce650>]}
[0m20:36:21.708049 [info ] [MainThread]: Found 6 models, 4 data tests, 2 sources, 862 macros
[0m20:36:21.708513 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '1379f160-52a9-462c-9a61-030b2b512ec8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1146c4490>]}
[0m20:36:21.710361 [info ] [MainThread]: 
[0m20:36:21.710848 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m20:36:21.711269 [info ] [MainThread]: 
[0m20:36:21.711855 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m20:36:21.712832 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_DATAEXPERT_STUDENT'
[0m20:36:21.723201 [debug] [ThreadPool]: Using snowflake connection "list_DATAEXPERT_STUDENT"
[0m20:36:21.723693 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "connection_name": "list_DATAEXPERT_STUDENT"} */
show terse schemas in database DATAEXPERT_STUDENT
    limit 10000
[0m20:36:21.724081 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:36:22.687717 [debug] [ThreadPool]: SQL status: SUCCESS 68 in 0.964 seconds
[0m20:36:22.691640 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT: Close
[0m20:36:22.906184 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_DATAEXPERT_STUDENT, now list_DATAEXPERT_STUDENT_andres)
[0m20:36:22.915202 [debug] [ThreadPool]: Using snowflake connection "list_DATAEXPERT_STUDENT_andres"
[0m20:36:22.915582 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "connection_name": "list_DATAEXPERT_STUDENT_andres"} */
show objects in DATAEXPERT_STUDENT.andres limit 10000
[0m20:36:22.915926 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m20:36:23.618101 [debug] [ThreadPool]: SQL status: SUCCESS 21 in 0.702 seconds
[0m20:36:23.621764 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: Close
[0m20:36:23.839182 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '1379f160-52a9-462c-9a61-030b2b512ec8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1147b4f50>]}
[0m20:36:23.843359 [debug] [Thread-1 (]: Began running node model.dbt_capstone_andres.dim_date
[0m20:36:23.844207 [info ] [Thread-1 (]: 1 of 1 START sql table model andres.dim_date ................................... [RUN]
[0m20:36:23.844942 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_DATAEXPERT_STUDENT_andres, now model.dbt_capstone_andres.dim_date)
[0m20:36:23.845476 [debug] [Thread-1 (]: Began compiling node model.dbt_capstone_andres.dim_date
[0m20:36:23.860784 [debug] [Thread-1 (]: Using snowflake connection "model.dbt_capstone_andres.dim_date"
[0m20:36:23.861520 [debug] [Thread-1 (]: On model.dbt_capstone_andres.dim_date: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "model.dbt_capstone_andres.dim_date"} */
select datediff(
        day,
        cast('1995-01-01' as timestamp),
        cast('2040-12-31' as timestamp)
        )
[0m20:36:23.862130 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m20:36:24.561725 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 0.700 seconds
[0m20:36:24.578780 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_capstone_andres.dim_date"
[0m20:36:24.579930 [debug] [Thread-1 (]: Began executing node model.dbt_capstone_andres.dim_date
[0m20:36:24.609153 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_capstone_andres.dim_date"
[0m20:36:24.616705 [debug] [Thread-1 (]: Using snowflake connection "model.dbt_capstone_andres.dim_date"
[0m20:36:24.617404 [debug] [Thread-1 (]: On model.dbt_capstone_andres.dim_date: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "model.dbt_capstone_andres.dim_date"} */
create or replace transient table DATAEXPERT_STUDENT.andres.dim_date
         as
        (

    
with base_dates as (
    
    with date_spine as
(

    





with rawdata as (

    

    

    with p as (
        select 0 as generated_number union all select 1
    ), unioned as (

    select

    
    p0.generated_number * power(2, 0)
     + 
    
    p1.generated_number * power(2, 1)
     + 
    
    p2.generated_number * power(2, 2)
     + 
    
    p3.generated_number * power(2, 3)
     + 
    
    p4.generated_number * power(2, 4)
     + 
    
    p5.generated_number * power(2, 5)
     + 
    
    p6.generated_number * power(2, 6)
     + 
    
    p7.generated_number * power(2, 7)
     + 
    
    p8.generated_number * power(2, 8)
     + 
    
    p9.generated_number * power(2, 9)
     + 
    
    p10.generated_number * power(2, 10)
     + 
    
    p11.generated_number * power(2, 11)
     + 
    
    p12.generated_number * power(2, 12)
     + 
    
    p13.generated_number * power(2, 13)
     + 
    
    p14.generated_number * power(2, 14)
    
    
    + 1
    as generated_number

    from

    
    p as p0
     cross join 
    
    p as p1
     cross join 
    
    p as p2
     cross join 
    
    p as p3
     cross join 
    
    p as p4
     cross join 
    
    p as p5
     cross join 
    
    p as p6
     cross join 
    
    p as p7
     cross join 
    
    p as p8
     cross join 
    
    p as p9
     cross join 
    
    p as p10
     cross join 
    
    p as p11
     cross join 
    
    p as p12
     cross join 
    
    p as p13
     cross join 
    
    p as p14
    
    

    )

    select *
    from unioned
    where generated_number <= 16801
    order by generated_number



),

all_periods as (

    select (
        

    dateadd(
        day,
        (row_number() over (order by 1) - 1),
        cast('1995-01-01' as timestamp)
        )


    ) as date_day
    from rawdata

),

filtered as (

    select *
    from all_periods
    where date_day <= cast('2040-12-31' as timestamp)

)

select * from filtered



)
select
    cast(d.date_day as timestamp) as date_day
from
    date_spine d


),
dates_with_prior_year_dates as (

    select
        cast(d.date_day as date) as date_day,
        cast(

    dateadd(
        year,
        -1,
        d.date_day
        )

 as date) as prior_year_date_day,
        cast(

    dateadd(
        day,
        -364,
        d.date_day
        )

 as date) as prior_year_over_year_date_day
    from
    	base_dates d

)
select
    d.date_day,
    cast(

    dateadd(
        day,
        -1,
        d.date_day
        )

 as date) as prior_date_day,
    cast(

    dateadd(
        day,
        1,
        d.date_day
        )

 as date) as next_date_day,
    d.prior_year_date_day as prior_year_date_day,
    d.prior_year_over_year_date_day,
    case
            when date_part('dayofweek', d.date_day) = 7 then 1
            else date_part('dayofweek', d.date_day) + 1
        end as day_of_week,
    date_part('dayofweekiso', d.date_day) as day_of_week_iso,
    -- long version not implemented on Snowflake so we're doing it manually :/
    case dayname(d.date_day)
        when 'Mon' then 'Monday'
        when 'Tue' then 'Tuesday'
        when 'Wed' then 'Wednesday'
        when 'Thu' then 'Thursday'
        when 'Fri' then 'Friday'
        when 'Sat' then 'Saturday'
        when 'Sun' then 'Sunday'
    end as day_of_week_name,
    dayname(d.date_day) as day_of_week_name_short,
    date_part('day', d.date_day) as day_of_month,
    date_part('dayofyear', d.date_day) as day_of_year,

    
    
    cast(

    dateadd(
        day,
        -1 * (case
            when date_part('dayofweek', d.date_day) = 7 then 1
            else date_part('dayofweek', d.date_day) + 1
        end - 1),
        d.date_day
        )

 as date) as week_start_date,
    cast(

    dateadd(
        day,
        6,
        
    
    cast(

    dateadd(
        day,
        -1 * (case
            when date_part('dayofweek', d.date_day) = 7 then 1
            else date_part('dayofweek', d.date_day) + 1
        end - 1),
        d.date_day
        )

 as date)
        )

 as date) as week_end_date,
    
    
    cast(

    dateadd(
        day,
        -1 * (case
            when date_part('dayofweek', d.prior_year_over_year_date_day) = 7 then 1
            else date_part('dayofweek', d.prior_year_over_year_date_day) + 1
        end - 1),
        d.prior_year_over_year_date_day
        )

 as date) as prior_year_week_start_date,
    cast(

    dateadd(
        day,
        6,
        
    
    cast(

    dateadd(
        day,
        -1 * (case
            when date_part('dayofweek', d.prior_year_over_year_date_day) = 7 then 1
            else date_part('dayofweek', d.prior_year_over_year_date_day) + 1
        end - 1),
        d.prior_year_over_year_date_day
        )

 as date)
        )

 as date) as prior_year_week_end_date,
    cast(date_part('week', d.date_day) as integer) as week_of_year,

    cast(date_trunc('week', d.date_day) as date) as iso_week_start_date,
    cast(

    dateadd(
        day,
        6,
        cast(date_trunc('week', d.date_day) as date)
        )

 as date) as iso_week_end_date,
    cast(date_trunc('week', d.prior_year_over_year_date_day) as date) as prior_year_iso_week_start_date,
    cast(

    dateadd(
        day,
        6,
        cast(date_trunc('week', d.prior_year_over_year_date_day) as date)
        )

 as date) as prior_year_iso_week_end_date,
    cast(date_part('weekiso', d.date_day) as integer) as iso_week_of_year,

    cast(date_part('week', d.prior_year_over_year_date_day) as integer) as prior_year_week_of_year,
    cast(date_part('weekiso', d.prior_year_over_year_date_day) as integer) as prior_year_iso_week_of_year,

    cast(date_part('month', d.date_day) as integer) as month_of_year,
    to_char(d.date_day, 'MMMM')  as month_name,
    to_char(d.date_day, 'MON')  as month_name_short,

    cast(date_trunc('month', d.date_day) as date) as month_start_date,
    cast(cast(
        

    dateadd(
        day,
        -1,
        

    dateadd(
        month,
        1,
        date_trunc('month', d.date_day)
        )


        )


        as date) as date) as month_end_date,

    cast(date_trunc('month', d.prior_year_date_day) as date) as prior_year_month_start_date,
    cast(cast(
        

    dateadd(
        day,
        -1,
        

    dateadd(
        month,
        1,
        date_trunc('month', d.prior_year_date_day)
        )


        )


        as date) as date) as prior_year_month_end_date,

    cast(date_part('quarter', d.date_day) as integer) as quarter_of_year,
    cast(date_trunc('quarter', d.date_day) as date) as quarter_start_date,
    cast(cast(
        

    dateadd(
        day,
        -1,
        

    dateadd(
        quarter,
        1,
        date_trunc('quarter', d.date_day)
        )


        )


        as date) as date) as quarter_end_date,

    cast(date_part('year', d.date_day) as integer) as year_number,
    cast(date_trunc('year', d.date_day) as date) as year_start_date,
    cast(cast(
        

    dateadd(
        day,
        -1,
        

    dateadd(
        year,
        1,
        date_trunc('year', d.date_day)
        )


        )


        as date) as date) as year_end_date
from
    dates_with_prior_year_dates d
order by 1


        );
[0m20:36:25.878063 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 1.260 seconds
[0m20:36:25.901613 [debug] [Thread-1 (]: On model.dbt_capstone_andres.dim_date: Close
[0m20:36:26.124994 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1379f160-52a9-462c-9a61-030b2b512ec8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1142ee5d0>]}
[0m20:36:26.126100 [info ] [Thread-1 (]: 1 of 1 OK created sql table model andres.dim_date .............................. [[32mSUCCESS 1[0m in 2.28s]
[0m20:36:26.127034 [debug] [Thread-1 (]: Finished running node model.dbt_capstone_andres.dim_date
[0m20:36:26.128520 [debug] [MainThread]: Connection 'master' was properly closed.
[0m20:36:26.128944 [debug] [MainThread]: Connection 'model.dbt_capstone_andres.dim_date' was properly closed.
[0m20:36:26.129373 [info ] [MainThread]: 
[0m20:36:26.129833 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 4.42 seconds (4.42s).
[0m20:36:26.130662 [debug] [MainThread]: Command end result
[0m20:36:26.186278 [info ] [MainThread]: 
[0m20:36:26.186873 [info ] [MainThread]: [32mCompleted successfully[0m
[0m20:36:26.187270 [info ] [MainThread]: 
[0m20:36:26.187670 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m20:36:26.191692 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 7.40655, "process_user_time": 3.454764, "process_kernel_time": 0.72013, "process_mem_max_rss": "195293184", "process_in_blocks": "0", "process_out_blocks": "0"}
[0m20:36:26.192421 [debug] [MainThread]: Command `dbt run` succeeded at 20:36:26.192306 after 7.41 seconds
[0m20:36:26.192879 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10de57350>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d1a6950>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a114d50>]}
[0m20:36:26.193315 [debug] [MainThread]: Flushing usage events
[0m20:36:26.635949 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m20:39:11.405306 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1119079d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111c76450>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111c76a50>]}


============================== 20:39:11.410718 | 05fcef48-c2f2-4399-b14e-1e08288904c2 ==============================
[0m20:39:11.410718 [info ] [MainThread]: Running with dbt=1.9.0-b3
[0m20:39:11.411414 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'profiles_dir': '.', 'log_path': 'logs', 'fail_fast': 'False', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt build', 'log_format': 'default', 'introspect': 'True', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m20:39:13.111514 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '05fcef48-c2f2-4399-b14e-1e08288904c2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x117dd7390>]}
[0m20:39:13.169323 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '05fcef48-c2f2-4399-b14e-1e08288904c2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111b08550>]}
[0m20:39:13.173370 [info ] [MainThread]: Registered adapter: snowflake=1.8.4
[0m20:39:13.391297 [debug] [MainThread]: checksum: 5f4fde221ca8627174a0341efc73099aaec05b8546408ceed6f59b1ec0755115, vars: {}, profile: , target: , version: 1.9.0b3
[0m20:39:13.662568 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 2 files changed.
[0m20:39:13.663197 [debug] [MainThread]: Partial parsing: updated file: dbt_capstone_andres://models/staging/stg_daily_bars.sql
[0m20:39:13.663589 [debug] [MainThread]: Partial parsing: updated file: dbt_capstone_andres://models/staging/stg_treasury_revenue_collections.sql
[0m20:39:13.907293 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.andres_capstone_dbt.example
[0m20:39:14.004569 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '05fcef48-c2f2-4399-b14e-1e08288904c2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11854ef50>]}
[0m20:39:14.167087 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '05fcef48-c2f2-4399-b14e-1e08288904c2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x117ecab90>]}
[0m20:39:14.167778 [info ] [MainThread]: Found 6 models, 4 data tests, 2 sources, 862 macros
[0m20:39:14.170362 [info ] [MainThread]: 
[0m20:39:14.170771 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m20:39:14.171135 [info ] [MainThread]: 
[0m20:39:14.171655 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m20:39:14.176765 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_DATAEXPERT_STUDENT'
[0m20:39:14.191948 [debug] [ThreadPool]: Using snowflake connection "list_DATAEXPERT_STUDENT"
[0m20:39:14.192467 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "connection_name": "list_DATAEXPERT_STUDENT"} */
show terse schemas in database DATAEXPERT_STUDENT
    limit 10000
[0m20:39:14.192876 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:39:15.049635 [debug] [ThreadPool]: SQL status: SUCCESS 68 in 0.857 seconds
[0m20:39:15.052373 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT: Close
[0m20:39:15.260164 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_DATAEXPERT_STUDENT, now list_DATAEXPERT_STUDENT_andres)
[0m20:39:15.269416 [debug] [ThreadPool]: Using snowflake connection "list_DATAEXPERT_STUDENT_andres"
[0m20:39:15.269835 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "connection_name": "list_DATAEXPERT_STUDENT_andres"} */
show objects in DATAEXPERT_STUDENT.andres limit 10000
[0m20:39:15.270151 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m20:39:15.913784 [debug] [ThreadPool]: SQL status: SUCCESS 22 in 0.644 seconds
[0m20:39:15.917476 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: Close
[0m20:39:16.145386 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '05fcef48-c2f2-4399-b14e-1e08288904c2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110beb390>]}
[0m20:39:16.149547 [debug] [Thread-1 (]: Began running node model.dbt_capstone_andres.dim_date
[0m20:39:16.150287 [info ] [Thread-1 (]: 1 of 9 START sql table model andres.dim_date ................................... [RUN]
[0m20:39:16.150926 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_DATAEXPERT_STUDENT_andres, now model.dbt_capstone_andres.dim_date)
[0m20:39:16.151426 [debug] [Thread-1 (]: Began compiling node model.dbt_capstone_andres.dim_date
[0m20:39:16.199478 [debug] [Thread-1 (]: Using snowflake connection "model.dbt_capstone_andres.dim_date"
[0m20:39:16.200262 [debug] [Thread-1 (]: On model.dbt_capstone_andres.dim_date: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "model.dbt_capstone_andres.dim_date"} */
select datediff(
        day,
        cast('1995-01-01' as timestamp),
        cast('2040-12-31' as timestamp)
        )
[0m20:39:16.200890 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m20:39:16.889416 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 0.688 seconds
[0m20:39:16.967628 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_capstone_andres.dim_date"
[0m20:39:16.971196 [debug] [Thread-1 (]: Began executing node model.dbt_capstone_andres.dim_date
[0m20:39:16.999851 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_capstone_andres.dim_date"
[0m20:39:17.007597 [debug] [Thread-1 (]: Using snowflake connection "model.dbt_capstone_andres.dim_date"
[0m20:39:17.008306 [debug] [Thread-1 (]: On model.dbt_capstone_andres.dim_date: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "model.dbt_capstone_andres.dim_date"} */
create or replace transient table DATAEXPERT_STUDENT.andres.dim_date
         as
        (

    
with base_dates as (
    
    with date_spine as
(

    





with rawdata as (

    

    

    with p as (
        select 0 as generated_number union all select 1
    ), unioned as (

    select

    
    p0.generated_number * power(2, 0)
     + 
    
    p1.generated_number * power(2, 1)
     + 
    
    p2.generated_number * power(2, 2)
     + 
    
    p3.generated_number * power(2, 3)
     + 
    
    p4.generated_number * power(2, 4)
     + 
    
    p5.generated_number * power(2, 5)
     + 
    
    p6.generated_number * power(2, 6)
     + 
    
    p7.generated_number * power(2, 7)
     + 
    
    p8.generated_number * power(2, 8)
     + 
    
    p9.generated_number * power(2, 9)
     + 
    
    p10.generated_number * power(2, 10)
     + 
    
    p11.generated_number * power(2, 11)
     + 
    
    p12.generated_number * power(2, 12)
     + 
    
    p13.generated_number * power(2, 13)
     + 
    
    p14.generated_number * power(2, 14)
    
    
    + 1
    as generated_number

    from

    
    p as p0
     cross join 
    
    p as p1
     cross join 
    
    p as p2
     cross join 
    
    p as p3
     cross join 
    
    p as p4
     cross join 
    
    p as p5
     cross join 
    
    p as p6
     cross join 
    
    p as p7
     cross join 
    
    p as p8
     cross join 
    
    p as p9
     cross join 
    
    p as p10
     cross join 
    
    p as p11
     cross join 
    
    p as p12
     cross join 
    
    p as p13
     cross join 
    
    p as p14
    
    

    )

    select *
    from unioned
    where generated_number <= 16801
    order by generated_number



),

all_periods as (

    select (
        

    dateadd(
        day,
        (row_number() over (order by 1) - 1),
        cast('1995-01-01' as timestamp)
        )


    ) as date_day
    from rawdata

),

filtered as (

    select *
    from all_periods
    where date_day <= cast('2040-12-31' as timestamp)

)

select * from filtered



)
select
    cast(d.date_day as timestamp) as date_day
from
    date_spine d


),
dates_with_prior_year_dates as (

    select
        cast(d.date_day as date) as date_day,
        cast(

    dateadd(
        year,
        -1,
        d.date_day
        )

 as date) as prior_year_date_day,
        cast(

    dateadd(
        day,
        -364,
        d.date_day
        )

 as date) as prior_year_over_year_date_day
    from
    	base_dates d

)
select
    d.date_day,
    cast(

    dateadd(
        day,
        -1,
        d.date_day
        )

 as date) as prior_date_day,
    cast(

    dateadd(
        day,
        1,
        d.date_day
        )

 as date) as next_date_day,
    d.prior_year_date_day as prior_year_date_day,
    d.prior_year_over_year_date_day,
    case
            when date_part('dayofweek', d.date_day) = 7 then 1
            else date_part('dayofweek', d.date_day) + 1
        end as day_of_week,
    date_part('dayofweekiso', d.date_day) as day_of_week_iso,
    -- long version not implemented on Snowflake so we're doing it manually :/
    case dayname(d.date_day)
        when 'Mon' then 'Monday'
        when 'Tue' then 'Tuesday'
        when 'Wed' then 'Wednesday'
        when 'Thu' then 'Thursday'
        when 'Fri' then 'Friday'
        when 'Sat' then 'Saturday'
        when 'Sun' then 'Sunday'
    end as day_of_week_name,
    dayname(d.date_day) as day_of_week_name_short,
    date_part('day', d.date_day) as day_of_month,
    date_part('dayofyear', d.date_day) as day_of_year,

    
    
    cast(

    dateadd(
        day,
        -1 * (case
            when date_part('dayofweek', d.date_day) = 7 then 1
            else date_part('dayofweek', d.date_day) + 1
        end - 1),
        d.date_day
        )

 as date) as week_start_date,
    cast(

    dateadd(
        day,
        6,
        
    
    cast(

    dateadd(
        day,
        -1 * (case
            when date_part('dayofweek', d.date_day) = 7 then 1
            else date_part('dayofweek', d.date_day) + 1
        end - 1),
        d.date_day
        )

 as date)
        )

 as date) as week_end_date,
    
    
    cast(

    dateadd(
        day,
        -1 * (case
            when date_part('dayofweek', d.prior_year_over_year_date_day) = 7 then 1
            else date_part('dayofweek', d.prior_year_over_year_date_day) + 1
        end - 1),
        d.prior_year_over_year_date_day
        )

 as date) as prior_year_week_start_date,
    cast(

    dateadd(
        day,
        6,
        
    
    cast(

    dateadd(
        day,
        -1 * (case
            when date_part('dayofweek', d.prior_year_over_year_date_day) = 7 then 1
            else date_part('dayofweek', d.prior_year_over_year_date_day) + 1
        end - 1),
        d.prior_year_over_year_date_day
        )

 as date)
        )

 as date) as prior_year_week_end_date,
    cast(date_part('week', d.date_day) as integer) as week_of_year,

    cast(date_trunc('week', d.date_day) as date) as iso_week_start_date,
    cast(

    dateadd(
        day,
        6,
        cast(date_trunc('week', d.date_day) as date)
        )

 as date) as iso_week_end_date,
    cast(date_trunc('week', d.prior_year_over_year_date_day) as date) as prior_year_iso_week_start_date,
    cast(

    dateadd(
        day,
        6,
        cast(date_trunc('week', d.prior_year_over_year_date_day) as date)
        )

 as date) as prior_year_iso_week_end_date,
    cast(date_part('weekiso', d.date_day) as integer) as iso_week_of_year,

    cast(date_part('week', d.prior_year_over_year_date_day) as integer) as prior_year_week_of_year,
    cast(date_part('weekiso', d.prior_year_over_year_date_day) as integer) as prior_year_iso_week_of_year,

    cast(date_part('month', d.date_day) as integer) as month_of_year,
    to_char(d.date_day, 'MMMM')  as month_name,
    to_char(d.date_day, 'MON')  as month_name_short,

    cast(date_trunc('month', d.date_day) as date) as month_start_date,
    cast(cast(
        

    dateadd(
        day,
        -1,
        

    dateadd(
        month,
        1,
        date_trunc('month', d.date_day)
        )


        )


        as date) as date) as month_end_date,

    cast(date_trunc('month', d.prior_year_date_day) as date) as prior_year_month_start_date,
    cast(cast(
        

    dateadd(
        day,
        -1,
        

    dateadd(
        month,
        1,
        date_trunc('month', d.prior_year_date_day)
        )


        )


        as date) as date) as prior_year_month_end_date,

    cast(date_part('quarter', d.date_day) as integer) as quarter_of_year,
    cast(date_trunc('quarter', d.date_day) as date) as quarter_start_date,
    cast(cast(
        

    dateadd(
        day,
        -1,
        

    dateadd(
        quarter,
        1,
        date_trunc('quarter', d.date_day)
        )


        )


        as date) as date) as quarter_end_date,

    cast(date_part('year', d.date_day) as integer) as year_number,
    cast(date_trunc('year', d.date_day) as date) as year_start_date,
    cast(cast(
        

    dateadd(
        day,
        -1,
        

    dateadd(
        year,
        1,
        date_trunc('year', d.date_day)
        )


        )


        as date) as date) as year_end_date
from
    dates_with_prior_year_dates d
order by 1


        );
[0m20:39:18.255863 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 1.247 seconds
[0m20:39:18.277576 [debug] [Thread-1 (]: On model.dbt_capstone_andres.dim_date: Close
[0m20:39:18.497676 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '05fcef48-c2f2-4399-b14e-1e08288904c2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x117f00e90>]}
[0m20:39:18.498370 [info ] [Thread-1 (]: 1 of 9 OK created sql table model andres.dim_date .............................. [[32mSUCCESS 1[0m in 2.35s]
[0m20:39:18.498956 [debug] [Thread-1 (]: Finished running node model.dbt_capstone_andres.dim_date
[0m20:39:18.499358 [debug] [Thread-1 (]: Began running node model.dbt_capstone_andres.my_first_dbt_model
[0m20:39:18.499859 [info ] [Thread-1 (]: 2 of 9 START sql table model andres.my_first_dbt_model ......................... [RUN]
[0m20:39:18.500302 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_capstone_andres.dim_date, now model.dbt_capstone_andres.my_first_dbt_model)
[0m20:39:18.500672 [debug] [Thread-1 (]: Began compiling node model.dbt_capstone_andres.my_first_dbt_model
[0m20:39:18.503626 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_capstone_andres.my_first_dbt_model"
[0m20:39:18.504822 [debug] [Thread-1 (]: Began executing node model.dbt_capstone_andres.my_first_dbt_model
[0m20:39:18.508501 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_capstone_andres.my_first_dbt_model"
[0m20:39:18.509752 [debug] [Thread-1 (]: Using snowflake connection "model.dbt_capstone_andres.my_first_dbt_model"
[0m20:39:18.510235 [debug] [Thread-1 (]: On model.dbt_capstone_andres.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "model.dbt_capstone_andres.my_first_dbt_model"} */
create or replace transient table DATAEXPERT_STUDENT.andres.my_first_dbt_model
         as
        (/*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
        );
[0m20:39:18.510631 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m20:39:19.748491 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 1.238 seconds
[0m20:39:19.750847 [debug] [Thread-1 (]: On model.dbt_capstone_andres.my_first_dbt_model: Close
[0m20:39:19.960837 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '05fcef48-c2f2-4399-b14e-1e08288904c2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x117f01e90>]}
[0m20:39:19.961891 [info ] [Thread-1 (]: 2 of 9 OK created sql table model andres.my_first_dbt_model .................... [[32mSUCCESS 1[0m in 1.46s]
[0m20:39:19.962779 [debug] [Thread-1 (]: Finished running node model.dbt_capstone_andres.my_first_dbt_model
[0m20:39:19.963311 [debug] [Thread-1 (]: Began running node model.dbt_capstone_andres.stg_daily_bars
[0m20:39:19.964126 [info ] [Thread-1 (]: 3 of 9 START sql view model andres.stg_daily_bars .............................. [RUN]
[0m20:39:19.964728 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_capstone_andres.my_first_dbt_model, now model.dbt_capstone_andres.stg_daily_bars)
[0m20:39:19.965228 [debug] [Thread-1 (]: Began compiling node model.dbt_capstone_andres.stg_daily_bars
[0m20:39:19.971029 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_capstone_andres.stg_daily_bars"
[0m20:39:19.972040 [debug] [Thread-1 (]: Began executing node model.dbt_capstone_andres.stg_daily_bars
[0m20:39:19.998928 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_capstone_andres.stg_daily_bars"
[0m20:39:20.001273 [debug] [Thread-1 (]: Using snowflake connection "model.dbt_capstone_andres.stg_daily_bars"
[0m20:39:20.002320 [debug] [Thread-1 (]: On model.dbt_capstone_andres.stg_daily_bars: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "model.dbt_capstone_andres.stg_daily_bars"} */
create or replace   view DATAEXPERT_STUDENT.andres.stg_daily_bars
  
   as (
    with source as (

    select * from dataexpert_student.andres.daily_bars

),

renamed as (

    select
        TO_DATE(TO_TIMESTAMP(timestamp / 1000)) as date,
        ticker as ticker,
        v/1000000  as trading_volume_in_millions,
        n/1000000  as number_of_trades_in_millions,
        vw as volume_weighted_avg_price,
        o  as open_price,
        c  as close_price,
        h  as high_price,
        l  as low_price
    from source

)

select * from renamed
  );
[0m20:39:20.002857 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m20:39:20.873660 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 0.871 seconds
[0m20:39:20.877077 [debug] [Thread-1 (]: On model.dbt_capstone_andres.stg_daily_bars: Close
[0m20:39:21.083352 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '05fcef48-c2f2-4399-b14e-1e08288904c2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11c524e90>]}
[0m20:39:21.084380 [info ] [Thread-1 (]: 3 of 9 OK created sql view model andres.stg_daily_bars ......................... [[32mSUCCESS 1[0m in 1.12s]
[0m20:39:21.085220 [debug] [Thread-1 (]: Finished running node model.dbt_capstone_andres.stg_daily_bars
[0m20:39:21.085760 [debug] [Thread-1 (]: Began running node model.dbt_capstone_andres.stg_treasury_revenue_collections
[0m20:39:21.086435 [info ] [Thread-1 (]: 4 of 9 START sql view model andres.stg_treasury_revenue_collections ............ [RUN]
[0m20:39:21.087042 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_capstone_andres.stg_daily_bars, now model.dbt_capstone_andres.stg_treasury_revenue_collections)
[0m20:39:21.087561 [debug] [Thread-1 (]: Began compiling node model.dbt_capstone_andres.stg_treasury_revenue_collections
[0m20:39:21.091695 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_capstone_andres.stg_treasury_revenue_collections"
[0m20:39:21.092639 [debug] [Thread-1 (]: Began executing node model.dbt_capstone_andres.stg_treasury_revenue_collections
[0m20:39:21.097322 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_capstone_andres.stg_treasury_revenue_collections"
[0m20:39:21.098883 [debug] [Thread-1 (]: Using snowflake connection "model.dbt_capstone_andres.stg_treasury_revenue_collections"
[0m20:39:21.099529 [debug] [Thread-1 (]: On model.dbt_capstone_andres.stg_treasury_revenue_collections: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "model.dbt_capstone_andres.stg_treasury_revenue_collections"} */
create or replace   view DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections
  
   as (
    with source as (

    select * from dataexpert_student.andres.fiscal_data

),

renamed as (

    select
        record_date as collection_date,
        channel_type_desc as channel_type,
        tax_category_desc as tax_category,
        net_collections_amt/1000000 as amount_in_millions

    from source

)

select * from renamed
  );
[0m20:39:21.100040 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m20:39:22.000820 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 0.901 seconds
[0m20:39:22.004253 [debug] [Thread-1 (]: On model.dbt_capstone_andres.stg_treasury_revenue_collections: Close
[0m20:39:22.233559 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '05fcef48-c2f2-4399-b14e-1e08288904c2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11c4c5810>]}
[0m20:39:22.234641 [info ] [Thread-1 (]: 4 of 9 OK created sql view model andres.stg_treasury_revenue_collections ....... [[32mSUCCESS 1[0m in 1.15s]
[0m20:39:22.235574 [debug] [Thread-1 (]: Finished running node model.dbt_capstone_andres.stg_treasury_revenue_collections
[0m20:39:22.236258 [debug] [Thread-1 (]: Began running node test.dbt_capstone_andres.not_null_my_first_dbt_model_id.5fb22c2710
[0m20:39:22.236935 [info ] [Thread-1 (]: 5 of 9 START test not_null_my_first_dbt_model_id ............................... [RUN]
[0m20:39:22.237563 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_capstone_andres.stg_treasury_revenue_collections, now test.dbt_capstone_andres.not_null_my_first_dbt_model_id.5fb22c2710)
[0m20:39:22.238084 [debug] [Thread-1 (]: Began compiling node test.dbt_capstone_andres.not_null_my_first_dbt_model_id.5fb22c2710
[0m20:39:22.253534 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_capstone_andres.not_null_my_first_dbt_model_id.5fb22c2710"
[0m20:39:22.254382 [debug] [Thread-1 (]: Began executing node test.dbt_capstone_andres.not_null_my_first_dbt_model_id.5fb22c2710
[0m20:39:22.276603 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_capstone_andres.not_null_my_first_dbt_model_id.5fb22c2710"
[0m20:39:22.277959 [debug] [Thread-1 (]: Using snowflake connection "test.dbt_capstone_andres.not_null_my_first_dbt_model_id.5fb22c2710"
[0m20:39:22.278462 [debug] [Thread-1 (]: On test.dbt_capstone_andres.not_null_my_first_dbt_model_id.5fb22c2710: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "test.dbt_capstone_andres.not_null_my_first_dbt_model_id.5fb22c2710"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from DATAEXPERT_STUDENT.andres.my_first_dbt_model
where id is null



      
    ) dbt_internal_test
[0m20:39:22.278934 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m20:39:22.995482 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 0.717 seconds
[0m20:39:22.998836 [debug] [Thread-1 (]: On test.dbt_capstone_andres.not_null_my_first_dbt_model_id.5fb22c2710: Close
[0m20:39:23.213614 [error] [Thread-1 (]: 5 of 9 FAIL 1 not_null_my_first_dbt_model_id ................................... [[31mFAIL 1[0m in 0.98s]
[0m20:39:23.214366 [debug] [Thread-1 (]: Finished running node test.dbt_capstone_andres.not_null_my_first_dbt_model_id.5fb22c2710
[0m20:39:23.214826 [debug] [Thread-1 (]: Began running node test.dbt_capstone_andres.unique_my_first_dbt_model_id.16e066b321
[0m20:39:23.215350 [debug] [Thread-4 (]: Marking all children of 'test.dbt_capstone_andres.not_null_my_first_dbt_model_id.5fb22c2710' to be skipped because of status 'fail'.  Reason: Got 1 result, configured to fail if != 0.
[0m20:39:23.215808 [info ] [Thread-1 (]: 6 of 9 START test unique_my_first_dbt_model_id ................................. [RUN]
[0m20:39:23.216654 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_capstone_andres.not_null_my_first_dbt_model_id.5fb22c2710, now test.dbt_capstone_andres.unique_my_first_dbt_model_id.16e066b321)
[0m20:39:23.217077 [debug] [Thread-1 (]: Began compiling node test.dbt_capstone_andres.unique_my_first_dbt_model_id.16e066b321
[0m20:39:23.224579 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_capstone_andres.unique_my_first_dbt_model_id.16e066b321"
[0m20:39:23.225473 [debug] [Thread-1 (]: Began executing node test.dbt_capstone_andres.unique_my_first_dbt_model_id.16e066b321
[0m20:39:23.229117 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_capstone_andres.unique_my_first_dbt_model_id.16e066b321"
[0m20:39:23.230358 [debug] [Thread-1 (]: Using snowflake connection "test.dbt_capstone_andres.unique_my_first_dbt_model_id.16e066b321"
[0m20:39:23.230936 [debug] [Thread-1 (]: On test.dbt_capstone_andres.unique_my_first_dbt_model_id.16e066b321: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "test.dbt_capstone_andres.unique_my_first_dbt_model_id.16e066b321"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from DATAEXPERT_STUDENT.andres.my_first_dbt_model
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
[0m20:39:23.231430 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m20:39:23.936277 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 0.705 seconds
[0m20:39:23.938315 [debug] [Thread-1 (]: On test.dbt_capstone_andres.unique_my_first_dbt_model_id.16e066b321: Close
[0m20:39:24.151171 [info ] [Thread-1 (]: 6 of 9 PASS unique_my_first_dbt_model_id ....................................... [[32mPASS[0m in 0.93s]
[0m20:39:24.151850 [debug] [Thread-1 (]: Finished running node test.dbt_capstone_andres.unique_my_first_dbt_model_id.16e066b321
[0m20:39:24.152512 [debug] [Thread-1 (]: Began running node model.dbt_capstone_andres.my_second_dbt_model
[0m20:39:24.153041 [info ] [Thread-1 (]: 7 of 9 SKIP relation andres.my_second_dbt_model ................................ [[33mSKIP[0m]
[0m20:39:24.153501 [debug] [Thread-1 (]: Finished running node model.dbt_capstone_andres.my_second_dbt_model
[0m20:39:24.153880 [debug] [Thread-4 (]: Marking all children of 'model.dbt_capstone_andres.my_second_dbt_model' to be skipped because of status 'skipped'. 
[0m20:39:24.154526 [debug] [Thread-1 (]: Began running node test.dbt_capstone_andres.not_null_my_second_dbt_model_id.151b76d778
[0m20:39:24.154969 [info ] [Thread-1 (]: 8 of 9 SKIP test not_null_my_second_dbt_model_id ............................... [[33mSKIP[0m]
[0m20:39:24.155392 [debug] [Thread-1 (]: Finished running node test.dbt_capstone_andres.not_null_my_second_dbt_model_id.151b76d778
[0m20:39:24.155777 [debug] [Thread-1 (]: Began running node test.dbt_capstone_andres.unique_my_second_dbt_model_id.57a0f8c493
[0m20:39:24.156082 [debug] [Thread-4 (]: Marking all children of 'test.dbt_capstone_andres.not_null_my_second_dbt_model_id.151b76d778' to be skipped because of status 'skipped'. 
[0m20:39:24.156475 [info ] [Thread-1 (]: 9 of 9 SKIP test unique_my_second_dbt_model_id ................................. [[33mSKIP[0m]
[0m20:39:24.156978 [debug] [Thread-1 (]: Finished running node test.dbt_capstone_andres.unique_my_second_dbt_model_id.57a0f8c493
[0m20:39:24.157496 [debug] [Thread-4 (]: Marking all children of 'test.dbt_capstone_andres.unique_my_second_dbt_model_id.57a0f8c493' to be skipped because of status 'skipped'. 
[0m20:39:24.158581 [debug] [MainThread]: Connection 'master' was properly closed.
[0m20:39:24.158906 [debug] [MainThread]: Connection 'test.dbt_capstone_andres.unique_my_first_dbt_model_id.16e066b321' was properly closed.
[0m20:39:24.159298 [info ] [MainThread]: 
[0m20:39:24.159650 [info ] [MainThread]: Finished running 2 table models, 4 data tests, 3 view models in 0 hours 0 minutes and 9.99 seconds (9.99s).
[0m20:39:24.161136 [debug] [MainThread]: Command end result
[0m20:39:24.210270 [info ] [MainThread]: 
[0m20:39:24.210789 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successs, and 0 warnings:[0m
[0m20:39:24.211168 [info ] [MainThread]: 
[0m20:39:24.211605 [error] [MainThread]: [31mFailure in test not_null_my_first_dbt_model_id (models/example/schema.yml)[0m
[0m20:39:24.212029 [error] [MainThread]:   Got 1 result, configured to fail if != 0
[0m20:39:24.212378 [info ] [MainThread]: 
[0m20:39:24.212789 [info ] [MainThread]:   compiled code at target/compiled/dbt_capstone_andres/models/example/schema.yml/not_null_my_first_dbt_model_id.sql
[0m20:39:24.213188 [info ] [MainThread]: 
[0m20:39:24.213576 [info ] [MainThread]: Done. PASS=5 WARN=0 ERROR=1 SKIP=3 TOTAL=9
[0m20:39:24.216172 [debug] [MainThread]: Resource report: {"command_name": "build", "command_wall_clock_time": 12.8840065, "process_user_time": 3.877187, "process_kernel_time": 0.745992, "process_mem_max_rss": "198246400", "command_success": false, "process_in_blocks": "0", "process_out_blocks": "0"}
[0m20:39:24.217045 [debug] [MainThread]: Command `dbt build` failed at 20:39:24.216927 after 12.88 seconds
[0m20:39:24.217711 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111c0cb90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111b0ad50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111bfbe10>]}
[0m20:39:24.218133 [debug] [MainThread]: Flushing usage events
[0m20:39:24.387576 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m20:43:32.530614 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e79fb50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e5b6c90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e80f310>]}


============================== 20:43:32.535868 | 4fb2940c-8f79-412f-9b90-637fa616a812 ==============================
[0m20:43:32.535868 [info ] [MainThread]: Running with dbt=1.9.0-b3
[0m20:43:32.536547 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'fail_fast': 'False', 'version_check': 'True', 'log_path': 'logs', 'warn_error': 'None', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'log_format': 'default', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt build', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m20:43:34.217202 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '4fb2940c-8f79-412f-9b90-637fa616a812', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11348db10>]}
[0m20:43:34.278891 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '4fb2940c-8f79-412f-9b90-637fa616a812', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d3c0e10>]}
[0m20:43:34.280337 [info ] [MainThread]: Registered adapter: snowflake=1.8.4
[0m20:43:34.473457 [debug] [MainThread]: checksum: 5f4fde221ca8627174a0341efc73099aaec05b8546408ceed6f59b1ec0755115, vars: {}, profile: , target: , version: 1.9.0b3
[0m20:43:34.684771 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 2 files changed.
[0m20:43:34.685410 [debug] [MainThread]: Partial parsing: updated file: dbt_capstone_andres://models/staging/stg_daily_bars.sql
[0m20:43:34.685801 [debug] [MainThread]: Partial parsing: updated file: dbt_capstone_andres://models/staging/stg_treasury_revenue_collections.sql
[0m20:43:34.918056 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.andres_capstone_dbt.example
[0m20:43:35.009488 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '4fb2940c-8f79-412f-9b90-637fa616a812', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x115152a50>]}
[0m20:43:35.164731 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '4fb2940c-8f79-412f-9b90-637fa616a812', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114a4f690>]}
[0m20:43:35.165342 [info ] [MainThread]: Found 6 models, 4 data tests, 2 sources, 862 macros
[0m20:43:35.167866 [info ] [MainThread]: 
[0m20:43:35.168282 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m20:43:35.168644 [info ] [MainThread]: 
[0m20:43:35.169174 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m20:43:35.173997 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_DATAEXPERT_STUDENT'
[0m20:43:35.188730 [debug] [ThreadPool]: Using snowflake connection "list_DATAEXPERT_STUDENT"
[0m20:43:35.189416 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "connection_name": "list_DATAEXPERT_STUDENT"} */
show terse schemas in database DATAEXPERT_STUDENT
    limit 10000
[0m20:43:35.189909 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:43:36.046350 [debug] [ThreadPool]: SQL status: SUCCESS 68 in 0.856 seconds
[0m20:43:36.048875 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT: Close
[0m20:43:36.252100 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_DATAEXPERT_STUDENT, now list_DATAEXPERT_STUDENT_andres)
[0m20:43:36.262216 [debug] [ThreadPool]: Using snowflake connection "list_DATAEXPERT_STUDENT_andres"
[0m20:43:36.262647 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "connection_name": "list_DATAEXPERT_STUDENT_andres"} */
show objects in DATAEXPERT_STUDENT.andres limit 10000
[0m20:43:36.262993 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m20:43:36.931068 [debug] [ThreadPool]: SQL status: SUCCESS 22 in 0.668 seconds
[0m20:43:36.934728 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: Close
[0m20:43:37.158936 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4fb2940c-8f79-412f-9b90-637fa616a812', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1127c5910>]}
[0m20:43:37.163351 [debug] [Thread-1 (]: Began running node model.dbt_capstone_andres.dim_date
[0m20:43:37.164168 [info ] [Thread-1 (]: 1 of 9 START sql table model andres.dim_date ................................... [RUN]
[0m20:43:37.164801 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_DATAEXPERT_STUDENT_andres, now model.dbt_capstone_andres.dim_date)
[0m20:43:37.165299 [debug] [Thread-1 (]: Began compiling node model.dbt_capstone_andres.dim_date
[0m20:43:37.213470 [debug] [Thread-1 (]: Using snowflake connection "model.dbt_capstone_andres.dim_date"
[0m20:43:37.214204 [debug] [Thread-1 (]: On model.dbt_capstone_andres.dim_date: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "model.dbt_capstone_andres.dim_date"} */
select datediff(
        day,
        cast('1995-01-01' as timestamp),
        cast('2040-12-31' as timestamp)
        )
[0m20:43:37.214791 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m20:43:37.844147 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 0.629 seconds
[0m20:43:37.924509 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_capstone_andres.dim_date"
[0m20:43:37.926756 [debug] [Thread-1 (]: Began executing node model.dbt_capstone_andres.dim_date
[0m20:43:37.956968 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_capstone_andres.dim_date"
[0m20:43:37.965338 [debug] [Thread-1 (]: Using snowflake connection "model.dbt_capstone_andres.dim_date"
[0m20:43:37.966145 [debug] [Thread-1 (]: On model.dbt_capstone_andres.dim_date: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "model.dbt_capstone_andres.dim_date"} */
create or replace transient table DATAEXPERT_STUDENT.andres.dim_date
         as
        (

    
with base_dates as (
    
    with date_spine as
(

    





with rawdata as (

    

    

    with p as (
        select 0 as generated_number union all select 1
    ), unioned as (

    select

    
    p0.generated_number * power(2, 0)
     + 
    
    p1.generated_number * power(2, 1)
     + 
    
    p2.generated_number * power(2, 2)
     + 
    
    p3.generated_number * power(2, 3)
     + 
    
    p4.generated_number * power(2, 4)
     + 
    
    p5.generated_number * power(2, 5)
     + 
    
    p6.generated_number * power(2, 6)
     + 
    
    p7.generated_number * power(2, 7)
     + 
    
    p8.generated_number * power(2, 8)
     + 
    
    p9.generated_number * power(2, 9)
     + 
    
    p10.generated_number * power(2, 10)
     + 
    
    p11.generated_number * power(2, 11)
     + 
    
    p12.generated_number * power(2, 12)
     + 
    
    p13.generated_number * power(2, 13)
     + 
    
    p14.generated_number * power(2, 14)
    
    
    + 1
    as generated_number

    from

    
    p as p0
     cross join 
    
    p as p1
     cross join 
    
    p as p2
     cross join 
    
    p as p3
     cross join 
    
    p as p4
     cross join 
    
    p as p5
     cross join 
    
    p as p6
     cross join 
    
    p as p7
     cross join 
    
    p as p8
     cross join 
    
    p as p9
     cross join 
    
    p as p10
     cross join 
    
    p as p11
     cross join 
    
    p as p12
     cross join 
    
    p as p13
     cross join 
    
    p as p14
    
    

    )

    select *
    from unioned
    where generated_number <= 16801
    order by generated_number



),

all_periods as (

    select (
        

    dateadd(
        day,
        (row_number() over (order by 1) - 1),
        cast('1995-01-01' as timestamp)
        )


    ) as date_day
    from rawdata

),

filtered as (

    select *
    from all_periods
    where date_day <= cast('2040-12-31' as timestamp)

)

select * from filtered



)
select
    cast(d.date_day as timestamp) as date_day
from
    date_spine d


),
dates_with_prior_year_dates as (

    select
        cast(d.date_day as date) as date_day,
        cast(

    dateadd(
        year,
        -1,
        d.date_day
        )

 as date) as prior_year_date_day,
        cast(

    dateadd(
        day,
        -364,
        d.date_day
        )

 as date) as prior_year_over_year_date_day
    from
    	base_dates d

)
select
    d.date_day,
    cast(

    dateadd(
        day,
        -1,
        d.date_day
        )

 as date) as prior_date_day,
    cast(

    dateadd(
        day,
        1,
        d.date_day
        )

 as date) as next_date_day,
    d.prior_year_date_day as prior_year_date_day,
    d.prior_year_over_year_date_day,
    case
            when date_part('dayofweek', d.date_day) = 7 then 1
            else date_part('dayofweek', d.date_day) + 1
        end as day_of_week,
    date_part('dayofweekiso', d.date_day) as day_of_week_iso,
    -- long version not implemented on Snowflake so we're doing it manually :/
    case dayname(d.date_day)
        when 'Mon' then 'Monday'
        when 'Tue' then 'Tuesday'
        when 'Wed' then 'Wednesday'
        when 'Thu' then 'Thursday'
        when 'Fri' then 'Friday'
        when 'Sat' then 'Saturday'
        when 'Sun' then 'Sunday'
    end as day_of_week_name,
    dayname(d.date_day) as day_of_week_name_short,
    date_part('day', d.date_day) as day_of_month,
    date_part('dayofyear', d.date_day) as day_of_year,

    
    
    cast(

    dateadd(
        day,
        -1 * (case
            when date_part('dayofweek', d.date_day) = 7 then 1
            else date_part('dayofweek', d.date_day) + 1
        end - 1),
        d.date_day
        )

 as date) as week_start_date,
    cast(

    dateadd(
        day,
        6,
        
    
    cast(

    dateadd(
        day,
        -1 * (case
            when date_part('dayofweek', d.date_day) = 7 then 1
            else date_part('dayofweek', d.date_day) + 1
        end - 1),
        d.date_day
        )

 as date)
        )

 as date) as week_end_date,
    
    
    cast(

    dateadd(
        day,
        -1 * (case
            when date_part('dayofweek', d.prior_year_over_year_date_day) = 7 then 1
            else date_part('dayofweek', d.prior_year_over_year_date_day) + 1
        end - 1),
        d.prior_year_over_year_date_day
        )

 as date) as prior_year_week_start_date,
    cast(

    dateadd(
        day,
        6,
        
    
    cast(

    dateadd(
        day,
        -1 * (case
            when date_part('dayofweek', d.prior_year_over_year_date_day) = 7 then 1
            else date_part('dayofweek', d.prior_year_over_year_date_day) + 1
        end - 1),
        d.prior_year_over_year_date_day
        )

 as date)
        )

 as date) as prior_year_week_end_date,
    cast(date_part('week', d.date_day) as integer) as week_of_year,

    cast(date_trunc('week', d.date_day) as date) as iso_week_start_date,
    cast(

    dateadd(
        day,
        6,
        cast(date_trunc('week', d.date_day) as date)
        )

 as date) as iso_week_end_date,
    cast(date_trunc('week', d.prior_year_over_year_date_day) as date) as prior_year_iso_week_start_date,
    cast(

    dateadd(
        day,
        6,
        cast(date_trunc('week', d.prior_year_over_year_date_day) as date)
        )

 as date) as prior_year_iso_week_end_date,
    cast(date_part('weekiso', d.date_day) as integer) as iso_week_of_year,

    cast(date_part('week', d.prior_year_over_year_date_day) as integer) as prior_year_week_of_year,
    cast(date_part('weekiso', d.prior_year_over_year_date_day) as integer) as prior_year_iso_week_of_year,

    cast(date_part('month', d.date_day) as integer) as month_of_year,
    to_char(d.date_day, 'MMMM')  as month_name,
    to_char(d.date_day, 'MON')  as month_name_short,

    cast(date_trunc('month', d.date_day) as date) as month_start_date,
    cast(cast(
        

    dateadd(
        day,
        -1,
        

    dateadd(
        month,
        1,
        date_trunc('month', d.date_day)
        )


        )


        as date) as date) as month_end_date,

    cast(date_trunc('month', d.prior_year_date_day) as date) as prior_year_month_start_date,
    cast(cast(
        

    dateadd(
        day,
        -1,
        

    dateadd(
        month,
        1,
        date_trunc('month', d.prior_year_date_day)
        )


        )


        as date) as date) as prior_year_month_end_date,

    cast(date_part('quarter', d.date_day) as integer) as quarter_of_year,
    cast(date_trunc('quarter', d.date_day) as date) as quarter_start_date,
    cast(cast(
        

    dateadd(
        day,
        -1,
        

    dateadd(
        quarter,
        1,
        date_trunc('quarter', d.date_day)
        )


        )


        as date) as date) as quarter_end_date,

    cast(date_part('year', d.date_day) as integer) as year_number,
    cast(date_trunc('year', d.date_day) as date) as year_start_date,
    cast(cast(
        

    dateadd(
        day,
        -1,
        

    dateadd(
        year,
        1,
        date_trunc('year', d.date_day)
        )


        )


        as date) as date) as year_end_date
from
    dates_with_prior_year_dates d
order by 1


        );
[0m20:43:39.188642 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 1.222 seconds
[0m20:43:39.211604 [debug] [Thread-1 (]: On model.dbt_capstone_andres.dim_date: Close
[0m20:43:39.434170 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4fb2940c-8f79-412f-9b90-637fa616a812', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c91eb90>]}
[0m20:43:39.435050 [info ] [Thread-1 (]: 1 of 9 OK created sql table model andres.dim_date .............................. [[32mSUCCESS 1[0m in 2.27s]
[0m20:43:39.435900 [debug] [Thread-1 (]: Finished running node model.dbt_capstone_andres.dim_date
[0m20:43:39.436454 [debug] [Thread-1 (]: Began running node model.dbt_capstone_andres.my_first_dbt_model
[0m20:43:39.437138 [info ] [Thread-1 (]: 2 of 9 START sql table model andres.my_first_dbt_model ......................... [RUN]
[0m20:43:39.437760 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_capstone_andres.dim_date, now model.dbt_capstone_andres.my_first_dbt_model)
[0m20:43:39.438293 [debug] [Thread-1 (]: Began compiling node model.dbt_capstone_andres.my_first_dbt_model
[0m20:43:39.442809 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_capstone_andres.my_first_dbt_model"
[0m20:43:39.444292 [debug] [Thread-1 (]: Began executing node model.dbt_capstone_andres.my_first_dbt_model
[0m20:43:39.450432 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_capstone_andres.my_first_dbt_model"
[0m20:43:39.451984 [debug] [Thread-1 (]: Using snowflake connection "model.dbt_capstone_andres.my_first_dbt_model"
[0m20:43:39.452626 [debug] [Thread-1 (]: On model.dbt_capstone_andres.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "model.dbt_capstone_andres.my_first_dbt_model"} */
create or replace transient table DATAEXPERT_STUDENT.andres.my_first_dbt_model
         as
        (/*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
        );
[0m20:43:39.453147 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m20:43:40.676139 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 1.223 seconds
[0m20:43:40.679544 [debug] [Thread-1 (]: On model.dbt_capstone_andres.my_first_dbt_model: Close
[0m20:43:40.887613 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4fb2940c-8f79-412f-9b90-637fa616a812', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114bf4890>]}
[0m20:43:40.888659 [info ] [Thread-1 (]: 2 of 9 OK created sql table model andres.my_first_dbt_model .................... [[32mSUCCESS 1[0m in 1.45s]
[0m20:43:40.889541 [debug] [Thread-1 (]: Finished running node model.dbt_capstone_andres.my_first_dbt_model
[0m20:43:40.890180 [debug] [Thread-1 (]: Began running node model.dbt_capstone_andres.stg_daily_bars
[0m20:43:40.891002 [info ] [Thread-1 (]: 3 of 9 START sql view model andres.stg_daily_bars .............................. [RUN]
[0m20:43:40.891595 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_capstone_andres.my_first_dbt_model, now model.dbt_capstone_andres.stg_daily_bars)
[0m20:43:40.892086 [debug] [Thread-1 (]: Began compiling node model.dbt_capstone_andres.stg_daily_bars
[0m20:43:40.897721 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_capstone_andres.stg_daily_bars"
[0m20:43:40.898663 [debug] [Thread-1 (]: Began executing node model.dbt_capstone_andres.stg_daily_bars
[0m20:43:40.927000 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_capstone_andres.stg_daily_bars"
[0m20:43:40.928728 [debug] [Thread-1 (]: Using snowflake connection "model.dbt_capstone_andres.stg_daily_bars"
[0m20:43:40.929250 [debug] [Thread-1 (]: On model.dbt_capstone_andres.stg_daily_bars: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "model.dbt_capstone_andres.stg_daily_bars"} */
create or replace   view DATAEXPERT_STUDENT.andres.stg_daily_bars
  
   as (
    with source as (

    select * from dataexpert_student.andres.daily_bars

),

renamed as (

    select
        TO_DATE(TO_TIMESTAMP(timestamp / 1000)) as date,
        ticker as ticker,
        v  as trading_volume,
        n  as number_of_trades,
        vw as volume_weighted_avg_price,
        o  as open_price,
        c  as close_price,
        h  as high_price,
        l  as low_price
    from source

)

select * from renamed
  );
[0m20:43:40.929750 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m20:43:42.006345 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 1.077 seconds
[0m20:43:42.008794 [debug] [Thread-1 (]: On model.dbt_capstone_andres.stg_daily_bars: Close
[0m20:43:42.213931 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4fb2940c-8f79-412f-9b90-637fa616a812', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11f01c410>]}
[0m20:43:42.214685 [info ] [Thread-1 (]: 3 of 9 OK created sql view model andres.stg_daily_bars ......................... [[32mSUCCESS 1[0m in 1.32s]
[0m20:43:42.215309 [debug] [Thread-1 (]: Finished running node model.dbt_capstone_andres.stg_daily_bars
[0m20:43:42.215750 [debug] [Thread-1 (]: Began running node model.dbt_capstone_andres.stg_treasury_revenue_collections
[0m20:43:42.216307 [info ] [Thread-1 (]: 4 of 9 START sql view model andres.stg_treasury_revenue_collections ............ [RUN]
[0m20:43:42.216810 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_capstone_andres.stg_daily_bars, now model.dbt_capstone_andres.stg_treasury_revenue_collections)
[0m20:43:42.217221 [debug] [Thread-1 (]: Began compiling node model.dbt_capstone_andres.stg_treasury_revenue_collections
[0m20:43:42.220584 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_capstone_andres.stg_treasury_revenue_collections"
[0m20:43:42.221594 [debug] [Thread-1 (]: Began executing node model.dbt_capstone_andres.stg_treasury_revenue_collections
[0m20:43:42.225773 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_capstone_andres.stg_treasury_revenue_collections"
[0m20:43:42.227161 [debug] [Thread-1 (]: Using snowflake connection "model.dbt_capstone_andres.stg_treasury_revenue_collections"
[0m20:43:42.227642 [debug] [Thread-1 (]: On model.dbt_capstone_andres.stg_treasury_revenue_collections: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "model.dbt_capstone_andres.stg_treasury_revenue_collections"} */
create or replace   view DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections
  
   as (
    with source as (

    select * from dataexpert_student.andres.fiscal_data

),

renamed as (

    select
        record_date as collection_date,
        channel_type_desc as channel_type,
        tax_category_desc as tax_category,
        net_collections_amt as amount

    from source

)

select * from renamed
  );
[0m20:43:42.228075 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m20:43:43.053970 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 0.826 seconds
[0m20:43:43.057529 [debug] [Thread-1 (]: On model.dbt_capstone_andres.stg_treasury_revenue_collections: Close
[0m20:43:43.263193 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4fb2940c-8f79-412f-9b90-637fa616a812', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11f159990>]}
[0m20:43:43.264215 [info ] [Thread-1 (]: 4 of 9 OK created sql view model andres.stg_treasury_revenue_collections ....... [[32mSUCCESS 1[0m in 1.05s]
[0m20:43:43.265111 [debug] [Thread-1 (]: Finished running node model.dbt_capstone_andres.stg_treasury_revenue_collections
[0m20:43:43.265636 [debug] [Thread-1 (]: Began running node test.dbt_capstone_andres.not_null_my_first_dbt_model_id.5fb22c2710
[0m20:43:43.266194 [info ] [Thread-1 (]: 5 of 9 START test not_null_my_first_dbt_model_id ............................... [RUN]
[0m20:43:43.266773 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_capstone_andres.stg_treasury_revenue_collections, now test.dbt_capstone_andres.not_null_my_first_dbt_model_id.5fb22c2710)
[0m20:43:43.267258 [debug] [Thread-1 (]: Began compiling node test.dbt_capstone_andres.not_null_my_first_dbt_model_id.5fb22c2710
[0m20:43:43.281305 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_capstone_andres.not_null_my_first_dbt_model_id.5fb22c2710"
[0m20:43:43.282163 [debug] [Thread-1 (]: Began executing node test.dbt_capstone_andres.not_null_my_first_dbt_model_id.5fb22c2710
[0m20:43:43.302526 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_capstone_andres.not_null_my_first_dbt_model_id.5fb22c2710"
[0m20:43:43.303763 [debug] [Thread-1 (]: Using snowflake connection "test.dbt_capstone_andres.not_null_my_first_dbt_model_id.5fb22c2710"
[0m20:43:43.304245 [debug] [Thread-1 (]: On test.dbt_capstone_andres.not_null_my_first_dbt_model_id.5fb22c2710: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "test.dbt_capstone_andres.not_null_my_first_dbt_model_id.5fb22c2710"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from DATAEXPERT_STUDENT.andres.my_first_dbt_model
where id is null



      
    ) dbt_internal_test
[0m20:43:43.304687 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m20:43:43.993835 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 0.689 seconds
[0m20:43:43.997209 [debug] [Thread-1 (]: On test.dbt_capstone_andres.not_null_my_first_dbt_model_id.5fb22c2710: Close
[0m20:43:44.204371 [error] [Thread-1 (]: 5 of 9 FAIL 1 not_null_my_first_dbt_model_id ................................... [[31mFAIL 1[0m in 0.94s]
[0m20:43:44.205033 [debug] [Thread-1 (]: Finished running node test.dbt_capstone_andres.not_null_my_first_dbt_model_id.5fb22c2710
[0m20:43:44.205435 [debug] [Thread-1 (]: Began running node test.dbt_capstone_andres.unique_my_first_dbt_model_id.16e066b321
[0m20:43:44.205912 [debug] [Thread-4 (]: Marking all children of 'test.dbt_capstone_andres.not_null_my_first_dbt_model_id.5fb22c2710' to be skipped because of status 'fail'.  Reason: Got 1 result, configured to fail if != 0.
[0m20:43:44.206382 [info ] [Thread-1 (]: 6 of 9 START test unique_my_first_dbt_model_id ................................. [RUN]
[0m20:43:44.207191 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_capstone_andres.not_null_my_first_dbt_model_id.5fb22c2710, now test.dbt_capstone_andres.unique_my_first_dbt_model_id.16e066b321)
[0m20:43:44.207589 [debug] [Thread-1 (]: Began compiling node test.dbt_capstone_andres.unique_my_first_dbt_model_id.16e066b321
[0m20:43:44.214975 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_capstone_andres.unique_my_first_dbt_model_id.16e066b321"
[0m20:43:44.215957 [debug] [Thread-1 (]: Began executing node test.dbt_capstone_andres.unique_my_first_dbt_model_id.16e066b321
[0m20:43:44.219709 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_capstone_andres.unique_my_first_dbt_model_id.16e066b321"
[0m20:43:44.221513 [debug] [Thread-1 (]: Using snowflake connection "test.dbt_capstone_andres.unique_my_first_dbt_model_id.16e066b321"
[0m20:43:44.222164 [debug] [Thread-1 (]: On test.dbt_capstone_andres.unique_my_first_dbt_model_id.16e066b321: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "test.dbt_capstone_andres.unique_my_first_dbt_model_id.16e066b321"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from DATAEXPERT_STUDENT.andres.my_first_dbt_model
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
[0m20:43:44.222689 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m20:43:44.959294 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 0.736 seconds
[0m20:43:44.962233 [debug] [Thread-1 (]: On test.dbt_capstone_andres.unique_my_first_dbt_model_id.16e066b321: Close
[0m20:43:45.184655 [info ] [Thread-1 (]: 6 of 9 PASS unique_my_first_dbt_model_id ....................................... [[32mPASS[0m in 0.98s]
[0m20:43:45.185627 [debug] [Thread-1 (]: Finished running node test.dbt_capstone_andres.unique_my_first_dbt_model_id.16e066b321
[0m20:43:45.186521 [debug] [Thread-1 (]: Began running node model.dbt_capstone_andres.my_second_dbt_model
[0m20:43:45.187162 [info ] [Thread-1 (]: 7 of 9 SKIP relation andres.my_second_dbt_model ................................ [[33mSKIP[0m]
[0m20:43:45.187849 [debug] [Thread-1 (]: Finished running node model.dbt_capstone_andres.my_second_dbt_model
[0m20:43:45.188416 [debug] [Thread-4 (]: Marking all children of 'model.dbt_capstone_andres.my_second_dbt_model' to be skipped because of status 'skipped'. 
[0m20:43:45.189293 [debug] [Thread-1 (]: Began running node test.dbt_capstone_andres.not_null_my_second_dbt_model_id.151b76d778
[0m20:43:45.189887 [info ] [Thread-1 (]: 8 of 9 SKIP test not_null_my_second_dbt_model_id ............................... [[33mSKIP[0m]
[0m20:43:45.190465 [debug] [Thread-1 (]: Finished running node test.dbt_capstone_andres.not_null_my_second_dbt_model_id.151b76d778
[0m20:43:45.191190 [debug] [Thread-1 (]: Began running node test.dbt_capstone_andres.unique_my_second_dbt_model_id.57a0f8c493
[0m20:43:45.191678 [debug] [Thread-4 (]: Marking all children of 'test.dbt_capstone_andres.not_null_my_second_dbt_model_id.151b76d778' to be skipped because of status 'skipped'. 
[0m20:43:45.192191 [info ] [Thread-1 (]: 9 of 9 SKIP test unique_my_second_dbt_model_id ................................. [[33mSKIP[0m]
[0m20:43:45.192920 [debug] [Thread-1 (]: Finished running node test.dbt_capstone_andres.unique_my_second_dbt_model_id.57a0f8c493
[0m20:43:45.193480 [debug] [Thread-4 (]: Marking all children of 'test.dbt_capstone_andres.unique_my_second_dbt_model_id.57a0f8c493' to be skipped because of status 'skipped'. 
[0m20:43:45.194948 [debug] [MainThread]: Connection 'master' was properly closed.
[0m20:43:45.195448 [debug] [MainThread]: Connection 'test.dbt_capstone_andres.unique_my_first_dbt_model_id.16e066b321' was properly closed.
[0m20:43:45.196000 [info ] [MainThread]: 
[0m20:43:45.196541 [info ] [MainThread]: Finished running 2 table models, 4 data tests, 3 view models in 0 hours 0 minutes and 10.03 seconds (10.03s).
[0m20:43:45.198599 [debug] [MainThread]: Command end result
[0m20:43:45.255993 [info ] [MainThread]: 
[0m20:43:45.256570 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successs, and 0 warnings:[0m
[0m20:43:45.256976 [info ] [MainThread]: 
[0m20:43:45.257427 [error] [MainThread]: [31mFailure in test not_null_my_first_dbt_model_id (models/example/schema.yml)[0m
[0m20:43:45.257862 [error] [MainThread]:   Got 1 result, configured to fail if != 0
[0m20:43:45.258221 [info ] [MainThread]: 
[0m20:43:45.258646 [info ] [MainThread]:   compiled code at target/compiled/dbt_capstone_andres/models/example/schema.yml/not_null_my_first_dbt_model_id.sql
[0m20:43:45.259044 [info ] [MainThread]: 
[0m20:43:45.259424 [info ] [MainThread]: Done. PASS=5 WARN=0 ERROR=1 SKIP=3 TOTAL=9
[0m20:43:45.262399 [debug] [MainThread]: Resource report: {"command_name": "build", "command_wall_clock_time": 12.806943, "process_user_time": 3.857544, "process_kernel_time": 0.707393, "process_mem_max_rss": "198045696", "command_success": false, "process_in_blocks": "0", "process_out_blocks": "0"}
[0m20:43:45.263197 [debug] [MainThread]: Command `dbt build` failed at 20:43:45.263037 after 12.81 seconds
[0m20:43:45.263805 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e7a4a90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e823590>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114bfcb10>]}
[0m20:43:45.264288 [debug] [MainThread]: Flushing usage events
[0m20:43:45.446469 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m20:45:18.920189 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b2287d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c631bd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d2657d0>]}


============================== 20:45:18.926343 | f016d3c3-2051-4c6c-bb2c-2dd6c62ac878 ==============================
[0m20:45:18.926343 [info ] [MainThread]: Running with dbt=1.9.0-b3
[0m20:45:18.927163 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'profiles_dir': '.', 'log_path': 'logs', 'fail_fast': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'target_path': 'None', 'invocation_command': 'dbt run -s stg_treasury_revenue_collections', 'send_anonymous_usage_stats': 'True'}
[0m20:45:20.548920 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'f016d3c3-2051-4c6c-bb2c-2dd6c62ac878', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x113541e10>]}
[0m20:45:20.602560 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'f016d3c3-2051-4c6c-bb2c-2dd6c62ac878', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10be54e10>]}
[0m20:45:20.604192 [info ] [MainThread]: Registered adapter: snowflake=1.8.4
[0m20:45:20.806626 [debug] [MainThread]: checksum: 5f4fde221ca8627174a0341efc73099aaec05b8546408ceed6f59b1ec0755115, vars: {}, profile: , target: , version: 1.9.0b3
[0m20:45:21.053881 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m20:45:21.054645 [debug] [MainThread]: Partial parsing: updated file: dbt_capstone_andres://models/staging/stg_treasury_revenue_collections.sql
[0m20:45:21.319004 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.andres_capstone_dbt.example
[0m20:45:21.410563 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'f016d3c3-2051-4c6c-bb2c-2dd6c62ac878', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x113b31fd0>]}
[0m20:45:21.548396 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'f016d3c3-2051-4c6c-bb2c-2dd6c62ac878', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11381ab90>]}
[0m20:45:21.549000 [info ] [MainThread]: Found 6 models, 4 data tests, 2 sources, 862 macros
[0m20:45:21.549412 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f016d3c3-2051-4c6c-bb2c-2dd6c62ac878', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10cd53d90>]}
[0m20:45:21.550976 [info ] [MainThread]: 
[0m20:45:21.551383 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m20:45:21.551745 [info ] [MainThread]: 
[0m20:45:21.552288 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m20:45:21.553146 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_DATAEXPERT_STUDENT'
[0m20:45:21.568011 [debug] [ThreadPool]: Using snowflake connection "list_DATAEXPERT_STUDENT"
[0m20:45:21.568512 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "connection_name": "list_DATAEXPERT_STUDENT"} */
show terse schemas in database DATAEXPERT_STUDENT
    limit 10000
[0m20:45:21.568890 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:45:22.525882 [debug] [ThreadPool]: SQL status: SUCCESS 68 in 0.957 seconds
[0m20:45:22.528679 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT: Close
[0m20:45:22.745265 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_DATAEXPERT_STUDENT, now list_DATAEXPERT_STUDENT_andres)
[0m20:45:22.755477 [debug] [ThreadPool]: Using snowflake connection "list_DATAEXPERT_STUDENT_andres"
[0m20:45:22.755965 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "connection_name": "list_DATAEXPERT_STUDENT_andres"} */
show objects in DATAEXPERT_STUDENT.andres limit 10000
[0m20:45:22.756330 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m20:45:23.555580 [debug] [ThreadPool]: SQL status: SUCCESS 22 in 0.799 seconds
[0m20:45:23.558475 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: Close
[0m20:45:23.771521 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f016d3c3-2051-4c6c-bb2c-2dd6c62ac878', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x113b97cd0>]}
[0m20:45:23.774639 [debug] [Thread-1 (]: Began running node model.dbt_capstone_andres.stg_treasury_revenue_collections
[0m20:45:23.775277 [info ] [Thread-1 (]: 1 of 1 START sql view model andres.stg_treasury_revenue_collections ............ [RUN]
[0m20:45:23.775820 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_DATAEXPERT_STUDENT_andres, now model.dbt_capstone_andres.stg_treasury_revenue_collections)
[0m20:45:23.776255 [debug] [Thread-1 (]: Began compiling node model.dbt_capstone_andres.stg_treasury_revenue_collections
[0m20:45:23.785498 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_capstone_andres.stg_treasury_revenue_collections"
[0m20:45:23.786366 [debug] [Thread-1 (]: Began executing node model.dbt_capstone_andres.stg_treasury_revenue_collections
[0m20:45:23.821366 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_capstone_andres.stg_treasury_revenue_collections"
[0m20:45:23.822898 [debug] [Thread-1 (]: Using snowflake connection "model.dbt_capstone_andres.stg_treasury_revenue_collections"
[0m20:45:23.823421 [debug] [Thread-1 (]: On model.dbt_capstone_andres.stg_treasury_revenue_collections: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "model.dbt_capstone_andres.stg_treasury_revenue_collections"} */
create or replace   view DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections
  
   as (
    with source as (

    select * from dataexpert_student.andres.fiscal_data

),

renamed as (

    select
        record_date as collection_date,
        channel_type_desc as channel_type,
        tax_category_desc as tax_category,
        round(net_collections_amt,0) as amount

    from source

)

select * from renamed
  );
[0m20:45:23.823886 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m20:45:24.640112 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 0.816 seconds
[0m20:45:24.665794 [debug] [Thread-1 (]: On model.dbt_capstone_andres.stg_treasury_revenue_collections: Close
[0m20:45:24.883373 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f016d3c3-2051-4c6c-bb2c-2dd6c62ac878', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111f29f90>]}
[0m20:45:24.884538 [info ] [Thread-1 (]: 1 of 1 OK created sql view model andres.stg_treasury_revenue_collections ....... [[32mSUCCESS 1[0m in 1.11s]
[0m20:45:24.885554 [debug] [Thread-1 (]: Finished running node model.dbt_capstone_andres.stg_treasury_revenue_collections
[0m20:45:24.887379 [debug] [MainThread]: Connection 'master' was properly closed.
[0m20:45:24.887890 [debug] [MainThread]: Connection 'model.dbt_capstone_andres.stg_treasury_revenue_collections' was properly closed.
[0m20:45:24.888394 [info ] [MainThread]: 
[0m20:45:24.889059 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 3.34 seconds (3.34s).
[0m20:45:24.890216 [debug] [MainThread]: Command end result
[0m20:45:24.945235 [info ] [MainThread]: 
[0m20:45:24.945829 [info ] [MainThread]: [32mCompleted successfully[0m
[0m20:45:24.946304 [info ] [MainThread]: 
[0m20:45:24.946729 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m20:45:24.950311 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 6.1085896, "process_user_time": 3.304502, "process_kernel_time": 0.699236, "process_mem_max_rss": "192622592", "process_in_blocks": "0", "process_out_blocks": "0"}
[0m20:45:24.951000 [debug] [MainThread]: Command `dbt run` succeeded at 20:45:24.950889 after 6.11 seconds
[0m20:45:24.951448 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109614c50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109598a90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10958a150>]}
[0m20:45:24.951879 [debug] [MainThread]: Flushing usage events
[0m20:45:25.177531 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m20:46:37.772598 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1089b8910>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1089bbf10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1089b87d0>]}


============================== 20:46:37.778877 | 1f4add15-6499-4324-b9e0-9d700b399a47 ==============================
[0m20:46:37.778877 [info ] [MainThread]: Running with dbt=1.9.0-b3
[0m20:46:37.779639 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'fail_fast': 'False', 'version_check': 'True', 'log_path': 'logs', 'warn_error': 'None', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'invocation_command': 'dbt run -s stg_treasury_revenue_collections', 'send_anonymous_usage_stats': 'True'}
[0m20:46:39.572684 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '1f4add15-6499-4324-b9e0-9d700b399a47', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10923bc90>]}
[0m20:46:39.634488 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '1f4add15-6499-4324-b9e0-9d700b399a47', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10edd1690>]}
[0m20:46:39.636310 [info ] [MainThread]: Registered adapter: snowflake=1.8.4
[0m20:46:39.836716 [debug] [MainThread]: checksum: 5f4fde221ca8627174a0341efc73099aaec05b8546408ceed6f59b1ec0755115, vars: {}, profile: , target: , version: 1.9.0b3
[0m20:46:40.086864 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m20:46:40.087525 [debug] [MainThread]: Partial parsing: updated file: dbt_capstone_andres://models/staging/stg_daily_bars.sql
[0m20:46:40.327488 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.andres_capstone_dbt.example
[0m20:46:40.423145 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '1f4add15-6499-4324-b9e0-9d700b399a47', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10f353bd0>]}
[0m20:46:40.555418 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '1f4add15-6499-4324-b9e0-9d700b399a47', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10efd51d0>]}
[0m20:46:40.555962 [info ] [MainThread]: Found 6 models, 4 data tests, 2 sources, 862 macros
[0m20:46:40.556323 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '1f4add15-6499-4324-b9e0-9d700b399a47', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10f025810>]}
[0m20:46:40.557760 [info ] [MainThread]: 
[0m20:46:40.558128 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m20:46:40.558467 [info ] [MainThread]: 
[0m20:46:40.558968 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m20:46:40.559737 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_DATAEXPERT_STUDENT'
[0m20:46:40.574993 [debug] [ThreadPool]: Using snowflake connection "list_DATAEXPERT_STUDENT"
[0m20:46:40.595083 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "connection_name": "list_DATAEXPERT_STUDENT"} */
show terse schemas in database DATAEXPERT_STUDENT
    limit 10000
[0m20:46:40.614490 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:46:41.508071 [debug] [ThreadPool]: SQL status: SUCCESS 68 in 0.894 seconds
[0m20:46:41.511866 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT: Close
[0m20:46:41.728339 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_DATAEXPERT_STUDENT, now list_DATAEXPERT_STUDENT_andres)
[0m20:46:41.739677 [debug] [ThreadPool]: Using snowflake connection "list_DATAEXPERT_STUDENT_andres"
[0m20:46:41.740162 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "connection_name": "list_DATAEXPERT_STUDENT_andres"} */
show objects in DATAEXPERT_STUDENT.andres limit 10000
[0m20:46:41.740578 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m20:46:42.456020 [debug] [ThreadPool]: SQL status: SUCCESS 22 in 0.715 seconds
[0m20:46:42.459747 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: Close
[0m20:46:42.682566 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '1f4add15-6499-4324-b9e0-9d700b399a47', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10f0e0c10>]}
[0m20:46:42.686611 [debug] [Thread-1 (]: Began running node model.dbt_capstone_andres.stg_treasury_revenue_collections
[0m20:46:42.687357 [info ] [Thread-1 (]: 1 of 1 START sql view model andres.stg_treasury_revenue_collections ............ [RUN]
[0m20:46:42.688064 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_DATAEXPERT_STUDENT_andres, now model.dbt_capstone_andres.stg_treasury_revenue_collections)
[0m20:46:42.688583 [debug] [Thread-1 (]: Began compiling node model.dbt_capstone_andres.stg_treasury_revenue_collections
[0m20:46:42.698720 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_capstone_andres.stg_treasury_revenue_collections"
[0m20:46:42.699597 [debug] [Thread-1 (]: Began executing node model.dbt_capstone_andres.stg_treasury_revenue_collections
[0m20:46:42.736093 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_capstone_andres.stg_treasury_revenue_collections"
[0m20:46:42.737712 [debug] [Thread-1 (]: Using snowflake connection "model.dbt_capstone_andres.stg_treasury_revenue_collections"
[0m20:46:42.738375 [debug] [Thread-1 (]: On model.dbt_capstone_andres.stg_treasury_revenue_collections: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "model.dbt_capstone_andres.stg_treasury_revenue_collections"} */
create or replace   view DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections
  
   as (
    with source as (

    select * from dataexpert_student.andres.fiscal_data

),

renamed as (

    select
        record_date as collection_date,
        channel_type_desc as channel_type,
        tax_category_desc as tax_category,
        round(net_collections_amt,0) as amount

    from source

)

select * from renamed
  );
[0m20:46:42.738856 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m20:46:43.571333 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 0.832 seconds
[0m20:46:43.595084 [debug] [Thread-1 (]: On model.dbt_capstone_andres.stg_treasury_revenue_collections: Close
[0m20:46:43.824116 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1f4add15-6499-4324-b9e0-9d700b399a47', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ef0bf10>]}
[0m20:46:43.825034 [info ] [Thread-1 (]: 1 of 1 OK created sql view model andres.stg_treasury_revenue_collections ....... [[32mSUCCESS 1[0m in 1.13s]
[0m20:46:43.825827 [debug] [Thread-1 (]: Finished running node model.dbt_capstone_andres.stg_treasury_revenue_collections
[0m20:46:43.827139 [debug] [MainThread]: Connection 'master' was properly closed.
[0m20:46:43.827523 [debug] [MainThread]: Connection 'model.dbt_capstone_andres.stg_treasury_revenue_collections' was properly closed.
[0m20:46:43.827916 [info ] [MainThread]: 
[0m20:46:43.828321 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 3.27 seconds (3.27s).
[0m20:46:43.829061 [debug] [MainThread]: Command end result
[0m20:46:43.879098 [info ] [MainThread]: 
[0m20:46:43.879652 [info ] [MainThread]: [32mCompleted successfully[0m
[0m20:46:43.880035 [info ] [MainThread]: 
[0m20:46:43.880419 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m20:46:43.883346 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 6.1836243, "process_user_time": 3.391253, "process_kernel_time": 0.734509, "process_mem_max_rss": "193150976", "process_in_blocks": "0", "process_out_blocks": "0"}
[0m20:46:43.884021 [debug] [MainThread]: Command `dbt run` succeeded at 20:46:43.883908 after 6.18 seconds
[0m20:46:43.884497 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1086f1050>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1190fab50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104d34c50>]}
[0m20:46:43.885154 [debug] [MainThread]: Flushing usage events
[0m20:46:44.108554 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m20:47:25.465433 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10546d0d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1054b34d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105463390>]}


============================== 20:47:25.469488 | ec86c8db-94a3-4a46-9063-1f2e9fe30f0d ==============================
[0m20:47:25.469488 [info ] [MainThread]: Running with dbt=1.9.0-b3
[0m20:47:25.470165 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'version_check': 'True', 'log_path': 'logs', 'profiles_dir': '.', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'invocation_command': 'dbt run -s stg_daily_bars', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m20:47:26.527340 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'ec86c8db-94a3-4a46-9063-1f2e9fe30f0d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104e76d50>]}
[0m20:47:26.603738 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'ec86c8db-94a3-4a46-9063-1f2e9fe30f0d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104151690>]}
[0m20:47:26.605138 [info ] [MainThread]: Registered adapter: snowflake=1.8.4
[0m20:47:26.839335 [debug] [MainThread]: checksum: 5f4fde221ca8627174a0341efc73099aaec05b8546408ceed6f59b1ec0755115, vars: {}, profile: , target: , version: 1.9.0b3
[0m20:47:27.076451 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m20:47:27.076948 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m20:47:27.082865 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.andres_capstone_dbt.example
[0m20:47:27.128345 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'ec86c8db-94a3-4a46-9063-1f2e9fe30f0d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a3e1810>]}
[0m20:47:27.244000 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'ec86c8db-94a3-4a46-9063-1f2e9fe30f0d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10baee6d0>]}
[0m20:47:27.244575 [info ] [MainThread]: Found 6 models, 4 data tests, 2 sources, 862 macros
[0m20:47:27.244935 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ec86c8db-94a3-4a46-9063-1f2e9fe30f0d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10badef50>]}
[0m20:47:27.246405 [info ] [MainThread]: 
[0m20:47:27.246769 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m20:47:27.247104 [info ] [MainThread]: 
[0m20:47:27.247615 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m20:47:27.248470 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_DATAEXPERT_STUDENT'
[0m20:47:27.265382 [debug] [ThreadPool]: Using snowflake connection "list_DATAEXPERT_STUDENT"
[0m20:47:27.265887 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "connection_name": "list_DATAEXPERT_STUDENT"} */
show terse schemas in database DATAEXPERT_STUDENT
    limit 10000
[0m20:47:27.266270 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:47:28.040208 [debug] [ThreadPool]: SQL status: SUCCESS 68 in 0.774 seconds
[0m20:47:28.043340 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT: Close
[0m20:47:28.273766 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_DATAEXPERT_STUDENT, now list_DATAEXPERT_STUDENT_andres)
[0m20:47:28.283982 [debug] [ThreadPool]: Using snowflake connection "list_DATAEXPERT_STUDENT_andres"
[0m20:47:28.284396 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "connection_name": "list_DATAEXPERT_STUDENT_andres"} */
show objects in DATAEXPERT_STUDENT.andres limit 10000
[0m20:47:28.284770 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m20:47:29.017540 [debug] [ThreadPool]: SQL status: SUCCESS 22 in 0.733 seconds
[0m20:47:29.020666 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: Close
[0m20:47:29.253870 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ec86c8db-94a3-4a46-9063-1f2e9fe30f0d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104184410>]}
[0m20:47:29.256319 [debug] [Thread-1 (]: Began running node model.dbt_capstone_andres.stg_daily_bars
[0m20:47:29.257004 [info ] [Thread-1 (]: 1 of 1 START sql view model andres.stg_daily_bars .............................. [RUN]
[0m20:47:29.257910 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_DATAEXPERT_STUDENT_andres, now model.dbt_capstone_andres.stg_daily_bars)
[0m20:47:29.259184 [debug] [Thread-1 (]: Began compiling node model.dbt_capstone_andres.stg_daily_bars
[0m20:47:29.275101 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_capstone_andres.stg_daily_bars"
[0m20:47:29.276033 [debug] [Thread-1 (]: Began executing node model.dbt_capstone_andres.stg_daily_bars
[0m20:47:29.310039 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_capstone_andres.stg_daily_bars"
[0m20:47:29.311511 [debug] [Thread-1 (]: Using snowflake connection "model.dbt_capstone_andres.stg_daily_bars"
[0m20:47:29.311976 [debug] [Thread-1 (]: On model.dbt_capstone_andres.stg_daily_bars: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "model.dbt_capstone_andres.stg_daily_bars"} */
create or replace   view DATAEXPERT_STUDENT.andres.stg_daily_bars
  
   as (
    with source as (

    select * from dataexpert_student.andres.daily_bars

),

renamed as (

    select
        TO_DATE(TO_TIMESTAMP(timestamp / 1000)) as date,
        ticker as ticker,
        v  as trading_volume,
        n  as number_of_trades,
        round(vw,2) as volume_weighted_avg_price,
        o  as open_price,
        c  as close_price,
        h  as high_price,
        l  as low_price
    from source

)

select * from renamed
  );
[0m20:47:29.312405 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m20:47:30.162254 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 0.850 seconds
[0m20:47:30.190760 [debug] [Thread-1 (]: On model.dbt_capstone_andres.stg_daily_bars: Close
[0m20:47:30.410761 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ec86c8db-94a3-4a46-9063-1f2e9fe30f0d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10bc40dd0>]}
[0m20:47:30.411617 [info ] [Thread-1 (]: 1 of 1 OK created sql view model andres.stg_daily_bars ......................... [[32mSUCCESS 1[0m in 1.15s]
[0m20:47:30.412431 [debug] [Thread-1 (]: Finished running node model.dbt_capstone_andres.stg_daily_bars
[0m20:47:30.413809 [debug] [MainThread]: Connection 'master' was properly closed.
[0m20:47:30.414238 [debug] [MainThread]: Connection 'model.dbt_capstone_andres.stg_daily_bars' was properly closed.
[0m20:47:30.414652 [info ] [MainThread]: 
[0m20:47:30.415088 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 3.17 seconds (3.17s).
[0m20:47:30.415884 [debug] [MainThread]: Command end result
[0m20:47:30.477085 [info ] [MainThread]: 
[0m20:47:30.477721 [info ] [MainThread]: [32mCompleted successfully[0m
[0m20:47:30.478164 [info ] [MainThread]: 
[0m20:47:30.478634 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m20:47:30.480661 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 5.0892205, "process_user_time": 3.145802, "process_kernel_time": 0.585056, "process_mem_max_rss": "190275584", "process_in_blocks": "0", "process_out_blocks": "0"}
[0m20:47:30.481401 [debug] [MainThread]: Command `dbt run` succeeded at 20:47:30.481277 after 5.09 seconds
[0m20:47:30.481900 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1054b5990>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1054d1f50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10181a9d0>]}
[0m20:47:30.482372 [debug] [MainThread]: Flushing usage events
[0m20:47:30.667627 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m20:55:35.692971 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10cbff810>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10cc22590>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10cc22fd0>]}


============================== 20:55:35.698554 | ddd5b039-7e68-4626-99bc-adbbb0685dc5 ==============================
[0m20:55:35.698554 [info ] [MainThread]: Running with dbt=1.9.0-b3
[0m20:55:35.699268 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'debug': 'False', 'log_path': 'logs', 'version_check': 'True', 'profiles_dir': '.', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt build', 'static_parser': 'True', 'introspect': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m20:55:37.355771 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'ddd5b039-7e68-4626-99bc-adbbb0685dc5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10beda690>]}
[0m20:55:37.413783 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'ddd5b039-7e68-4626-99bc-adbbb0685dc5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b854fd0>]}
[0m20:55:37.414978 [info ] [MainThread]: Registered adapter: snowflake=1.8.4
[0m20:55:37.614225 [debug] [MainThread]: checksum: 5f4fde221ca8627174a0341efc73099aaec05b8546408ceed6f59b1ec0755115, vars: {}, profile: , target: , version: 1.9.0b3
[0m20:55:37.831325 [debug] [MainThread]: Partial parsing enabled: 1 files deleted, 1 files added, 0 files changed.
[0m20:55:37.831923 [debug] [MainThread]: Partial parsing: added file: dbt_capstone_andres://models/marts/weekly_metrics.sql
[0m20:55:37.832246 [debug] [MainThread]: Partial parsing: deleted file: dbt_capstone_andres://models/marts/daily_metrics.sql
[0m20:55:38.092084 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.andres_capstone_dbt.example
[0m20:55:38.184629 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'ddd5b039-7e68-4626-99bc-adbbb0685dc5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11356a010>]}
[0m20:55:38.344472 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'ddd5b039-7e68-4626-99bc-adbbb0685dc5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10cbefd50>]}
[0m20:55:38.345076 [info ] [MainThread]: Found 6 models, 4 data tests, 2 sources, 862 macros
[0m20:55:38.347579 [info ] [MainThread]: 
[0m20:55:38.347994 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m20:55:38.348356 [info ] [MainThread]: 
[0m20:55:38.348894 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m20:55:38.353736 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_DATAEXPERT_STUDENT'
[0m20:55:38.368040 [debug] [ThreadPool]: Using snowflake connection "list_DATAEXPERT_STUDENT"
[0m20:55:38.368533 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "connection_name": "list_DATAEXPERT_STUDENT"} */
show terse schemas in database DATAEXPERT_STUDENT
    limit 10000
[0m20:55:38.368903 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:55:39.280233 [debug] [ThreadPool]: SQL status: SUCCESS 68 in 0.911 seconds
[0m20:55:39.282699 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT: Close
[0m20:55:39.503701 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_DATAEXPERT_STUDENT, now list_DATAEXPERT_STUDENT_andres)
[0m20:55:39.515500 [debug] [ThreadPool]: Using snowflake connection "list_DATAEXPERT_STUDENT_andres"
[0m20:55:39.515911 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "connection_name": "list_DATAEXPERT_STUDENT_andres"} */
show objects in DATAEXPERT_STUDENT.andres limit 10000
[0m20:55:39.516287 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m20:55:40.191061 [debug] [ThreadPool]: SQL status: SUCCESS 22 in 0.675 seconds
[0m20:55:40.194768 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: Close
[0m20:55:40.414661 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ddd5b039-7e68-4626-99bc-adbbb0685dc5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11359ba90>]}
[0m20:55:40.418053 [debug] [Thread-1 (]: Began running node model.dbt_capstone_andres.dim_date
[0m20:55:40.418662 [info ] [Thread-1 (]: 1 of 10 START sql table model andres.dim_date .................................. [RUN]
[0m20:55:40.419224 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_DATAEXPERT_STUDENT_andres, now model.dbt_capstone_andres.dim_date)
[0m20:55:40.419678 [debug] [Thread-1 (]: Began compiling node model.dbt_capstone_andres.dim_date
[0m20:55:40.462020 [debug] [Thread-1 (]: Using snowflake connection "model.dbt_capstone_andres.dim_date"
[0m20:55:40.462711 [debug] [Thread-1 (]: On model.dbt_capstone_andres.dim_date: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "model.dbt_capstone_andres.dim_date"} */
select datediff(
        day,
        cast('1995-01-01' as timestamp),
        cast('2040-12-31' as timestamp)
        )
[0m20:55:40.463314 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m20:55:41.182938 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 0.720 seconds
[0m20:55:41.247481 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_capstone_andres.dim_date"
[0m20:55:41.249678 [debug] [Thread-1 (]: Began executing node model.dbt_capstone_andres.dim_date
[0m20:55:41.276550 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_capstone_andres.dim_date"
[0m20:55:41.283575 [debug] [Thread-1 (]: Using snowflake connection "model.dbt_capstone_andres.dim_date"
[0m20:55:41.284266 [debug] [Thread-1 (]: On model.dbt_capstone_andres.dim_date: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "model.dbt_capstone_andres.dim_date"} */
create or replace transient table DATAEXPERT_STUDENT.andres.dim_date
         as
        (

    
with base_dates as (
    
    with date_spine as
(

    





with rawdata as (

    

    

    with p as (
        select 0 as generated_number union all select 1
    ), unioned as (

    select

    
    p0.generated_number * power(2, 0)
     + 
    
    p1.generated_number * power(2, 1)
     + 
    
    p2.generated_number * power(2, 2)
     + 
    
    p3.generated_number * power(2, 3)
     + 
    
    p4.generated_number * power(2, 4)
     + 
    
    p5.generated_number * power(2, 5)
     + 
    
    p6.generated_number * power(2, 6)
     + 
    
    p7.generated_number * power(2, 7)
     + 
    
    p8.generated_number * power(2, 8)
     + 
    
    p9.generated_number * power(2, 9)
     + 
    
    p10.generated_number * power(2, 10)
     + 
    
    p11.generated_number * power(2, 11)
     + 
    
    p12.generated_number * power(2, 12)
     + 
    
    p13.generated_number * power(2, 13)
     + 
    
    p14.generated_number * power(2, 14)
    
    
    + 1
    as generated_number

    from

    
    p as p0
     cross join 
    
    p as p1
     cross join 
    
    p as p2
     cross join 
    
    p as p3
     cross join 
    
    p as p4
     cross join 
    
    p as p5
     cross join 
    
    p as p6
     cross join 
    
    p as p7
     cross join 
    
    p as p8
     cross join 
    
    p as p9
     cross join 
    
    p as p10
     cross join 
    
    p as p11
     cross join 
    
    p as p12
     cross join 
    
    p as p13
     cross join 
    
    p as p14
    
    

    )

    select *
    from unioned
    where generated_number <= 16801
    order by generated_number



),

all_periods as (

    select (
        

    dateadd(
        day,
        (row_number() over (order by 1) - 1),
        cast('1995-01-01' as timestamp)
        )


    ) as date_day
    from rawdata

),

filtered as (

    select *
    from all_periods
    where date_day <= cast('2040-12-31' as timestamp)

)

select * from filtered



)
select
    cast(d.date_day as timestamp) as date_day
from
    date_spine d


),
dates_with_prior_year_dates as (

    select
        cast(d.date_day as date) as date_day,
        cast(

    dateadd(
        year,
        -1,
        d.date_day
        )

 as date) as prior_year_date_day,
        cast(

    dateadd(
        day,
        -364,
        d.date_day
        )

 as date) as prior_year_over_year_date_day
    from
    	base_dates d

)
select
    d.date_day,
    cast(

    dateadd(
        day,
        -1,
        d.date_day
        )

 as date) as prior_date_day,
    cast(

    dateadd(
        day,
        1,
        d.date_day
        )

 as date) as next_date_day,
    d.prior_year_date_day as prior_year_date_day,
    d.prior_year_over_year_date_day,
    case
            when date_part('dayofweek', d.date_day) = 7 then 1
            else date_part('dayofweek', d.date_day) + 1
        end as day_of_week,
    date_part('dayofweekiso', d.date_day) as day_of_week_iso,
    -- long version not implemented on Snowflake so we're doing it manually :/
    case dayname(d.date_day)
        when 'Mon' then 'Monday'
        when 'Tue' then 'Tuesday'
        when 'Wed' then 'Wednesday'
        when 'Thu' then 'Thursday'
        when 'Fri' then 'Friday'
        when 'Sat' then 'Saturday'
        when 'Sun' then 'Sunday'
    end as day_of_week_name,
    dayname(d.date_day) as day_of_week_name_short,
    date_part('day', d.date_day) as day_of_month,
    date_part('dayofyear', d.date_day) as day_of_year,

    
    
    cast(

    dateadd(
        day,
        -1 * (case
            when date_part('dayofweek', d.date_day) = 7 then 1
            else date_part('dayofweek', d.date_day) + 1
        end - 1),
        d.date_day
        )

 as date) as week_start_date,
    cast(

    dateadd(
        day,
        6,
        
    
    cast(

    dateadd(
        day,
        -1 * (case
            when date_part('dayofweek', d.date_day) = 7 then 1
            else date_part('dayofweek', d.date_day) + 1
        end - 1),
        d.date_day
        )

 as date)
        )

 as date) as week_end_date,
    
    
    cast(

    dateadd(
        day,
        -1 * (case
            when date_part('dayofweek', d.prior_year_over_year_date_day) = 7 then 1
            else date_part('dayofweek', d.prior_year_over_year_date_day) + 1
        end - 1),
        d.prior_year_over_year_date_day
        )

 as date) as prior_year_week_start_date,
    cast(

    dateadd(
        day,
        6,
        
    
    cast(

    dateadd(
        day,
        -1 * (case
            when date_part('dayofweek', d.prior_year_over_year_date_day) = 7 then 1
            else date_part('dayofweek', d.prior_year_over_year_date_day) + 1
        end - 1),
        d.prior_year_over_year_date_day
        )

 as date)
        )

 as date) as prior_year_week_end_date,
    cast(date_part('week', d.date_day) as integer) as week_of_year,

    cast(date_trunc('week', d.date_day) as date) as iso_week_start_date,
    cast(

    dateadd(
        day,
        6,
        cast(date_trunc('week', d.date_day) as date)
        )

 as date) as iso_week_end_date,
    cast(date_trunc('week', d.prior_year_over_year_date_day) as date) as prior_year_iso_week_start_date,
    cast(

    dateadd(
        day,
        6,
        cast(date_trunc('week', d.prior_year_over_year_date_day) as date)
        )

 as date) as prior_year_iso_week_end_date,
    cast(date_part('weekiso', d.date_day) as integer) as iso_week_of_year,

    cast(date_part('week', d.prior_year_over_year_date_day) as integer) as prior_year_week_of_year,
    cast(date_part('weekiso', d.prior_year_over_year_date_day) as integer) as prior_year_iso_week_of_year,

    cast(date_part('month', d.date_day) as integer) as month_of_year,
    to_char(d.date_day, 'MMMM')  as month_name,
    to_char(d.date_day, 'MON')  as month_name_short,

    cast(date_trunc('month', d.date_day) as date) as month_start_date,
    cast(cast(
        

    dateadd(
        day,
        -1,
        

    dateadd(
        month,
        1,
        date_trunc('month', d.date_day)
        )


        )


        as date) as date) as month_end_date,

    cast(date_trunc('month', d.prior_year_date_day) as date) as prior_year_month_start_date,
    cast(cast(
        

    dateadd(
        day,
        -1,
        

    dateadd(
        month,
        1,
        date_trunc('month', d.prior_year_date_day)
        )


        )


        as date) as date) as prior_year_month_end_date,

    cast(date_part('quarter', d.date_day) as integer) as quarter_of_year,
    cast(date_trunc('quarter', d.date_day) as date) as quarter_start_date,
    cast(cast(
        

    dateadd(
        day,
        -1,
        

    dateadd(
        quarter,
        1,
        date_trunc('quarter', d.date_day)
        )


        )


        as date) as date) as quarter_end_date,

    cast(date_part('year', d.date_day) as integer) as year_number,
    cast(date_trunc('year', d.date_day) as date) as year_start_date,
    cast(cast(
        

    dateadd(
        day,
        -1,
        

    dateadd(
        year,
        1,
        date_trunc('year', d.date_day)
        )


        )


        as date) as date) as year_end_date
from
    dates_with_prior_year_dates d
order by 1


        );
[0m20:55:42.502722 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 1.218 seconds
[0m20:55:42.527168 [debug] [Thread-1 (]: On model.dbt_capstone_andres.dim_date: Close
[0m20:55:42.746277 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ddd5b039-7e68-4626-99bc-adbbb0685dc5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112fb0650>]}
[0m20:55:42.747545 [info ] [Thread-1 (]: 1 of 10 OK created sql table model andres.dim_date ............................. [[32mSUCCESS 1[0m in 2.32s]
[0m20:55:42.748474 [debug] [Thread-1 (]: Finished running node model.dbt_capstone_andres.dim_date
[0m20:55:42.749113 [debug] [Thread-1 (]: Began running node model.dbt_capstone_andres.my_first_dbt_model
[0m20:55:42.749916 [info ] [Thread-1 (]: 2 of 10 START sql table model andres.my_first_dbt_model ........................ [RUN]
[0m20:55:42.750649 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_capstone_andres.dim_date, now model.dbt_capstone_andres.my_first_dbt_model)
[0m20:55:42.751178 [debug] [Thread-1 (]: Began compiling node model.dbt_capstone_andres.my_first_dbt_model
[0m20:55:42.755359 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_capstone_andres.my_first_dbt_model"
[0m20:55:42.756246 [debug] [Thread-1 (]: Began executing node model.dbt_capstone_andres.my_first_dbt_model
[0m20:55:42.760593 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_capstone_andres.my_first_dbt_model"
[0m20:55:42.762002 [debug] [Thread-1 (]: Using snowflake connection "model.dbt_capstone_andres.my_first_dbt_model"
[0m20:55:42.762526 [debug] [Thread-1 (]: On model.dbt_capstone_andres.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "model.dbt_capstone_andres.my_first_dbt_model"} */
create or replace transient table DATAEXPERT_STUDENT.andres.my_first_dbt_model
         as
        (/*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
        );
[0m20:55:42.763023 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m20:55:43.923536 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 1.160 seconds
[0m20:55:43.926989 [debug] [Thread-1 (]: On model.dbt_capstone_andres.my_first_dbt_model: Close
[0m20:55:44.148132 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ddd5b039-7e68-4626-99bc-adbbb0685dc5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11743e090>]}
[0m20:55:44.149214 [info ] [Thread-1 (]: 2 of 10 OK created sql table model andres.my_first_dbt_model ................... [[32mSUCCESS 1[0m in 1.40s]
[0m20:55:44.150105 [debug] [Thread-1 (]: Finished running node model.dbt_capstone_andres.my_first_dbt_model
[0m20:55:44.150646 [debug] [Thread-1 (]: Began running node model.dbt_capstone_andres.stg_daily_bars
[0m20:55:44.151339 [info ] [Thread-1 (]: 3 of 10 START sql view model andres.stg_daily_bars ............................. [RUN]
[0m20:55:44.152075 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_capstone_andres.my_first_dbt_model, now model.dbt_capstone_andres.stg_daily_bars)
[0m20:55:44.152582 [debug] [Thread-1 (]: Began compiling node model.dbt_capstone_andres.stg_daily_bars
[0m20:55:44.158149 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_capstone_andres.stg_daily_bars"
[0m20:55:44.159062 [debug] [Thread-1 (]: Began executing node model.dbt_capstone_andres.stg_daily_bars
[0m20:55:44.186361 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_capstone_andres.stg_daily_bars"
[0m20:55:44.188108 [debug] [Thread-1 (]: Using snowflake connection "model.dbt_capstone_andres.stg_daily_bars"
[0m20:55:44.188725 [debug] [Thread-1 (]: On model.dbt_capstone_andres.stg_daily_bars: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "model.dbt_capstone_andres.stg_daily_bars"} */
create or replace   view DATAEXPERT_STUDENT.andres.stg_daily_bars
  
   as (
    with source as (

    select * from dataexpert_student.andres.daily_bars

),

renamed as (

    select
        TO_DATE(TO_TIMESTAMP(timestamp / 1000)) as date,
        ticker as ticker,
        v  as trading_volume,
        n  as number_of_trades,
        round(vw,2) as volume_weighted_avg_price,
        o  as open_price,
        c  as close_price,
        h  as high_price,
        l  as low_price
    from source

)

select * from renamed
  );
[0m20:55:44.189250 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m20:55:45.043483 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 0.854 seconds
[0m20:55:45.045568 [debug] [Thread-1 (]: On model.dbt_capstone_andres.stg_daily_bars: Close
[0m20:55:45.254475 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ddd5b039-7e68-4626-99bc-adbbb0685dc5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1175b0c10>]}
[0m20:55:45.255182 [info ] [Thread-1 (]: 3 of 10 OK created sql view model andres.stg_daily_bars ........................ [[32mSUCCESS 1[0m in 1.10s]
[0m20:55:45.255880 [debug] [Thread-1 (]: Finished running node model.dbt_capstone_andres.stg_daily_bars
[0m20:55:45.256276 [debug] [Thread-1 (]: Began running node model.dbt_capstone_andres.stg_treasury_revenue_collections
[0m20:55:45.256934 [info ] [Thread-1 (]: 4 of 10 START sql view model andres.stg_treasury_revenue_collections ........... [RUN]
[0m20:55:45.257461 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_capstone_andres.stg_daily_bars, now model.dbt_capstone_andres.stg_treasury_revenue_collections)
[0m20:55:45.257826 [debug] [Thread-1 (]: Began compiling node model.dbt_capstone_andres.stg_treasury_revenue_collections
[0m20:55:45.261312 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_capstone_andres.stg_treasury_revenue_collections"
[0m20:55:45.262063 [debug] [Thread-1 (]: Began executing node model.dbt_capstone_andres.stg_treasury_revenue_collections
[0m20:55:45.266864 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_capstone_andres.stg_treasury_revenue_collections"
[0m20:55:45.269419 [debug] [Thread-1 (]: Using snowflake connection "model.dbt_capstone_andres.stg_treasury_revenue_collections"
[0m20:55:45.269950 [debug] [Thread-1 (]: On model.dbt_capstone_andres.stg_treasury_revenue_collections: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "model.dbt_capstone_andres.stg_treasury_revenue_collections"} */
create or replace   view DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections
  
   as (
    with source as (

    select * from dataexpert_student.andres.fiscal_data

),

renamed as (

    select
        record_date as collection_date,
        channel_type_desc as channel_type,
        tax_category_desc as tax_category,
        round(net_collections_amt,0) as amount

    from source

)

select * from renamed
  );
[0m20:55:45.270330 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m20:55:46.176719 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 0.906 seconds
[0m20:55:46.179974 [debug] [Thread-1 (]: On model.dbt_capstone_andres.stg_treasury_revenue_collections: Close
[0m20:55:46.406874 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ddd5b039-7e68-4626-99bc-adbbb0685dc5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1133f6850>]}
[0m20:55:46.407897 [info ] [Thread-1 (]: 4 of 10 OK created sql view model andres.stg_treasury_revenue_collections ...... [[32mSUCCESS 1[0m in 1.15s]
[0m20:55:46.408781 [debug] [Thread-1 (]: Finished running node model.dbt_capstone_andres.stg_treasury_revenue_collections
[0m20:55:46.409416 [debug] [Thread-1 (]: Began running node test.dbt_capstone_andres.not_null_my_first_dbt_model_id.5fb22c2710
[0m20:55:46.410100 [info ] [Thread-1 (]: 5 of 10 START test not_null_my_first_dbt_model_id .............................. [RUN]
[0m20:55:46.410693 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_capstone_andres.stg_treasury_revenue_collections, now test.dbt_capstone_andres.not_null_my_first_dbt_model_id.5fb22c2710)
[0m20:55:46.411178 [debug] [Thread-1 (]: Began compiling node test.dbt_capstone_andres.not_null_my_first_dbt_model_id.5fb22c2710
[0m20:55:46.425277 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_capstone_andres.not_null_my_first_dbt_model_id.5fb22c2710"
[0m20:55:46.426146 [debug] [Thread-1 (]: Began executing node test.dbt_capstone_andres.not_null_my_first_dbt_model_id.5fb22c2710
[0m20:55:46.445096 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_capstone_andres.not_null_my_first_dbt_model_id.5fb22c2710"
[0m20:55:46.446217 [debug] [Thread-1 (]: Using snowflake connection "test.dbt_capstone_andres.not_null_my_first_dbt_model_id.5fb22c2710"
[0m20:55:46.446653 [debug] [Thread-1 (]: On test.dbt_capstone_andres.not_null_my_first_dbt_model_id.5fb22c2710: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "test.dbt_capstone_andres.not_null_my_first_dbt_model_id.5fb22c2710"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from DATAEXPERT_STUDENT.andres.my_first_dbt_model
where id is null



      
    ) dbt_internal_test
[0m20:55:46.447067 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m20:55:47.162356 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 0.715 seconds
[0m20:55:47.166326 [debug] [Thread-1 (]: On test.dbt_capstone_andres.not_null_my_first_dbt_model_id.5fb22c2710: Close
[0m20:55:47.379148 [error] [Thread-1 (]: 5 of 10 FAIL 1 not_null_my_first_dbt_model_id .................................. [[31mFAIL 1[0m in 0.97s]
[0m20:55:47.379804 [debug] [Thread-1 (]: Finished running node test.dbt_capstone_andres.not_null_my_first_dbt_model_id.5fb22c2710
[0m20:55:47.380190 [debug] [Thread-1 (]: Began running node test.dbt_capstone_andres.unique_my_first_dbt_model_id.16e066b321
[0m20:55:47.380643 [debug] [Thread-4 (]: Marking all children of 'test.dbt_capstone_andres.not_null_my_first_dbt_model_id.5fb22c2710' to be skipped because of status 'fail'.  Reason: Got 1 result, configured to fail if != 0.
[0m20:55:47.381078 [info ] [Thread-1 (]: 6 of 10 START test unique_my_first_dbt_model_id ................................ [RUN]
[0m20:55:47.381982 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_capstone_andres.not_null_my_first_dbt_model_id.5fb22c2710, now test.dbt_capstone_andres.unique_my_first_dbt_model_id.16e066b321)
[0m20:55:47.382350 [debug] [Thread-1 (]: Began compiling node test.dbt_capstone_andres.unique_my_first_dbt_model_id.16e066b321
[0m20:55:47.390736 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_capstone_andres.unique_my_first_dbt_model_id.16e066b321"
[0m20:55:47.391637 [debug] [Thread-1 (]: Began executing node test.dbt_capstone_andres.unique_my_first_dbt_model_id.16e066b321
[0m20:55:47.394836 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_capstone_andres.unique_my_first_dbt_model_id.16e066b321"
[0m20:55:47.395954 [debug] [Thread-1 (]: Using snowflake connection "test.dbt_capstone_andres.unique_my_first_dbt_model_id.16e066b321"
[0m20:55:47.396383 [debug] [Thread-1 (]: On test.dbt_capstone_andres.unique_my_first_dbt_model_id.16e066b321: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "test.dbt_capstone_andres.unique_my_first_dbt_model_id.16e066b321"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from DATAEXPERT_STUDENT.andres.my_first_dbt_model
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
[0m20:55:47.396803 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m20:55:48.079896 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 0.683 seconds
[0m20:55:48.081595 [debug] [Thread-1 (]: On test.dbt_capstone_andres.unique_my_first_dbt_model_id.16e066b321: Close
[0m20:55:48.288933 [info ] [Thread-1 (]: 6 of 10 PASS unique_my_first_dbt_model_id ...................................... [[32mPASS[0m in 0.91s]
[0m20:55:48.289741 [debug] [Thread-1 (]: Finished running node test.dbt_capstone_andres.unique_my_first_dbt_model_id.16e066b321
[0m20:55:48.290123 [debug] [Thread-1 (]: Began running node model.dbt_capstone_andres.weekly_metrics
[0m20:55:48.290748 [info ] [Thread-1 (]: 7 of 10 START sql view model andres.weekly_metrics ............................. [RUN]
[0m20:55:48.291316 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_capstone_andres.unique_my_first_dbt_model_id.16e066b321, now model.dbt_capstone_andres.weekly_metrics)
[0m20:55:48.291694 [debug] [Thread-1 (]: Began compiling node model.dbt_capstone_andres.weekly_metrics
[0m20:55:48.295231 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_capstone_andres.weekly_metrics"
[0m20:55:48.296120 [debug] [Thread-1 (]: Began executing node model.dbt_capstone_andres.weekly_metrics
[0m20:55:48.301368 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_capstone_andres.weekly_metrics"
[0m20:55:48.303892 [debug] [Thread-1 (]: Using snowflake connection "model.dbt_capstone_andres.weekly_metrics"
[0m20:55:48.304345 [debug] [Thread-1 (]: On model.dbt_capstone_andres.weekly_metrics: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "model.dbt_capstone_andres.weekly_metrics"} */
create or replace   view DATAEXPERT_STUDENT.andres.weekly_metrics
  
   as (
    with pol as (
SELECT week_start_date, 
sum(trading_volume) trading_volume,
sum(number_of_trades)
from  DATAEXPERT_STUDENT.andres.stg_daily_bars a 
JOIN DATAEXPERT_STUDENT.andres.dim_date d on d.date_day=a.date
group by 1)

, rev as (
select week_start_date,
sum(amount) amount_millions,
sum(case when tax_category = 'IRS Tax' then amount else 0 end) as irs_tax_amount_millions, 
sum(case when tax_category = 'Non-Tax' then amount else 0 end) as non_tax_amount,
sum(case when tax_category = 'IRS Non-Tax' then amount else 0 end) as irs_non_tax_amount
FROM DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections a 
JOIN DATAEXPERT_STUDENT.andres.dim_date d on d.date_day=a.collection_date
GROUP BY ALL)

select a.week_start_date, a.trading_volume/1000000000, b.amount_millions/1000000000 
from pol a 
INNER JOIN rev b on b.week_start_date = a.week_start_date
;
[0m20:55:48.304716 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m20:55:48.940454 [debug] [Thread-1 (]: Snowflake adapter: Snowflake query id: 01b87733-0004-32d9-0000-33eb00181626
[0m20:55:48.940894 [debug] [Thread-1 (]: Snowflake adapter: Snowflake error: 001003 (42000): SQL compilation error:
syntax error line 25 at position 0 unexpected ';'.
[0m20:55:48.941352 [debug] [Thread-1 (]: On model.dbt_capstone_andres.weekly_metrics: Close
[0m20:55:49.159131 [debug] [Thread-1 (]: Database Error in model weekly_metrics (models/marts/weekly_metrics.sql)
  001003 (42000): SQL compilation error:
  syntax error line 25 at position 0 unexpected ';'.
  compiled code at target/run/dbt_capstone_andres/models/marts/weekly_metrics.sql
[0m20:55:49.159684 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ddd5b039-7e68-4626-99bc-adbbb0685dc5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11751d750>]}
[0m20:55:49.160261 [error] [Thread-1 (]: 7 of 10 ERROR creating sql view model andres.weekly_metrics .................... [[31mERROR[0m in 0.87s]
[0m20:55:49.160821 [debug] [Thread-1 (]: Finished running node model.dbt_capstone_andres.weekly_metrics
[0m20:55:49.161213 [debug] [Thread-1 (]: Began running node model.dbt_capstone_andres.my_second_dbt_model
[0m20:55:49.161699 [debug] [Thread-4 (]: Marking all children of 'model.dbt_capstone_andres.weekly_metrics' to be skipped because of status 'error'.  Reason: Database Error in model weekly_metrics (models/marts/weekly_metrics.sql)
  001003 (42000): SQL compilation error:
  syntax error line 25 at position 0 unexpected ';'.
  compiled code at target/run/dbt_capstone_andres/models/marts/weekly_metrics.sql.
[0m20:55:49.162072 [info ] [Thread-1 (]: 8 of 10 SKIP relation andres.my_second_dbt_model ............................... [[33mSKIP[0m]
[0m20:55:49.162602 [debug] [Thread-1 (]: Finished running node model.dbt_capstone_andres.my_second_dbt_model
[0m20:55:49.162958 [debug] [Thread-4 (]: Marking all children of 'model.dbt_capstone_andres.my_second_dbt_model' to be skipped because of status 'skipped'. 
[0m20:55:49.163576 [debug] [Thread-1 (]: Began running node test.dbt_capstone_andres.not_null_my_second_dbt_model_id.151b76d778
[0m20:55:49.164073 [info ] [Thread-1 (]: 9 of 10 SKIP test not_null_my_second_dbt_model_id .............................. [[33mSKIP[0m]
[0m20:55:49.164564 [debug] [Thread-1 (]: Finished running node test.dbt_capstone_andres.not_null_my_second_dbt_model_id.151b76d778
[0m20:55:49.164951 [debug] [Thread-1 (]: Began running node test.dbt_capstone_andres.unique_my_second_dbt_model_id.57a0f8c493
[0m20:55:49.165256 [debug] [Thread-4 (]: Marking all children of 'test.dbt_capstone_andres.not_null_my_second_dbt_model_id.151b76d778' to be skipped because of status 'skipped'. 
[0m20:55:49.165665 [info ] [Thread-1 (]: 10 of 10 SKIP test unique_my_second_dbt_model_id ............................... [[33mSKIP[0m]
[0m20:55:49.166418 [debug] [Thread-1 (]: Finished running node test.dbt_capstone_andres.unique_my_second_dbt_model_id.57a0f8c493
[0m20:55:49.166789 [debug] [Thread-4 (]: Marking all children of 'test.dbt_capstone_andres.unique_my_second_dbt_model_id.57a0f8c493' to be skipped because of status 'skipped'. 
[0m20:55:49.167832 [debug] [MainThread]: Connection 'master' was properly closed.
[0m20:55:49.168148 [debug] [MainThread]: Connection 'model.dbt_capstone_andres.weekly_metrics' was properly closed.
[0m20:55:49.168538 [info ] [MainThread]: 
[0m20:55:49.168882 [info ] [MainThread]: Finished running 2 table models, 4 data tests, 4 view models in 0 hours 0 minutes and 10.82 seconds (10.82s).
[0m20:55:49.170402 [debug] [MainThread]: Command end result
[0m20:55:49.223132 [info ] [MainThread]: 
[0m20:55:49.223723 [info ] [MainThread]: [31mCompleted with 2 errors, 0 partial successs, and 0 warnings:[0m
[0m20:55:49.224144 [info ] [MainThread]: 
[0m20:55:49.224598 [error] [MainThread]: [31mFailure in test not_null_my_first_dbt_model_id (models/example/schema.yml)[0m
[0m20:55:49.225275 [error] [MainThread]:   Got 1 result, configured to fail if != 0
[0m20:55:49.225676 [info ] [MainThread]: 
[0m20:55:49.226099 [info ] [MainThread]:   compiled code at target/compiled/dbt_capstone_andres/models/example/schema.yml/not_null_my_first_dbt_model_id.sql
[0m20:55:49.226470 [info ] [MainThread]: 
[0m20:55:49.226896 [error] [MainThread]:   Database Error in model weekly_metrics (models/marts/weekly_metrics.sql)
  001003 (42000): SQL compilation error:
  syntax error line 25 at position 0 unexpected ';'.
  compiled code at target/run/dbt_capstone_andres/models/marts/weekly_metrics.sql
[0m20:55:49.227266 [info ] [MainThread]: 
[0m20:55:49.227635 [info ] [MainThread]: Done. PASS=5 WARN=0 ERROR=2 SKIP=3 TOTAL=10
[0m20:55:49.230383 [debug] [MainThread]: Resource report: {"command_name": "build", "command_wall_clock_time": 13.608666, "process_user_time": 3.936303, "process_kernel_time": 0.743599, "process_mem_max_rss": "199565312", "command_success": false, "process_in_blocks": "0", "process_out_blocks": "0"}
[0m20:55:49.231263 [debug] [MainThread]: Command `dbt build` failed at 20:55:49.231131 after 13.61 seconds
[0m20:55:49.231872 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10cc37650>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11763ab90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108edb390>]}
[0m20:55:49.232329 [debug] [MainThread]: Flushing usage events
[0m20:55:49.531722 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m20:56:14.671420 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106a37810>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106a5a8d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106a5aed0>]}


============================== 20:56:14.675663 | 0a106dc8-8a13-4eeb-bd0f-929086ef106a ==============================
[0m20:56:14.675663 [info ] [MainThread]: Running with dbt=1.9.0-b3
[0m20:56:14.676313 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'debug': 'False', 'warn_error': 'None', 'log_path': 'logs', 'version_check': 'True', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'invocation_command': 'dbt build -s weekly_metrics', 'introspect': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m20:56:15.608843 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '0a106dc8-8a13-4eeb-bd0f-929086ef106a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ccfc610>]}
[0m20:56:15.663614 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '0a106dc8-8a13-4eeb-bd0f-929086ef106a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108db5f90>]}
[0m20:56:15.664383 [info ] [MainThread]: Registered adapter: snowflake=1.8.4
[0m20:56:15.841562 [debug] [MainThread]: checksum: 5f4fde221ca8627174a0341efc73099aaec05b8546408ceed6f59b1ec0755115, vars: {}, profile: , target: , version: 1.9.0b3
[0m20:56:16.128580 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m20:56:16.129095 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m20:56:16.135346 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.andres_capstone_dbt.example
[0m20:56:16.180912 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '0a106dc8-8a13-4eeb-bd0f-929086ef106a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d15a1d0>]}
[0m20:56:16.315860 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '0a106dc8-8a13-4eeb-bd0f-929086ef106a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10cd04e10>]}
[0m20:56:16.316513 [info ] [MainThread]: Found 6 models, 4 data tests, 2 sources, 862 macros
[0m20:56:16.318619 [info ] [MainThread]: 
[0m20:56:16.319134 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m20:56:16.319524 [info ] [MainThread]: 
[0m20:56:16.320142 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m20:56:16.321085 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_DATAEXPERT_STUDENT'
[0m20:56:16.418306 [debug] [ThreadPool]: Using snowflake connection "list_DATAEXPERT_STUDENT"
[0m20:56:16.418900 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "connection_name": "list_DATAEXPERT_STUDENT"} */
show terse schemas in database DATAEXPERT_STUDENT
    limit 10000
[0m20:56:16.419356 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:56:17.116934 [debug] [ThreadPool]: SQL status: SUCCESS 68 in 0.698 seconds
[0m20:56:17.120644 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT: Close
[0m20:56:17.343519 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_DATAEXPERT_STUDENT, now list_DATAEXPERT_STUDENT_andres)
[0m20:56:17.354778 [debug] [ThreadPool]: Using snowflake connection "list_DATAEXPERT_STUDENT_andres"
[0m20:56:17.355237 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "connection_name": "list_DATAEXPERT_STUDENT_andres"} */
show objects in DATAEXPERT_STUDENT.andres limit 10000
[0m20:56:17.355650 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m20:56:18.078614 [debug] [ThreadPool]: SQL status: SUCCESS 22 in 0.723 seconds
[0m20:56:18.082285 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: Close
[0m20:56:18.314349 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '0a106dc8-8a13-4eeb-bd0f-929086ef106a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10cff3390>]}
[0m20:56:18.316257 [debug] [Thread-1 (]: Began running node model.dbt_capstone_andres.weekly_metrics
[0m20:56:18.316739 [info ] [Thread-1 (]: 1 of 1 START sql view model andres.weekly_metrics .............................. [RUN]
[0m20:56:18.317176 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_DATAEXPERT_STUDENT_andres, now model.dbt_capstone_andres.weekly_metrics)
[0m20:56:18.317722 [debug] [Thread-1 (]: Began compiling node model.dbt_capstone_andres.weekly_metrics
[0m20:56:18.328409 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_capstone_andres.weekly_metrics"
[0m20:56:18.329651 [debug] [Thread-1 (]: Began executing node model.dbt_capstone_andres.weekly_metrics
[0m20:56:18.360659 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_capstone_andres.weekly_metrics"
[0m20:56:18.362618 [debug] [Thread-1 (]: Using snowflake connection "model.dbt_capstone_andres.weekly_metrics"
[0m20:56:18.363075 [debug] [Thread-1 (]: On model.dbt_capstone_andres.weekly_metrics: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "model.dbt_capstone_andres.weekly_metrics"} */
create or replace   view DATAEXPERT_STUDENT.andres.weekly_metrics
  
   as (
    with pol as (
SELECT week_start_date, 
sum(trading_volume) trading_volume,
sum(number_of_trades)
from  DATAEXPERT_STUDENT.andres.stg_daily_bars a 
JOIN DATAEXPERT_STUDENT.andres.dim_date d on d.date_day=a.date
group by 1)

, rev as (
select week_start_date,
sum(amount) amount_millions,
sum(case when tax_category = 'IRS Tax' then amount else 0 end) as irs_tax_amount_millions, 
sum(case when tax_category = 'Non-Tax' then amount else 0 end) as non_tax_amount,
sum(case when tax_category = 'IRS Non-Tax' then amount else 0 end) as irs_non_tax_amount
FROM DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections a 
JOIN DATAEXPERT_STUDENT.andres.dim_date d on d.date_day=a.collection_date
GROUP BY ALL)

select a.week_start_date, a.trading_volume/1000000000, b.amount_millions/1000000000 
from pol a 
INNER JOIN rev b on b.week_start_date = a.week_start_date
;
[0m20:56:18.363508 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m20:56:19.026122 [debug] [Thread-1 (]: Snowflake adapter: Snowflake query id: 01b87734-0004-351a-0000-33eb001807f2
[0m20:56:19.026708 [debug] [Thread-1 (]: Snowflake adapter: Snowflake error: 001003 (42000): SQL compilation error:
syntax error line 25 at position 0 unexpected ';'.
[0m20:56:19.027338 [debug] [Thread-1 (]: On model.dbt_capstone_andres.weekly_metrics: Close
[0m20:56:19.245643 [debug] [Thread-1 (]: Database Error in model weekly_metrics (models/marts/weekly_metrics.sql)
  001003 (42000): SQL compilation error:
  syntax error line 25 at position 0 unexpected ';'.
  compiled code at target/run/dbt_capstone_andres/models/marts/weekly_metrics.sql
[0m20:56:19.247637 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0a106dc8-8a13-4eeb-bd0f-929086ef106a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x117105990>]}
[0m20:56:19.248441 [error] [Thread-1 (]: 1 of 1 ERROR creating sql view model andres.weekly_metrics ..................... [[31mERROR[0m in 0.93s]
[0m20:56:19.249237 [debug] [Thread-1 (]: Finished running node model.dbt_capstone_andres.weekly_metrics
[0m20:56:19.249948 [debug] [Thread-4 (]: Marking all children of 'model.dbt_capstone_andres.weekly_metrics' to be skipped because of status 'error'.  Reason: Database Error in model weekly_metrics (models/marts/weekly_metrics.sql)
  001003 (42000): SQL compilation error:
  syntax error line 25 at position 0 unexpected ';'.
  compiled code at target/run/dbt_capstone_andres/models/marts/weekly_metrics.sql.
[0m20:56:19.251622 [debug] [MainThread]: Connection 'master' was properly closed.
[0m20:56:19.252052 [debug] [MainThread]: Connection 'model.dbt_capstone_andres.weekly_metrics' was properly closed.
[0m20:56:19.252487 [info ] [MainThread]: 
[0m20:56:19.253005 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 2.93 seconds (2.93s).
[0m20:56:19.253884 [debug] [MainThread]: Command end result
[0m20:56:19.307380 [info ] [MainThread]: 
[0m20:56:19.307902 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successs, and 0 warnings:[0m
[0m20:56:19.308286 [info ] [MainThread]: 
[0m20:56:19.308725 [error] [MainThread]:   Database Error in model weekly_metrics (models/marts/weekly_metrics.sql)
  001003 (42000): SQL compilation error:
  syntax error line 25 at position 0 unexpected ';'.
  compiled code at target/run/dbt_capstone_andres/models/marts/weekly_metrics.sql
[0m20:56:19.309090 [info ] [MainThread]: 
[0m20:56:19.309467 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m20:56:19.311319 [debug] [MainThread]: Resource report: {"command_name": "build", "command_wall_clock_time": 4.702451, "process_user_time": 2.935829, "process_kernel_time": 0.491428, "process_mem_max_rss": "192909312", "command_success": false, "process_in_blocks": "0", "process_out_blocks": "0"}
[0m20:56:19.311978 [debug] [MainThread]: Command `dbt build` failed at 20:56:19.311846 after 4.70 seconds
[0m20:56:19.312559 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1069f0ad0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106a375d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x102d7cc90>]}
[0m20:56:19.313025 [debug] [MainThread]: Flushing usage events
[0m20:56:19.487352 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m20:56:29.769562 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a1b3cd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a222b10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a223110>]}


============================== 20:56:29.774376 | 0b985737-a010-4407-8de5-15045f7546e1 ==============================
[0m20:56:29.774376 [info ] [MainThread]: Running with dbt=1.9.0-b3
[0m20:56:29.775273 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'fail_fast': 'False', 'log_path': 'logs', 'profiles_dir': '.', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'log_format': 'default', 'invocation_command': 'dbt build -s weekly_metrics', 'introspect': 'True', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m20:56:30.666059 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '0b985737-a010-4407-8de5-15045f7546e1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10f02a750>]}
[0m20:56:30.730508 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '0b985737-a010-4407-8de5-15045f7546e1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108e50b90>]}
[0m20:56:30.731536 [info ] [MainThread]: Registered adapter: snowflake=1.8.4
[0m20:56:30.931456 [debug] [MainThread]: checksum: 5f4fde221ca8627174a0341efc73099aaec05b8546408ceed6f59b1ec0755115, vars: {}, profile: , target: , version: 1.9.0b3
[0m20:56:31.111496 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m20:56:31.112119 [debug] [MainThread]: Partial parsing: updated file: dbt_capstone_andres://models/marts/weekly_metrics.sql
[0m20:56:31.337761 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.andres_capstone_dbt.example
[0m20:56:31.429245 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '0b985737-a010-4407-8de5-15045f7546e1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110b60410>]}
[0m20:56:31.560127 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '0b985737-a010-4407-8de5-15045f7546e1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1083ea810>]}
[0m20:56:31.560709 [info ] [MainThread]: Found 6 models, 4 data tests, 2 sources, 862 macros
[0m20:56:31.562341 [info ] [MainThread]: 
[0m20:56:31.562735 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m20:56:31.563055 [info ] [MainThread]: 
[0m20:56:31.563538 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m20:56:31.564302 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_DATAEXPERT_STUDENT'
[0m20:56:31.578380 [debug] [ThreadPool]: Using snowflake connection "list_DATAEXPERT_STUDENT"
[0m20:56:31.578952 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "connection_name": "list_DATAEXPERT_STUDENT"} */
show terse schemas in database DATAEXPERT_STUDENT
    limit 10000
[0m20:56:31.579320 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:56:32.273268 [debug] [ThreadPool]: SQL status: SUCCESS 68 in 0.694 seconds
[0m20:56:32.276146 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT: Close
[0m20:56:32.481406 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_DATAEXPERT_STUDENT, now list_DATAEXPERT_STUDENT_andres)
[0m20:56:32.492491 [debug] [ThreadPool]: Using snowflake connection "list_DATAEXPERT_STUDENT_andres"
[0m20:56:32.492989 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "connection_name": "list_DATAEXPERT_STUDENT_andres"} */
show objects in DATAEXPERT_STUDENT.andres limit 10000
[0m20:56:32.493354 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m20:56:33.094282 [debug] [ThreadPool]: SQL status: SUCCESS 22 in 0.601 seconds
[0m20:56:33.096526 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: Close
[0m20:56:33.337355 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '0b985737-a010-4407-8de5-15045f7546e1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109c0ad90>]}
[0m20:56:33.339685 [debug] [Thread-1 (]: Began running node model.dbt_capstone_andres.weekly_metrics
[0m20:56:33.340328 [info ] [Thread-1 (]: 1 of 1 START sql view model andres.weekly_metrics .............................. [RUN]
[0m20:56:33.340951 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_DATAEXPERT_STUDENT_andres, now model.dbt_capstone_andres.weekly_metrics)
[0m20:56:33.341412 [debug] [Thread-1 (]: Began compiling node model.dbt_capstone_andres.weekly_metrics
[0m20:56:33.350787 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_capstone_andres.weekly_metrics"
[0m20:56:33.351600 [debug] [Thread-1 (]: Began executing node model.dbt_capstone_andres.weekly_metrics
[0m20:56:33.382381 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_capstone_andres.weekly_metrics"
[0m20:56:33.384434 [debug] [Thread-1 (]: Using snowflake connection "model.dbt_capstone_andres.weekly_metrics"
[0m20:56:33.384895 [debug] [Thread-1 (]: On model.dbt_capstone_andres.weekly_metrics: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "model.dbt_capstone_andres.weekly_metrics"} */
create or replace   view DATAEXPERT_STUDENT.andres.weekly_metrics
  
   as (
    with pol as (
SELECT week_start_date, 
sum(trading_volume) trading_volume,
sum(number_of_trades)
from  DATAEXPERT_STUDENT.andres.stg_daily_bars a 
JOIN DATAEXPERT_STUDENT.andres.dim_date d on d.date_day=a.date
group by 1)

, rev as (
select week_start_date,
sum(amount) amount_millions,
sum(case when tax_category = 'IRS Tax' then amount else 0 end) as irs_tax_amount_millions, 
sum(case when tax_category = 'Non-Tax' then amount else 0 end) as non_tax_amount,
sum(case when tax_category = 'IRS Non-Tax' then amount else 0 end) as irs_non_tax_amount
FROM DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections a 
JOIN DATAEXPERT_STUDENT.andres.dim_date d on d.date_day=a.collection_date
GROUP BY ALL)

select a.week_start_date, a.trading_volume/1000000000, b.amount_millions/1000000000 
from pol a 
INNER JOIN rev b on b.week_start_date = a.week_start_date
  );
[0m20:56:33.385354 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m20:56:34.139716 [debug] [Thread-1 (]: Snowflake adapter: Snowflake query id: 01b87734-0004-351a-0000-33eb00180806
[0m20:56:34.140355 [debug] [Thread-1 (]: Snowflake adapter: Snowflake error: 002022 (42601): SQL compilation error:
Missing column specification
[0m20:56:34.140891 [debug] [Thread-1 (]: On model.dbt_capstone_andres.weekly_metrics: Close
[0m20:56:34.368198 [debug] [Thread-1 (]: Database Error in model weekly_metrics (models/marts/weekly_metrics.sql)
  002022 (42601): SQL compilation error:
  Missing column specification
  compiled code at target/run/dbt_capstone_andres/models/marts/weekly_metrics.sql
[0m20:56:34.369711 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0b985737-a010-4407-8de5-15045f7546e1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1149cb310>]}
[0m20:56:34.370320 [error] [Thread-1 (]: 1 of 1 ERROR creating sql view model andres.weekly_metrics ..................... [[31mERROR[0m in 1.03s]
[0m20:56:34.370897 [debug] [Thread-1 (]: Finished running node model.dbt_capstone_andres.weekly_metrics
[0m20:56:34.371432 [debug] [Thread-4 (]: Marking all children of 'model.dbt_capstone_andres.weekly_metrics' to be skipped because of status 'error'.  Reason: Database Error in model weekly_metrics (models/marts/weekly_metrics.sql)
  002022 (42601): SQL compilation error:
  Missing column specification
  compiled code at target/run/dbt_capstone_andres/models/marts/weekly_metrics.sql.
[0m20:56:34.372643 [debug] [MainThread]: Connection 'master' was properly closed.
[0m20:56:34.372951 [debug] [MainThread]: Connection 'model.dbt_capstone_andres.weekly_metrics' was properly closed.
[0m20:56:34.373268 [info ] [MainThread]: 
[0m20:56:34.373603 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 2.81 seconds (2.81s).
[0m20:56:34.374221 [debug] [MainThread]: Command end result
[0m20:56:34.422295 [info ] [MainThread]: 
[0m20:56:34.422837 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successs, and 0 warnings:[0m
[0m20:56:34.423214 [info ] [MainThread]: 
[0m20:56:34.423636 [error] [MainThread]:   Database Error in model weekly_metrics (models/marts/weekly_metrics.sql)
  002022 (42601): SQL compilation error:
  Missing column specification
  compiled code at target/run/dbt_capstone_andres/models/marts/weekly_metrics.sql
[0m20:56:34.423998 [info ] [MainThread]: 
[0m20:56:34.424374 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m20:56:34.426169 [debug] [MainThread]: Resource report: {"command_name": "build", "command_wall_clock_time": 4.7235613, "process_user_time": 3.108095, "process_kernel_time": 0.478747, "process_mem_max_rss": "196026368", "command_success": false, "process_in_blocks": "0", "process_out_blocks": "0"}
[0m20:56:34.426876 [debug] [MainThread]: Command `dbt build` failed at 20:56:34.426762 after 4.72 seconds
[0m20:56:34.427369 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a237f90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a237850>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1064e8c90>]}
[0m20:56:34.427877 [debug] [MainThread]: Flushing usage events
[0m20:56:34.642047 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m20:57:04.344061 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10479b810>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10480aad0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10480b0d0>]}


============================== 20:57:04.348495 | 3d1233ff-9cc4-4912-9e1c-9d4ddd93eb0e ==============================
[0m20:57:04.348495 [info ] [MainThread]: Running with dbt=1.9.0-b3
[0m20:57:04.349221 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'fail_fast': 'False', 'version_check': 'True', 'log_path': 'logs', 'warn_error': 'None', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt build -s weekly_metrics', 'static_parser': 'True', 'log_format': 'default', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m20:57:05.191794 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '3d1233ff-9cc4-4912-9e1c-9d4ddd93eb0e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109ce7690>]}
[0m20:57:05.248528 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '3d1233ff-9cc4-4912-9e1c-9d4ddd93eb0e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1046a4850>]}
[0m20:57:05.249393 [info ] [MainThread]: Registered adapter: snowflake=1.8.4
[0m20:57:05.426606 [debug] [MainThread]: checksum: 5f4fde221ca8627174a0341efc73099aaec05b8546408ceed6f59b1ec0755115, vars: {}, profile: , target: , version: 1.9.0b3
[0m20:57:05.603429 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m20:57:05.604123 [debug] [MainThread]: Partial parsing: updated file: dbt_capstone_andres://models/marts/weekly_metrics.sql
[0m20:57:05.843211 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.andres_capstone_dbt.example
[0m20:57:05.939107 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '3d1233ff-9cc4-4912-9e1c-9d4ddd93eb0e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ebea350>]}
[0m20:57:06.074299 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '3d1233ff-9cc4-4912-9e1c-9d4ddd93eb0e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10aa2e290>]}
[0m20:57:06.074898 [info ] [MainThread]: Found 6 models, 4 data tests, 2 sources, 862 macros
[0m20:57:06.076682 [info ] [MainThread]: 
[0m20:57:06.077115 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m20:57:06.077478 [info ] [MainThread]: 
[0m20:57:06.078102 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m20:57:06.079017 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_DATAEXPERT_STUDENT'
[0m20:57:06.094805 [debug] [ThreadPool]: Using snowflake connection "list_DATAEXPERT_STUDENT"
[0m20:57:06.095314 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "connection_name": "list_DATAEXPERT_STUDENT"} */
show terse schemas in database DATAEXPERT_STUDENT
    limit 10000
[0m20:57:06.095681 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:57:06.843501 [debug] [ThreadPool]: SQL status: SUCCESS 68 in 0.748 seconds
[0m20:57:06.846325 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT: Close
[0m20:57:07.064591 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_DATAEXPERT_STUDENT, now list_DATAEXPERT_STUDENT_andres)
[0m20:57:07.077057 [debug] [ThreadPool]: Using snowflake connection "list_DATAEXPERT_STUDENT_andres"
[0m20:57:07.077594 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "connection_name": "list_DATAEXPERT_STUDENT_andres"} */
show objects in DATAEXPERT_STUDENT.andres limit 10000
[0m20:57:07.077996 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m20:57:07.791389 [debug] [ThreadPool]: SQL status: SUCCESS 22 in 0.713 seconds
[0m20:57:07.795178 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: Close
[0m20:57:08.004756 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '3d1233ff-9cc4-4912-9e1c-9d4ddd93eb0e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ab5b090>]}
[0m20:57:08.007028 [debug] [Thread-1 (]: Began running node model.dbt_capstone_andres.weekly_metrics
[0m20:57:08.007674 [info ] [Thread-1 (]: 1 of 1 START sql view model andres.weekly_metrics .............................. [RUN]
[0m20:57:08.008276 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_DATAEXPERT_STUDENT_andres, now model.dbt_capstone_andres.weekly_metrics)
[0m20:57:08.008745 [debug] [Thread-1 (]: Began compiling node model.dbt_capstone_andres.weekly_metrics
[0m20:57:08.018814 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_capstone_andres.weekly_metrics"
[0m20:57:08.019726 [debug] [Thread-1 (]: Began executing node model.dbt_capstone_andres.weekly_metrics
[0m20:57:08.049528 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_capstone_andres.weekly_metrics"
[0m20:57:08.051878 [debug] [Thread-1 (]: Using snowflake connection "model.dbt_capstone_andres.weekly_metrics"
[0m20:57:08.052528 [debug] [Thread-1 (]: On model.dbt_capstone_andres.weekly_metrics: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "model.dbt_capstone_andres.weekly_metrics"} */
create or replace   view DATAEXPERT_STUDENT.andres.weekly_metrics
  
   as (
    with pol as (
SELECT week_start_date, 
sum(trading_volume) trading_volume,
sum(number_of_trades)
from  DATAEXPERT_STUDENT.andres.stg_daily_bars a 
JOIN DATAEXPERT_STUDENT.andres.dim_date d on d.date_day=a.date
group by 1)

, rev as (
select week_start_date,
sum(amount) amount_millions,
sum(case when tax_category = 'IRS Tax' then amount else 0 end) as irs_tax_amount_millions, 
sum(case when tax_category = 'Non-Tax' then amount else 0 end) as non_tax_amount,
sum(case when tax_category = 'IRS Non-Tax' then amount else 0 end) as irs_non_tax_amount
FROM DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections a 
JOIN DATAEXPERT_STUDENT.andres.dim_date d on d.date_day=a.collection_date
GROUP BY ALL)

select a.week_start_date, a.trading_volume/1000000000 trading_vol_billions, b.amount_millions/1000000000 amount_vol_billions
from pol a 
INNER JOIN rev b on b.week_start_date = a.week_start_date
  );
[0m20:57:08.053019 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m20:57:08.815817 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 0.763 seconds
[0m20:57:08.835701 [debug] [Thread-1 (]: On model.dbt_capstone_andres.weekly_metrics: Close
[0m20:57:09.057623 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3d1233ff-9cc4-4912-9e1c-9d4ddd93eb0e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10abb9610>]}
[0m20:57:09.058546 [info ] [Thread-1 (]: 1 of 1 OK created sql view model andres.weekly_metrics ......................... [[32mSUCCESS 1[0m in 1.05s]
[0m20:57:09.059327 [debug] [Thread-1 (]: Finished running node model.dbt_capstone_andres.weekly_metrics
[0m20:57:09.060727 [debug] [MainThread]: Connection 'master' was properly closed.
[0m20:57:09.061159 [debug] [MainThread]: Connection 'model.dbt_capstone_andres.weekly_metrics' was properly closed.
[0m20:57:09.061591 [info ] [MainThread]: 
[0m20:57:09.062053 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 2.98 seconds (2.98s).
[0m20:57:09.062896 [debug] [MainThread]: Command end result
[0m20:57:09.117657 [info ] [MainThread]: 
[0m20:57:09.118264 [info ] [MainThread]: [32mCompleted successfully[0m
[0m20:57:09.118668 [info ] [MainThread]: 
[0m20:57:09.119083 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m20:57:09.121218 [debug] [MainThread]: Resource report: {"command_name": "build", "command_success": true, "command_wall_clock_time": 4.846089, "process_user_time": 2.994912, "process_kernel_time": 0.463439, "process_mem_max_rss": "195592192", "process_in_blocks": "0", "process_out_blocks": "0"}
[0m20:57:09.122136 [debug] [MainThread]: Command `dbt build` succeeded at 20:57:09.121961 after 4.85 seconds
[0m20:57:09.122728 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x100b40d90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104823d50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104823490>]}
[0m20:57:09.123300 [debug] [MainThread]: Flushing usage events
[0m20:57:09.400988 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:15:03.682013 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c8af810>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c8ce390>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c8ce990>]}


============================== 15:15:03.687367 | 2c0883ce-c60a-419b-ba1c-36180a0ca6d5 ==============================
[0m15:15:03.687367 [info ] [MainThread]: Running with dbt=1.9.0-b3
[0m15:15:03.688061 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'fail_fast': 'False', 'warn_error': 'None', 'log_path': 'logs', 'debug': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'log_format': 'default', 'invocation_command': 'dbt compile -m weekly_metrics', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m15:15:09.713041 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '2c0883ce-c60a-419b-ba1c-36180a0ca6d5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110b6f2d0>]}
[0m15:15:09.772989 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '2c0883ce-c60a-419b-ba1c-36180a0ca6d5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112cd8550>]}
[0m15:15:09.774528 [info ] [MainThread]: Registered adapter: snowflake=1.8.4
[0m15:15:09.982433 [debug] [MainThread]: checksum: 5f4fde221ca8627174a0341efc73099aaec05b8546408ceed6f59b1ec0755115, vars: {}, profile: , target: , version: 1.9.0b3
[0m15:15:10.080389 [info ] [MainThread]: Unable to do partial parsing because a project config has changed
[0m15:15:10.080944 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '2c0883ce-c60a-419b-ba1c-36180a0ca6d5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10bb56010>]}
[0m15:15:12.204356 [debug] [MainThread]: STDERR: "Unit Test fixtures benefit from having at least one row free of Null values to ensure consistent column types. Failure to meet this recommendation can result in type mismatch errors between unit test source models and `expected` fixtures."
[0m15:15:12.321858 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '2c0883ce-c60a-419b-ba1c-36180a0ca6d5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x116ef7390>]}
[0m15:15:12.458732 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '2c0883ce-c60a-419b-ba1c-36180a0ca6d5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x116faaa90>]}
[0m15:15:12.459339 [info ] [MainThread]: Found 6 models, 4 data tests, 2 sources, 862 macros, 1 unit test
[0m15:15:12.459744 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2c0883ce-c60a-419b-ba1c-36180a0ca6d5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x116ec3090>]}
[0m15:15:12.461366 [info ] [MainThread]: 
[0m15:15:12.461771 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m15:15:12.462146 [info ] [MainThread]: 
[0m15:15:12.462665 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m15:15:12.468065 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_DATAEXPERT_STUDENT_andres'
[0m15:15:12.555481 [debug] [ThreadPool]: Using snowflake connection "list_DATAEXPERT_STUDENT_andres"
[0m15:15:12.556065 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "connection_name": "list_DATAEXPERT_STUDENT_andres"} */
show objects in DATAEXPERT_STUDENT.andres limit 10000
[0m15:15:12.556510 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:15:15.403135 [debug] [ThreadPool]: SQL status: SUCCESS 25 in 2.847 seconds
[0m15:15:15.405990 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: Close
[0m15:15:15.638750 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2c0883ce-c60a-419b-ba1c-36180a0ca6d5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a94a950>]}
[0m15:15:15.641966 [debug] [Thread-1 (]: Began running node model.dbt_capstone_andres.weekly_metrics
[0m15:15:15.642396 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_DATAEXPERT_STUDENT_andres, now model.dbt_capstone_andres.weekly_metrics)
[0m15:15:15.642750 [debug] [Thread-1 (]: Began compiling node model.dbt_capstone_andres.weekly_metrics
[0m15:15:15.651271 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_capstone_andres.weekly_metrics"
[0m15:15:15.652596 [debug] [Thread-1 (]: Began executing node model.dbt_capstone_andres.weekly_metrics
[0m15:15:15.653245 [debug] [Thread-1 (]: Finished running node model.dbt_capstone_andres.weekly_metrics
[0m15:15:15.654368 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:15:15.654670 [debug] [MainThread]: Connection 'model.dbt_capstone_andres.weekly_metrics' was properly closed.
[0m15:15:15.655230 [debug] [MainThread]: Command end result
[0m15:15:15.701629 [info ] [MainThread]: Compiled node 'weekly_metrics' is:
with pol as (
SELECT week_start_date, 
sum(number_of_shares_traded) number_of_shares_traded,
sum(number_of_trades) number_of_trades
from  DATAEXPERT_STUDENT.andres.stg_daily_bars a 
JOIN DATAEXPERT_STUDENT.andres.dim_date d on d.date_day=a.date
group by 1)

, rev as (
select week_start_date,
sum(dollars_collected_by_treasury) dollars_collected_by_treasury,
sum(case when tax_category = 'IRS Tax' then dollars_collected_by_treasury else 0 end) as irs_tax_dollars_collected, 
sum(case when tax_category = 'Non-Tax' then dollars_collected_by_treasury else 0 end) as non_irs_non_tax_dollars,
sum(case when tax_category = 'IRS Non-Tax' then dollars_collected_by_treasury else 0 end) as irs_non_tax_dollars
FROM DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections a 
JOIN DATAEXPERT_STUDENT.andres.dim_date d on d.date_day=a.collection_date
GROUP BY ALL)

select 
    a.week_start_date, 
    a.number_of_shares_traded, 
    a.number_of_trades,
    b.dollars_collected_by_treasury,
    b.irs_tax_dollars

from pol a 
INNER JOIN rev b on b.week_start_date = a.week_start_date
[0m15:15:15.707368 [debug] [MainThread]: Resource report: {"command_name": "compile", "command_success": true, "command_wall_clock_time": 12.109072, "process_user_time": 5.048458, "process_kernel_time": 0.79657, "process_mem_max_rss": "196296704", "process_in_blocks": "0", "process_out_blocks": "0"}
[0m15:15:15.708678 [debug] [MainThread]: Command `dbt compile` succeeded at 15:15:15.708346 after 12.11 seconds
[0m15:15:15.709301 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108c74d90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108c1ea10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108c662d0>]}
[0m15:15:15.709677 [debug] [MainThread]: Flushing usage events
[0m15:15:16.017506 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:17:07.388290 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10f1b3590>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10f1d6890>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10f1d6e90>]}


============================== 15:17:07.394573 | ad1bdf79-0f15-4720-b79c-b6347c53530c ==============================
[0m15:17:07.394573 [info ] [MainThread]: Running with dbt=1.9.0-b3
[0m15:17:07.395290 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'version_check': 'True', 'debug': 'False', 'log_path': 'logs', 'fail_fast': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'invocation_command': 'dbt build', 'send_anonymous_usage_stats': 'True'}
[0m15:17:08.922421 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'ad1bdf79-0f15-4720-b79c-b6347c53530c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10de581d0>]}
[0m15:17:08.978046 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'ad1bdf79-0f15-4720-b79c-b6347c53530c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10de58f10>]}
[0m15:17:08.979232 [info ] [MainThread]: Registered adapter: snowflake=1.8.4
[0m15:17:09.179361 [debug] [MainThread]: checksum: 5f4fde221ca8627174a0341efc73099aaec05b8546408ceed6f59b1ec0755115, vars: {}, profile: , target: , version: 1.9.0b3
[0m15:17:09.435382 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m15:17:09.435924 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m15:17:09.492900 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'ad1bdf79-0f15-4720-b79c-b6347c53530c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x115871cd0>]}
[0m15:17:09.730686 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'ad1bdf79-0f15-4720-b79c-b6347c53530c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11545b190>]}
[0m15:17:09.731389 [info ] [MainThread]: Found 6 models, 4 data tests, 2 sources, 862 macros, 1 unit test
[0m15:17:09.734384 [info ] [MainThread]: 
[0m15:17:09.734904 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m15:17:09.735361 [info ] [MainThread]: 
[0m15:17:09.736059 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m15:17:09.742221 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_DATAEXPERT_STUDENT'
[0m15:17:09.758703 [debug] [ThreadPool]: Using snowflake connection "list_DATAEXPERT_STUDENT"
[0m15:17:09.759165 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "connection_name": "list_DATAEXPERT_STUDENT"} */
show terse schemas in database DATAEXPERT_STUDENT
    limit 10000
[0m15:17:09.759536 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:17:10.687767 [debug] [ThreadPool]: SQL status: SUCCESS 73 in 0.928 seconds
[0m15:17:10.691537 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT: Close
[0m15:17:10.924067 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_DATAEXPERT_STUDENT, now list_DATAEXPERT_STUDENT_andres)
[0m15:17:10.936473 [debug] [ThreadPool]: Using snowflake connection "list_DATAEXPERT_STUDENT_andres"
[0m15:17:10.936970 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "connection_name": "list_DATAEXPERT_STUDENT_andres"} */
show objects in DATAEXPERT_STUDENT.andres limit 10000
[0m15:17:10.937384 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:17:11.687914 [debug] [ThreadPool]: SQL status: SUCCESS 25 in 0.750 seconds
[0m15:17:11.691034 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: Close
[0m15:17:11.931495 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ad1bdf79-0f15-4720-b79c-b6347c53530c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112571810>]}
[0m15:17:11.935039 [debug] [Thread-1 (]: Began running node model.dbt_capstone_andres.dim_date
[0m15:17:11.935696 [info ] [Thread-1 (]: 1 of 11 START sql table model andres.dim_date .................................. [RUN]
[0m15:17:11.936262 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_DATAEXPERT_STUDENT_andres, now model.dbt_capstone_andres.dim_date)
[0m15:17:11.936710 [debug] [Thread-1 (]: Began compiling node model.dbt_capstone_andres.dim_date
[0m15:17:11.980340 [debug] [Thread-1 (]: Using snowflake connection "model.dbt_capstone_andres.dim_date"
[0m15:17:11.980993 [debug] [Thread-1 (]: On model.dbt_capstone_andres.dim_date: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "model.dbt_capstone_andres.dim_date"} */
select datediff(
        day,
        cast('1995-01-01' as timestamp),
        cast('2040-12-31' as timestamp)
        )
[0m15:17:11.981515 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:17:12.616876 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 0.635 seconds
[0m15:17:12.681293 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_capstone_andres.dim_date"
[0m15:17:12.683241 [debug] [Thread-1 (]: Began executing node model.dbt_capstone_andres.dim_date
[0m15:17:12.710030 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_capstone_andres.dim_date"
[0m15:17:12.716818 [debug] [Thread-1 (]: Using snowflake connection "model.dbt_capstone_andres.dim_date"
[0m15:17:12.717502 [debug] [Thread-1 (]: On model.dbt_capstone_andres.dim_date: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "model.dbt_capstone_andres.dim_date"} */
create or replace transient table DATAEXPERT_STUDENT.andres.dim_date
         as
        (

    
with base_dates as (
    
    with date_spine as
(

    





with rawdata as (

    

    

    with p as (
        select 0 as generated_number union all select 1
    ), unioned as (

    select

    
    p0.generated_number * power(2, 0)
     + 
    
    p1.generated_number * power(2, 1)
     + 
    
    p2.generated_number * power(2, 2)
     + 
    
    p3.generated_number * power(2, 3)
     + 
    
    p4.generated_number * power(2, 4)
     + 
    
    p5.generated_number * power(2, 5)
     + 
    
    p6.generated_number * power(2, 6)
     + 
    
    p7.generated_number * power(2, 7)
     + 
    
    p8.generated_number * power(2, 8)
     + 
    
    p9.generated_number * power(2, 9)
     + 
    
    p10.generated_number * power(2, 10)
     + 
    
    p11.generated_number * power(2, 11)
     + 
    
    p12.generated_number * power(2, 12)
     + 
    
    p13.generated_number * power(2, 13)
     + 
    
    p14.generated_number * power(2, 14)
    
    
    + 1
    as generated_number

    from

    
    p as p0
     cross join 
    
    p as p1
     cross join 
    
    p as p2
     cross join 
    
    p as p3
     cross join 
    
    p as p4
     cross join 
    
    p as p5
     cross join 
    
    p as p6
     cross join 
    
    p as p7
     cross join 
    
    p as p8
     cross join 
    
    p as p9
     cross join 
    
    p as p10
     cross join 
    
    p as p11
     cross join 
    
    p as p12
     cross join 
    
    p as p13
     cross join 
    
    p as p14
    
    

    )

    select *
    from unioned
    where generated_number <= 16801
    order by generated_number



),

all_periods as (

    select (
        

    dateadd(
        day,
        (row_number() over (order by 1) - 1),
        cast('1995-01-01' as timestamp)
        )


    ) as date_day
    from rawdata

),

filtered as (

    select *
    from all_periods
    where date_day <= cast('2040-12-31' as timestamp)

)

select * from filtered



)
select
    cast(d.date_day as timestamp) as date_day
from
    date_spine d


),
dates_with_prior_year_dates as (

    select
        cast(d.date_day as date) as date_day,
        cast(

    dateadd(
        year,
        -1,
        d.date_day
        )

 as date) as prior_year_date_day,
        cast(

    dateadd(
        day,
        -364,
        d.date_day
        )

 as date) as prior_year_over_year_date_day
    from
    	base_dates d

)
select
    d.date_day,
    cast(

    dateadd(
        day,
        -1,
        d.date_day
        )

 as date) as prior_date_day,
    cast(

    dateadd(
        day,
        1,
        d.date_day
        )

 as date) as next_date_day,
    d.prior_year_date_day as prior_year_date_day,
    d.prior_year_over_year_date_day,
    case
            when date_part('dayofweek', d.date_day) = 7 then 1
            else date_part('dayofweek', d.date_day) + 1
        end as day_of_week,
    date_part('dayofweekiso', d.date_day) as day_of_week_iso,
    -- long version not implemented on Snowflake so we're doing it manually :/
    case dayname(d.date_day)
        when 'Mon' then 'Monday'
        when 'Tue' then 'Tuesday'
        when 'Wed' then 'Wednesday'
        when 'Thu' then 'Thursday'
        when 'Fri' then 'Friday'
        when 'Sat' then 'Saturday'
        when 'Sun' then 'Sunday'
    end as day_of_week_name,
    dayname(d.date_day) as day_of_week_name_short,
    date_part('day', d.date_day) as day_of_month,
    date_part('dayofyear', d.date_day) as day_of_year,

    
    
    cast(

    dateadd(
        day,
        -1 * (case
            when date_part('dayofweek', d.date_day) = 7 then 1
            else date_part('dayofweek', d.date_day) + 1
        end - 1),
        d.date_day
        )

 as date) as week_start_date,
    cast(

    dateadd(
        day,
        6,
        
    
    cast(

    dateadd(
        day,
        -1 * (case
            when date_part('dayofweek', d.date_day) = 7 then 1
            else date_part('dayofweek', d.date_day) + 1
        end - 1),
        d.date_day
        )

 as date)
        )

 as date) as week_end_date,
    
    
    cast(

    dateadd(
        day,
        -1 * (case
            when date_part('dayofweek', d.prior_year_over_year_date_day) = 7 then 1
            else date_part('dayofweek', d.prior_year_over_year_date_day) + 1
        end - 1),
        d.prior_year_over_year_date_day
        )

 as date) as prior_year_week_start_date,
    cast(

    dateadd(
        day,
        6,
        
    
    cast(

    dateadd(
        day,
        -1 * (case
            when date_part('dayofweek', d.prior_year_over_year_date_day) = 7 then 1
            else date_part('dayofweek', d.prior_year_over_year_date_day) + 1
        end - 1),
        d.prior_year_over_year_date_day
        )

 as date)
        )

 as date) as prior_year_week_end_date,
    cast(date_part('week', d.date_day) as integer) as week_of_year,

    cast(date_trunc('week', d.date_day) as date) as iso_week_start_date,
    cast(

    dateadd(
        day,
        6,
        cast(date_trunc('week', d.date_day) as date)
        )

 as date) as iso_week_end_date,
    cast(date_trunc('week', d.prior_year_over_year_date_day) as date) as prior_year_iso_week_start_date,
    cast(

    dateadd(
        day,
        6,
        cast(date_trunc('week', d.prior_year_over_year_date_day) as date)
        )

 as date) as prior_year_iso_week_end_date,
    cast(date_part('weekiso', d.date_day) as integer) as iso_week_of_year,

    cast(date_part('week', d.prior_year_over_year_date_day) as integer) as prior_year_week_of_year,
    cast(date_part('weekiso', d.prior_year_over_year_date_day) as integer) as prior_year_iso_week_of_year,

    cast(date_part('month', d.date_day) as integer) as month_of_year,
    to_char(d.date_day, 'MMMM')  as month_name,
    to_char(d.date_day, 'MON')  as month_name_short,

    cast(date_trunc('month', d.date_day) as date) as month_start_date,
    cast(cast(
        

    dateadd(
        day,
        -1,
        

    dateadd(
        month,
        1,
        date_trunc('month', d.date_day)
        )


        )


        as date) as date) as month_end_date,

    cast(date_trunc('month', d.prior_year_date_day) as date) as prior_year_month_start_date,
    cast(cast(
        

    dateadd(
        day,
        -1,
        

    dateadd(
        month,
        1,
        date_trunc('month', d.prior_year_date_day)
        )


        )


        as date) as date) as prior_year_month_end_date,

    cast(date_part('quarter', d.date_day) as integer) as quarter_of_year,
    cast(date_trunc('quarter', d.date_day) as date) as quarter_start_date,
    cast(cast(
        

    dateadd(
        day,
        -1,
        

    dateadd(
        quarter,
        1,
        date_trunc('quarter', d.date_day)
        )


        )


        as date) as date) as quarter_end_date,

    cast(date_part('year', d.date_day) as integer) as year_number,
    cast(date_trunc('year', d.date_day) as date) as year_start_date,
    cast(cast(
        

    dateadd(
        day,
        -1,
        

    dateadd(
        year,
        1,
        date_trunc('year', d.date_day)
        )


        )


        as date) as date) as year_end_date
from
    dates_with_prior_year_dates d
order by 1


        );
[0m15:17:14.253027 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 1.535 seconds
[0m15:17:14.280543 [debug] [Thread-1 (]: On model.dbt_capstone_andres.dim_date: Close
[0m15:17:14.511295 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ad1bdf79-0f15-4720-b79c-b6347c53530c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119796b10>]}
[0m15:17:14.512333 [info ] [Thread-1 (]: 1 of 11 OK created sql table model andres.dim_date ............................. [[32mSUCCESS 1[0m in 2.57s]
[0m15:17:14.513155 [debug] [Thread-1 (]: Finished running node model.dbt_capstone_andres.dim_date
[0m15:17:14.513688 [debug] [Thread-1 (]: Began running node model.dbt_capstone_andres.my_first_dbt_model
[0m15:17:14.514514 [info ] [Thread-1 (]: 2 of 11 START sql table model andres.my_first_dbt_model ........................ [RUN]
[0m15:17:14.515931 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_capstone_andres.dim_date, now model.dbt_capstone_andres.my_first_dbt_model)
[0m15:17:14.516666 [debug] [Thread-1 (]: Began compiling node model.dbt_capstone_andres.my_first_dbt_model
[0m15:17:14.521309 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_capstone_andres.my_first_dbt_model"
[0m15:17:14.523724 [debug] [Thread-1 (]: Began executing node model.dbt_capstone_andres.my_first_dbt_model
[0m15:17:14.529133 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_capstone_andres.my_first_dbt_model"
[0m15:17:14.530783 [debug] [Thread-1 (]: Using snowflake connection "model.dbt_capstone_andres.my_first_dbt_model"
[0m15:17:14.531396 [debug] [Thread-1 (]: On model.dbt_capstone_andres.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "model.dbt_capstone_andres.my_first_dbt_model"} */
create or replace transient table DATAEXPERT_STUDENT.andres.my_first_dbt_model
         as
        (/*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
        );
[0m15:17:14.531921 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:17:15.888099 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 1.356 seconds
[0m15:17:15.891577 [debug] [Thread-1 (]: On model.dbt_capstone_andres.my_first_dbt_model: Close
[0m15:17:16.139806 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ad1bdf79-0f15-4720-b79c-b6347c53530c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11f9a2710>]}
[0m15:17:16.140646 [info ] [Thread-1 (]: 2 of 11 OK created sql table model andres.my_first_dbt_model ................... [[32mSUCCESS 1[0m in 1.62s]
[0m15:17:16.141343 [debug] [Thread-1 (]: Finished running node model.dbt_capstone_andres.my_first_dbt_model
[0m15:17:16.141909 [debug] [Thread-1 (]: Began running node model.dbt_capstone_andres.stg_daily_bars
[0m15:17:16.142658 [info ] [Thread-1 (]: 3 of 11 START sql table model andres.stg_daily_bars ............................ [RUN]
[0m15:17:16.143197 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_capstone_andres.my_first_dbt_model, now model.dbt_capstone_andres.stg_daily_bars)
[0m15:17:16.143659 [debug] [Thread-1 (]: Began compiling node model.dbt_capstone_andres.stg_daily_bars
[0m15:17:16.148834 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_capstone_andres.stg_daily_bars"
[0m15:17:16.150066 [debug] [Thread-1 (]: Began executing node model.dbt_capstone_andres.stg_daily_bars
[0m15:17:16.154309 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_capstone_andres.stg_daily_bars"
[0m15:17:16.155875 [debug] [Thread-1 (]: Using snowflake connection "model.dbt_capstone_andres.stg_daily_bars"
[0m15:17:16.156361 [debug] [Thread-1 (]: On model.dbt_capstone_andres.stg_daily_bars: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "model.dbt_capstone_andres.stg_daily_bars"} */
create or replace transient table DATAEXPERT_STUDENT.andres.stg_daily_bars
         as
        (with source as (

    select * from dataexpert_student.andres.daily_bars

),

renamed as (

    select
        TO_DATE(TO_TIMESTAMP(timestamp / 1000)) as date,
        ticker as ticker,
        v  as number_of_shares_traded,
        n  as number_of_trades,
        round(vw,2) as volume_weighted_avg_price,
        o  as open_price,
        c  as close_price,
        h  as high_price,
        l  as low_price
    from source

)

select * from renamed
        );
[0m15:17:16.156837 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:17:18.648154 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 2.491 seconds
[0m15:17:18.650833 [debug] [Thread-1 (]: On model.dbt_capstone_andres.stg_daily_bars: Close
[0m15:17:18.868972 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ad1bdf79-0f15-4720-b79c-b6347c53530c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11f9d7850>]}
[0m15:17:18.869979 [info ] [Thread-1 (]: 3 of 11 OK created sql table model andres.stg_daily_bars ....................... [[32mSUCCESS 1[0m in 2.73s]
[0m15:17:18.870874 [debug] [Thread-1 (]: Finished running node model.dbt_capstone_andres.stg_daily_bars
[0m15:17:18.871506 [debug] [Thread-1 (]: Began running node model.dbt_capstone_andres.stg_treasury_revenue_collections
[0m15:17:18.872218 [info ] [Thread-1 (]: 4 of 11 START sql table model andres.stg_treasury_revenue_collections .......... [RUN]
[0m15:17:18.872801 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_capstone_andres.stg_daily_bars, now model.dbt_capstone_andres.stg_treasury_revenue_collections)
[0m15:17:18.873299 [debug] [Thread-1 (]: Began compiling node model.dbt_capstone_andres.stg_treasury_revenue_collections
[0m15:17:18.877627 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_capstone_andres.stg_treasury_revenue_collections"
[0m15:17:18.878498 [debug] [Thread-1 (]: Began executing node model.dbt_capstone_andres.stg_treasury_revenue_collections
[0m15:17:18.882808 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_capstone_andres.stg_treasury_revenue_collections"
[0m15:17:18.884353 [debug] [Thread-1 (]: Using snowflake connection "model.dbt_capstone_andres.stg_treasury_revenue_collections"
[0m15:17:18.884871 [debug] [Thread-1 (]: On model.dbt_capstone_andres.stg_treasury_revenue_collections: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "model.dbt_capstone_andres.stg_treasury_revenue_collections"} */
create or replace transient table DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections
         as
        (with source as (

    select * from dataexpert_student.andres.fiscal_data

),

renamed as (

    select
        record_date as collection_date,
        channel_type_desc as channel_type,
        tax_category_desc as tax_category,
        round(net_collections_amt,0) as dollars_collected_by_treasury

    from source

)

select * from renamed
        );
[0m15:17:18.885365 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:17:20.293498 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 1.408 seconds
[0m15:17:20.295932 [debug] [Thread-1 (]: On model.dbt_capstone_andres.stg_treasury_revenue_collections: Close
[0m15:17:20.531754 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ad1bdf79-0f15-4720-b79c-b6347c53530c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11545b290>]}
[0m15:17:20.532449 [info ] [Thread-1 (]: 4 of 11 OK created sql table model andres.stg_treasury_revenue_collections ..... [[32mSUCCESS 1[0m in 1.66s]
[0m15:17:20.533140 [debug] [Thread-1 (]: Finished running node model.dbt_capstone_andres.stg_treasury_revenue_collections
[0m15:17:20.533536 [debug] [Thread-1 (]: Began running node test.dbt_capstone_andres.not_null_my_first_dbt_model_id.5fb22c2710
[0m15:17:20.534033 [info ] [Thread-1 (]: 5 of 11 START test not_null_my_first_dbt_model_id .............................. [RUN]
[0m15:17:20.534581 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_capstone_andres.stg_treasury_revenue_collections, now test.dbt_capstone_andres.not_null_my_first_dbt_model_id.5fb22c2710)
[0m15:17:20.534940 [debug] [Thread-1 (]: Began compiling node test.dbt_capstone_andres.not_null_my_first_dbt_model_id.5fb22c2710
[0m15:17:20.549393 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_capstone_andres.not_null_my_first_dbt_model_id.5fb22c2710"
[0m15:17:20.550201 [debug] [Thread-1 (]: Began executing node test.dbt_capstone_andres.not_null_my_first_dbt_model_id.5fb22c2710
[0m15:17:20.567703 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_capstone_andres.not_null_my_first_dbt_model_id.5fb22c2710"
[0m15:17:20.568838 [debug] [Thread-1 (]: Using snowflake connection "test.dbt_capstone_andres.not_null_my_first_dbt_model_id.5fb22c2710"
[0m15:17:20.569264 [debug] [Thread-1 (]: On test.dbt_capstone_andres.not_null_my_first_dbt_model_id.5fb22c2710: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "test.dbt_capstone_andres.not_null_my_first_dbt_model_id.5fb22c2710"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from DATAEXPERT_STUDENT.andres.my_first_dbt_model
where id is null



      
    ) dbt_internal_test
[0m15:17:20.569661 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:17:21.355616 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 0.786 seconds
[0m15:17:21.360150 [debug] [Thread-1 (]: On test.dbt_capstone_andres.not_null_my_first_dbt_model_id.5fb22c2710: Close
[0m15:17:21.614826 [error] [Thread-1 (]: 5 of 11 FAIL 1 not_null_my_first_dbt_model_id .................................. [[31mFAIL 1[0m in 1.08s]
[0m15:17:21.615876 [debug] [Thread-1 (]: Finished running node test.dbt_capstone_andres.not_null_my_first_dbt_model_id.5fb22c2710
[0m15:17:21.616527 [debug] [Thread-1 (]: Began running node test.dbt_capstone_andres.unique_my_first_dbt_model_id.16e066b321
[0m15:17:21.617186 [debug] [Thread-4 (]: Marking all children of 'test.dbt_capstone_andres.not_null_my_first_dbt_model_id.5fb22c2710' to be skipped because of status 'fail'.  Reason: Got 1 result, configured to fail if != 0.
[0m15:17:21.617636 [info ] [Thread-1 (]: 6 of 11 START test unique_my_first_dbt_model_id ................................ [RUN]
[0m15:17:21.618553 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_capstone_andres.not_null_my_first_dbt_model_id.5fb22c2710, now test.dbt_capstone_andres.unique_my_first_dbt_model_id.16e066b321)
[0m15:17:21.619027 [debug] [Thread-1 (]: Began compiling node test.dbt_capstone_andres.unique_my_first_dbt_model_id.16e066b321
[0m15:17:21.626873 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_capstone_andres.unique_my_first_dbt_model_id.16e066b321"
[0m15:17:21.627720 [debug] [Thread-1 (]: Began executing node test.dbt_capstone_andres.unique_my_first_dbt_model_id.16e066b321
[0m15:17:21.631249 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_capstone_andres.unique_my_first_dbt_model_id.16e066b321"
[0m15:17:21.632418 [debug] [Thread-1 (]: Using snowflake connection "test.dbt_capstone_andres.unique_my_first_dbt_model_id.16e066b321"
[0m15:17:21.632896 [debug] [Thread-1 (]: On test.dbt_capstone_andres.unique_my_first_dbt_model_id.16e066b321: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "test.dbt_capstone_andres.unique_my_first_dbt_model_id.16e066b321"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from DATAEXPERT_STUDENT.andres.my_first_dbt_model
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
[0m15:17:21.633347 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:17:22.501771 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 0.868 seconds
[0m15:17:22.504633 [debug] [Thread-1 (]: On test.dbt_capstone_andres.unique_my_first_dbt_model_id.16e066b321: Close
[0m15:17:22.745974 [info ] [Thread-1 (]: 6 of 11 PASS unique_my_first_dbt_model_id ...................................... [[32mPASS[0m in 1.13s]
[0m15:17:22.746960 [debug] [Thread-1 (]: Finished running node test.dbt_capstone_andres.unique_my_first_dbt_model_id.16e066b321
[0m15:17:22.747620 [debug] [Thread-1 (]: Began running node unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum
[0m15:17:22.748277 [info ] [Thread-1 (]: 7 of 11 START unit_test weekly_metrics::test_amount_sum ........................ [RUN]
[0m15:17:22.748842 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_capstone_andres.unique_my_first_dbt_model_id.16e066b321, now unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum)
[0m15:17:22.749311 [debug] [Thread-1 (]: Began compiling node unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum
[0m15:17:22.749768 [debug] [Thread-1 (]: Began executing node unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum
[0m15:17:22.784784 [debug] [Thread-1 (]: Compilation Error in unit_test test_amount_sum (models/marts/_marts.yml)
  Unit_Test 'unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum' (models/marts/_marts.yml) depends on a node named 'dim_date' which was not found
[0m15:17:22.785430 [error] [Thread-1 (]: 7 of 11 ERROR weekly_metrics::test_amount_sum .................................. [[31mERROR[0m in 0.04s]
[0m15:17:22.786092 [debug] [Thread-1 (]: Finished running node unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum
[0m15:17:22.786648 [debug] [Thread-1 (]: Marking all children of 'unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum' to be skipped because of status 'error'.  Reason: Compilation Error in unit_test test_amount_sum (models/marts/_marts.yml)
  Unit_Test 'unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum' (models/marts/_marts.yml) depends on a node named 'dim_date' which was not found.
[0m15:17:22.787129 [debug] [Thread-1 (]: Began running node model.dbt_capstone_andres.weekly_metrics
[0m15:17:22.787574 [info ] [Thread-1 (]: 9 of 11 SKIP relation andres.weekly_metrics .................................... [[33mSKIP[0m]
[0m15:17:22.788126 [debug] [Thread-1 (]: Finished running node model.dbt_capstone_andres.weekly_metrics
[0m15:17:22.788578 [debug] [Thread-1 (]: Began running node model.dbt_capstone_andres.my_second_dbt_model
[0m15:17:22.788942 [debug] [Thread-4 (]: Marking all children of 'model.dbt_capstone_andres.weekly_metrics' to be skipped because of status 'skipped'. 
[0m15:17:22.789410 [info ] [Thread-1 (]: 8 of 11 SKIP relation andres.my_second_dbt_model ............................... [[33mSKIP[0m]
[0m15:17:22.790003 [debug] [Thread-1 (]: Finished running node model.dbt_capstone_andres.my_second_dbt_model
[0m15:17:22.790412 [debug] [Thread-4 (]: Marking all children of 'model.dbt_capstone_andres.my_second_dbt_model' to be skipped because of status 'skipped'. 
[0m15:17:22.791452 [debug] [Thread-1 (]: Began running node test.dbt_capstone_andres.not_null_my_second_dbt_model_id.151b76d778
[0m15:17:22.791992 [info ] [Thread-1 (]: 10 of 11 SKIP test not_null_my_second_dbt_model_id ............................. [[33mSKIP[0m]
[0m15:17:22.792488 [debug] [Thread-1 (]: Finished running node test.dbt_capstone_andres.not_null_my_second_dbt_model_id.151b76d778
[0m15:17:22.792928 [debug] [Thread-1 (]: Began running node test.dbt_capstone_andres.unique_my_second_dbt_model_id.57a0f8c493
[0m15:17:22.793367 [info ] [Thread-1 (]: 11 of 11 SKIP test unique_my_second_dbt_model_id ............................... [[33mSKIP[0m]
[0m15:17:22.793802 [debug] [Thread-4 (]: Marking all children of 'test.dbt_capstone_andres.not_null_my_second_dbt_model_id.151b76d778' to be skipped because of status 'skipped'. 
[0m15:17:22.794229 [debug] [Thread-1 (]: Finished running node test.dbt_capstone_andres.unique_my_second_dbt_model_id.57a0f8c493
[0m15:17:22.794747 [debug] [Thread-4 (]: Marking all children of 'test.dbt_capstone_andres.unique_my_second_dbt_model_id.57a0f8c493' to be skipped because of status 'skipped'. 
[0m15:17:22.795881 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:17:22.796234 [debug] [MainThread]: Connection 'unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum' was properly closed.
[0m15:17:22.796654 [info ] [MainThread]: 
[0m15:17:22.797041 [info ] [MainThread]: Finished running 6 table models, 4 data tests, 1 unit test in 0 hours 0 minutes and 13.06 seconds (13.06s).
[0m15:17:22.799087 [debug] [MainThread]: Command end result
[0m15:17:22.849191 [info ] [MainThread]: 
[0m15:17:22.849693 [info ] [MainThread]: [31mCompleted with 2 errors, 0 partial successs, and 0 warnings:[0m
[0m15:17:22.850068 [info ] [MainThread]: 
[0m15:17:22.850496 [error] [MainThread]: [31mFailure in test not_null_my_first_dbt_model_id (models/example/schema.yml)[0m
[0m15:17:22.850998 [error] [MainThread]:   Got 1 result, configured to fail if != 0
[0m15:17:22.851418 [info ] [MainThread]: 
[0m15:17:22.851863 [info ] [MainThread]:   compiled code at target/compiled/dbt_capstone_andres/models/example/schema.yml/not_null_my_first_dbt_model_id.sql
[0m15:17:22.852233 [info ] [MainThread]: 
[0m15:17:22.852675 [error] [MainThread]:   Compilation Error in unit_test test_amount_sum (models/marts/_marts.yml)
  Unit_Test 'unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum' (models/marts/_marts.yml) depends on a node named 'dim_date' which was not found
[0m15:17:22.853244 [info ] [MainThread]: 
[0m15:17:22.853793 [info ] [MainThread]: Done. PASS=5 WARN=0 ERROR=2 SKIP=4 TOTAL=11
[0m15:17:22.856089 [debug] [MainThread]: Resource report: {"command_name": "build", "command_wall_clock_time": 15.541595, "process_user_time": 3.708343, "process_kernel_time": 0.73376, "process_mem_max_rss": "197255168", "command_success": false, "process_in_blocks": "0", "process_out_blocks": "0"}
[0m15:17:22.856763 [debug] [MainThread]: Command `dbt build` failed at 15:17:22.856653 after 15.54 seconds
[0m15:17:22.857214 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10f16cc50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10f1d7350>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b54b510>]}
[0m15:17:22.857628 [debug] [MainThread]: Flushing usage events
[0m15:17:23.120298 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:19:45.013728 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112ab7bd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112b2e950>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112b2ef50>]}


============================== 15:19:45.019198 | 4d2c7628-d1e8-47a3-a2f8-e2583df76510 ==============================
[0m15:19:45.019198 [info ] [MainThread]: Running with dbt=1.9.0-b3
[0m15:19:45.019911 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'profiles_dir': '.', 'log_path': 'logs', 'fail_fast': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt test', 'log_format': 'default', 'static_parser': 'True', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m15:19:46.702272 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '4d2c7628-d1e8-47a3-a2f8-e2583df76510', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x118e03ed0>]}
[0m15:19:46.756997 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '4d2c7628-d1e8-47a3-a2f8-e2583df76510', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x113338610>]}
[0m15:19:46.758736 [info ] [MainThread]: Registered adapter: snowflake=1.8.4
[0m15:19:46.941513 [debug] [MainThread]: checksum: 5f4fde221ca8627174a0341efc73099aaec05b8546408ceed6f59b1ec0755115, vars: {}, profile: , target: , version: 1.9.0b3
[0m15:19:47.179652 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m15:19:47.180378 [debug] [MainThread]: Partial parsing: updated file: dbt_capstone_andres://models/marts/_marts.yml
[0m15:19:47.224063 [debug] [MainThread]: STDERR: "Unit Test fixtures benefit from having at least one row free of Null values to ensure consistent column types. Failure to meet this recommendation can result in type mismatch errors between unit test source models and `expected` fixtures."
[0m15:19:47.317898 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '4d2c7628-d1e8-47a3-a2f8-e2583df76510', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11908cd90>]}
[0m15:19:47.552062 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '4d2c7628-d1e8-47a3-a2f8-e2583df76510', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11921b3d0>]}
[0m15:19:47.552715 [info ] [MainThread]: Found 6 models, 4 data tests, 2 sources, 862 macros, 1 unit test
[0m15:19:47.553128 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4d2c7628-d1e8-47a3-a2f8-e2583df76510', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1190cb590>]}
[0m15:19:47.555228 [info ] [MainThread]: 
[0m15:19:47.555619 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m15:19:47.555975 [info ] [MainThread]: 
[0m15:19:47.556515 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m15:19:47.562112 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_DATAEXPERT_STUDENT_andres'
[0m15:19:47.578794 [debug] [ThreadPool]: Using snowflake connection "list_DATAEXPERT_STUDENT_andres"
[0m15:19:47.579219 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "connection_name": "list_DATAEXPERT_STUDENT_andres"} */
show objects in DATAEXPERT_STUDENT.andres limit 10000
[0m15:19:47.579558 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:19:48.887571 [debug] [ThreadPool]: SQL status: SUCCESS 25 in 1.308 seconds
[0m15:19:48.891514 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: Close
[0m15:19:49.165983 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4d2c7628-d1e8-47a3-a2f8-e2583df76510', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1129bd390>]}
[0m15:19:49.169326 [debug] [Thread-1 (]: Began running node test.dbt_capstone_andres.not_null_my_first_dbt_model_id.5fb22c2710
[0m15:19:49.169760 [info ] [Thread-1 (]: 1 of 5 START test not_null_my_first_dbt_model_id ............................... [RUN]
[0m15:19:49.170222 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_DATAEXPERT_STUDENT_andres, now test.dbt_capstone_andres.not_null_my_first_dbt_model_id.5fb22c2710)
[0m15:19:49.170594 [debug] [Thread-1 (]: Began compiling node test.dbt_capstone_andres.not_null_my_first_dbt_model_id.5fb22c2710
[0m15:19:49.186369 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_capstone_andres.not_null_my_first_dbt_model_id.5fb22c2710"
[0m15:19:49.187641 [debug] [Thread-1 (]: Began executing node test.dbt_capstone_andres.not_null_my_first_dbt_model_id.5fb22c2710
[0m15:19:49.207281 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_capstone_andres.not_null_my_first_dbt_model_id.5fb22c2710"
[0m15:19:49.208342 [debug] [Thread-1 (]: Using snowflake connection "test.dbt_capstone_andres.not_null_my_first_dbt_model_id.5fb22c2710"
[0m15:19:49.208752 [debug] [Thread-1 (]: On test.dbt_capstone_andres.not_null_my_first_dbt_model_id.5fb22c2710: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "test.dbt_capstone_andres.not_null_my_first_dbt_model_id.5fb22c2710"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from DATAEXPERT_STUDENT.andres.my_first_dbt_model
where id is null



      
    ) dbt_internal_test
[0m15:19:49.209117 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:19:49.921465 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 0.712 seconds
[0m15:19:49.933172 [debug] [Thread-1 (]: On test.dbt_capstone_andres.not_null_my_first_dbt_model_id.5fb22c2710: Close
[0m15:19:50.188931 [error] [Thread-1 (]: 1 of 5 FAIL 1 not_null_my_first_dbt_model_id ................................... [[31mFAIL 1[0m in 1.02s]
[0m15:19:50.189948 [debug] [Thread-1 (]: Finished running node test.dbt_capstone_andres.not_null_my_first_dbt_model_id.5fb22c2710
[0m15:19:50.190582 [debug] [Thread-1 (]: Began running node test.dbt_capstone_andres.not_null_my_second_dbt_model_id.151b76d778
[0m15:19:50.191277 [info ] [Thread-1 (]: 2 of 5 START test not_null_my_second_dbt_model_id .............................. [RUN]
[0m15:19:50.191948 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_capstone_andres.not_null_my_first_dbt_model_id.5fb22c2710, now test.dbt_capstone_andres.not_null_my_second_dbt_model_id.151b76d778)
[0m15:19:50.192494 [debug] [Thread-1 (]: Began compiling node test.dbt_capstone_andres.not_null_my_second_dbt_model_id.151b76d778
[0m15:19:50.198498 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_capstone_andres.not_null_my_second_dbt_model_id.151b76d778"
[0m15:19:50.199449 [debug] [Thread-1 (]: Began executing node test.dbt_capstone_andres.not_null_my_second_dbt_model_id.151b76d778
[0m15:19:50.203382 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_capstone_andres.not_null_my_second_dbt_model_id.151b76d778"
[0m15:19:50.204649 [debug] [Thread-1 (]: Using snowflake connection "test.dbt_capstone_andres.not_null_my_second_dbt_model_id.151b76d778"
[0m15:19:50.205169 [debug] [Thread-1 (]: On test.dbt_capstone_andres.not_null_my_second_dbt_model_id.151b76d778: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "test.dbt_capstone_andres.not_null_my_second_dbt_model_id.151b76d778"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from DATAEXPERT_STUDENT.andres.my_second_dbt_model
where id is null



      
    ) dbt_internal_test
[0m15:19:50.205662 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:19:50.908280 [debug] [Thread-1 (]: Snowflake adapter: Snowflake query id: 01b8ca43-0004-4530-0000-33eb0039f17a
[0m15:19:50.908795 [debug] [Thread-1 (]: Snowflake adapter: Snowflake error: 002003 (42S02): SQL compilation error:
Object 'DATAEXPERT_STUDENT.ANDRES.MY_SECOND_DBT_MODEL' does not exist or not authorized.
[0m15:19:50.909368 [debug] [Thread-1 (]: On test.dbt_capstone_andres.not_null_my_second_dbt_model_id.151b76d778: Close
[0m15:19:51.129424 [debug] [Thread-1 (]: Database Error in test not_null_my_second_dbt_model_id (models/example/schema.yml)
  002003 (42S02): SQL compilation error:
  Object 'DATAEXPERT_STUDENT.ANDRES.MY_SECOND_DBT_MODEL' does not exist or not authorized.
  compiled code at target/run/dbt_capstone_andres/models/example/schema.yml/not_null_my_second_dbt_model_id.sql
[0m15:19:51.130098 [error] [Thread-1 (]: 2 of 5 ERROR not_null_my_second_dbt_model_id ................................... [[31mERROR[0m in 0.94s]
[0m15:19:51.130745 [debug] [Thread-1 (]: Finished running node test.dbt_capstone_andres.not_null_my_second_dbt_model_id.151b76d778
[0m15:19:51.131192 [debug] [Thread-1 (]: Began running node test.dbt_capstone_andres.unique_my_first_dbt_model_id.16e066b321
[0m15:19:51.131766 [debug] [Thread-4 (]: Marking all children of 'test.dbt_capstone_andres.not_null_my_second_dbt_model_id.151b76d778' to be skipped because of status 'error'.  Reason: Database Error in test not_null_my_second_dbt_model_id (models/example/schema.yml)
  002003 (42S02): SQL compilation error:
  Object 'DATAEXPERT_STUDENT.ANDRES.MY_SECOND_DBT_MODEL' does not exist or not authorized.
  compiled code at target/run/dbt_capstone_andres/models/example/schema.yml/not_null_my_second_dbt_model_id.sql.
[0m15:19:51.132179 [info ] [Thread-1 (]: 3 of 5 START test unique_my_first_dbt_model_id ................................. [RUN]
[0m15:19:51.133340 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_capstone_andres.not_null_my_second_dbt_model_id.151b76d778, now test.dbt_capstone_andres.unique_my_first_dbt_model_id.16e066b321)
[0m15:19:51.133766 [debug] [Thread-1 (]: Began compiling node test.dbt_capstone_andres.unique_my_first_dbt_model_id.16e066b321
[0m15:19:51.142197 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_capstone_andres.unique_my_first_dbt_model_id.16e066b321"
[0m15:19:51.143089 [debug] [Thread-1 (]: Began executing node test.dbt_capstone_andres.unique_my_first_dbt_model_id.16e066b321
[0m15:19:51.146537 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_capstone_andres.unique_my_first_dbt_model_id.16e066b321"
[0m15:19:51.147759 [debug] [Thread-1 (]: Using snowflake connection "test.dbt_capstone_andres.unique_my_first_dbt_model_id.16e066b321"
[0m15:19:51.148208 [debug] [Thread-1 (]: On test.dbt_capstone_andres.unique_my_first_dbt_model_id.16e066b321: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "test.dbt_capstone_andres.unique_my_first_dbt_model_id.16e066b321"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from DATAEXPERT_STUDENT.andres.my_first_dbt_model
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
[0m15:19:51.148625 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:19:51.794728 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 0.646 seconds
[0m15:19:51.797725 [debug] [Thread-1 (]: On test.dbt_capstone_andres.unique_my_first_dbt_model_id.16e066b321: Close
[0m15:19:52.011599 [info ] [Thread-1 (]: 3 of 5 PASS unique_my_first_dbt_model_id ....................................... [[32mPASS[0m in 0.88s]
[0m15:19:52.012330 [debug] [Thread-1 (]: Finished running node test.dbt_capstone_andres.unique_my_first_dbt_model_id.16e066b321
[0m15:19:52.012784 [debug] [Thread-1 (]: Began running node test.dbt_capstone_andres.unique_my_second_dbt_model_id.57a0f8c493
[0m15:19:52.013277 [info ] [Thread-1 (]: 4 of 5 START test unique_my_second_dbt_model_id ................................ [RUN]
[0m15:19:52.013776 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_capstone_andres.unique_my_first_dbt_model_id.16e066b321, now test.dbt_capstone_andres.unique_my_second_dbt_model_id.57a0f8c493)
[0m15:19:52.014190 [debug] [Thread-1 (]: Began compiling node test.dbt_capstone_andres.unique_my_second_dbt_model_id.57a0f8c493
[0m15:19:52.019181 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_capstone_andres.unique_my_second_dbt_model_id.57a0f8c493"
[0m15:19:52.020019 [debug] [Thread-1 (]: Began executing node test.dbt_capstone_andres.unique_my_second_dbt_model_id.57a0f8c493
[0m15:19:52.023436 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_capstone_andres.unique_my_second_dbt_model_id.57a0f8c493"
[0m15:19:52.024659 [debug] [Thread-1 (]: Using snowflake connection "test.dbt_capstone_andres.unique_my_second_dbt_model_id.57a0f8c493"
[0m15:19:52.025099 [debug] [Thread-1 (]: On test.dbt_capstone_andres.unique_my_second_dbt_model_id.57a0f8c493: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "test.dbt_capstone_andres.unique_my_second_dbt_model_id.57a0f8c493"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from DATAEXPERT_STUDENT.andres.my_second_dbt_model
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
[0m15:19:52.025516 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:19:52.742048 [debug] [Thread-1 (]: Snowflake adapter: Snowflake query id: 01b8ca43-0004-455e-0000-33eb0039ddfa
[0m15:19:52.742634 [debug] [Thread-1 (]: Snowflake adapter: Snowflake error: 002003 (42S02): SQL compilation error:
Object 'DATAEXPERT_STUDENT.ANDRES.MY_SECOND_DBT_MODEL' does not exist or not authorized.
[0m15:19:52.743256 [debug] [Thread-1 (]: On test.dbt_capstone_andres.unique_my_second_dbt_model_id.57a0f8c493: Close
[0m15:19:52.991905 [debug] [Thread-1 (]: Database Error in test unique_my_second_dbt_model_id (models/example/schema.yml)
  002003 (42S02): SQL compilation error:
  Object 'DATAEXPERT_STUDENT.ANDRES.MY_SECOND_DBT_MODEL' does not exist or not authorized.
  compiled code at target/run/dbt_capstone_andres/models/example/schema.yml/unique_my_second_dbt_model_id.sql
[0m15:19:52.992495 [error] [Thread-1 (]: 4 of 5 ERROR unique_my_second_dbt_model_id ..................................... [[31mERROR[0m in 0.98s]
[0m15:19:52.993137 [debug] [Thread-1 (]: Finished running node test.dbt_capstone_andres.unique_my_second_dbt_model_id.57a0f8c493
[0m15:19:52.993518 [debug] [Thread-1 (]: Began running node unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum
[0m15:19:52.993987 [debug] [Thread-4 (]: Marking all children of 'test.dbt_capstone_andres.unique_my_second_dbt_model_id.57a0f8c493' to be skipped because of status 'error'.  Reason: Database Error in test unique_my_second_dbt_model_id (models/example/schema.yml)
  002003 (42S02): SQL compilation error:
  Object 'DATAEXPERT_STUDENT.ANDRES.MY_SECOND_DBT_MODEL' does not exist or not authorized.
  compiled code at target/run/dbt_capstone_andres/models/example/schema.yml/unique_my_second_dbt_model_id.sql.
[0m15:19:52.994374 [info ] [Thread-1 (]: 5 of 5 START unit_test weekly_metrics::test_amount_sum ......................... [RUN]
[0m15:19:52.994996 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_capstone_andres.unique_my_second_dbt_model_id.57a0f8c493, now unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum)
[0m15:19:52.995345 [debug] [Thread-1 (]: Began compiling node unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum
[0m15:19:52.995683 [debug] [Thread-1 (]: Began executing node unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum
[0m15:19:53.039045 [debug] [Thread-1 (]: Using snowflake connection "unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum"
[0m15:19:53.039586 [debug] [Thread-1 (]: On unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum"} */
describe table DATAEXPERT_STUDENT.andres.stg_daily_bars
[0m15:19:53.040002 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:19:53.734543 [debug] [Thread-1 (]: SQL status: SUCCESS 9 in 0.694 seconds
[0m15:19:53.768521 [debug] [Thread-1 (]: Writing injected SQL for node "unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum"
[0m15:19:53.776296 [debug] [Thread-1 (]: Using snowflake connection "unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum"
[0m15:19:53.776796 [debug] [Thread-1 (]: On unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum"} */
describe table DATAEXPERT_STUDENT.andres.dim_date
[0m15:19:53.938615 [debug] [Thread-1 (]: SQL status: SUCCESS 36 in 0.161 seconds
[0m15:19:53.952852 [debug] [Thread-1 (]: Writing injected SQL for node "unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum"
[0m15:19:53.960748 [debug] [Thread-1 (]: Using snowflake connection "unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum"
[0m15:19:53.961240 [debug] [Thread-1 (]: On unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum"} */
describe table DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections
[0m15:19:54.098814 [debug] [Thread-1 (]: SQL status: SUCCESS 4 in 0.137 seconds
[0m15:19:54.109835 [debug] [Thread-1 (]: Writing injected SQL for node "unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum"
[0m15:19:54.118364 [debug] [Thread-1 (]: Writing injected SQL for node "unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum"
[0m15:19:54.158586 [debug] [Thread-1 (]: Using snowflake connection "unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum"
[0m15:19:54.159347 [debug] [Thread-1 (]: On unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum"} */
create or replace temporary table DATAEXPERT_STUDENT.andres.test_amount_sum__dbt_tmp
         as
        (select * from (
        with  __dbt__cte__stg_daily_bars as (

-- Fixture for stg_daily_bars
select 
    
        try_cast('2020-11-20' as DATE)
     as date, 
    
        try_cast('\'L\'' as character varying(16777216))
     as ticker, 
    
        try_cast('100' as FLOAT)
     as number_of_shares_traded, 
    
        try_cast('10' as NUMBER(38,0))
     as number_of_trades, try_cast(null as FLOAT) as volume_weighted_avg_price, try_cast(null as FLOAT) as open_price, try_cast(null as FLOAT) as close_price, try_cast(null as FLOAT) as high_price, try_cast(null as FLOAT) as low_price
union all
select 
    
        try_cast('2020-11-20' as DATE)
     as date, 
    
        try_cast('\'S\'' as character varying(16777216))
     as ticker, 
    
        try_cast('200' as FLOAT)
     as number_of_shares_traded, 
    
        try_cast('20' as NUMBER(38,0))
     as number_of_trades, try_cast(null as FLOAT) as volume_weighted_avg_price, try_cast(null as FLOAT) as open_price, try_cast(null as FLOAT) as close_price, try_cast(null as FLOAT) as high_price, try_cast(null as FLOAT) as low_price
union all
select 
    
        try_cast('2020-11-21' as DATE)
     as date, 
    
        try_cast('\'L\'' as character varying(16777216))
     as ticker, 
    
        try_cast('300' as FLOAT)
     as number_of_shares_traded, 
    
        try_cast('40' as NUMBER(38,0))
     as number_of_trades, try_cast(null as FLOAT) as volume_weighted_avg_price, try_cast(null as FLOAT) as open_price, try_cast(null as FLOAT) as close_price, try_cast(null as FLOAT) as high_price, try_cast(null as FLOAT) as low_price
union all
select 
    
        try_cast('2020-11-21' as DATE)
     as date, 
    
        try_cast('\'S\'' as character varying(16777216))
     as ticker, 
    
        try_cast('200' as FLOAT)
     as number_of_shares_traded, 
    
        try_cast('50' as NUMBER(38,0))
     as number_of_trades, try_cast(null as FLOAT) as volume_weighted_avg_price, try_cast(null as FLOAT) as open_price, try_cast(null as FLOAT) as close_price, try_cast(null as FLOAT) as high_price, try_cast(null as FLOAT) as low_price
),  __dbt__cte__dim_date as (

-- Fixture for dim_date
select 
    
        try_cast('2020-11-20' as DATE)
     as date_day, try_cast(null as DATE) as prior_date_day, try_cast(null as DATE) as next_date_day, try_cast(null as DATE) as prior_year_date_day, try_cast(null as DATE) as prior_year_over_year_date_day, try_cast(null as NUMBER(3,0)) as day_of_week, try_cast(null as NUMBER(2,0)) as day_of_week_iso, try_cast(null as character varying(16777216)) as day_of_week_name, try_cast(null as character varying(3)) as day_of_week_name_short, try_cast(null as NUMBER(2,0)) as day_of_month, try_cast(null as NUMBER(4,0)) as day_of_year, try_cast(null as DATE) as week_start_date, try_cast(null as DATE) as week_end_date, try_cast(null as DATE) as prior_year_week_start_date, try_cast(null as DATE) as prior_year_week_end_date, try_cast(null as NUMBER(38,0)) as week_of_year, try_cast(null as DATE) as iso_week_start_date, try_cast(null as DATE) as iso_week_end_date, try_cast(null as DATE) as prior_year_iso_week_start_date, try_cast(null as DATE) as prior_year_iso_week_end_date, try_cast(null as NUMBER(38,0)) as iso_week_of_year, try_cast(null as NUMBER(38,0)) as prior_year_week_of_year, try_cast(null as NUMBER(38,0)) as prior_year_iso_week_of_year, try_cast(null as NUMBER(38,0)) as month_of_year, try_cast(null as character varying(16777216)) as month_name, try_cast(null as character varying(16777216)) as month_name_short, try_cast(null as DATE) as month_start_date, try_cast(null as DATE) as month_end_date, try_cast(null as DATE) as prior_year_month_start_date, try_cast(null as DATE) as prior_year_month_end_date, try_cast(null as NUMBER(38,0)) as quarter_of_year, try_cast(null as DATE) as quarter_start_date, try_cast(null as DATE) as quarter_end_date, try_cast(null as NUMBER(38,0)) as year_number, try_cast(null as DATE) as year_start_date, try_cast(null as DATE) as year_end_date
union all
select 
    
        try_cast('2020-11-21' as DATE)
     as date_day, try_cast(null as DATE) as prior_date_day, try_cast(null as DATE) as next_date_day, try_cast(null as DATE) as prior_year_date_day, try_cast(null as DATE) as prior_year_over_year_date_day, try_cast(null as NUMBER(3,0)) as day_of_week, try_cast(null as NUMBER(2,0)) as day_of_week_iso, try_cast(null as character varying(16777216)) as day_of_week_name, try_cast(null as character varying(3)) as day_of_week_name_short, try_cast(null as NUMBER(2,0)) as day_of_month, try_cast(null as NUMBER(4,0)) as day_of_year, try_cast(null as DATE) as week_start_date, try_cast(null as DATE) as week_end_date, try_cast(null as DATE) as prior_year_week_start_date, try_cast(null as DATE) as prior_year_week_end_date, try_cast(null as NUMBER(38,0)) as week_of_year, try_cast(null as DATE) as iso_week_start_date, try_cast(null as DATE) as iso_week_end_date, try_cast(null as DATE) as prior_year_iso_week_start_date, try_cast(null as DATE) as prior_year_iso_week_end_date, try_cast(null as NUMBER(38,0)) as iso_week_of_year, try_cast(null as NUMBER(38,0)) as prior_year_week_of_year, try_cast(null as NUMBER(38,0)) as prior_year_iso_week_of_year, try_cast(null as NUMBER(38,0)) as month_of_year, try_cast(null as character varying(16777216)) as month_name, try_cast(null as character varying(16777216)) as month_name_short, try_cast(null as DATE) as month_start_date, try_cast(null as DATE) as month_end_date, try_cast(null as DATE) as prior_year_month_start_date, try_cast(null as DATE) as prior_year_month_end_date, try_cast(null as NUMBER(38,0)) as quarter_of_year, try_cast(null as DATE) as quarter_start_date, try_cast(null as DATE) as quarter_end_date, try_cast(null as NUMBER(38,0)) as year_number, try_cast(null as DATE) as year_start_date, try_cast(null as DATE) as year_end_date
),  __dbt__cte__stg_treasury_revenue_collections as (

-- Fixture for stg_treasury_revenue_collections
select 
    
        try_cast('2020-11-20' as DATE)
     as collection_date, try_cast(null as character varying(16777216)) as channel_type, 
    
        try_cast('\'IRS Tax\'' as character varying(16777216))
     as tax_category, 
    
        try_cast('1000' as NUMBER(19,0))
     as dollars_collected_by_treasury
union all
select 
    
        try_cast('2020-11-20' as DATE)
     as collection_date, try_cast(null as character varying(16777216)) as channel_type, 
    
        try_cast('\'IRS Non Tax\'' as character varying(16777216))
     as tax_category, 
    
        try_cast('2000' as NUMBER(19,0))
     as dollars_collected_by_treasury
union all
select 
    
        try_cast('2020-11-21' as DATE)
     as collection_date, try_cast(null as character varying(16777216)) as channel_type, 
    
        try_cast('\'IRS Tax\'' as character varying(16777216))
     as tax_category, 
    
        try_cast('3000' as NUMBER(19,0))
     as dollars_collected_by_treasury
union all
select 
    
        try_cast('2020-11-21' as DATE)
     as collection_date, try_cast(null as character varying(16777216)) as channel_type, 
    
        try_cast('\'IRS Tax\'' as character varying(16777216))
     as tax_category, 
    
        try_cast('2000' as NUMBER(19,0))
     as dollars_collected_by_treasury
), pol as (
SELECT week_start_date, 
sum(number_of_shares_traded) number_of_shares_traded,
sum(number_of_trades) number_of_trades
from  __dbt__cte__stg_daily_bars a 
JOIN __dbt__cte__dim_date d on d.date_day=a.date
group by 1)

, rev as (
select week_start_date,
sum(dollars_collected_by_treasury) dollars_collected_by_treasury,
sum(case when tax_category = 'IRS Tax' then dollars_collected_by_treasury else 0 end) as irs_tax_dollars_collected, 
sum(case when tax_category = 'Non-Tax' then dollars_collected_by_treasury else 0 end) as non_irs_non_tax_dollars,
sum(case when tax_category = 'IRS Non-Tax' then dollars_collected_by_treasury else 0 end) as irs_non_tax_dollars
FROM __dbt__cte__stg_treasury_revenue_collections a 
JOIN __dbt__cte__dim_date d on d.date_day=a.collection_date
GROUP BY ALL)

select 
    a.week_start_date, 
    a.number_of_shares_traded, 
    a.number_of_trades,
    b.dollars_collected_by_treasury,
    b.irs_tax_dollars

from pol a 
INNER JOIN rev b on b.week_start_date = a.week_start_date
    ) as __dbt_sbq
    where false
    limit 0

        );
[0m15:19:54.336782 [debug] [Thread-1 (]: Snowflake adapter: Snowflake query id: 01b8ca43-0004-455e-0000-33eb0039de02
[0m15:19:54.337587 [debug] [Thread-1 (]: Snowflake adapter: Snowflake error: 000904 (42000): SQL compilation error: error line 143 at position 4
invalid identifier 'B.IRS_TAX_DOLLARS'
[0m15:19:54.338537 [debug] [Thread-1 (]: On unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum: Close
[0m15:19:54.588559 [debug] [Thread-1 (]: Runtime Error in unit_test test_amount_sum (models/marts/_marts.yml)
  An error occurred during execution of unit test 'test_amount_sum'. There may be an error in the unit test definition: check the data types.
   Database Error
    000904 (42000): SQL compilation error: error line 143 at position 4
    invalid identifier 'B.IRS_TAX_DOLLARS'
[0m15:19:54.589282 [error] [Thread-1 (]: 5 of 5 ERROR weekly_metrics::test_amount_sum ................................... [[31mERROR[0m in 1.59s]
[0m15:19:54.590034 [debug] [Thread-1 (]: Finished running node unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum
[0m15:19:54.590705 [debug] [Thread-4 (]: Marking all children of 'unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum' to be skipped because of status 'error'.  Reason: Runtime Error in unit_test test_amount_sum (models/marts/_marts.yml)
  An error occurred during execution of unit test 'test_amount_sum'. There may be an error in the unit test definition: check the data types.
   Database Error
    000904 (42000): SQL compilation error: error line 143 at position 4
    invalid identifier 'B.IRS_TAX_DOLLARS'.
[0m15:19:54.592043 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:19:54.592454 [debug] [MainThread]: Connection 'unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum' was properly closed.
[0m15:19:54.592910 [info ] [MainThread]: 
[0m15:19:54.593358 [info ] [MainThread]: Finished running 4 data tests, 1 unit test in 0 hours 0 minutes and 7.04 seconds (7.04s).
[0m15:19:54.594851 [debug] [MainThread]: Command end result
[0m15:19:54.644661 [info ] [MainThread]: 
[0m15:19:54.645164 [info ] [MainThread]: [31mCompleted with 4 errors, 0 partial successs, and 0 warnings:[0m
[0m15:19:54.645493 [info ] [MainThread]: 
[0m15:19:54.645870 [error] [MainThread]: [31mFailure in test not_null_my_first_dbt_model_id (models/example/schema.yml)[0m
[0m15:19:54.646269 [error] [MainThread]:   Got 1 result, configured to fail if != 0
[0m15:19:54.646566 [info ] [MainThread]: 
[0m15:19:54.646983 [info ] [MainThread]:   compiled code at target/compiled/dbt_capstone_andres/models/example/schema.yml/not_null_my_first_dbt_model_id.sql
[0m15:19:54.647385 [info ] [MainThread]: 
[0m15:19:54.647875 [error] [MainThread]:   Database Error in test not_null_my_second_dbt_model_id (models/example/schema.yml)
  002003 (42S02): SQL compilation error:
  Object 'DATAEXPERT_STUDENT.ANDRES.MY_SECOND_DBT_MODEL' does not exist or not authorized.
  compiled code at target/run/dbt_capstone_andres/models/example/schema.yml/not_null_my_second_dbt_model_id.sql
[0m15:19:54.648204 [info ] [MainThread]: 
[0m15:19:54.648604 [error] [MainThread]:   Database Error in test unique_my_second_dbt_model_id (models/example/schema.yml)
  002003 (42S02): SQL compilation error:
  Object 'DATAEXPERT_STUDENT.ANDRES.MY_SECOND_DBT_MODEL' does not exist or not authorized.
  compiled code at target/run/dbt_capstone_andres/models/example/schema.yml/unique_my_second_dbt_model_id.sql
[0m15:19:54.648968 [info ] [MainThread]: 
[0m15:19:54.649346 [error] [MainThread]:   Runtime Error in unit_test test_amount_sum (models/marts/_marts.yml)
  An error occurred during execution of unit test 'test_amount_sum'. There may be an error in the unit test definition: check the data types.
   Database Error
    000904 (42000): SQL compilation error: error line 143 at position 4
    invalid identifier 'B.IRS_TAX_DOLLARS'
[0m15:19:54.649807 [info ] [MainThread]: 
[0m15:19:54.650575 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=4 SKIP=0 TOTAL=5
[0m15:19:54.653422 [debug] [MainThread]: Resource report: {"command_name": "test", "command_wall_clock_time": 9.716449, "process_user_time": 3.663694, "process_kernel_time": 0.763321, "process_mem_max_rss": "196141056", "command_success": false, "process_in_blocks": "0", "process_out_blocks": "0"}
[0m15:19:54.654096 [debug] [MainThread]: Command `dbt test` failed at 15:19:54.653984 after 9.72 seconds
[0m15:19:54.654557 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112ac0d90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112b3fd50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111e81f50>]}
[0m15:19:54.655013 [debug] [MainThread]: Flushing usage events
[0m15:19:55.003895 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:19:59.694330 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ae9af10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10af0aad0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10af0b0d0>]}


============================== 15:19:59.698077 | 28b07765-eaab-4e9c-ba2c-c406c00afc16 ==============================
[0m15:19:59.698077 [info ] [MainThread]: Running with dbt=1.9.0-b3
[0m15:19:59.698630 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'profiles_dir': '.', 'log_path': 'logs', 'debug': 'False', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'invocation_command': 'dbt build', 'introspect': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m15:20:00.538105 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '28b07765-eaab-4e9c-ba2c-c406c00afc16', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111257510>]}
[0m15:20:00.593183 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '28b07765-eaab-4e9c-ba2c-c406c00afc16', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e7fc510>]}
[0m15:20:00.593985 [info ] [MainThread]: Registered adapter: snowflake=1.8.4
[0m15:20:00.778099 [debug] [MainThread]: checksum: 5f4fde221ca8627174a0341efc73099aaec05b8546408ceed6f59b1ec0755115, vars: {}, profile: , target: , version: 1.9.0b3
[0m15:20:00.986478 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m15:20:00.986936 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m15:20:01.029808 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '28b07765-eaab-4e9c-ba2c-c406c00afc16', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11166f650>]}
[0m15:20:01.283826 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '28b07765-eaab-4e9c-ba2c-c406c00afc16', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111268a50>]}
[0m15:20:01.284585 [info ] [MainThread]: Found 6 models, 4 data tests, 2 sources, 862 macros, 1 unit test
[0m15:20:01.287910 [info ] [MainThread]: 
[0m15:20:01.288532 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m15:20:01.288991 [info ] [MainThread]: 
[0m15:20:01.289700 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m15:20:01.296697 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_DATAEXPERT_STUDENT'
[0m15:20:01.317113 [debug] [ThreadPool]: Using snowflake connection "list_DATAEXPERT_STUDENT"
[0m15:20:01.317662 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "connection_name": "list_DATAEXPERT_STUDENT"} */
show terse schemas in database DATAEXPERT_STUDENT
    limit 10000
[0m15:20:01.318208 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:20:02.128118 [debug] [ThreadPool]: SQL status: SUCCESS 73 in 0.810 seconds
[0m15:20:02.130977 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT: Close
[0m15:20:02.358393 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_DATAEXPERT_STUDENT, now list_DATAEXPERT_STUDENT_andres)
[0m15:20:02.367186 [debug] [ThreadPool]: Using snowflake connection "list_DATAEXPERT_STUDENT_andres"
[0m15:20:02.367541 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "connection_name": "list_DATAEXPERT_STUDENT_andres"} */
show objects in DATAEXPERT_STUDENT.andres limit 10000
[0m15:20:02.367835 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:20:03.212945 [debug] [ThreadPool]: SQL status: SUCCESS 25 in 0.845 seconds
[0m15:20:03.215654 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: Close
[0m15:20:03.472261 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '28b07765-eaab-4e9c-ba2c-c406c00afc16', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11126a490>]}
[0m15:20:03.474285 [debug] [Thread-1 (]: Began running node model.dbt_capstone_andres.dim_date
[0m15:20:03.474838 [info ] [Thread-1 (]: 1 of 11 START sql table model andres.dim_date .................................. [RUN]
[0m15:20:03.475339 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_DATAEXPERT_STUDENT_andres, now model.dbt_capstone_andres.dim_date)
[0m15:20:03.475745 [debug] [Thread-1 (]: Began compiling node model.dbt_capstone_andres.dim_date
[0m15:20:03.516840 [debug] [Thread-1 (]: Using snowflake connection "model.dbt_capstone_andres.dim_date"
[0m15:20:03.517453 [debug] [Thread-1 (]: On model.dbt_capstone_andres.dim_date: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "model.dbt_capstone_andres.dim_date"} */
select datediff(
        day,
        cast('1995-01-01' as timestamp),
        cast('2040-12-31' as timestamp)
        )
[0m15:20:03.517946 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:20:04.161084 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 0.643 seconds
[0m15:20:04.243417 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_capstone_andres.dim_date"
[0m15:20:04.245164 [debug] [Thread-1 (]: Began executing node model.dbt_capstone_andres.dim_date
[0m15:20:04.276439 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_capstone_andres.dim_date"
[0m15:20:04.284243 [debug] [Thread-1 (]: Using snowflake connection "model.dbt_capstone_andres.dim_date"
[0m15:20:04.285127 [debug] [Thread-1 (]: On model.dbt_capstone_andres.dim_date: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "model.dbt_capstone_andres.dim_date"} */
create or replace transient table DATAEXPERT_STUDENT.andres.dim_date
         as
        (

    
with base_dates as (
    
    with date_spine as
(

    





with rawdata as (

    

    

    with p as (
        select 0 as generated_number union all select 1
    ), unioned as (

    select

    
    p0.generated_number * power(2, 0)
     + 
    
    p1.generated_number * power(2, 1)
     + 
    
    p2.generated_number * power(2, 2)
     + 
    
    p3.generated_number * power(2, 3)
     + 
    
    p4.generated_number * power(2, 4)
     + 
    
    p5.generated_number * power(2, 5)
     + 
    
    p6.generated_number * power(2, 6)
     + 
    
    p7.generated_number * power(2, 7)
     + 
    
    p8.generated_number * power(2, 8)
     + 
    
    p9.generated_number * power(2, 9)
     + 
    
    p10.generated_number * power(2, 10)
     + 
    
    p11.generated_number * power(2, 11)
     + 
    
    p12.generated_number * power(2, 12)
     + 
    
    p13.generated_number * power(2, 13)
     + 
    
    p14.generated_number * power(2, 14)
    
    
    + 1
    as generated_number

    from

    
    p as p0
     cross join 
    
    p as p1
     cross join 
    
    p as p2
     cross join 
    
    p as p3
     cross join 
    
    p as p4
     cross join 
    
    p as p5
     cross join 
    
    p as p6
     cross join 
    
    p as p7
     cross join 
    
    p as p8
     cross join 
    
    p as p9
     cross join 
    
    p as p10
     cross join 
    
    p as p11
     cross join 
    
    p as p12
     cross join 
    
    p as p13
     cross join 
    
    p as p14
    
    

    )

    select *
    from unioned
    where generated_number <= 16801
    order by generated_number



),

all_periods as (

    select (
        

    dateadd(
        day,
        (row_number() over (order by 1) - 1),
        cast('1995-01-01' as timestamp)
        )


    ) as date_day
    from rawdata

),

filtered as (

    select *
    from all_periods
    where date_day <= cast('2040-12-31' as timestamp)

)

select * from filtered



)
select
    cast(d.date_day as timestamp) as date_day
from
    date_spine d


),
dates_with_prior_year_dates as (

    select
        cast(d.date_day as date) as date_day,
        cast(

    dateadd(
        year,
        -1,
        d.date_day
        )

 as date) as prior_year_date_day,
        cast(

    dateadd(
        day,
        -364,
        d.date_day
        )

 as date) as prior_year_over_year_date_day
    from
    	base_dates d

)
select
    d.date_day,
    cast(

    dateadd(
        day,
        -1,
        d.date_day
        )

 as date) as prior_date_day,
    cast(

    dateadd(
        day,
        1,
        d.date_day
        )

 as date) as next_date_day,
    d.prior_year_date_day as prior_year_date_day,
    d.prior_year_over_year_date_day,
    case
            when date_part('dayofweek', d.date_day) = 7 then 1
            else date_part('dayofweek', d.date_day) + 1
        end as day_of_week,
    date_part('dayofweekiso', d.date_day) as day_of_week_iso,
    -- long version not implemented on Snowflake so we're doing it manually :/
    case dayname(d.date_day)
        when 'Mon' then 'Monday'
        when 'Tue' then 'Tuesday'
        when 'Wed' then 'Wednesday'
        when 'Thu' then 'Thursday'
        when 'Fri' then 'Friday'
        when 'Sat' then 'Saturday'
        when 'Sun' then 'Sunday'
    end as day_of_week_name,
    dayname(d.date_day) as day_of_week_name_short,
    date_part('day', d.date_day) as day_of_month,
    date_part('dayofyear', d.date_day) as day_of_year,

    
    
    cast(

    dateadd(
        day,
        -1 * (case
            when date_part('dayofweek', d.date_day) = 7 then 1
            else date_part('dayofweek', d.date_day) + 1
        end - 1),
        d.date_day
        )

 as date) as week_start_date,
    cast(

    dateadd(
        day,
        6,
        
    
    cast(

    dateadd(
        day,
        -1 * (case
            when date_part('dayofweek', d.date_day) = 7 then 1
            else date_part('dayofweek', d.date_day) + 1
        end - 1),
        d.date_day
        )

 as date)
        )

 as date) as week_end_date,
    
    
    cast(

    dateadd(
        day,
        -1 * (case
            when date_part('dayofweek', d.prior_year_over_year_date_day) = 7 then 1
            else date_part('dayofweek', d.prior_year_over_year_date_day) + 1
        end - 1),
        d.prior_year_over_year_date_day
        )

 as date) as prior_year_week_start_date,
    cast(

    dateadd(
        day,
        6,
        
    
    cast(

    dateadd(
        day,
        -1 * (case
            when date_part('dayofweek', d.prior_year_over_year_date_day) = 7 then 1
            else date_part('dayofweek', d.prior_year_over_year_date_day) + 1
        end - 1),
        d.prior_year_over_year_date_day
        )

 as date)
        )

 as date) as prior_year_week_end_date,
    cast(date_part('week', d.date_day) as integer) as week_of_year,

    cast(date_trunc('week', d.date_day) as date) as iso_week_start_date,
    cast(

    dateadd(
        day,
        6,
        cast(date_trunc('week', d.date_day) as date)
        )

 as date) as iso_week_end_date,
    cast(date_trunc('week', d.prior_year_over_year_date_day) as date) as prior_year_iso_week_start_date,
    cast(

    dateadd(
        day,
        6,
        cast(date_trunc('week', d.prior_year_over_year_date_day) as date)
        )

 as date) as prior_year_iso_week_end_date,
    cast(date_part('weekiso', d.date_day) as integer) as iso_week_of_year,

    cast(date_part('week', d.prior_year_over_year_date_day) as integer) as prior_year_week_of_year,
    cast(date_part('weekiso', d.prior_year_over_year_date_day) as integer) as prior_year_iso_week_of_year,

    cast(date_part('month', d.date_day) as integer) as month_of_year,
    to_char(d.date_day, 'MMMM')  as month_name,
    to_char(d.date_day, 'MON')  as month_name_short,

    cast(date_trunc('month', d.date_day) as date) as month_start_date,
    cast(cast(
        

    dateadd(
        day,
        -1,
        

    dateadd(
        month,
        1,
        date_trunc('month', d.date_day)
        )


        )


        as date) as date) as month_end_date,

    cast(date_trunc('month', d.prior_year_date_day) as date) as prior_year_month_start_date,
    cast(cast(
        

    dateadd(
        day,
        -1,
        

    dateadd(
        month,
        1,
        date_trunc('month', d.prior_year_date_day)
        )


        )


        as date) as date) as prior_year_month_end_date,

    cast(date_part('quarter', d.date_day) as integer) as quarter_of_year,
    cast(date_trunc('quarter', d.date_day) as date) as quarter_start_date,
    cast(cast(
        

    dateadd(
        day,
        -1,
        

    dateadd(
        quarter,
        1,
        date_trunc('quarter', d.date_day)
        )


        )


        as date) as date) as quarter_end_date,

    cast(date_part('year', d.date_day) as integer) as year_number,
    cast(date_trunc('year', d.date_day) as date) as year_start_date,
    cast(cast(
        

    dateadd(
        day,
        -1,
        

    dateadd(
        year,
        1,
        date_trunc('year', d.date_day)
        )


        )


        as date) as date) as year_end_date
from
    dates_with_prior_year_dates d
order by 1


        );
[0m15:20:06.159503 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 1.874 seconds
[0m15:20:06.192474 [debug] [Thread-1 (]: On model.dbt_capstone_andres.dim_date: Close
[0m15:20:06.419887 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '28b07765-eaab-4e9c-ba2c-c406c00afc16', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11563add0>]}
[0m15:20:06.421174 [info ] [Thread-1 (]: 1 of 11 OK created sql table model andres.dim_date ............................. [[32mSUCCESS 1[0m in 2.94s]
[0m15:20:06.422057 [debug] [Thread-1 (]: Finished running node model.dbt_capstone_andres.dim_date
[0m15:20:06.422742 [debug] [Thread-1 (]: Began running node model.dbt_capstone_andres.my_first_dbt_model
[0m15:20:06.423549 [info ] [Thread-1 (]: 2 of 11 START sql table model andres.my_first_dbt_model ........................ [RUN]
[0m15:20:06.424391 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_capstone_andres.dim_date, now model.dbt_capstone_andres.my_first_dbt_model)
[0m15:20:06.424936 [debug] [Thread-1 (]: Began compiling node model.dbt_capstone_andres.my_first_dbt_model
[0m15:20:06.430550 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_capstone_andres.my_first_dbt_model"
[0m15:20:06.432680 [debug] [Thread-1 (]: Began executing node model.dbt_capstone_andres.my_first_dbt_model
[0m15:20:06.440202 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_capstone_andres.my_first_dbt_model"
[0m15:20:06.442424 [debug] [Thread-1 (]: Using snowflake connection "model.dbt_capstone_andres.my_first_dbt_model"
[0m15:20:06.443003 [debug] [Thread-1 (]: On model.dbt_capstone_andres.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "model.dbt_capstone_andres.my_first_dbt_model"} */
create or replace transient table DATAEXPERT_STUDENT.andres.my_first_dbt_model
         as
        (/*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
        );
[0m15:20:06.443779 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:20:07.863336 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 1.419 seconds
[0m15:20:07.872505 [debug] [Thread-1 (]: On model.dbt_capstone_andres.my_first_dbt_model: Close
[0m15:20:08.136792 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '28b07765-eaab-4e9c-ba2c-c406c00afc16', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1111bd190>]}
[0m15:20:08.139244 [info ] [Thread-1 (]: 2 of 11 OK created sql table model andres.my_first_dbt_model ................... [[32mSUCCESS 1[0m in 1.71s]
[0m15:20:08.140921 [debug] [Thread-1 (]: Finished running node model.dbt_capstone_andres.my_first_dbt_model
[0m15:20:08.142315 [debug] [Thread-1 (]: Began running node model.dbt_capstone_andres.stg_daily_bars
[0m15:20:08.153852 [info ] [Thread-1 (]: 3 of 11 START sql table model andres.stg_daily_bars ............................ [RUN]
[0m15:20:08.157379 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_capstone_andres.my_first_dbt_model, now model.dbt_capstone_andres.stg_daily_bars)
[0m15:20:08.164289 [debug] [Thread-1 (]: Began compiling node model.dbt_capstone_andres.stg_daily_bars
[0m15:20:08.288670 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_capstone_andres.stg_daily_bars"
[0m15:20:08.299600 [debug] [Thread-1 (]: Began executing node model.dbt_capstone_andres.stg_daily_bars
[0m15:20:08.324306 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_capstone_andres.stg_daily_bars"
[0m15:20:08.328648 [debug] [Thread-1 (]: Using snowflake connection "model.dbt_capstone_andres.stg_daily_bars"
[0m15:20:08.329850 [debug] [Thread-1 (]: On model.dbt_capstone_andres.stg_daily_bars: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "model.dbt_capstone_andres.stg_daily_bars"} */
create or replace transient table DATAEXPERT_STUDENT.andres.stg_daily_bars
         as
        (with source as (

    select * from dataexpert_student.andres.daily_bars

),

renamed as (

    select
        TO_DATE(TO_TIMESTAMP(timestamp / 1000)) as date,
        ticker as ticker,
        v  as number_of_shares_traded,
        n  as number_of_trades,
        round(vw,2) as volume_weighted_avg_price,
        o  as open_price,
        c  as close_price,
        h  as high_price,
        l  as low_price
    from source

)

select * from renamed
        );
[0m15:20:08.331268 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:20:10.713368 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 2.383 seconds
[0m15:20:10.716488 [debug] [Thread-1 (]: On model.dbt_capstone_andres.stg_daily_bars: Close
[0m15:20:10.928534 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '28b07765-eaab-4e9c-ba2c-c406c00afc16', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11571a390>]}
[0m15:20:10.929904 [info ] [Thread-1 (]: 3 of 11 OK created sql table model andres.stg_daily_bars ....................... [[32mSUCCESS 1[0m in 2.77s]
[0m15:20:10.931320 [debug] [Thread-1 (]: Finished running node model.dbt_capstone_andres.stg_daily_bars
[0m15:20:10.931885 [debug] [Thread-1 (]: Began running node model.dbt_capstone_andres.stg_treasury_revenue_collections
[0m15:20:10.932576 [info ] [Thread-1 (]: 4 of 11 START sql table model andres.stg_treasury_revenue_collections .......... [RUN]
[0m15:20:10.933308 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_capstone_andres.stg_daily_bars, now model.dbt_capstone_andres.stg_treasury_revenue_collections)
[0m15:20:10.933932 [debug] [Thread-1 (]: Began compiling node model.dbt_capstone_andres.stg_treasury_revenue_collections
[0m15:20:10.938859 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_capstone_andres.stg_treasury_revenue_collections"
[0m15:20:10.940236 [debug] [Thread-1 (]: Began executing node model.dbt_capstone_andres.stg_treasury_revenue_collections
[0m15:20:10.946345 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_capstone_andres.stg_treasury_revenue_collections"
[0m15:20:10.948167 [debug] [Thread-1 (]: Using snowflake connection "model.dbt_capstone_andres.stg_treasury_revenue_collections"
[0m15:20:10.948927 [debug] [Thread-1 (]: On model.dbt_capstone_andres.stg_treasury_revenue_collections: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "model.dbt_capstone_andres.stg_treasury_revenue_collections"} */
create or replace transient table DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections
         as
        (with source as (

    select * from dataexpert_student.andres.fiscal_data

),

renamed as (

    select
        record_date as collection_date,
        channel_type_desc as channel_type,
        tax_category_desc as tax_category,
        round(net_collections_amt,0) as dollars_collected_by_treasury

    from source

)

select * from renamed
        );
[0m15:20:10.949405 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:20:12.510639 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 1.561 seconds
[0m15:20:12.514217 [debug] [Thread-1 (]: On model.dbt_capstone_andres.stg_treasury_revenue_collections: Close
[0m15:20:12.761014 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '28b07765-eaab-4e9c-ba2c-c406c00afc16', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1157a8e90>]}
[0m15:20:12.761785 [info ] [Thread-1 (]: 4 of 11 OK created sql table model andres.stg_treasury_revenue_collections ..... [[32mSUCCESS 1[0m in 1.83s]
[0m15:20:12.762475 [debug] [Thread-1 (]: Finished running node model.dbt_capstone_andres.stg_treasury_revenue_collections
[0m15:20:12.762977 [debug] [Thread-1 (]: Began running node test.dbt_capstone_andres.not_null_my_first_dbt_model_id.5fb22c2710
[0m15:20:12.763630 [info ] [Thread-1 (]: 5 of 11 START test not_null_my_first_dbt_model_id .............................. [RUN]
[0m15:20:12.764185 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_capstone_andres.stg_treasury_revenue_collections, now test.dbt_capstone_andres.not_null_my_first_dbt_model_id.5fb22c2710)
[0m15:20:12.764624 [debug] [Thread-1 (]: Began compiling node test.dbt_capstone_andres.not_null_my_first_dbt_model_id.5fb22c2710
[0m15:20:12.779051 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_capstone_andres.not_null_my_first_dbt_model_id.5fb22c2710"
[0m15:20:12.779880 [debug] [Thread-1 (]: Began executing node test.dbt_capstone_andres.not_null_my_first_dbt_model_id.5fb22c2710
[0m15:20:12.797230 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_capstone_andres.not_null_my_first_dbt_model_id.5fb22c2710"
[0m15:20:12.798371 [debug] [Thread-1 (]: Using snowflake connection "test.dbt_capstone_andres.not_null_my_first_dbt_model_id.5fb22c2710"
[0m15:20:12.798814 [debug] [Thread-1 (]: On test.dbt_capstone_andres.not_null_my_first_dbt_model_id.5fb22c2710: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "test.dbt_capstone_andres.not_null_my_first_dbt_model_id.5fb22c2710"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from DATAEXPERT_STUDENT.andres.my_first_dbt_model
where id is null



      
    ) dbt_internal_test
[0m15:20:12.799226 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:20:13.597870 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 0.799 seconds
[0m15:20:13.602654 [debug] [Thread-1 (]: On test.dbt_capstone_andres.not_null_my_first_dbt_model_id.5fb22c2710: Close
[0m15:20:13.832437 [error] [Thread-1 (]: 5 of 11 FAIL 1 not_null_my_first_dbt_model_id .................................. [[31mFAIL 1[0m in 1.07s]
[0m15:20:13.833225 [debug] [Thread-1 (]: Finished running node test.dbt_capstone_andres.not_null_my_first_dbt_model_id.5fb22c2710
[0m15:20:13.833704 [debug] [Thread-1 (]: Began running node test.dbt_capstone_andres.unique_my_first_dbt_model_id.16e066b321
[0m15:20:13.834300 [debug] [Thread-4 (]: Marking all children of 'test.dbt_capstone_andres.not_null_my_first_dbt_model_id.5fb22c2710' to be skipped because of status 'fail'.  Reason: Got 1 result, configured to fail if != 0.
[0m15:20:13.834695 [info ] [Thread-1 (]: 6 of 11 START test unique_my_first_dbt_model_id ................................ [RUN]
[0m15:20:13.835913 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_capstone_andres.not_null_my_first_dbt_model_id.5fb22c2710, now test.dbt_capstone_andres.unique_my_first_dbt_model_id.16e066b321)
[0m15:20:13.836350 [debug] [Thread-1 (]: Began compiling node test.dbt_capstone_andres.unique_my_first_dbt_model_id.16e066b321
[0m15:20:13.843813 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_capstone_andres.unique_my_first_dbt_model_id.16e066b321"
[0m15:20:13.844651 [debug] [Thread-1 (]: Began executing node test.dbt_capstone_andres.unique_my_first_dbt_model_id.16e066b321
[0m15:20:13.847964 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_capstone_andres.unique_my_first_dbt_model_id.16e066b321"
[0m15:20:13.849066 [debug] [Thread-1 (]: Using snowflake connection "test.dbt_capstone_andres.unique_my_first_dbt_model_id.16e066b321"
[0m15:20:13.849505 [debug] [Thread-1 (]: On test.dbt_capstone_andres.unique_my_first_dbt_model_id.16e066b321: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "test.dbt_capstone_andres.unique_my_first_dbt_model_id.16e066b321"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from DATAEXPERT_STUDENT.andres.my_first_dbt_model
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
[0m15:20:13.849924 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:20:14.541354 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 0.691 seconds
[0m15:20:14.544022 [debug] [Thread-1 (]: On test.dbt_capstone_andres.unique_my_first_dbt_model_id.16e066b321: Close
[0m15:20:14.768612 [info ] [Thread-1 (]: 6 of 11 PASS unique_my_first_dbt_model_id ...................................... [[32mPASS[0m in 0.93s]
[0m15:20:14.769734 [debug] [Thread-1 (]: Finished running node test.dbt_capstone_andres.unique_my_first_dbt_model_id.16e066b321
[0m15:20:14.770553 [debug] [Thread-1 (]: Began running node unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum
[0m15:20:14.771384 [info ] [Thread-1 (]: 7 of 11 START unit_test weekly_metrics::test_amount_sum ........................ [RUN]
[0m15:20:14.772369 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_capstone_andres.unique_my_first_dbt_model_id.16e066b321, now unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum)
[0m15:20:14.773040 [debug] [Thread-1 (]: Began compiling node unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum
[0m15:20:14.773690 [debug] [Thread-1 (]: Began executing node unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum
[0m15:20:14.834517 [debug] [Thread-1 (]: Using snowflake connection "unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum"
[0m15:20:14.835303 [debug] [Thread-1 (]: On unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum"} */
describe table DATAEXPERT_STUDENT.andres.stg_daily_bars
[0m15:20:14.835858 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:20:15.835404 [debug] [Thread-1 (]: SQL status: SUCCESS 9 in 1.000 seconds
[0m15:20:15.873915 [debug] [Thread-1 (]: Writing injected SQL for node "unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum"
[0m15:20:15.883844 [debug] [Thread-1 (]: Using snowflake connection "unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum"
[0m15:20:15.884675 [debug] [Thread-1 (]: On unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum"} */
describe table DATAEXPERT_STUDENT.andres.dim_date
[0m15:20:16.075435 [debug] [Thread-1 (]: SQL status: SUCCESS 36 in 0.190 seconds
[0m15:20:16.092465 [debug] [Thread-1 (]: Writing injected SQL for node "unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum"
[0m15:20:16.101844 [debug] [Thread-1 (]: Using snowflake connection "unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum"
[0m15:20:16.102428 [debug] [Thread-1 (]: On unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum"} */
describe table DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections
[0m15:20:16.267140 [debug] [Thread-1 (]: SQL status: SUCCESS 4 in 0.164 seconds
[0m15:20:16.278997 [debug] [Thread-1 (]: Writing injected SQL for node "unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum"
[0m15:20:16.288120 [debug] [Thread-1 (]: Writing injected SQL for node "unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum"
[0m15:20:16.320978 [debug] [Thread-1 (]: Using snowflake connection "unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum"
[0m15:20:16.321942 [debug] [Thread-1 (]: On unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum"} */
create or replace temporary table DATAEXPERT_STUDENT.andres.test_amount_sum__dbt_tmp
         as
        (select * from (
        with  __dbt__cte__stg_daily_bars as (

-- Fixture for stg_daily_bars
select 
    
        try_cast('2020-11-20' as DATE)
     as date, 
    
        try_cast('\'L\'' as character varying(16777216))
     as ticker, 
    
        try_cast('100' as FLOAT)
     as number_of_shares_traded, 
    
        try_cast('10' as NUMBER(38,0))
     as number_of_trades, try_cast(null as FLOAT) as volume_weighted_avg_price, try_cast(null as FLOAT) as open_price, try_cast(null as FLOAT) as close_price, try_cast(null as FLOAT) as high_price, try_cast(null as FLOAT) as low_price
union all
select 
    
        try_cast('2020-11-20' as DATE)
     as date, 
    
        try_cast('\'S\'' as character varying(16777216))
     as ticker, 
    
        try_cast('200' as FLOAT)
     as number_of_shares_traded, 
    
        try_cast('20' as NUMBER(38,0))
     as number_of_trades, try_cast(null as FLOAT) as volume_weighted_avg_price, try_cast(null as FLOAT) as open_price, try_cast(null as FLOAT) as close_price, try_cast(null as FLOAT) as high_price, try_cast(null as FLOAT) as low_price
union all
select 
    
        try_cast('2020-11-21' as DATE)
     as date, 
    
        try_cast('\'L\'' as character varying(16777216))
     as ticker, 
    
        try_cast('300' as FLOAT)
     as number_of_shares_traded, 
    
        try_cast('40' as NUMBER(38,0))
     as number_of_trades, try_cast(null as FLOAT) as volume_weighted_avg_price, try_cast(null as FLOAT) as open_price, try_cast(null as FLOAT) as close_price, try_cast(null as FLOAT) as high_price, try_cast(null as FLOAT) as low_price
union all
select 
    
        try_cast('2020-11-21' as DATE)
     as date, 
    
        try_cast('\'S\'' as character varying(16777216))
     as ticker, 
    
        try_cast('200' as FLOAT)
     as number_of_shares_traded, 
    
        try_cast('50' as NUMBER(38,0))
     as number_of_trades, try_cast(null as FLOAT) as volume_weighted_avg_price, try_cast(null as FLOAT) as open_price, try_cast(null as FLOAT) as close_price, try_cast(null as FLOAT) as high_price, try_cast(null as FLOAT) as low_price
),  __dbt__cte__dim_date as (

-- Fixture for dim_date
select 
    
        try_cast('2020-11-20' as DATE)
     as date_day, try_cast(null as DATE) as prior_date_day, try_cast(null as DATE) as next_date_day, try_cast(null as DATE) as prior_year_date_day, try_cast(null as DATE) as prior_year_over_year_date_day, try_cast(null as NUMBER(3,0)) as day_of_week, try_cast(null as NUMBER(2,0)) as day_of_week_iso, try_cast(null as character varying(16777216)) as day_of_week_name, try_cast(null as character varying(3)) as day_of_week_name_short, try_cast(null as NUMBER(2,0)) as day_of_month, try_cast(null as NUMBER(4,0)) as day_of_year, try_cast(null as DATE) as week_start_date, try_cast(null as DATE) as week_end_date, try_cast(null as DATE) as prior_year_week_start_date, try_cast(null as DATE) as prior_year_week_end_date, try_cast(null as NUMBER(38,0)) as week_of_year, try_cast(null as DATE) as iso_week_start_date, try_cast(null as DATE) as iso_week_end_date, try_cast(null as DATE) as prior_year_iso_week_start_date, try_cast(null as DATE) as prior_year_iso_week_end_date, try_cast(null as NUMBER(38,0)) as iso_week_of_year, try_cast(null as NUMBER(38,0)) as prior_year_week_of_year, try_cast(null as NUMBER(38,0)) as prior_year_iso_week_of_year, try_cast(null as NUMBER(38,0)) as month_of_year, try_cast(null as character varying(16777216)) as month_name, try_cast(null as character varying(16777216)) as month_name_short, try_cast(null as DATE) as month_start_date, try_cast(null as DATE) as month_end_date, try_cast(null as DATE) as prior_year_month_start_date, try_cast(null as DATE) as prior_year_month_end_date, try_cast(null as NUMBER(38,0)) as quarter_of_year, try_cast(null as DATE) as quarter_start_date, try_cast(null as DATE) as quarter_end_date, try_cast(null as NUMBER(38,0)) as year_number, try_cast(null as DATE) as year_start_date, try_cast(null as DATE) as year_end_date
union all
select 
    
        try_cast('2020-11-21' as DATE)
     as date_day, try_cast(null as DATE) as prior_date_day, try_cast(null as DATE) as next_date_day, try_cast(null as DATE) as prior_year_date_day, try_cast(null as DATE) as prior_year_over_year_date_day, try_cast(null as NUMBER(3,0)) as day_of_week, try_cast(null as NUMBER(2,0)) as day_of_week_iso, try_cast(null as character varying(16777216)) as day_of_week_name, try_cast(null as character varying(3)) as day_of_week_name_short, try_cast(null as NUMBER(2,0)) as day_of_month, try_cast(null as NUMBER(4,0)) as day_of_year, try_cast(null as DATE) as week_start_date, try_cast(null as DATE) as week_end_date, try_cast(null as DATE) as prior_year_week_start_date, try_cast(null as DATE) as prior_year_week_end_date, try_cast(null as NUMBER(38,0)) as week_of_year, try_cast(null as DATE) as iso_week_start_date, try_cast(null as DATE) as iso_week_end_date, try_cast(null as DATE) as prior_year_iso_week_start_date, try_cast(null as DATE) as prior_year_iso_week_end_date, try_cast(null as NUMBER(38,0)) as iso_week_of_year, try_cast(null as NUMBER(38,0)) as prior_year_week_of_year, try_cast(null as NUMBER(38,0)) as prior_year_iso_week_of_year, try_cast(null as NUMBER(38,0)) as month_of_year, try_cast(null as character varying(16777216)) as month_name, try_cast(null as character varying(16777216)) as month_name_short, try_cast(null as DATE) as month_start_date, try_cast(null as DATE) as month_end_date, try_cast(null as DATE) as prior_year_month_start_date, try_cast(null as DATE) as prior_year_month_end_date, try_cast(null as NUMBER(38,0)) as quarter_of_year, try_cast(null as DATE) as quarter_start_date, try_cast(null as DATE) as quarter_end_date, try_cast(null as NUMBER(38,0)) as year_number, try_cast(null as DATE) as year_start_date, try_cast(null as DATE) as year_end_date
),  __dbt__cte__stg_treasury_revenue_collections as (

-- Fixture for stg_treasury_revenue_collections
select 
    
        try_cast('2020-11-20' as DATE)
     as collection_date, try_cast(null as character varying(16777216)) as channel_type, 
    
        try_cast('\'IRS Tax\'' as character varying(16777216))
     as tax_category, 
    
        try_cast('1000' as NUMBER(19,0))
     as dollars_collected_by_treasury
union all
select 
    
        try_cast('2020-11-20' as DATE)
     as collection_date, try_cast(null as character varying(16777216)) as channel_type, 
    
        try_cast('\'IRS Non Tax\'' as character varying(16777216))
     as tax_category, 
    
        try_cast('2000' as NUMBER(19,0))
     as dollars_collected_by_treasury
union all
select 
    
        try_cast('2020-11-21' as DATE)
     as collection_date, try_cast(null as character varying(16777216)) as channel_type, 
    
        try_cast('\'IRS Tax\'' as character varying(16777216))
     as tax_category, 
    
        try_cast('3000' as NUMBER(19,0))
     as dollars_collected_by_treasury
union all
select 
    
        try_cast('2020-11-21' as DATE)
     as collection_date, try_cast(null as character varying(16777216)) as channel_type, 
    
        try_cast('\'IRS Tax\'' as character varying(16777216))
     as tax_category, 
    
        try_cast('2000' as NUMBER(19,0))
     as dollars_collected_by_treasury
), pol as (
SELECT week_start_date, 
sum(number_of_shares_traded) number_of_shares_traded,
sum(number_of_trades) number_of_trades
from  __dbt__cte__stg_daily_bars a 
JOIN __dbt__cte__dim_date d on d.date_day=a.date
group by 1)

, rev as (
select week_start_date,
sum(dollars_collected_by_treasury) dollars_collected_by_treasury,
sum(case when tax_category = 'IRS Tax' then dollars_collected_by_treasury else 0 end) as irs_tax_dollars_collected, 
sum(case when tax_category = 'Non-Tax' then dollars_collected_by_treasury else 0 end) as non_irs_non_tax_dollars,
sum(case when tax_category = 'IRS Non-Tax' then dollars_collected_by_treasury else 0 end) as irs_non_tax_dollars
FROM __dbt__cte__stg_treasury_revenue_collections a 
JOIN __dbt__cte__dim_date d on d.date_day=a.collection_date
GROUP BY ALL)

select 
    a.week_start_date, 
    a.number_of_shares_traded, 
    a.number_of_trades,
    b.dollars_collected_by_treasury,
    b.irs_tax_dollars

from pol a 
INNER JOIN rev b on b.week_start_date = a.week_start_date
    ) as __dbt_sbq
    where false
    limit 0

        );
[0m15:20:16.544245 [debug] [Thread-1 (]: Snowflake adapter: Snowflake query id: 01b8ca44-0004-4530-0000-33eb0039f1a2
[0m15:20:16.544819 [debug] [Thread-1 (]: Snowflake adapter: Snowflake error: 000904 (42000): SQL compilation error: error line 143 at position 4
invalid identifier 'B.IRS_TAX_DOLLARS'
[0m15:20:16.545528 [debug] [Thread-1 (]: On unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum: Close
[0m15:20:16.791100 [debug] [Thread-1 (]: Runtime Error in unit_test test_amount_sum (models/marts/_marts.yml)
  An error occurred during execution of unit test 'test_amount_sum'. There may be an error in the unit test definition: check the data types.
   Database Error
    000904 (42000): SQL compilation error: error line 143 at position 4
    invalid identifier 'B.IRS_TAX_DOLLARS'
[0m15:20:16.791838 [error] [Thread-1 (]: 7 of 11 ERROR weekly_metrics::test_amount_sum .................................. [[31mERROR[0m in 2.02s]
[0m15:20:16.792570 [debug] [Thread-1 (]: Finished running node unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum
[0m15:20:16.793156 [debug] [Thread-1 (]: Marking all children of 'unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum' to be skipped because of status 'error'.  Reason: Runtime Error in unit_test test_amount_sum (models/marts/_marts.yml)
  An error occurred during execution of unit test 'test_amount_sum'. There may be an error in the unit test definition: check the data types.
   Database Error
    000904 (42000): SQL compilation error: error line 143 at position 4
    invalid identifier 'B.IRS_TAX_DOLLARS'.
[0m15:20:16.793679 [debug] [Thread-1 (]: Began running node model.dbt_capstone_andres.weekly_metrics
[0m15:20:16.794212 [info ] [Thread-1 (]: 9 of 11 SKIP relation andres.weekly_metrics .................................... [[33mSKIP[0m]
[0m15:20:16.794814 [debug] [Thread-1 (]: Finished running node model.dbt_capstone_andres.weekly_metrics
[0m15:20:16.795320 [debug] [Thread-1 (]: Began running node model.dbt_capstone_andres.my_second_dbt_model
[0m15:20:16.795770 [debug] [Thread-4 (]: Marking all children of 'model.dbt_capstone_andres.weekly_metrics' to be skipped because of status 'skipped'. 
[0m15:20:16.796195 [info ] [Thread-1 (]: 8 of 11 SKIP relation andres.my_second_dbt_model ............................... [[33mSKIP[0m]
[0m15:20:16.796818 [debug] [Thread-1 (]: Finished running node model.dbt_capstone_andres.my_second_dbt_model
[0m15:20:16.797240 [debug] [Thread-4 (]: Marking all children of 'model.dbt_capstone_andres.my_second_dbt_model' to be skipped because of status 'skipped'. 
[0m15:20:16.798604 [debug] [Thread-1 (]: Began running node test.dbt_capstone_andres.not_null_my_second_dbt_model_id.151b76d778
[0m15:20:16.799138 [info ] [Thread-1 (]: 10 of 11 SKIP test not_null_my_second_dbt_model_id ............................. [[33mSKIP[0m]
[0m15:20:16.799639 [debug] [Thread-1 (]: Finished running node test.dbt_capstone_andres.not_null_my_second_dbt_model_id.151b76d778
[0m15:20:16.800088 [debug] [Thread-1 (]: Began running node test.dbt_capstone_andres.unique_my_second_dbt_model_id.57a0f8c493
[0m15:20:16.800601 [debug] [Thread-4 (]: Marking all children of 'test.dbt_capstone_andres.not_null_my_second_dbt_model_id.151b76d778' to be skipped because of status 'skipped'. 
[0m15:20:16.801154 [info ] [Thread-1 (]: 11 of 11 SKIP test unique_my_second_dbt_model_id ............................... [[33mSKIP[0m]
[0m15:20:16.801855 [debug] [Thread-1 (]: Finished running node test.dbt_capstone_andres.unique_my_second_dbt_model_id.57a0f8c493
[0m15:20:16.802311 [debug] [Thread-4 (]: Marking all children of 'test.dbt_capstone_andres.unique_my_second_dbt_model_id.57a0f8c493' to be skipped because of status 'skipped'. 
[0m15:20:16.803719 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:20:16.804295 [debug] [MainThread]: Connection 'unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum' was properly closed.
[0m15:20:16.804921 [info ] [MainThread]: 
[0m15:20:16.805391 [info ] [MainThread]: Finished running 6 table models, 4 data tests, 1 unit test in 0 hours 0 minutes and 15.52 seconds (15.52s).
[0m15:20:16.807243 [debug] [MainThread]: Command end result
[0m15:20:16.870518 [info ] [MainThread]: 
[0m15:20:16.871549 [info ] [MainThread]: [31mCompleted with 2 errors, 0 partial successs, and 0 warnings:[0m
[0m15:20:16.872204 [info ] [MainThread]: 
[0m15:20:16.872994 [error] [MainThread]: [31mFailure in test not_null_my_first_dbt_model_id (models/example/schema.yml)[0m
[0m15:20:16.873962 [error] [MainThread]:   Got 1 result, configured to fail if != 0
[0m15:20:16.874666 [info ] [MainThread]: 
[0m15:20:16.875432 [info ] [MainThread]:   compiled code at target/compiled/dbt_capstone_andres/models/example/schema.yml/not_null_my_first_dbt_model_id.sql
[0m15:20:16.876922 [info ] [MainThread]: 
[0m15:20:16.877648 [error] [MainThread]:   Runtime Error in unit_test test_amount_sum (models/marts/_marts.yml)
  An error occurred during execution of unit test 'test_amount_sum'. There may be an error in the unit test definition: check the data types.
   Database Error
    000904 (42000): SQL compilation error: error line 143 at position 4
    invalid identifier 'B.IRS_TAX_DOLLARS'
[0m15:20:16.878217 [info ] [MainThread]: 
[0m15:20:16.878724 [info ] [MainThread]: Done. PASS=5 WARN=0 ERROR=2 SKIP=4 TOTAL=11
[0m15:20:16.883333 [debug] [MainThread]: Resource report: {"command_name": "build", "command_wall_clock_time": 17.249271, "process_user_time": 3.754279, "process_kernel_time": 0.610818, "process_mem_max_rss": "192020480", "command_success": false, "process_in_blocks": "0", "process_out_blocks": "0"}
[0m15:20:16.884846 [debug] [MainThread]: Command `dbt build` failed at 15:20:16.884668 after 17.25 seconds
[0m15:20:16.885733 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10af23f10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10af23c90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11b82c350>]}
[0m15:20:16.886326 [debug] [MainThread]: Flushing usage events
[0m15:20:17.064194 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:20:51.579788 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105691d10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105706710>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105706d10>]}


============================== 15:20:51.585397 | 422f580c-14f5-4adc-9a46-514b42025efd ==============================
[0m15:20:51.585397 [info ] [MainThread]: Running with dbt=1.9.0-b3
[0m15:20:51.586140 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'version_check': 'True', 'debug': 'False', 'log_path': 'logs', 'fail_fast': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'log_format': 'default', 'static_parser': 'True', 'target_path': 'None', 'invocation_command': 'dbt build', 'send_anonymous_usage_stats': 'True'}
[0m15:20:53.267618 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '422f580c-14f5-4adc-9a46-514b42025efd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1090b72d0>]}
[0m15:20:53.326382 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '422f580c-14f5-4adc-9a46-514b42025efd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1042b8f10>]}
[0m15:20:53.327759 [info ] [MainThread]: Registered adapter: snowflake=1.8.4
[0m15:20:53.532948 [debug] [MainThread]: checksum: 5f4fde221ca8627174a0341efc73099aaec05b8546408ceed6f59b1ec0755115, vars: {}, profile: , target: , version: 1.9.0b3
[0m15:20:53.775916 [debug] [MainThread]: Partial parsing enabled: 3 files deleted, 0 files added, 0 files changed.
[0m15:20:53.776549 [debug] [MainThread]: Partial parsing: deleted file: dbt_capstone_andres://models/example/my_first_dbt_model.sql
[0m15:20:53.776868 [debug] [MainThread]: Partial parsing: deleted file: dbt_capstone_andres://models/example/my_second_dbt_model.sql
[0m15:20:53.863275 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '422f580c-14f5-4adc-9a46-514b42025efd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ac5e990>]}
[0m15:20:54.073854 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '422f580c-14f5-4adc-9a46-514b42025efd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10be85c90>]}
[0m15:20:54.074418 [info ] [MainThread]: Found 4 models, 2 sources, 862 macros, 1 unit test
[0m15:20:54.076510 [info ] [MainThread]: 
[0m15:20:54.076879 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m15:20:54.077205 [info ] [MainThread]: 
[0m15:20:54.077702 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m15:20:54.082842 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_DATAEXPERT_STUDENT'
[0m15:20:54.098110 [debug] [ThreadPool]: Using snowflake connection "list_DATAEXPERT_STUDENT"
[0m15:20:54.098545 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "connection_name": "list_DATAEXPERT_STUDENT"} */
show terse schemas in database DATAEXPERT_STUDENT
    limit 10000
[0m15:20:54.098868 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:20:54.882519 [debug] [ThreadPool]: SQL status: SUCCESS 73 in 0.784 seconds
[0m15:20:54.885296 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT: Close
[0m15:20:55.114946 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_DATAEXPERT_STUDENT, now list_DATAEXPERT_STUDENT_andres)
[0m15:20:55.124823 [debug] [ThreadPool]: Using snowflake connection "list_DATAEXPERT_STUDENT_andres"
[0m15:20:55.125292 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "connection_name": "list_DATAEXPERT_STUDENT_andres"} */
show objects in DATAEXPERT_STUDENT.andres limit 10000
[0m15:20:55.125650 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:20:55.839672 [debug] [ThreadPool]: SQL status: SUCCESS 25 in 0.714 seconds
[0m15:20:55.842382 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: Close
[0m15:20:56.093890 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '422f580c-14f5-4adc-9a46-514b42025efd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ba7c910>]}
[0m15:20:56.096777 [debug] [Thread-1 (]: Began running node model.dbt_capstone_andres.dim_date
[0m15:20:56.097368 [info ] [Thread-1 (]: 1 of 5 START sql table model andres.dim_date ................................... [RUN]
[0m15:20:56.097878 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_DATAEXPERT_STUDENT_andres, now model.dbt_capstone_andres.dim_date)
[0m15:20:56.098278 [debug] [Thread-1 (]: Began compiling node model.dbt_capstone_andres.dim_date
[0m15:20:56.138920 [debug] [Thread-1 (]: Using snowflake connection "model.dbt_capstone_andres.dim_date"
[0m15:20:56.139550 [debug] [Thread-1 (]: On model.dbt_capstone_andres.dim_date: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "model.dbt_capstone_andres.dim_date"} */
select datediff(
        day,
        cast('1995-01-01' as timestamp),
        cast('2040-12-31' as timestamp)
        )
[0m15:20:56.140056 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:20:56.881603 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 0.741 seconds
[0m15:20:56.959786 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_capstone_andres.dim_date"
[0m15:20:56.961481 [debug] [Thread-1 (]: Began executing node model.dbt_capstone_andres.dim_date
[0m15:20:57.000400 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_capstone_andres.dim_date"
[0m15:20:57.008898 [debug] [Thread-1 (]: Using snowflake connection "model.dbt_capstone_andres.dim_date"
[0m15:20:57.009755 [debug] [Thread-1 (]: On model.dbt_capstone_andres.dim_date: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "model.dbt_capstone_andres.dim_date"} */
create or replace transient table DATAEXPERT_STUDENT.andres.dim_date
         as
        (

    
with base_dates as (
    
    with date_spine as
(

    





with rawdata as (

    

    

    with p as (
        select 0 as generated_number union all select 1
    ), unioned as (

    select

    
    p0.generated_number * power(2, 0)
     + 
    
    p1.generated_number * power(2, 1)
     + 
    
    p2.generated_number * power(2, 2)
     + 
    
    p3.generated_number * power(2, 3)
     + 
    
    p4.generated_number * power(2, 4)
     + 
    
    p5.generated_number * power(2, 5)
     + 
    
    p6.generated_number * power(2, 6)
     + 
    
    p7.generated_number * power(2, 7)
     + 
    
    p8.generated_number * power(2, 8)
     + 
    
    p9.generated_number * power(2, 9)
     + 
    
    p10.generated_number * power(2, 10)
     + 
    
    p11.generated_number * power(2, 11)
     + 
    
    p12.generated_number * power(2, 12)
     + 
    
    p13.generated_number * power(2, 13)
     + 
    
    p14.generated_number * power(2, 14)
    
    
    + 1
    as generated_number

    from

    
    p as p0
     cross join 
    
    p as p1
     cross join 
    
    p as p2
     cross join 
    
    p as p3
     cross join 
    
    p as p4
     cross join 
    
    p as p5
     cross join 
    
    p as p6
     cross join 
    
    p as p7
     cross join 
    
    p as p8
     cross join 
    
    p as p9
     cross join 
    
    p as p10
     cross join 
    
    p as p11
     cross join 
    
    p as p12
     cross join 
    
    p as p13
     cross join 
    
    p as p14
    
    

    )

    select *
    from unioned
    where generated_number <= 16801
    order by generated_number



),

all_periods as (

    select (
        

    dateadd(
        day,
        (row_number() over (order by 1) - 1),
        cast('1995-01-01' as timestamp)
        )


    ) as date_day
    from rawdata

),

filtered as (

    select *
    from all_periods
    where date_day <= cast('2040-12-31' as timestamp)

)

select * from filtered



)
select
    cast(d.date_day as timestamp) as date_day
from
    date_spine d


),
dates_with_prior_year_dates as (

    select
        cast(d.date_day as date) as date_day,
        cast(

    dateadd(
        year,
        -1,
        d.date_day
        )

 as date) as prior_year_date_day,
        cast(

    dateadd(
        day,
        -364,
        d.date_day
        )

 as date) as prior_year_over_year_date_day
    from
    	base_dates d

)
select
    d.date_day,
    cast(

    dateadd(
        day,
        -1,
        d.date_day
        )

 as date) as prior_date_day,
    cast(

    dateadd(
        day,
        1,
        d.date_day
        )

 as date) as next_date_day,
    d.prior_year_date_day as prior_year_date_day,
    d.prior_year_over_year_date_day,
    case
            when date_part('dayofweek', d.date_day) = 7 then 1
            else date_part('dayofweek', d.date_day) + 1
        end as day_of_week,
    date_part('dayofweekiso', d.date_day) as day_of_week_iso,
    -- long version not implemented on Snowflake so we're doing it manually :/
    case dayname(d.date_day)
        when 'Mon' then 'Monday'
        when 'Tue' then 'Tuesday'
        when 'Wed' then 'Wednesday'
        when 'Thu' then 'Thursday'
        when 'Fri' then 'Friday'
        when 'Sat' then 'Saturday'
        when 'Sun' then 'Sunday'
    end as day_of_week_name,
    dayname(d.date_day) as day_of_week_name_short,
    date_part('day', d.date_day) as day_of_month,
    date_part('dayofyear', d.date_day) as day_of_year,

    
    
    cast(

    dateadd(
        day,
        -1 * (case
            when date_part('dayofweek', d.date_day) = 7 then 1
            else date_part('dayofweek', d.date_day) + 1
        end - 1),
        d.date_day
        )

 as date) as week_start_date,
    cast(

    dateadd(
        day,
        6,
        
    
    cast(

    dateadd(
        day,
        -1 * (case
            when date_part('dayofweek', d.date_day) = 7 then 1
            else date_part('dayofweek', d.date_day) + 1
        end - 1),
        d.date_day
        )

 as date)
        )

 as date) as week_end_date,
    
    
    cast(

    dateadd(
        day,
        -1 * (case
            when date_part('dayofweek', d.prior_year_over_year_date_day) = 7 then 1
            else date_part('dayofweek', d.prior_year_over_year_date_day) + 1
        end - 1),
        d.prior_year_over_year_date_day
        )

 as date) as prior_year_week_start_date,
    cast(

    dateadd(
        day,
        6,
        
    
    cast(

    dateadd(
        day,
        -1 * (case
            when date_part('dayofweek', d.prior_year_over_year_date_day) = 7 then 1
            else date_part('dayofweek', d.prior_year_over_year_date_day) + 1
        end - 1),
        d.prior_year_over_year_date_day
        )

 as date)
        )

 as date) as prior_year_week_end_date,
    cast(date_part('week', d.date_day) as integer) as week_of_year,

    cast(date_trunc('week', d.date_day) as date) as iso_week_start_date,
    cast(

    dateadd(
        day,
        6,
        cast(date_trunc('week', d.date_day) as date)
        )

 as date) as iso_week_end_date,
    cast(date_trunc('week', d.prior_year_over_year_date_day) as date) as prior_year_iso_week_start_date,
    cast(

    dateadd(
        day,
        6,
        cast(date_trunc('week', d.prior_year_over_year_date_day) as date)
        )

 as date) as prior_year_iso_week_end_date,
    cast(date_part('weekiso', d.date_day) as integer) as iso_week_of_year,

    cast(date_part('week', d.prior_year_over_year_date_day) as integer) as prior_year_week_of_year,
    cast(date_part('weekiso', d.prior_year_over_year_date_day) as integer) as prior_year_iso_week_of_year,

    cast(date_part('month', d.date_day) as integer) as month_of_year,
    to_char(d.date_day, 'MMMM')  as month_name,
    to_char(d.date_day, 'MON')  as month_name_short,

    cast(date_trunc('month', d.date_day) as date) as month_start_date,
    cast(cast(
        

    dateadd(
        day,
        -1,
        

    dateadd(
        month,
        1,
        date_trunc('month', d.date_day)
        )


        )


        as date) as date) as month_end_date,

    cast(date_trunc('month', d.prior_year_date_day) as date) as prior_year_month_start_date,
    cast(cast(
        

    dateadd(
        day,
        -1,
        

    dateadd(
        month,
        1,
        date_trunc('month', d.prior_year_date_day)
        )


        )


        as date) as date) as prior_year_month_end_date,

    cast(date_part('quarter', d.date_day) as integer) as quarter_of_year,
    cast(date_trunc('quarter', d.date_day) as date) as quarter_start_date,
    cast(cast(
        

    dateadd(
        day,
        -1,
        

    dateadd(
        quarter,
        1,
        date_trunc('quarter', d.date_day)
        )


        )


        as date) as date) as quarter_end_date,

    cast(date_part('year', d.date_day) as integer) as year_number,
    cast(date_trunc('year', d.date_day) as date) as year_start_date,
    cast(cast(
        

    dateadd(
        day,
        -1,
        

    dateadd(
        year,
        1,
        date_trunc('year', d.date_day)
        )


        )


        as date) as date) as year_end_date
from
    dates_with_prior_year_dates d
order by 1


        );
[0m15:20:58.452212 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 1.442 seconds
[0m15:20:58.480147 [debug] [Thread-1 (]: On model.dbt_capstone_andres.dim_date: Close
[0m15:20:58.741391 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '422f580c-14f5-4adc-9a46-514b42025efd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10fdbed50>]}
[0m15:20:58.742276 [info ] [Thread-1 (]: 1 of 5 OK created sql table model andres.dim_date .............................. [[32mSUCCESS 1[0m in 2.64s]
[0m15:20:58.743055 [debug] [Thread-1 (]: Finished running node model.dbt_capstone_andres.dim_date
[0m15:20:58.743590 [debug] [Thread-1 (]: Began running node model.dbt_capstone_andres.stg_daily_bars
[0m15:20:58.744270 [info ] [Thread-1 (]: 2 of 5 START sql table model andres.stg_daily_bars ............................. [RUN]
[0m15:20:58.744879 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_capstone_andres.dim_date, now model.dbt_capstone_andres.stg_daily_bars)
[0m15:20:58.745375 [debug] [Thread-1 (]: Began compiling node model.dbt_capstone_andres.stg_daily_bars
[0m15:20:58.750417 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_capstone_andres.stg_daily_bars"
[0m15:20:58.751513 [debug] [Thread-1 (]: Began executing node model.dbt_capstone_andres.stg_daily_bars
[0m15:20:58.756144 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_capstone_andres.stg_daily_bars"
[0m15:20:58.757818 [debug] [Thread-1 (]: Using snowflake connection "model.dbt_capstone_andres.stg_daily_bars"
[0m15:20:58.758346 [debug] [Thread-1 (]: On model.dbt_capstone_andres.stg_daily_bars: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "model.dbt_capstone_andres.stg_daily_bars"} */
create or replace transient table DATAEXPERT_STUDENT.andres.stg_daily_bars
         as
        (with source as (

    select * from dataexpert_student.andres.daily_bars

),

renamed as (

    select
        TO_DATE(TO_TIMESTAMP(timestamp / 1000)) as date,
        ticker as ticker,
        v  as number_of_shares_traded,
        n  as number_of_trades,
        round(vw,2) as volume_weighted_avg_price,
        o  as open_price,
        c  as close_price,
        h  as high_price,
        l  as low_price
    from source

)

select * from renamed
        );
[0m15:20:58.758854 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:21:00.682046 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 1.923 seconds
[0m15:21:00.685426 [debug] [Thread-1 (]: On model.dbt_capstone_andres.stg_daily_bars: Close
[0m15:21:00.937469 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '422f580c-14f5-4adc-9a46-514b42025efd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b97c590>]}
[0m15:21:00.938594 [info ] [Thread-1 (]: 2 of 5 OK created sql table model andres.stg_daily_bars ........................ [[32mSUCCESS 1[0m in 2.19s]
[0m15:21:00.939511 [debug] [Thread-1 (]: Finished running node model.dbt_capstone_andres.stg_daily_bars
[0m15:21:00.940148 [debug] [Thread-1 (]: Began running node model.dbt_capstone_andres.stg_treasury_revenue_collections
[0m15:21:00.940848 [info ] [Thread-1 (]: 3 of 5 START sql table model andres.stg_treasury_revenue_collections ........... [RUN]
[0m15:21:00.941482 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_capstone_andres.stg_daily_bars, now model.dbt_capstone_andres.stg_treasury_revenue_collections)
[0m15:21:00.941991 [debug] [Thread-1 (]: Began compiling node model.dbt_capstone_andres.stg_treasury_revenue_collections
[0m15:21:00.946726 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_capstone_andres.stg_treasury_revenue_collections"
[0m15:21:00.947736 [debug] [Thread-1 (]: Began executing node model.dbt_capstone_andres.stg_treasury_revenue_collections
[0m15:21:00.952479 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_capstone_andres.stg_treasury_revenue_collections"
[0m15:21:00.954178 [debug] [Thread-1 (]: Using snowflake connection "model.dbt_capstone_andres.stg_treasury_revenue_collections"
[0m15:21:00.954730 [debug] [Thread-1 (]: On model.dbt_capstone_andres.stg_treasury_revenue_collections: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "model.dbt_capstone_andres.stg_treasury_revenue_collections"} */
create or replace transient table DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections
         as
        (with source as (

    select * from dataexpert_student.andres.fiscal_data

),

renamed as (

    select
        record_date as collection_date,
        channel_type_desc as channel_type,
        tax_category_desc as tax_category,
        round(net_collections_amt,0) as dollars_collected_by_treasury

    from source

)

select * from renamed
        );
[0m15:21:00.955249 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:21:02.481720 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 1.526 seconds
[0m15:21:02.484733 [debug] [Thread-1 (]: On model.dbt_capstone_andres.stg_treasury_revenue_collections: Close
[0m15:21:02.731395 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '422f580c-14f5-4adc-9a46-514b42025efd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x116051410>]}
[0m15:21:02.732823 [info ] [Thread-1 (]: 3 of 5 OK created sql table model andres.stg_treasury_revenue_collections ...... [[32mSUCCESS 1[0m in 1.79s]
[0m15:21:02.734173 [debug] [Thread-1 (]: Finished running node model.dbt_capstone_andres.stg_treasury_revenue_collections
[0m15:21:02.735952 [debug] [Thread-1 (]: Began running node unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum
[0m15:21:02.736998 [info ] [Thread-1 (]: 4 of 5 START unit_test weekly_metrics::test_amount_sum ......................... [RUN]
[0m15:21:02.738640 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_capstone_andres.stg_treasury_revenue_collections, now unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum)
[0m15:21:02.739611 [debug] [Thread-1 (]: Began compiling node unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum
[0m15:21:02.740449 [debug] [Thread-1 (]: Began executing node unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum
[0m15:21:02.830512 [debug] [Thread-1 (]: Using snowflake connection "unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum"
[0m15:21:02.831675 [debug] [Thread-1 (]: On unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum"} */
describe table DATAEXPERT_STUDENT.andres.stg_daily_bars
[0m15:21:02.832447 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:21:03.600352 [debug] [Thread-1 (]: SQL status: SUCCESS 9 in 0.768 seconds
[0m15:21:03.637187 [debug] [Thread-1 (]: Writing injected SQL for node "unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum"
[0m15:21:03.649933 [debug] [Thread-1 (]: Using snowflake connection "unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum"
[0m15:21:03.650640 [debug] [Thread-1 (]: On unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum"} */
describe table DATAEXPERT_STUDENT.andres.dim_date
[0m15:21:03.808997 [debug] [Thread-1 (]: SQL status: SUCCESS 36 in 0.158 seconds
[0m15:21:03.824066 [debug] [Thread-1 (]: Writing injected SQL for node "unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum"
[0m15:21:03.834038 [debug] [Thread-1 (]: Using snowflake connection "unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum"
[0m15:21:03.834570 [debug] [Thread-1 (]: On unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum"} */
describe table DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections
[0m15:21:04.010341 [debug] [Thread-1 (]: SQL status: SUCCESS 4 in 0.175 seconds
[0m15:21:04.033724 [debug] [Thread-1 (]: Writing injected SQL for node "unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum"
[0m15:21:04.053423 [debug] [Thread-1 (]: Writing injected SQL for node "unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum"
[0m15:21:04.109905 [debug] [Thread-1 (]: Using snowflake connection "unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum"
[0m15:21:04.110861 [debug] [Thread-1 (]: On unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum"} */
create or replace temporary table DATAEXPERT_STUDENT.andres.test_amount_sum__dbt_tmp
         as
        (select * from (
        with  __dbt__cte__stg_daily_bars as (

-- Fixture for stg_daily_bars
select 
    
        try_cast('2020-11-20' as DATE)
     as date, 
    
        try_cast('\'L\'' as character varying(16777216))
     as ticker, 
    
        try_cast('100' as FLOAT)
     as number_of_shares_traded, 
    
        try_cast('10' as NUMBER(38,0))
     as number_of_trades, try_cast(null as FLOAT) as volume_weighted_avg_price, try_cast(null as FLOAT) as open_price, try_cast(null as FLOAT) as close_price, try_cast(null as FLOAT) as high_price, try_cast(null as FLOAT) as low_price
union all
select 
    
        try_cast('2020-11-20' as DATE)
     as date, 
    
        try_cast('\'S\'' as character varying(16777216))
     as ticker, 
    
        try_cast('200' as FLOAT)
     as number_of_shares_traded, 
    
        try_cast('20' as NUMBER(38,0))
     as number_of_trades, try_cast(null as FLOAT) as volume_weighted_avg_price, try_cast(null as FLOAT) as open_price, try_cast(null as FLOAT) as close_price, try_cast(null as FLOAT) as high_price, try_cast(null as FLOAT) as low_price
union all
select 
    
        try_cast('2020-11-21' as DATE)
     as date, 
    
        try_cast('\'L\'' as character varying(16777216))
     as ticker, 
    
        try_cast('300' as FLOAT)
     as number_of_shares_traded, 
    
        try_cast('40' as NUMBER(38,0))
     as number_of_trades, try_cast(null as FLOAT) as volume_weighted_avg_price, try_cast(null as FLOAT) as open_price, try_cast(null as FLOAT) as close_price, try_cast(null as FLOAT) as high_price, try_cast(null as FLOAT) as low_price
union all
select 
    
        try_cast('2020-11-21' as DATE)
     as date, 
    
        try_cast('\'S\'' as character varying(16777216))
     as ticker, 
    
        try_cast('200' as FLOAT)
     as number_of_shares_traded, 
    
        try_cast('50' as NUMBER(38,0))
     as number_of_trades, try_cast(null as FLOAT) as volume_weighted_avg_price, try_cast(null as FLOAT) as open_price, try_cast(null as FLOAT) as close_price, try_cast(null as FLOAT) as high_price, try_cast(null as FLOAT) as low_price
),  __dbt__cte__dim_date as (

-- Fixture for dim_date
select 
    
        try_cast('2020-11-20' as DATE)
     as date_day, try_cast(null as DATE) as prior_date_day, try_cast(null as DATE) as next_date_day, try_cast(null as DATE) as prior_year_date_day, try_cast(null as DATE) as prior_year_over_year_date_day, try_cast(null as NUMBER(3,0)) as day_of_week, try_cast(null as NUMBER(2,0)) as day_of_week_iso, try_cast(null as character varying(16777216)) as day_of_week_name, try_cast(null as character varying(3)) as day_of_week_name_short, try_cast(null as NUMBER(2,0)) as day_of_month, try_cast(null as NUMBER(4,0)) as day_of_year, try_cast(null as DATE) as week_start_date, try_cast(null as DATE) as week_end_date, try_cast(null as DATE) as prior_year_week_start_date, try_cast(null as DATE) as prior_year_week_end_date, try_cast(null as NUMBER(38,0)) as week_of_year, try_cast(null as DATE) as iso_week_start_date, try_cast(null as DATE) as iso_week_end_date, try_cast(null as DATE) as prior_year_iso_week_start_date, try_cast(null as DATE) as prior_year_iso_week_end_date, try_cast(null as NUMBER(38,0)) as iso_week_of_year, try_cast(null as NUMBER(38,0)) as prior_year_week_of_year, try_cast(null as NUMBER(38,0)) as prior_year_iso_week_of_year, try_cast(null as NUMBER(38,0)) as month_of_year, try_cast(null as character varying(16777216)) as month_name, try_cast(null as character varying(16777216)) as month_name_short, try_cast(null as DATE) as month_start_date, try_cast(null as DATE) as month_end_date, try_cast(null as DATE) as prior_year_month_start_date, try_cast(null as DATE) as prior_year_month_end_date, try_cast(null as NUMBER(38,0)) as quarter_of_year, try_cast(null as DATE) as quarter_start_date, try_cast(null as DATE) as quarter_end_date, try_cast(null as NUMBER(38,0)) as year_number, try_cast(null as DATE) as year_start_date, try_cast(null as DATE) as year_end_date
union all
select 
    
        try_cast('2020-11-21' as DATE)
     as date_day, try_cast(null as DATE) as prior_date_day, try_cast(null as DATE) as next_date_day, try_cast(null as DATE) as prior_year_date_day, try_cast(null as DATE) as prior_year_over_year_date_day, try_cast(null as NUMBER(3,0)) as day_of_week, try_cast(null as NUMBER(2,0)) as day_of_week_iso, try_cast(null as character varying(16777216)) as day_of_week_name, try_cast(null as character varying(3)) as day_of_week_name_short, try_cast(null as NUMBER(2,0)) as day_of_month, try_cast(null as NUMBER(4,0)) as day_of_year, try_cast(null as DATE) as week_start_date, try_cast(null as DATE) as week_end_date, try_cast(null as DATE) as prior_year_week_start_date, try_cast(null as DATE) as prior_year_week_end_date, try_cast(null as NUMBER(38,0)) as week_of_year, try_cast(null as DATE) as iso_week_start_date, try_cast(null as DATE) as iso_week_end_date, try_cast(null as DATE) as prior_year_iso_week_start_date, try_cast(null as DATE) as prior_year_iso_week_end_date, try_cast(null as NUMBER(38,0)) as iso_week_of_year, try_cast(null as NUMBER(38,0)) as prior_year_week_of_year, try_cast(null as NUMBER(38,0)) as prior_year_iso_week_of_year, try_cast(null as NUMBER(38,0)) as month_of_year, try_cast(null as character varying(16777216)) as month_name, try_cast(null as character varying(16777216)) as month_name_short, try_cast(null as DATE) as month_start_date, try_cast(null as DATE) as month_end_date, try_cast(null as DATE) as prior_year_month_start_date, try_cast(null as DATE) as prior_year_month_end_date, try_cast(null as NUMBER(38,0)) as quarter_of_year, try_cast(null as DATE) as quarter_start_date, try_cast(null as DATE) as quarter_end_date, try_cast(null as NUMBER(38,0)) as year_number, try_cast(null as DATE) as year_start_date, try_cast(null as DATE) as year_end_date
),  __dbt__cte__stg_treasury_revenue_collections as (

-- Fixture for stg_treasury_revenue_collections
select 
    
        try_cast('2020-11-20' as DATE)
     as collection_date, try_cast(null as character varying(16777216)) as channel_type, 
    
        try_cast('\'IRS Tax\'' as character varying(16777216))
     as tax_category, 
    
        try_cast('1000' as NUMBER(19,0))
     as dollars_collected_by_treasury
union all
select 
    
        try_cast('2020-11-20' as DATE)
     as collection_date, try_cast(null as character varying(16777216)) as channel_type, 
    
        try_cast('\'IRS Non Tax\'' as character varying(16777216))
     as tax_category, 
    
        try_cast('2000' as NUMBER(19,0))
     as dollars_collected_by_treasury
union all
select 
    
        try_cast('2020-11-21' as DATE)
     as collection_date, try_cast(null as character varying(16777216)) as channel_type, 
    
        try_cast('\'IRS Tax\'' as character varying(16777216))
     as tax_category, 
    
        try_cast('3000' as NUMBER(19,0))
     as dollars_collected_by_treasury
union all
select 
    
        try_cast('2020-11-21' as DATE)
     as collection_date, try_cast(null as character varying(16777216)) as channel_type, 
    
        try_cast('\'IRS Tax\'' as character varying(16777216))
     as tax_category, 
    
        try_cast('2000' as NUMBER(19,0))
     as dollars_collected_by_treasury
), pol as (
SELECT week_start_date, 
sum(number_of_shares_traded) number_of_shares_traded,
sum(number_of_trades) number_of_trades
from  __dbt__cte__stg_daily_bars a 
JOIN __dbt__cte__dim_date d on d.date_day=a.date
group by 1)

, rev as (
select week_start_date,
sum(dollars_collected_by_treasury) dollars_collected_by_treasury,
sum(case when tax_category = 'IRS Tax' then dollars_collected_by_treasury else 0 end) as irs_tax_dollars_collected, 
sum(case when tax_category = 'Non-Tax' then dollars_collected_by_treasury else 0 end) as non_irs_non_tax_dollars,
sum(case when tax_category = 'IRS Non-Tax' then dollars_collected_by_treasury else 0 end) as irs_non_tax_dollars
FROM __dbt__cte__stg_treasury_revenue_collections a 
JOIN __dbt__cte__dim_date d on d.date_day=a.collection_date
GROUP BY ALL)

select 
    a.week_start_date, 
    a.number_of_shares_traded, 
    a.number_of_trades,
    b.dollars_collected_by_treasury,
    b.irs_tax_dollars

from pol a 
INNER JOIN rev b on b.week_start_date = a.week_start_date
    ) as __dbt_sbq
    where false
    limit 0

        );
[0m15:21:04.337154 [debug] [Thread-1 (]: Snowflake adapter: Snowflake query id: 01b8ca45-0004-455e-0000-33eb0039de46
[0m15:21:04.337659 [debug] [Thread-1 (]: Snowflake adapter: Snowflake error: 000904 (42000): SQL compilation error: error line 143 at position 4
invalid identifier 'B.IRS_TAX_DOLLARS'
[0m15:21:04.338279 [debug] [Thread-1 (]: On unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum: Close
[0m15:21:04.559916 [debug] [Thread-1 (]: Runtime Error in unit_test test_amount_sum (models/marts/_marts.yml)
  An error occurred during execution of unit test 'test_amount_sum'. There may be an error in the unit test definition: check the data types.
   Database Error
    000904 (42000): SQL compilation error: error line 143 at position 4
    invalid identifier 'B.IRS_TAX_DOLLARS'
[0m15:21:04.560748 [error] [Thread-1 (]: 4 of 5 ERROR weekly_metrics::test_amount_sum ................................... [[31mERROR[0m in 1.82s]
[0m15:21:04.561544 [debug] [Thread-1 (]: Finished running node unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum
[0m15:21:04.562231 [debug] [Thread-1 (]: Marking all children of 'unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum' to be skipped because of status 'error'.  Reason: Runtime Error in unit_test test_amount_sum (models/marts/_marts.yml)
  An error occurred during execution of unit test 'test_amount_sum'. There may be an error in the unit test definition: check the data types.
   Database Error
    000904 (42000): SQL compilation error: error line 143 at position 4
    invalid identifier 'B.IRS_TAX_DOLLARS'.
[0m15:21:04.563064 [debug] [Thread-1 (]: Began running node model.dbt_capstone_andres.weekly_metrics
[0m15:21:04.563618 [info ] [Thread-1 (]: 5 of 5 SKIP relation andres.weekly_metrics ..................................... [[33mSKIP[0m]
[0m15:21:04.564201 [debug] [Thread-1 (]: Finished running node model.dbt_capstone_andres.weekly_metrics
[0m15:21:04.564722 [debug] [Thread-4 (]: Marking all children of 'model.dbt_capstone_andres.weekly_metrics' to be skipped because of status 'skipped'. 
[0m15:21:04.566205 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:21:04.566662 [debug] [MainThread]: Connection 'unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum' was properly closed.
[0m15:21:04.567555 [info ] [MainThread]: 
[0m15:21:04.568138 [info ] [MainThread]: Finished running 4 table models, 1 unit test in 0 hours 0 minutes and 10.49 seconds (10.49s).
[0m15:21:04.569661 [debug] [MainThread]: Command end result
[0m15:21:04.633139 [info ] [MainThread]: 
[0m15:21:04.633755 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successs, and 0 warnings:[0m
[0m15:21:04.634243 [info ] [MainThread]: 
[0m15:21:04.634783 [error] [MainThread]:   Runtime Error in unit_test test_amount_sum (models/marts/_marts.yml)
  An error occurred during execution of unit test 'test_amount_sum'. There may be an error in the unit test definition: check the data types.
   Database Error
    000904 (42000): SQL compilation error: error line 143 at position 4
    invalid identifier 'B.IRS_TAX_DOLLARS'
[0m15:21:04.635282 [info ] [MainThread]: 
[0m15:21:04.635773 [info ] [MainThread]: Done. PASS=3 WARN=0 ERROR=1 SKIP=1 TOTAL=5
[0m15:21:04.639268 [debug] [MainThread]: Resource report: {"command_name": "build", "command_wall_clock_time": 13.13148, "process_user_time": 3.701245, "process_kernel_time": 0.737397, "process_mem_max_rss": "198701056", "command_success": false, "process_in_blocks": "0", "process_out_blocks": "0"}
[0m15:21:04.640125 [debug] [MainThread]: Command `dbt build` failed at 15:21:04.639987 after 13.13 seconds
[0m15:21:04.640694 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10569cd50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10571bf10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10571bed0>]}
[0m15:21:04.641206 [debug] [MainThread]: Flushing usage events
[0m15:21:04.832581 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:23:15.206981 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10edba250>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ee2b110>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ee2b710>]}


============================== 15:23:15.212652 | 82d9d709-375e-4cfd-b3f5-00dfa99af32e ==============================
[0m15:23:15.212652 [info ] [MainThread]: Running with dbt=1.9.0-b3
[0m15:23:15.213323 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'version_check': 'True', 'warn_error': 'None', 'log_path': 'logs', 'fail_fast': 'False', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'log_format': 'default', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'target_path': 'None', 'invocation_command': 'dbt build', 'send_anonymous_usage_stats': 'True'}
[0m15:23:16.781655 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '82d9d709-375e-4cfd-b3f5-00dfa99af32e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11119ed50>]}
[0m15:23:16.845812 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '82d9d709-375e-4cfd-b3f5-00dfa99af32e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10da60fd0>]}
[0m15:23:16.847147 [info ] [MainThread]: Registered adapter: snowflake=1.8.4
[0m15:23:17.070888 [debug] [MainThread]: checksum: 5f4fde221ca8627174a0341efc73099aaec05b8546408ceed6f59b1ec0755115, vars: {}, profile: , target: , version: 1.9.0b3
[0m15:23:17.306481 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 2 files changed.
[0m15:23:17.307214 [debug] [MainThread]: Partial parsing: updated file: dbt_capstone_andres://models/marts/_marts.yml
[0m15:23:17.307669 [debug] [MainThread]: Partial parsing: updated file: dbt_capstone_andres://models/marts/weekly_metrics.sql
[0m15:23:17.667571 [debug] [MainThread]: STDERR: "Unit Test fixtures benefit from having at least one row free of Null values to ensure consistent column types. Failure to meet this recommendation can result in type mismatch errors between unit test source models and `expected` fixtures."
[0m15:23:17.746870 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '82d9d709-375e-4cfd-b3f5-00dfa99af32e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1150f7b50>]}
[0m15:23:17.886968 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '82d9d709-375e-4cfd-b3f5-00dfa99af32e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11517e510>]}
[0m15:23:17.887603 [info ] [MainThread]: Found 4 models, 2 sources, 862 macros, 1 unit test
[0m15:23:17.889601 [info ] [MainThread]: 
[0m15:23:17.889952 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m15:23:17.890262 [info ] [MainThread]: 
[0m15:23:17.890730 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m15:23:17.895810 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_DATAEXPERT_STUDENT'
[0m15:23:17.908123 [debug] [ThreadPool]: Using snowflake connection "list_DATAEXPERT_STUDENT"
[0m15:23:17.908529 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "connection_name": "list_DATAEXPERT_STUDENT"} */
show terse schemas in database DATAEXPERT_STUDENT
    limit 10000
[0m15:23:17.908839 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:23:18.825357 [debug] [ThreadPool]: SQL status: SUCCESS 73 in 0.916 seconds
[0m15:23:18.827869 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT: Close
[0m15:23:19.069664 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_DATAEXPERT_STUDENT, now list_DATAEXPERT_STUDENT_andres)
[0m15:23:19.078435 [debug] [ThreadPool]: Using snowflake connection "list_DATAEXPERT_STUDENT_andres"
[0m15:23:19.078881 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "connection_name": "list_DATAEXPERT_STUDENT_andres"} */
show objects in DATAEXPERT_STUDENT.andres limit 10000
[0m15:23:19.079179 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:23:19.814506 [debug] [ThreadPool]: SQL status: SUCCESS 25 in 0.735 seconds
[0m15:23:19.817528 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: Close
[0m15:23:20.045452 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '82d9d709-375e-4cfd-b3f5-00dfa99af32e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1150f57d0>]}
[0m15:23:20.048608 [debug] [Thread-1 (]: Began running node model.dbt_capstone_andres.dim_date
[0m15:23:20.049192 [info ] [Thread-1 (]: 1 of 5 START sql table model andres.dim_date ................................... [RUN]
[0m15:23:20.049815 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_DATAEXPERT_STUDENT_andres, now model.dbt_capstone_andres.dim_date)
[0m15:23:20.050171 [debug] [Thread-1 (]: Began compiling node model.dbt_capstone_andres.dim_date
[0m15:23:20.089040 [debug] [Thread-1 (]: Using snowflake connection "model.dbt_capstone_andres.dim_date"
[0m15:23:20.089629 [debug] [Thread-1 (]: On model.dbt_capstone_andres.dim_date: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "model.dbt_capstone_andres.dim_date"} */
select datediff(
        day,
        cast('1995-01-01' as timestamp),
        cast('2040-12-31' as timestamp)
        )
[0m15:23:20.090080 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:23:20.751244 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 0.661 seconds
[0m15:23:20.836311 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_capstone_andres.dim_date"
[0m15:23:20.837864 [debug] [Thread-1 (]: Began executing node model.dbt_capstone_andres.dim_date
[0m15:23:20.868608 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_capstone_andres.dim_date"
[0m15:23:20.876277 [debug] [Thread-1 (]: Using snowflake connection "model.dbt_capstone_andres.dim_date"
[0m15:23:20.877319 [debug] [Thread-1 (]: On model.dbt_capstone_andres.dim_date: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "model.dbt_capstone_andres.dim_date"} */
create or replace transient table DATAEXPERT_STUDENT.andres.dim_date
         as
        (

    
with base_dates as (
    
    with date_spine as
(

    





with rawdata as (

    

    

    with p as (
        select 0 as generated_number union all select 1
    ), unioned as (

    select

    
    p0.generated_number * power(2, 0)
     + 
    
    p1.generated_number * power(2, 1)
     + 
    
    p2.generated_number * power(2, 2)
     + 
    
    p3.generated_number * power(2, 3)
     + 
    
    p4.generated_number * power(2, 4)
     + 
    
    p5.generated_number * power(2, 5)
     + 
    
    p6.generated_number * power(2, 6)
     + 
    
    p7.generated_number * power(2, 7)
     + 
    
    p8.generated_number * power(2, 8)
     + 
    
    p9.generated_number * power(2, 9)
     + 
    
    p10.generated_number * power(2, 10)
     + 
    
    p11.generated_number * power(2, 11)
     + 
    
    p12.generated_number * power(2, 12)
     + 
    
    p13.generated_number * power(2, 13)
     + 
    
    p14.generated_number * power(2, 14)
    
    
    + 1
    as generated_number

    from

    
    p as p0
     cross join 
    
    p as p1
     cross join 
    
    p as p2
     cross join 
    
    p as p3
     cross join 
    
    p as p4
     cross join 
    
    p as p5
     cross join 
    
    p as p6
     cross join 
    
    p as p7
     cross join 
    
    p as p8
     cross join 
    
    p as p9
     cross join 
    
    p as p10
     cross join 
    
    p as p11
     cross join 
    
    p as p12
     cross join 
    
    p as p13
     cross join 
    
    p as p14
    
    

    )

    select *
    from unioned
    where generated_number <= 16801
    order by generated_number



),

all_periods as (

    select (
        

    dateadd(
        day,
        (row_number() over (order by 1) - 1),
        cast('1995-01-01' as timestamp)
        )


    ) as date_day
    from rawdata

),

filtered as (

    select *
    from all_periods
    where date_day <= cast('2040-12-31' as timestamp)

)

select * from filtered



)
select
    cast(d.date_day as timestamp) as date_day
from
    date_spine d


),
dates_with_prior_year_dates as (

    select
        cast(d.date_day as date) as date_day,
        cast(

    dateadd(
        year,
        -1,
        d.date_day
        )

 as date) as prior_year_date_day,
        cast(

    dateadd(
        day,
        -364,
        d.date_day
        )

 as date) as prior_year_over_year_date_day
    from
    	base_dates d

)
select
    d.date_day,
    cast(

    dateadd(
        day,
        -1,
        d.date_day
        )

 as date) as prior_date_day,
    cast(

    dateadd(
        day,
        1,
        d.date_day
        )

 as date) as next_date_day,
    d.prior_year_date_day as prior_year_date_day,
    d.prior_year_over_year_date_day,
    case
            when date_part('dayofweek', d.date_day) = 7 then 1
            else date_part('dayofweek', d.date_day) + 1
        end as day_of_week,
    date_part('dayofweekiso', d.date_day) as day_of_week_iso,
    -- long version not implemented on Snowflake so we're doing it manually :/
    case dayname(d.date_day)
        when 'Mon' then 'Monday'
        when 'Tue' then 'Tuesday'
        when 'Wed' then 'Wednesday'
        when 'Thu' then 'Thursday'
        when 'Fri' then 'Friday'
        when 'Sat' then 'Saturday'
        when 'Sun' then 'Sunday'
    end as day_of_week_name,
    dayname(d.date_day) as day_of_week_name_short,
    date_part('day', d.date_day) as day_of_month,
    date_part('dayofyear', d.date_day) as day_of_year,

    
    
    cast(

    dateadd(
        day,
        -1 * (case
            when date_part('dayofweek', d.date_day) = 7 then 1
            else date_part('dayofweek', d.date_day) + 1
        end - 1),
        d.date_day
        )

 as date) as week_start_date,
    cast(

    dateadd(
        day,
        6,
        
    
    cast(

    dateadd(
        day,
        -1 * (case
            when date_part('dayofweek', d.date_day) = 7 then 1
            else date_part('dayofweek', d.date_day) + 1
        end - 1),
        d.date_day
        )

 as date)
        )

 as date) as week_end_date,
    
    
    cast(

    dateadd(
        day,
        -1 * (case
            when date_part('dayofweek', d.prior_year_over_year_date_day) = 7 then 1
            else date_part('dayofweek', d.prior_year_over_year_date_day) + 1
        end - 1),
        d.prior_year_over_year_date_day
        )

 as date) as prior_year_week_start_date,
    cast(

    dateadd(
        day,
        6,
        
    
    cast(

    dateadd(
        day,
        -1 * (case
            when date_part('dayofweek', d.prior_year_over_year_date_day) = 7 then 1
            else date_part('dayofweek', d.prior_year_over_year_date_day) + 1
        end - 1),
        d.prior_year_over_year_date_day
        )

 as date)
        )

 as date) as prior_year_week_end_date,
    cast(date_part('week', d.date_day) as integer) as week_of_year,

    cast(date_trunc('week', d.date_day) as date) as iso_week_start_date,
    cast(

    dateadd(
        day,
        6,
        cast(date_trunc('week', d.date_day) as date)
        )

 as date) as iso_week_end_date,
    cast(date_trunc('week', d.prior_year_over_year_date_day) as date) as prior_year_iso_week_start_date,
    cast(

    dateadd(
        day,
        6,
        cast(date_trunc('week', d.prior_year_over_year_date_day) as date)
        )

 as date) as prior_year_iso_week_end_date,
    cast(date_part('weekiso', d.date_day) as integer) as iso_week_of_year,

    cast(date_part('week', d.prior_year_over_year_date_day) as integer) as prior_year_week_of_year,
    cast(date_part('weekiso', d.prior_year_over_year_date_day) as integer) as prior_year_iso_week_of_year,

    cast(date_part('month', d.date_day) as integer) as month_of_year,
    to_char(d.date_day, 'MMMM')  as month_name,
    to_char(d.date_day, 'MON')  as month_name_short,

    cast(date_trunc('month', d.date_day) as date) as month_start_date,
    cast(cast(
        

    dateadd(
        day,
        -1,
        

    dateadd(
        month,
        1,
        date_trunc('month', d.date_day)
        )


        )


        as date) as date) as month_end_date,

    cast(date_trunc('month', d.prior_year_date_day) as date) as prior_year_month_start_date,
    cast(cast(
        

    dateadd(
        day,
        -1,
        

    dateadd(
        month,
        1,
        date_trunc('month', d.prior_year_date_day)
        )


        )


        as date) as date) as prior_year_month_end_date,

    cast(date_part('quarter', d.date_day) as integer) as quarter_of_year,
    cast(date_trunc('quarter', d.date_day) as date) as quarter_start_date,
    cast(cast(
        

    dateadd(
        day,
        -1,
        

    dateadd(
        quarter,
        1,
        date_trunc('quarter', d.date_day)
        )


        )


        as date) as date) as quarter_end_date,

    cast(date_part('year', d.date_day) as integer) as year_number,
    cast(date_trunc('year', d.date_day) as date) as year_start_date,
    cast(cast(
        

    dateadd(
        day,
        -1,
        

    dateadd(
        year,
        1,
        date_trunc('year', d.date_day)
        )


        )


        as date) as date) as year_end_date
from
    dates_with_prior_year_dates d
order by 1


        );
[0m15:23:22.605917 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 1.728 seconds
[0m15:23:22.634741 [debug] [Thread-1 (]: On model.dbt_capstone_andres.dim_date: Close
[0m15:23:22.859602 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '82d9d709-375e-4cfd-b3f5-00dfa99af32e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10cf35310>]}
[0m15:23:22.860392 [info ] [Thread-1 (]: 1 of 5 OK created sql table model andres.dim_date .............................. [[32mSUCCESS 1[0m in 2.81s]
[0m15:23:22.861054 [debug] [Thread-1 (]: Finished running node model.dbt_capstone_andres.dim_date
[0m15:23:22.861501 [debug] [Thread-1 (]: Began running node model.dbt_capstone_andres.stg_daily_bars
[0m15:23:22.862066 [info ] [Thread-1 (]: 2 of 5 START sql table model andres.stg_daily_bars ............................. [RUN]
[0m15:23:22.862570 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_capstone_andres.dim_date, now model.dbt_capstone_andres.stg_daily_bars)
[0m15:23:22.862979 [debug] [Thread-1 (]: Began compiling node model.dbt_capstone_andres.stg_daily_bars
[0m15:23:22.866379 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_capstone_andres.stg_daily_bars"
[0m15:23:22.867098 [debug] [Thread-1 (]: Began executing node model.dbt_capstone_andres.stg_daily_bars
[0m15:23:22.870954 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_capstone_andres.stg_daily_bars"
[0m15:23:22.872361 [debug] [Thread-1 (]: Using snowflake connection "model.dbt_capstone_andres.stg_daily_bars"
[0m15:23:22.872803 [debug] [Thread-1 (]: On model.dbt_capstone_andres.stg_daily_bars: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "model.dbt_capstone_andres.stg_daily_bars"} */
create or replace transient table DATAEXPERT_STUDENT.andres.stg_daily_bars
         as
        (with source as (

    select * from dataexpert_student.andres.daily_bars

),

renamed as (

    select
        TO_DATE(TO_TIMESTAMP(timestamp / 1000)) as date,
        ticker as ticker,
        v  as number_of_shares_traded,
        n  as number_of_trades,
        round(vw,2) as volume_weighted_avg_price,
        o  as open_price,
        c  as close_price,
        h  as high_price,
        l  as low_price
    from source

)

select * from renamed
        );
[0m15:23:22.873224 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:23:25.571867 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 2.699 seconds
[0m15:23:25.574965 [debug] [Thread-1 (]: On model.dbt_capstone_andres.stg_daily_bars: Close
[0m15:23:25.803209 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '82d9d709-375e-4cfd-b3f5-00dfa99af32e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11f85ed50>]}
[0m15:23:25.804222 [info ] [Thread-1 (]: 2 of 5 OK created sql table model andres.stg_daily_bars ........................ [[32mSUCCESS 1[0m in 2.94s]
[0m15:23:25.805037 [debug] [Thread-1 (]: Finished running node model.dbt_capstone_andres.stg_daily_bars
[0m15:23:25.805544 [debug] [Thread-1 (]: Began running node model.dbt_capstone_andres.stg_treasury_revenue_collections
[0m15:23:25.806197 [info ] [Thread-1 (]: 3 of 5 START sql table model andres.stg_treasury_revenue_collections ........... [RUN]
[0m15:23:25.806764 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_capstone_andres.stg_daily_bars, now model.dbt_capstone_andres.stg_treasury_revenue_collections)
[0m15:23:25.807244 [debug] [Thread-1 (]: Began compiling node model.dbt_capstone_andres.stg_treasury_revenue_collections
[0m15:23:25.811696 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_capstone_andres.stg_treasury_revenue_collections"
[0m15:23:25.813943 [debug] [Thread-1 (]: Began executing node model.dbt_capstone_andres.stg_treasury_revenue_collections
[0m15:23:25.819315 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_capstone_andres.stg_treasury_revenue_collections"
[0m15:23:25.820947 [debug] [Thread-1 (]: Using snowflake connection "model.dbt_capstone_andres.stg_treasury_revenue_collections"
[0m15:23:25.821455 [debug] [Thread-1 (]: On model.dbt_capstone_andres.stg_treasury_revenue_collections: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "model.dbt_capstone_andres.stg_treasury_revenue_collections"} */
create or replace transient table DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections
         as
        (with source as (

    select * from dataexpert_student.andres.fiscal_data

),

renamed as (

    select
        record_date as collection_date,
        channel_type_desc as channel_type,
        tax_category_desc as tax_category,
        round(net_collections_amt,0) as dollars_collected_by_treasury

    from source

)

select * from renamed
        );
[0m15:23:25.822337 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:23:27.695133 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 1.873 seconds
[0m15:23:27.697297 [debug] [Thread-1 (]: On model.dbt_capstone_andres.stg_treasury_revenue_collections: Close
[0m15:23:27.952548 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '82d9d709-375e-4cfd-b3f5-00dfa99af32e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11f845f10>]}
[0m15:23:27.953259 [info ] [Thread-1 (]: 3 of 5 OK created sql table model andres.stg_treasury_revenue_collections ...... [[32mSUCCESS 1[0m in 2.15s]
[0m15:23:27.953945 [debug] [Thread-1 (]: Finished running node model.dbt_capstone_andres.stg_treasury_revenue_collections
[0m15:23:27.954791 [debug] [Thread-1 (]: Began running node unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum
[0m15:23:27.955234 [info ] [Thread-1 (]: 4 of 5 START unit_test weekly_metrics::test_amount_sum ......................... [RUN]
[0m15:23:27.955958 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_capstone_andres.stg_treasury_revenue_collections, now unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum)
[0m15:23:27.956335 [debug] [Thread-1 (]: Began compiling node unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum
[0m15:23:27.956676 [debug] [Thread-1 (]: Began executing node unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum
[0m15:23:27.997452 [debug] [Thread-1 (]: Using snowflake connection "unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum"
[0m15:23:27.997949 [debug] [Thread-1 (]: On unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum"} */
describe table DATAEXPERT_STUDENT.andres.stg_daily_bars
[0m15:23:27.998342 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:23:28.643087 [debug] [Thread-1 (]: SQL status: SUCCESS 9 in 0.645 seconds
[0m15:23:28.678048 [debug] [Thread-1 (]: Writing injected SQL for node "unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum"
[0m15:23:28.685799 [debug] [Thread-1 (]: Using snowflake connection "unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum"
[0m15:23:28.686307 [debug] [Thread-1 (]: On unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum"} */
describe table DATAEXPERT_STUDENT.andres.dim_date
[0m15:23:28.833843 [debug] [Thread-1 (]: SQL status: SUCCESS 36 in 0.147 seconds
[0m15:23:28.846020 [debug] [Thread-1 (]: Writing injected SQL for node "unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum"
[0m15:23:28.856059 [debug] [Thread-1 (]: Using snowflake connection "unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum"
[0m15:23:28.856715 [debug] [Thread-1 (]: On unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum"} */
describe table DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections
[0m15:23:29.012873 [debug] [Thread-1 (]: SQL status: SUCCESS 4 in 0.156 seconds
[0m15:23:29.020952 [debug] [Thread-1 (]: Writing injected SQL for node "unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum"
[0m15:23:29.028620 [debug] [Thread-1 (]: Writing injected SQL for node "unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum"
[0m15:23:29.055999 [debug] [Thread-1 (]: Using snowflake connection "unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum"
[0m15:23:29.056780 [debug] [Thread-1 (]: On unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum"} */
create or replace temporary table DATAEXPERT_STUDENT.andres.test_amount_sum__dbt_tmp
         as
        (select * from (
        with  __dbt__cte__stg_daily_bars as (

-- Fixture for stg_daily_bars
select 
    
        try_cast('2020-11-20' as DATE)
     as date, 
    
        try_cast('\'L\'' as character varying(16777216))
     as ticker, 
    
        try_cast('100' as FLOAT)
     as number_of_shares_traded, 
    
        try_cast('10' as NUMBER(38,0))
     as number_of_trades, try_cast(null as FLOAT) as volume_weighted_avg_price, try_cast(null as FLOAT) as open_price, try_cast(null as FLOAT) as close_price, try_cast(null as FLOAT) as high_price, try_cast(null as FLOAT) as low_price
union all
select 
    
        try_cast('2020-11-20' as DATE)
     as date, 
    
        try_cast('\'S\'' as character varying(16777216))
     as ticker, 
    
        try_cast('200' as FLOAT)
     as number_of_shares_traded, 
    
        try_cast('20' as NUMBER(38,0))
     as number_of_trades, try_cast(null as FLOAT) as volume_weighted_avg_price, try_cast(null as FLOAT) as open_price, try_cast(null as FLOAT) as close_price, try_cast(null as FLOAT) as high_price, try_cast(null as FLOAT) as low_price
union all
select 
    
        try_cast('2020-11-21' as DATE)
     as date, 
    
        try_cast('\'L\'' as character varying(16777216))
     as ticker, 
    
        try_cast('300' as FLOAT)
     as number_of_shares_traded, 
    
        try_cast('40' as NUMBER(38,0))
     as number_of_trades, try_cast(null as FLOAT) as volume_weighted_avg_price, try_cast(null as FLOAT) as open_price, try_cast(null as FLOAT) as close_price, try_cast(null as FLOAT) as high_price, try_cast(null as FLOAT) as low_price
union all
select 
    
        try_cast('2020-11-21' as DATE)
     as date, 
    
        try_cast('\'S\'' as character varying(16777216))
     as ticker, 
    
        try_cast('200' as FLOAT)
     as number_of_shares_traded, 
    
        try_cast('50' as NUMBER(38,0))
     as number_of_trades, try_cast(null as FLOAT) as volume_weighted_avg_price, try_cast(null as FLOAT) as open_price, try_cast(null as FLOAT) as close_price, try_cast(null as FLOAT) as high_price, try_cast(null as FLOAT) as low_price
),  __dbt__cte__dim_date as (

-- Fixture for dim_date
select 
    
        try_cast('2020-11-20' as DATE)
     as date_day, try_cast(null as DATE) as prior_date_day, try_cast(null as DATE) as next_date_day, try_cast(null as DATE) as prior_year_date_day, try_cast(null as DATE) as prior_year_over_year_date_day, try_cast(null as NUMBER(3,0)) as day_of_week, try_cast(null as NUMBER(2,0)) as day_of_week_iso, try_cast(null as character varying(16777216)) as day_of_week_name, try_cast(null as character varying(3)) as day_of_week_name_short, try_cast(null as NUMBER(2,0)) as day_of_month, try_cast(null as NUMBER(4,0)) as day_of_year, try_cast(null as DATE) as week_start_date, try_cast(null as DATE) as week_end_date, try_cast(null as DATE) as prior_year_week_start_date, try_cast(null as DATE) as prior_year_week_end_date, try_cast(null as NUMBER(38,0)) as week_of_year, try_cast(null as DATE) as iso_week_start_date, try_cast(null as DATE) as iso_week_end_date, try_cast(null as DATE) as prior_year_iso_week_start_date, try_cast(null as DATE) as prior_year_iso_week_end_date, try_cast(null as NUMBER(38,0)) as iso_week_of_year, try_cast(null as NUMBER(38,0)) as prior_year_week_of_year, try_cast(null as NUMBER(38,0)) as prior_year_iso_week_of_year, try_cast(null as NUMBER(38,0)) as month_of_year, try_cast(null as character varying(16777216)) as month_name, try_cast(null as character varying(16777216)) as month_name_short, try_cast(null as DATE) as month_start_date, try_cast(null as DATE) as month_end_date, try_cast(null as DATE) as prior_year_month_start_date, try_cast(null as DATE) as prior_year_month_end_date, try_cast(null as NUMBER(38,0)) as quarter_of_year, try_cast(null as DATE) as quarter_start_date, try_cast(null as DATE) as quarter_end_date, try_cast(null as NUMBER(38,0)) as year_number, try_cast(null as DATE) as year_start_date, try_cast(null as DATE) as year_end_date
union all
select 
    
        try_cast('2020-11-21' as DATE)
     as date_day, try_cast(null as DATE) as prior_date_day, try_cast(null as DATE) as next_date_day, try_cast(null as DATE) as prior_year_date_day, try_cast(null as DATE) as prior_year_over_year_date_day, try_cast(null as NUMBER(3,0)) as day_of_week, try_cast(null as NUMBER(2,0)) as day_of_week_iso, try_cast(null as character varying(16777216)) as day_of_week_name, try_cast(null as character varying(3)) as day_of_week_name_short, try_cast(null as NUMBER(2,0)) as day_of_month, try_cast(null as NUMBER(4,0)) as day_of_year, try_cast(null as DATE) as week_start_date, try_cast(null as DATE) as week_end_date, try_cast(null as DATE) as prior_year_week_start_date, try_cast(null as DATE) as prior_year_week_end_date, try_cast(null as NUMBER(38,0)) as week_of_year, try_cast(null as DATE) as iso_week_start_date, try_cast(null as DATE) as iso_week_end_date, try_cast(null as DATE) as prior_year_iso_week_start_date, try_cast(null as DATE) as prior_year_iso_week_end_date, try_cast(null as NUMBER(38,0)) as iso_week_of_year, try_cast(null as NUMBER(38,0)) as prior_year_week_of_year, try_cast(null as NUMBER(38,0)) as prior_year_iso_week_of_year, try_cast(null as NUMBER(38,0)) as month_of_year, try_cast(null as character varying(16777216)) as month_name, try_cast(null as character varying(16777216)) as month_name_short, try_cast(null as DATE) as month_start_date, try_cast(null as DATE) as month_end_date, try_cast(null as DATE) as prior_year_month_start_date, try_cast(null as DATE) as prior_year_month_end_date, try_cast(null as NUMBER(38,0)) as quarter_of_year, try_cast(null as DATE) as quarter_start_date, try_cast(null as DATE) as quarter_end_date, try_cast(null as NUMBER(38,0)) as year_number, try_cast(null as DATE) as year_start_date, try_cast(null as DATE) as year_end_date
),  __dbt__cte__stg_treasury_revenue_collections as (

-- Fixture for stg_treasury_revenue_collections
select 
    
        try_cast('2020-11-20' as DATE)
     as collection_date, try_cast(null as character varying(16777216)) as channel_type, 
    
        try_cast('\'IRS Tax\'' as character varying(16777216))
     as tax_category, 
    
        try_cast('1000' as NUMBER(19,0))
     as dollars_collected_by_treasury
union all
select 
    
        try_cast('2020-11-20' as DATE)
     as collection_date, try_cast(null as character varying(16777216)) as channel_type, 
    
        try_cast('\'IRS Non Tax\'' as character varying(16777216))
     as tax_category, 
    
        try_cast('2000' as NUMBER(19,0))
     as dollars_collected_by_treasury
union all
select 
    
        try_cast('2020-11-21' as DATE)
     as collection_date, try_cast(null as character varying(16777216)) as channel_type, 
    
        try_cast('\'IRS Tax\'' as character varying(16777216))
     as tax_category, 
    
        try_cast('3000' as NUMBER(19,0))
     as dollars_collected_by_treasury
union all
select 
    
        try_cast('2020-11-21' as DATE)
     as collection_date, try_cast(null as character varying(16777216)) as channel_type, 
    
        try_cast('\'IRS Tax\'' as character varying(16777216))
     as tax_category, 
    
        try_cast('2000' as NUMBER(19,0))
     as dollars_collected_by_treasury
), pol as (
SELECT week_start_date, 
sum(number_of_shares_traded) number_of_shares_traded,
sum(number_of_trades) number_of_trades
from  __dbt__cte__stg_daily_bars a 
JOIN __dbt__cte__dim_date d on d.date_day=a.date
group by 1)

, rev as (
select week_start_date,
sum(dollars_collected_by_treasury) dollars_collected_by_treasury,
sum(case when tax_category = 'IRS Tax' then dollars_collected_by_treasury else 0 end) as irs_tax_dollars_collected_by_treasury, 
sum(case when tax_category = 'Non-Tax' then dollars_collected_by_treasury else 0 end) as non_irs_non_tax_dollars_collected_by_treasury,
sum(case when tax_category = 'IRS Non-Tax' then dollars_collected_by_treasury else 0 end) as irs_non_tax_dollars_collected_by_treasury
FROM __dbt__cte__stg_treasury_revenue_collections a 
JOIN __dbt__cte__dim_date d on d.date_day=a.collection_date
GROUP BY ALL)

select 
    a.week_start_date, 
    a.number_of_shares_traded, 
    a.number_of_trades,
    b.dollars_collected_by_treasury,
    b.irs_tax_dollars_collected_by_treasury

from pol a 
INNER JOIN rev b on b.week_start_date = a.week_start_date
    ) as __dbt_sbq
    where false
    limit 0

        );
[0m15:23:29.660190 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 0.603 seconds
[0m15:23:29.663800 [debug] [Thread-1 (]: Using snowflake connection "unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum"
[0m15:23:29.664190 [debug] [Thread-1 (]: On unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum"} */
describe table DATAEXPERT_STUDENT.andres.test_amount_sum__dbt_tmp
[0m15:23:29.809776 [debug] [Thread-1 (]: SQL status: SUCCESS 5 in 0.145 seconds
[0m15:23:29.823079 [debug] [Thread-1 (]: Writing runtime sql for node "unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum"
[0m15:23:29.835029 [debug] [Thread-1 (]: Using snowflake connection "unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum"
[0m15:23:29.835913 [debug] [Thread-1 (]: On unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum"} */
-- Build actual result given inputs
with dbt_internal_unit_test_actual as (
  select
    week_start_date,number_of_shares_traded,number_of_trades,dollars_collected_by_treasury,irs_tax_dollars_collected_by_treasury, 'actual' as "actual_or_expected"
  from (
    with  __dbt__cte__stg_daily_bars as (

-- Fixture for stg_daily_bars
select 
    
        try_cast('2020-11-20' as DATE)
     as date, 
    
        try_cast('\'L\'' as character varying(16777216))
     as ticker, 
    
        try_cast('100' as FLOAT)
     as number_of_shares_traded, 
    
        try_cast('10' as NUMBER(38,0))
     as number_of_trades, try_cast(null as FLOAT) as volume_weighted_avg_price, try_cast(null as FLOAT) as open_price, try_cast(null as FLOAT) as close_price, try_cast(null as FLOAT) as high_price, try_cast(null as FLOAT) as low_price
union all
select 
    
        try_cast('2020-11-20' as DATE)
     as date, 
    
        try_cast('\'S\'' as character varying(16777216))
     as ticker, 
    
        try_cast('200' as FLOAT)
     as number_of_shares_traded, 
    
        try_cast('20' as NUMBER(38,0))
     as number_of_trades, try_cast(null as FLOAT) as volume_weighted_avg_price, try_cast(null as FLOAT) as open_price, try_cast(null as FLOAT) as close_price, try_cast(null as FLOAT) as high_price, try_cast(null as FLOAT) as low_price
union all
select 
    
        try_cast('2020-11-21' as DATE)
     as date, 
    
        try_cast('\'L\'' as character varying(16777216))
     as ticker, 
    
        try_cast('300' as FLOAT)
     as number_of_shares_traded, 
    
        try_cast('40' as NUMBER(38,0))
     as number_of_trades, try_cast(null as FLOAT) as volume_weighted_avg_price, try_cast(null as FLOAT) as open_price, try_cast(null as FLOAT) as close_price, try_cast(null as FLOAT) as high_price, try_cast(null as FLOAT) as low_price
union all
select 
    
        try_cast('2020-11-21' as DATE)
     as date, 
    
        try_cast('\'S\'' as character varying(16777216))
     as ticker, 
    
        try_cast('200' as FLOAT)
     as number_of_shares_traded, 
    
        try_cast('50' as NUMBER(38,0))
     as number_of_trades, try_cast(null as FLOAT) as volume_weighted_avg_price, try_cast(null as FLOAT) as open_price, try_cast(null as FLOAT) as close_price, try_cast(null as FLOAT) as high_price, try_cast(null as FLOAT) as low_price
),  __dbt__cte__dim_date as (

-- Fixture for dim_date
select 
    
        try_cast('2020-11-20' as DATE)
     as date_day, try_cast(null as DATE) as prior_date_day, try_cast(null as DATE) as next_date_day, try_cast(null as DATE) as prior_year_date_day, try_cast(null as DATE) as prior_year_over_year_date_day, try_cast(null as NUMBER(3,0)) as day_of_week, try_cast(null as NUMBER(2,0)) as day_of_week_iso, try_cast(null as character varying(16777216)) as day_of_week_name, try_cast(null as character varying(3)) as day_of_week_name_short, try_cast(null as NUMBER(2,0)) as day_of_month, try_cast(null as NUMBER(4,0)) as day_of_year, try_cast(null as DATE) as week_start_date, try_cast(null as DATE) as week_end_date, try_cast(null as DATE) as prior_year_week_start_date, try_cast(null as DATE) as prior_year_week_end_date, try_cast(null as NUMBER(38,0)) as week_of_year, try_cast(null as DATE) as iso_week_start_date, try_cast(null as DATE) as iso_week_end_date, try_cast(null as DATE) as prior_year_iso_week_start_date, try_cast(null as DATE) as prior_year_iso_week_end_date, try_cast(null as NUMBER(38,0)) as iso_week_of_year, try_cast(null as NUMBER(38,0)) as prior_year_week_of_year, try_cast(null as NUMBER(38,0)) as prior_year_iso_week_of_year, try_cast(null as NUMBER(38,0)) as month_of_year, try_cast(null as character varying(16777216)) as month_name, try_cast(null as character varying(16777216)) as month_name_short, try_cast(null as DATE) as month_start_date, try_cast(null as DATE) as month_end_date, try_cast(null as DATE) as prior_year_month_start_date, try_cast(null as DATE) as prior_year_month_end_date, try_cast(null as NUMBER(38,0)) as quarter_of_year, try_cast(null as DATE) as quarter_start_date, try_cast(null as DATE) as quarter_end_date, try_cast(null as NUMBER(38,0)) as year_number, try_cast(null as DATE) as year_start_date, try_cast(null as DATE) as year_end_date
union all
select 
    
        try_cast('2020-11-21' as DATE)
     as date_day, try_cast(null as DATE) as prior_date_day, try_cast(null as DATE) as next_date_day, try_cast(null as DATE) as prior_year_date_day, try_cast(null as DATE) as prior_year_over_year_date_day, try_cast(null as NUMBER(3,0)) as day_of_week, try_cast(null as NUMBER(2,0)) as day_of_week_iso, try_cast(null as character varying(16777216)) as day_of_week_name, try_cast(null as character varying(3)) as day_of_week_name_short, try_cast(null as NUMBER(2,0)) as day_of_month, try_cast(null as NUMBER(4,0)) as day_of_year, try_cast(null as DATE) as week_start_date, try_cast(null as DATE) as week_end_date, try_cast(null as DATE) as prior_year_week_start_date, try_cast(null as DATE) as prior_year_week_end_date, try_cast(null as NUMBER(38,0)) as week_of_year, try_cast(null as DATE) as iso_week_start_date, try_cast(null as DATE) as iso_week_end_date, try_cast(null as DATE) as prior_year_iso_week_start_date, try_cast(null as DATE) as prior_year_iso_week_end_date, try_cast(null as NUMBER(38,0)) as iso_week_of_year, try_cast(null as NUMBER(38,0)) as prior_year_week_of_year, try_cast(null as NUMBER(38,0)) as prior_year_iso_week_of_year, try_cast(null as NUMBER(38,0)) as month_of_year, try_cast(null as character varying(16777216)) as month_name, try_cast(null as character varying(16777216)) as month_name_short, try_cast(null as DATE) as month_start_date, try_cast(null as DATE) as month_end_date, try_cast(null as DATE) as prior_year_month_start_date, try_cast(null as DATE) as prior_year_month_end_date, try_cast(null as NUMBER(38,0)) as quarter_of_year, try_cast(null as DATE) as quarter_start_date, try_cast(null as DATE) as quarter_end_date, try_cast(null as NUMBER(38,0)) as year_number, try_cast(null as DATE) as year_start_date, try_cast(null as DATE) as year_end_date
),  __dbt__cte__stg_treasury_revenue_collections as (

-- Fixture for stg_treasury_revenue_collections
select 
    
        try_cast('2020-11-20' as DATE)
     as collection_date, try_cast(null as character varying(16777216)) as channel_type, 
    
        try_cast('\'IRS Tax\'' as character varying(16777216))
     as tax_category, 
    
        try_cast('1000' as NUMBER(19,0))
     as dollars_collected_by_treasury
union all
select 
    
        try_cast('2020-11-20' as DATE)
     as collection_date, try_cast(null as character varying(16777216)) as channel_type, 
    
        try_cast('\'IRS Non Tax\'' as character varying(16777216))
     as tax_category, 
    
        try_cast('2000' as NUMBER(19,0))
     as dollars_collected_by_treasury
union all
select 
    
        try_cast('2020-11-21' as DATE)
     as collection_date, try_cast(null as character varying(16777216)) as channel_type, 
    
        try_cast('\'IRS Tax\'' as character varying(16777216))
     as tax_category, 
    
        try_cast('3000' as NUMBER(19,0))
     as dollars_collected_by_treasury
union all
select 
    
        try_cast('2020-11-21' as DATE)
     as collection_date, try_cast(null as character varying(16777216)) as channel_type, 
    
        try_cast('\'IRS Tax\'' as character varying(16777216))
     as tax_category, 
    
        try_cast('2000' as NUMBER(19,0))
     as dollars_collected_by_treasury
), pol as (
SELECT week_start_date, 
sum(number_of_shares_traded) number_of_shares_traded,
sum(number_of_trades) number_of_trades
from  __dbt__cte__stg_daily_bars a 
JOIN __dbt__cte__dim_date d on d.date_day=a.date
group by 1)

, rev as (
select week_start_date,
sum(dollars_collected_by_treasury) dollars_collected_by_treasury,
sum(case when tax_category = 'IRS Tax' then dollars_collected_by_treasury else 0 end) as irs_tax_dollars_collected_by_treasury, 
sum(case when tax_category = 'Non-Tax' then dollars_collected_by_treasury else 0 end) as non_irs_non_tax_dollars_collected_by_treasury,
sum(case when tax_category = 'IRS Non-Tax' then dollars_collected_by_treasury else 0 end) as irs_non_tax_dollars_collected_by_treasury
FROM __dbt__cte__stg_treasury_revenue_collections a 
JOIN __dbt__cte__dim_date d on d.date_day=a.collection_date
GROUP BY ALL)

select 
    a.week_start_date, 
    a.number_of_shares_traded, 
    a.number_of_trades,
    b.dollars_collected_by_treasury,
    b.irs_tax_dollars_collected_by_treasury

from pol a 
INNER JOIN rev b on b.week_start_date = a.week_start_date
  ) _dbt_internal_unit_test_actual
),
-- Build expected result
dbt_internal_unit_test_expected as (
  select
    week_start_date, number_of_shares_traded, number_of_trades, dollars_collected_by_treasury, irs_tax_dollars_collected_by_treasury, 'expected' as "actual_or_expected"
  from (
    select 
    
        try_cast('\'2020-11-15\'' as DATE)
     as week_start_date, 
    
        try_cast('800' as FLOAT)
     as number_of_shares_traded, 
    
        try_cast('8000' as NUMBER(38,0))
     as number_of_trades, 
    
        try_cast('6000' as NUMBER(31,0))
     as dollars_collected_by_treasury, 
    
        try_cast(null as NUMBER(31,0))
     as irs_tax_dollars_collected_by_treasury
  ) _dbt_internal_unit_test_expected
)
-- Union actual and expected results
select * from dbt_internal_unit_test_actual
union all
select * from dbt_internal_unit_test_expected
[0m15:23:30.036722 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 0.200 seconds
[0m15:23:30.046412 [debug] [Thread-1 (]: Applying DROP to: DATAEXPERT_STUDENT.andres.test_amount_sum__dbt_tmp
[0m15:23:30.053840 [debug] [Thread-1 (]: Using snowflake connection "unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum"
[0m15:23:30.054281 [debug] [Thread-1 (]: On unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum"} */
drop table if exists DATAEXPERT_STUDENT.andres.test_amount_sum__dbt_tmp cascade
[0m15:23:30.239623 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 0.185 seconds
[0m15:23:30.249192 [debug] [Thread-1 (]: On unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum: Close
[0m15:23:30.486411 [error] [Thread-1 (]: 4 of 5 FAIL 1 weekly_metrics::test_amount_sum .................................. [[31mFAIL 1[0m in 2.53s]
[0m15:23:30.487446 [debug] [Thread-1 (]: Finished running node unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum
[0m15:23:30.488369 [debug] [Thread-1 (]: Marking all children of 'unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum' to be skipped because of status 'fail'.  Reason: 

[32mactual[0m differs from [31mexpected[0m:

[0;1m@@[0m [33m,[0m[0;1mWEEK_START_DATE[0m[33m,[0m[0;1mNUMBER_OF_SHARES_TRADED[0m[33m,[0m[0;1mNUMBER_OF_TRADES[0m[33m,[0m[0;1mDOLLARS_COLLECTED_BY_TREASURY[0m[33m,[0m[0;1mIRS_TAX_DOLLARS_COLLECTED_BY_TREASURY[0m
[31;1m---[0m[33m,[0m[31;1mnull[0m           [33m,[0m[31;1mDecimal('800.0')[0m                  [33m,[0m[31;1m8000[0m            [33m,[0m[31;1m6000[0m                         [33m,[0m[31;1mnull[0m

.
[0m15:23:30.489290 [debug] [Thread-1 (]: Began running node model.dbt_capstone_andres.weekly_metrics
[0m15:23:30.489821 [info ] [Thread-1 (]: 5 of 5 SKIP relation andres.weekly_metrics ..................................... [[33mSKIP[0m]
[0m15:23:30.490367 [debug] [Thread-1 (]: Finished running node model.dbt_capstone_andres.weekly_metrics
[0m15:23:30.490922 [debug] [Thread-4 (]: Marking all children of 'model.dbt_capstone_andres.weekly_metrics' to be skipped because of status 'skipped'. 
[0m15:23:30.492423 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:23:30.492865 [debug] [MainThread]: Connection 'unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum' was properly closed.
[0m15:23:30.493665 [info ] [MainThread]: 
[0m15:23:30.494149 [info ] [MainThread]: Finished running 4 table models, 1 unit test in 0 hours 0 minutes and 12.60 seconds (12.60s).
[0m15:23:30.495578 [debug] [MainThread]: Command end result
[0m15:23:30.549631 [info ] [MainThread]: 
[0m15:23:30.550328 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successs, and 0 warnings:[0m
[0m15:23:30.550835 [info ] [MainThread]: 
[0m15:23:30.551366 [error] [MainThread]: [31mFailure in unit_test test_amount_sum (models/marts/_marts.yml)[0m
[0m15:23:30.551965 [error] [MainThread]:   

[32mactual[0m differs from [31mexpected[0m:

[0;1m@@[0m [33m,[0m[0;1mWEEK_START_DATE[0m[33m,[0m[0;1mNUMBER_OF_SHARES_TRADED[0m[33m,[0m[0;1mNUMBER_OF_TRADES[0m[33m,[0m[0;1mDOLLARS_COLLECTED_BY_TREASURY[0m[33m,[0m[0;1mIRS_TAX_DOLLARS_COLLECTED_BY_TREASURY[0m
[31;1m---[0m[33m,[0m[31;1mnull[0m           [33m,[0m[31;1mDecimal('800.0')[0m                  [33m,[0m[31;1m8000[0m            [33m,[0m[31;1m6000[0m                         [33m,[0m[31;1mnull[0m


[0m15:23:30.552383 [info ] [MainThread]: 
[0m15:23:30.553188 [info ] [MainThread]:   compiled code at target/compiled/dbt_capstone_andres/models/marts/_marts.yml/models/marts/test_amount_sum.sql
[0m15:23:30.553650 [info ] [MainThread]: 
[0m15:23:30.554043 [info ] [MainThread]: Done. PASS=3 WARN=0 ERROR=1 SKIP=1 TOTAL=5
[0m15:23:30.557143 [debug] [MainThread]: Resource report: {"command_name": "build", "command_wall_clock_time": 15.422707, "process_user_time": 3.780792, "process_kernel_time": 0.695267, "process_mem_max_rss": "200220672", "command_success": false, "process_in_blocks": "0", "process_out_blocks": "0"}
[0m15:23:30.557900 [debug] [MainThread]: Command `dbt build` failed at 15:23:30.557784 after 15.42 seconds
[0m15:23:30.558371 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10edc4d10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ee43f50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ee43bd0>]}
[0m15:23:30.558802 [debug] [MainThread]: Flushing usage events
[0m15:23:30.742806 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:26:07.325633 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b3b3a10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b4225d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b422bd0>]}


============================== 15:26:07.331942 | 734aafdb-e8a4-414f-8a1a-2e78dcda236e ==============================
[0m15:26:07.331942 [info ] [MainThread]: Running with dbt=1.9.0-b3
[0m15:26:07.332676 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'fail_fast': 'False', 'log_path': 'logs', 'version_check': 'True', 'profiles_dir': '.', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'static_parser': 'True', 'invocation_command': 'dbt build', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m15:26:09.014373 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '734aafdb-e8a4-414f-8a1a-2e78dcda236e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10dae4c90>]}
[0m15:26:09.070626 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '734aafdb-e8a4-414f-8a1a-2e78dcda236e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a054e90>]}
[0m15:26:09.072246 [info ] [MainThread]: Registered adapter: snowflake=1.8.4
[0m15:26:09.261756 [debug] [MainThread]: checksum: 5f4fde221ca8627174a0341efc73099aaec05b8546408ceed6f59b1ec0755115, vars: {}, profile: , target: , version: 1.9.0b3
[0m15:26:09.500705 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m15:26:09.501626 [debug] [MainThread]: Partial parsing: updated file: dbt_capstone_andres://models/marts/_marts.yml
[0m15:26:09.913981 [debug] [MainThread]: STDERR: "Unit Test fixtures benefit from having at least one row free of Null values to ensure consistent column types. Failure to meet this recommendation can result in type mismatch errors between unit test source models and `expected` fixtures."
[0m15:26:09.992693 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '734aafdb-e8a4-414f-8a1a-2e78dcda236e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111738d50>]}
[0m15:26:10.142437 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '734aafdb-e8a4-414f-8a1a-2e78dcda236e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111b9d9d0>]}
[0m15:26:10.143009 [info ] [MainThread]: Found 4 models, 2 sources, 862 macros, 1 unit test
[0m15:26:10.145152 [info ] [MainThread]: 
[0m15:26:10.145552 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m15:26:10.145914 [info ] [MainThread]: 
[0m15:26:10.146440 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m15:26:10.151456 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_DATAEXPERT_STUDENT'
[0m15:26:10.165610 [debug] [ThreadPool]: Using snowflake connection "list_DATAEXPERT_STUDENT"
[0m15:26:10.166081 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "connection_name": "list_DATAEXPERT_STUDENT"} */
show terse schemas in database DATAEXPERT_STUDENT
    limit 10000
[0m15:26:10.166438 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:26:11.137714 [debug] [ThreadPool]: SQL status: SUCCESS 73 in 0.971 seconds
[0m15:26:11.143792 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT: Close
[0m15:26:11.379649 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_DATAEXPERT_STUDENT, now list_DATAEXPERT_STUDENT_andres)
[0m15:26:11.390195 [debug] [ThreadPool]: Using snowflake connection "list_DATAEXPERT_STUDENT_andres"
[0m15:26:11.390692 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "connection_name": "list_DATAEXPERT_STUDENT_andres"} */
show objects in DATAEXPERT_STUDENT.andres limit 10000
[0m15:26:11.391069 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:26:12.078035 [debug] [ThreadPool]: SQL status: SUCCESS 25 in 0.687 seconds
[0m15:26:12.080377 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: Close
[0m15:26:12.296260 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '734aafdb-e8a4-414f-8a1a-2e78dcda236e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111719e10>]}
[0m15:26:12.299576 [debug] [Thread-1 (]: Began running node model.dbt_capstone_andres.dim_date
[0m15:26:12.300296 [info ] [Thread-1 (]: 1 of 5 START sql table model andres.dim_date ................................... [RUN]
[0m15:26:12.300894 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_DATAEXPERT_STUDENT_andres, now model.dbt_capstone_andres.dim_date)
[0m15:26:12.301368 [debug] [Thread-1 (]: Began compiling node model.dbt_capstone_andres.dim_date
[0m15:26:12.342853 [debug] [Thread-1 (]: Using snowflake connection "model.dbt_capstone_andres.dim_date"
[0m15:26:12.343475 [debug] [Thread-1 (]: On model.dbt_capstone_andres.dim_date: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "model.dbt_capstone_andres.dim_date"} */
select datediff(
        day,
        cast('1995-01-01' as timestamp),
        cast('2040-12-31' as timestamp)
        )
[0m15:26:12.343984 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:26:12.993764 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 0.650 seconds
[0m15:26:13.056948 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_capstone_andres.dim_date"
[0m15:26:13.058471 [debug] [Thread-1 (]: Began executing node model.dbt_capstone_andres.dim_date
[0m15:26:13.086325 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_capstone_andres.dim_date"
[0m15:26:13.093098 [debug] [Thread-1 (]: Using snowflake connection "model.dbt_capstone_andres.dim_date"
[0m15:26:13.093784 [debug] [Thread-1 (]: On model.dbt_capstone_andres.dim_date: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "model.dbt_capstone_andres.dim_date"} */
create or replace transient table DATAEXPERT_STUDENT.andres.dim_date
         as
        (

    
with base_dates as (
    
    with date_spine as
(

    





with rawdata as (

    

    

    with p as (
        select 0 as generated_number union all select 1
    ), unioned as (

    select

    
    p0.generated_number * power(2, 0)
     + 
    
    p1.generated_number * power(2, 1)
     + 
    
    p2.generated_number * power(2, 2)
     + 
    
    p3.generated_number * power(2, 3)
     + 
    
    p4.generated_number * power(2, 4)
     + 
    
    p5.generated_number * power(2, 5)
     + 
    
    p6.generated_number * power(2, 6)
     + 
    
    p7.generated_number * power(2, 7)
     + 
    
    p8.generated_number * power(2, 8)
     + 
    
    p9.generated_number * power(2, 9)
     + 
    
    p10.generated_number * power(2, 10)
     + 
    
    p11.generated_number * power(2, 11)
     + 
    
    p12.generated_number * power(2, 12)
     + 
    
    p13.generated_number * power(2, 13)
     + 
    
    p14.generated_number * power(2, 14)
    
    
    + 1
    as generated_number

    from

    
    p as p0
     cross join 
    
    p as p1
     cross join 
    
    p as p2
     cross join 
    
    p as p3
     cross join 
    
    p as p4
     cross join 
    
    p as p5
     cross join 
    
    p as p6
     cross join 
    
    p as p7
     cross join 
    
    p as p8
     cross join 
    
    p as p9
     cross join 
    
    p as p10
     cross join 
    
    p as p11
     cross join 
    
    p as p12
     cross join 
    
    p as p13
     cross join 
    
    p as p14
    
    

    )

    select *
    from unioned
    where generated_number <= 16801
    order by generated_number



),

all_periods as (

    select (
        

    dateadd(
        day,
        (row_number() over (order by 1) - 1),
        cast('1995-01-01' as timestamp)
        )


    ) as date_day
    from rawdata

),

filtered as (

    select *
    from all_periods
    where date_day <= cast('2040-12-31' as timestamp)

)

select * from filtered



)
select
    cast(d.date_day as timestamp) as date_day
from
    date_spine d


),
dates_with_prior_year_dates as (

    select
        cast(d.date_day as date) as date_day,
        cast(

    dateadd(
        year,
        -1,
        d.date_day
        )

 as date) as prior_year_date_day,
        cast(

    dateadd(
        day,
        -364,
        d.date_day
        )

 as date) as prior_year_over_year_date_day
    from
    	base_dates d

)
select
    d.date_day,
    cast(

    dateadd(
        day,
        -1,
        d.date_day
        )

 as date) as prior_date_day,
    cast(

    dateadd(
        day,
        1,
        d.date_day
        )

 as date) as next_date_day,
    d.prior_year_date_day as prior_year_date_day,
    d.prior_year_over_year_date_day,
    case
            when date_part('dayofweek', d.date_day) = 7 then 1
            else date_part('dayofweek', d.date_day) + 1
        end as day_of_week,
    date_part('dayofweekiso', d.date_day) as day_of_week_iso,
    -- long version not implemented on Snowflake so we're doing it manually :/
    case dayname(d.date_day)
        when 'Mon' then 'Monday'
        when 'Tue' then 'Tuesday'
        when 'Wed' then 'Wednesday'
        when 'Thu' then 'Thursday'
        when 'Fri' then 'Friday'
        when 'Sat' then 'Saturday'
        when 'Sun' then 'Sunday'
    end as day_of_week_name,
    dayname(d.date_day) as day_of_week_name_short,
    date_part('day', d.date_day) as day_of_month,
    date_part('dayofyear', d.date_day) as day_of_year,

    
    
    cast(

    dateadd(
        day,
        -1 * (case
            when date_part('dayofweek', d.date_day) = 7 then 1
            else date_part('dayofweek', d.date_day) + 1
        end - 1),
        d.date_day
        )

 as date) as week_start_date,
    cast(

    dateadd(
        day,
        6,
        
    
    cast(

    dateadd(
        day,
        -1 * (case
            when date_part('dayofweek', d.date_day) = 7 then 1
            else date_part('dayofweek', d.date_day) + 1
        end - 1),
        d.date_day
        )

 as date)
        )

 as date) as week_end_date,
    
    
    cast(

    dateadd(
        day,
        -1 * (case
            when date_part('dayofweek', d.prior_year_over_year_date_day) = 7 then 1
            else date_part('dayofweek', d.prior_year_over_year_date_day) + 1
        end - 1),
        d.prior_year_over_year_date_day
        )

 as date) as prior_year_week_start_date,
    cast(

    dateadd(
        day,
        6,
        
    
    cast(

    dateadd(
        day,
        -1 * (case
            when date_part('dayofweek', d.prior_year_over_year_date_day) = 7 then 1
            else date_part('dayofweek', d.prior_year_over_year_date_day) + 1
        end - 1),
        d.prior_year_over_year_date_day
        )

 as date)
        )

 as date) as prior_year_week_end_date,
    cast(date_part('week', d.date_day) as integer) as week_of_year,

    cast(date_trunc('week', d.date_day) as date) as iso_week_start_date,
    cast(

    dateadd(
        day,
        6,
        cast(date_trunc('week', d.date_day) as date)
        )

 as date) as iso_week_end_date,
    cast(date_trunc('week', d.prior_year_over_year_date_day) as date) as prior_year_iso_week_start_date,
    cast(

    dateadd(
        day,
        6,
        cast(date_trunc('week', d.prior_year_over_year_date_day) as date)
        )

 as date) as prior_year_iso_week_end_date,
    cast(date_part('weekiso', d.date_day) as integer) as iso_week_of_year,

    cast(date_part('week', d.prior_year_over_year_date_day) as integer) as prior_year_week_of_year,
    cast(date_part('weekiso', d.prior_year_over_year_date_day) as integer) as prior_year_iso_week_of_year,

    cast(date_part('month', d.date_day) as integer) as month_of_year,
    to_char(d.date_day, 'MMMM')  as month_name,
    to_char(d.date_day, 'MON')  as month_name_short,

    cast(date_trunc('month', d.date_day) as date) as month_start_date,
    cast(cast(
        

    dateadd(
        day,
        -1,
        

    dateadd(
        month,
        1,
        date_trunc('month', d.date_day)
        )


        )


        as date) as date) as month_end_date,

    cast(date_trunc('month', d.prior_year_date_day) as date) as prior_year_month_start_date,
    cast(cast(
        

    dateadd(
        day,
        -1,
        

    dateadd(
        month,
        1,
        date_trunc('month', d.prior_year_date_day)
        )


        )


        as date) as date) as prior_year_month_end_date,

    cast(date_part('quarter', d.date_day) as integer) as quarter_of_year,
    cast(date_trunc('quarter', d.date_day) as date) as quarter_start_date,
    cast(cast(
        

    dateadd(
        day,
        -1,
        

    dateadd(
        quarter,
        1,
        date_trunc('quarter', d.date_day)
        )


        )


        as date) as date) as quarter_end_date,

    cast(date_part('year', d.date_day) as integer) as year_number,
    cast(date_trunc('year', d.date_day) as date) as year_start_date,
    cast(cast(
        

    dateadd(
        day,
        -1,
        

    dateadd(
        year,
        1,
        date_trunc('year', d.date_day)
        )


        )


        as date) as date) as year_end_date
from
    dates_with_prior_year_dates d
order by 1


        );
[0m15:26:14.896054 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 1.801 seconds
[0m15:26:14.922617 [debug] [Thread-1 (]: On model.dbt_capstone_andres.dim_date: Close
[0m15:26:15.169962 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '734aafdb-e8a4-414f-8a1a-2e78dcda236e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109528e50>]}
[0m15:26:15.170817 [info ] [Thread-1 (]: 1 of 5 OK created sql table model andres.dim_date .............................. [[32mSUCCESS 1[0m in 2.87s]
[0m15:26:15.171547 [debug] [Thread-1 (]: Finished running node model.dbt_capstone_andres.dim_date
[0m15:26:15.172043 [debug] [Thread-1 (]: Began running node model.dbt_capstone_andres.stg_daily_bars
[0m15:26:15.172683 [info ] [Thread-1 (]: 2 of 5 START sql table model andres.stg_daily_bars ............................. [RUN]
[0m15:26:15.173245 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_capstone_andres.dim_date, now model.dbt_capstone_andres.stg_daily_bars)
[0m15:26:15.173699 [debug] [Thread-1 (]: Began compiling node model.dbt_capstone_andres.stg_daily_bars
[0m15:26:15.177542 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_capstone_andres.stg_daily_bars"
[0m15:26:15.178335 [debug] [Thread-1 (]: Began executing node model.dbt_capstone_andres.stg_daily_bars
[0m15:26:15.182042 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_capstone_andres.stg_daily_bars"
[0m15:26:15.183452 [debug] [Thread-1 (]: Using snowflake connection "model.dbt_capstone_andres.stg_daily_bars"
[0m15:26:15.183929 [debug] [Thread-1 (]: On model.dbt_capstone_andres.stg_daily_bars: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "model.dbt_capstone_andres.stg_daily_bars"} */
create or replace transient table DATAEXPERT_STUDENT.andres.stg_daily_bars
         as
        (with source as (

    select * from dataexpert_student.andres.daily_bars

),

renamed as (

    select
        TO_DATE(TO_TIMESTAMP(timestamp / 1000)) as date,
        ticker as ticker,
        v  as number_of_shares_traded,
        n  as number_of_trades,
        round(vw,2) as volume_weighted_avg_price,
        o  as open_price,
        c  as close_price,
        h  as high_price,
        l  as low_price
    from source

)

select * from renamed
        );
[0m15:26:15.184390 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:26:17.361338 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 2.177 seconds
[0m15:26:17.364158 [debug] [Thread-1 (]: On model.dbt_capstone_andres.stg_daily_bars: Close
[0m15:26:17.598024 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '734aafdb-e8a4-414f-8a1a-2e78dcda236e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x115ede650>]}
[0m15:26:17.598898 [info ] [Thread-1 (]: 2 of 5 OK created sql table model andres.stg_daily_bars ........................ [[32mSUCCESS 1[0m in 2.42s]
[0m15:26:17.599663 [debug] [Thread-1 (]: Finished running node model.dbt_capstone_andres.stg_daily_bars
[0m15:26:17.600191 [debug] [Thread-1 (]: Began running node model.dbt_capstone_andres.stg_treasury_revenue_collections
[0m15:26:17.600912 [info ] [Thread-1 (]: 3 of 5 START sql table model andres.stg_treasury_revenue_collections ........... [RUN]
[0m15:26:17.601509 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_capstone_andres.stg_daily_bars, now model.dbt_capstone_andres.stg_treasury_revenue_collections)
[0m15:26:17.601968 [debug] [Thread-1 (]: Began compiling node model.dbt_capstone_andres.stg_treasury_revenue_collections
[0m15:26:17.606293 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_capstone_andres.stg_treasury_revenue_collections"
[0m15:26:17.607246 [debug] [Thread-1 (]: Began executing node model.dbt_capstone_andres.stg_treasury_revenue_collections
[0m15:26:17.613440 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_capstone_andres.stg_treasury_revenue_collections"
[0m15:26:17.615148 [debug] [Thread-1 (]: Using snowflake connection "model.dbt_capstone_andres.stg_treasury_revenue_collections"
[0m15:26:17.615719 [debug] [Thread-1 (]: On model.dbt_capstone_andres.stg_treasury_revenue_collections: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "model.dbt_capstone_andres.stg_treasury_revenue_collections"} */
create or replace transient table DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections
         as
        (with source as (

    select * from dataexpert_student.andres.fiscal_data

),

renamed as (

    select
        record_date as collection_date,
        channel_type_desc as channel_type,
        tax_category_desc as tax_category,
        round(net_collections_amt,0) as dollars_collected_by_treasury

    from source

)

select * from renamed
        );
[0m15:26:17.616227 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:26:19.116663 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 1.500 seconds
[0m15:26:19.118793 [debug] [Thread-1 (]: On model.dbt_capstone_andres.stg_treasury_revenue_collections: Close
[0m15:26:19.331282 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '734aafdb-e8a4-414f-8a1a-2e78dcda236e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x115ecbc50>]}
[0m15:26:19.332018 [info ] [Thread-1 (]: 3 of 5 OK created sql table model andres.stg_treasury_revenue_collections ...... [[32mSUCCESS 1[0m in 1.73s]
[0m15:26:19.332632 [debug] [Thread-1 (]: Finished running node model.dbt_capstone_andres.stg_treasury_revenue_collections
[0m15:26:19.333432 [debug] [Thread-1 (]: Began running node unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum
[0m15:26:19.333841 [info ] [Thread-1 (]: 4 of 5 START unit_test weekly_metrics::test_amount_sum ......................... [RUN]
[0m15:26:19.334310 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_capstone_andres.stg_treasury_revenue_collections, now unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum)
[0m15:26:19.334692 [debug] [Thread-1 (]: Began compiling node unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum
[0m15:26:19.335071 [debug] [Thread-1 (]: Began executing node unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum
[0m15:26:19.375100 [debug] [Thread-1 (]: Using snowflake connection "unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum"
[0m15:26:19.375596 [debug] [Thread-1 (]: On unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum"} */
describe table DATAEXPERT_STUDENT.andres.stg_daily_bars
[0m15:26:19.375992 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:26:20.017213 [debug] [Thread-1 (]: SQL status: SUCCESS 9 in 0.641 seconds
[0m15:26:20.043075 [debug] [Thread-1 (]: Writing injected SQL for node "unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum"
[0m15:26:20.048538 [debug] [Thread-1 (]: Using snowflake connection "unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum"
[0m15:26:20.048952 [debug] [Thread-1 (]: On unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum"} */
describe table DATAEXPERT_STUDENT.andres.dim_date
[0m15:26:20.192521 [debug] [Thread-1 (]: SQL status: SUCCESS 36 in 0.143 seconds
[0m15:26:20.204303 [debug] [Thread-1 (]: Writing injected SQL for node "unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum"
[0m15:26:20.210796 [debug] [Thread-1 (]: Using snowflake connection "unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum"
[0m15:26:20.211246 [debug] [Thread-1 (]: On unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum"} */
describe table DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections
[0m15:26:20.397573 [debug] [Thread-1 (]: SQL status: SUCCESS 4 in 0.186 seconds
[0m15:26:20.409595 [debug] [Thread-1 (]: Writing injected SQL for node "unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum"
[0m15:26:20.419696 [debug] [Thread-1 (]: Writing injected SQL for node "unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum"
[0m15:26:20.453462 [debug] [Thread-1 (]: Using snowflake connection "unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum"
[0m15:26:20.454385 [debug] [Thread-1 (]: On unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum"} */
create or replace temporary table DATAEXPERT_STUDENT.andres.test_amount_sum__dbt_tmp
         as
        (select * from (
        with  __dbt__cte__stg_daily_bars as (

-- Fixture for stg_daily_bars
select 
    
        try_cast('2020-11-20' as DATE)
     as date, 
    
        try_cast('\'L\'' as character varying(16777216))
     as ticker, 
    
        try_cast('100' as FLOAT)
     as number_of_shares_traded, 
    
        try_cast('10' as NUMBER(38,0))
     as number_of_trades, try_cast(null as FLOAT) as volume_weighted_avg_price, try_cast(null as FLOAT) as open_price, try_cast(null as FLOAT) as close_price, try_cast(null as FLOAT) as high_price, try_cast(null as FLOAT) as low_price
union all
select 
    
        try_cast('2020-11-20' as DATE)
     as date, 
    
        try_cast('\'S\'' as character varying(16777216))
     as ticker, 
    
        try_cast('200' as FLOAT)
     as number_of_shares_traded, 
    
        try_cast('20' as NUMBER(38,0))
     as number_of_trades, try_cast(null as FLOAT) as volume_weighted_avg_price, try_cast(null as FLOAT) as open_price, try_cast(null as FLOAT) as close_price, try_cast(null as FLOAT) as high_price, try_cast(null as FLOAT) as low_price
union all
select 
    
        try_cast('2020-11-21' as DATE)
     as date, 
    
        try_cast('\'L\'' as character varying(16777216))
     as ticker, 
    
        try_cast('300' as FLOAT)
     as number_of_shares_traded, 
    
        try_cast('40' as NUMBER(38,0))
     as number_of_trades, try_cast(null as FLOAT) as volume_weighted_avg_price, try_cast(null as FLOAT) as open_price, try_cast(null as FLOAT) as close_price, try_cast(null as FLOAT) as high_price, try_cast(null as FLOAT) as low_price
union all
select 
    
        try_cast('2020-11-21' as DATE)
     as date, 
    
        try_cast('\'S\'' as character varying(16777216))
     as ticker, 
    
        try_cast('200' as FLOAT)
     as number_of_shares_traded, 
    
        try_cast('50' as NUMBER(38,0))
     as number_of_trades, try_cast(null as FLOAT) as volume_weighted_avg_price, try_cast(null as FLOAT) as open_price, try_cast(null as FLOAT) as close_price, try_cast(null as FLOAT) as high_price, try_cast(null as FLOAT) as low_price
),  __dbt__cte__dim_date as (

-- Fixture for dim_date
select 
    
        try_cast('2020-11-20' as DATE)
     as date_day, try_cast(null as DATE) as prior_date_day, try_cast(null as DATE) as next_date_day, try_cast(null as DATE) as prior_year_date_day, try_cast(null as DATE) as prior_year_over_year_date_day, try_cast(null as NUMBER(3,0)) as day_of_week, try_cast(null as NUMBER(2,0)) as day_of_week_iso, try_cast(null as character varying(16777216)) as day_of_week_name, try_cast(null as character varying(3)) as day_of_week_name_short, try_cast(null as NUMBER(2,0)) as day_of_month, try_cast(null as NUMBER(4,0)) as day_of_year, try_cast(null as DATE) as week_start_date, try_cast(null as DATE) as week_end_date, try_cast(null as DATE) as prior_year_week_start_date, try_cast(null as DATE) as prior_year_week_end_date, try_cast(null as NUMBER(38,0)) as week_of_year, try_cast(null as DATE) as iso_week_start_date, try_cast(null as DATE) as iso_week_end_date, try_cast(null as DATE) as prior_year_iso_week_start_date, try_cast(null as DATE) as prior_year_iso_week_end_date, try_cast(null as NUMBER(38,0)) as iso_week_of_year, try_cast(null as NUMBER(38,0)) as prior_year_week_of_year, try_cast(null as NUMBER(38,0)) as prior_year_iso_week_of_year, try_cast(null as NUMBER(38,0)) as month_of_year, try_cast(null as character varying(16777216)) as month_name, try_cast(null as character varying(16777216)) as month_name_short, try_cast(null as DATE) as month_start_date, try_cast(null as DATE) as month_end_date, try_cast(null as DATE) as prior_year_month_start_date, try_cast(null as DATE) as prior_year_month_end_date, try_cast(null as NUMBER(38,0)) as quarter_of_year, try_cast(null as DATE) as quarter_start_date, try_cast(null as DATE) as quarter_end_date, try_cast(null as NUMBER(38,0)) as year_number, try_cast(null as DATE) as year_start_date, try_cast(null as DATE) as year_end_date
union all
select 
    
        try_cast('2020-11-21' as DATE)
     as date_day, try_cast(null as DATE) as prior_date_day, try_cast(null as DATE) as next_date_day, try_cast(null as DATE) as prior_year_date_day, try_cast(null as DATE) as prior_year_over_year_date_day, try_cast(null as NUMBER(3,0)) as day_of_week, try_cast(null as NUMBER(2,0)) as day_of_week_iso, try_cast(null as character varying(16777216)) as day_of_week_name, try_cast(null as character varying(3)) as day_of_week_name_short, try_cast(null as NUMBER(2,0)) as day_of_month, try_cast(null as NUMBER(4,0)) as day_of_year, try_cast(null as DATE) as week_start_date, try_cast(null as DATE) as week_end_date, try_cast(null as DATE) as prior_year_week_start_date, try_cast(null as DATE) as prior_year_week_end_date, try_cast(null as NUMBER(38,0)) as week_of_year, try_cast(null as DATE) as iso_week_start_date, try_cast(null as DATE) as iso_week_end_date, try_cast(null as DATE) as prior_year_iso_week_start_date, try_cast(null as DATE) as prior_year_iso_week_end_date, try_cast(null as NUMBER(38,0)) as iso_week_of_year, try_cast(null as NUMBER(38,0)) as prior_year_week_of_year, try_cast(null as NUMBER(38,0)) as prior_year_iso_week_of_year, try_cast(null as NUMBER(38,0)) as month_of_year, try_cast(null as character varying(16777216)) as month_name, try_cast(null as character varying(16777216)) as month_name_short, try_cast(null as DATE) as month_start_date, try_cast(null as DATE) as month_end_date, try_cast(null as DATE) as prior_year_month_start_date, try_cast(null as DATE) as prior_year_month_end_date, try_cast(null as NUMBER(38,0)) as quarter_of_year, try_cast(null as DATE) as quarter_start_date, try_cast(null as DATE) as quarter_end_date, try_cast(null as NUMBER(38,0)) as year_number, try_cast(null as DATE) as year_start_date, try_cast(null as DATE) as year_end_date
),  __dbt__cte__stg_treasury_revenue_collections as (

-- Fixture for stg_treasury_revenue_collections
select 
    
        try_cast('2020-11-20' as DATE)
     as collection_date, try_cast(null as character varying(16777216)) as channel_type, 
    
        try_cast('\'IRS Tax\'' as character varying(16777216))
     as tax_category, 
    
        try_cast('1000' as NUMBER(19,0))
     as dollars_collected_by_treasury
union all
select 
    
        try_cast('2020-11-20' as DATE)
     as collection_date, try_cast(null as character varying(16777216)) as channel_type, 
    
        try_cast('\'IRS Non Tax\'' as character varying(16777216))
     as tax_category, 
    
        try_cast('2000' as NUMBER(19,0))
     as dollars_collected_by_treasury
union all
select 
    
        try_cast('2020-11-21' as DATE)
     as collection_date, try_cast(null as character varying(16777216)) as channel_type, 
    
        try_cast('\'IRS Tax\'' as character varying(16777216))
     as tax_category, 
    
        try_cast('3000' as NUMBER(19,0))
     as dollars_collected_by_treasury
union all
select 
    
        try_cast('2020-11-21' as DATE)
     as collection_date, try_cast(null as character varying(16777216)) as channel_type, 
    
        try_cast('\'IRS Tax\'' as character varying(16777216))
     as tax_category, 
    
        try_cast('2000' as NUMBER(19,0))
     as dollars_collected_by_treasury
), pol as (
SELECT week_start_date, 
sum(number_of_shares_traded) number_of_shares_traded,
sum(number_of_trades) number_of_trades
from  __dbt__cte__stg_daily_bars a 
JOIN __dbt__cte__dim_date d on d.date_day=a.date
group by 1)

, rev as (
select week_start_date,
sum(dollars_collected_by_treasury) dollars_collected_by_treasury,
sum(case when tax_category = 'IRS Tax' then dollars_collected_by_treasury else 0 end) as irs_tax_dollars_collected_by_treasury, 
sum(case when tax_category = 'Non-Tax' then dollars_collected_by_treasury else 0 end) as non_irs_non_tax_dollars_collected_by_treasury,
sum(case when tax_category = 'IRS Non-Tax' then dollars_collected_by_treasury else 0 end) as irs_non_tax_dollars_collected_by_treasury
FROM __dbt__cte__stg_treasury_revenue_collections a 
JOIN __dbt__cte__dim_date d on d.date_day=a.collection_date
GROUP BY ALL)

select 
    a.week_start_date, 
    a.number_of_shares_traded, 
    a.number_of_trades,
    b.dollars_collected_by_treasury,
    b.irs_tax_dollars_collected_by_treasury

from pol a 
INNER JOIN rev b on b.week_start_date = a.week_start_date
    ) as __dbt_sbq
    where false
    limit 0

        );
[0m15:26:21.394536 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 0.939 seconds
[0m15:26:21.398045 [debug] [Thread-1 (]: Using snowflake connection "unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum"
[0m15:26:21.398449 [debug] [Thread-1 (]: On unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum"} */
describe table DATAEXPERT_STUDENT.andres.test_amount_sum__dbt_tmp
[0m15:26:21.625820 [debug] [Thread-1 (]: SQL status: SUCCESS 5 in 0.227 seconds
[0m15:26:21.636506 [debug] [Thread-1 (]: Writing runtime sql for node "unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum"
[0m15:26:21.646782 [debug] [Thread-1 (]: Using snowflake connection "unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum"
[0m15:26:21.647586 [debug] [Thread-1 (]: On unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum"} */
-- Build actual result given inputs
with dbt_internal_unit_test_actual as (
  select
    week_start_date,number_of_shares_traded,number_of_trades,dollars_collected_by_treasury,irs_tax_dollars_collected_by_treasury, 'actual' as "actual_or_expected"
  from (
    with  __dbt__cte__stg_daily_bars as (

-- Fixture for stg_daily_bars
select 
    
        try_cast('2020-11-20' as DATE)
     as date, 
    
        try_cast('\'L\'' as character varying(16777216))
     as ticker, 
    
        try_cast('100' as FLOAT)
     as number_of_shares_traded, 
    
        try_cast('10' as NUMBER(38,0))
     as number_of_trades, try_cast(null as FLOAT) as volume_weighted_avg_price, try_cast(null as FLOAT) as open_price, try_cast(null as FLOAT) as close_price, try_cast(null as FLOAT) as high_price, try_cast(null as FLOAT) as low_price
union all
select 
    
        try_cast('2020-11-20' as DATE)
     as date, 
    
        try_cast('\'S\'' as character varying(16777216))
     as ticker, 
    
        try_cast('200' as FLOAT)
     as number_of_shares_traded, 
    
        try_cast('20' as NUMBER(38,0))
     as number_of_trades, try_cast(null as FLOAT) as volume_weighted_avg_price, try_cast(null as FLOAT) as open_price, try_cast(null as FLOAT) as close_price, try_cast(null as FLOAT) as high_price, try_cast(null as FLOAT) as low_price
union all
select 
    
        try_cast('2020-11-21' as DATE)
     as date, 
    
        try_cast('\'L\'' as character varying(16777216))
     as ticker, 
    
        try_cast('300' as FLOAT)
     as number_of_shares_traded, 
    
        try_cast('40' as NUMBER(38,0))
     as number_of_trades, try_cast(null as FLOAT) as volume_weighted_avg_price, try_cast(null as FLOAT) as open_price, try_cast(null as FLOAT) as close_price, try_cast(null as FLOAT) as high_price, try_cast(null as FLOAT) as low_price
union all
select 
    
        try_cast('2020-11-21' as DATE)
     as date, 
    
        try_cast('\'S\'' as character varying(16777216))
     as ticker, 
    
        try_cast('200' as FLOAT)
     as number_of_shares_traded, 
    
        try_cast('50' as NUMBER(38,0))
     as number_of_trades, try_cast(null as FLOAT) as volume_weighted_avg_price, try_cast(null as FLOAT) as open_price, try_cast(null as FLOAT) as close_price, try_cast(null as FLOAT) as high_price, try_cast(null as FLOAT) as low_price
),  __dbt__cte__dim_date as (

-- Fixture for dim_date
select 
    
        try_cast('2020-11-20' as DATE)
     as date_day, try_cast(null as DATE) as prior_date_day, try_cast(null as DATE) as next_date_day, try_cast(null as DATE) as prior_year_date_day, try_cast(null as DATE) as prior_year_over_year_date_day, try_cast(null as NUMBER(3,0)) as day_of_week, try_cast(null as NUMBER(2,0)) as day_of_week_iso, try_cast(null as character varying(16777216)) as day_of_week_name, try_cast(null as character varying(3)) as day_of_week_name_short, try_cast(null as NUMBER(2,0)) as day_of_month, try_cast(null as NUMBER(4,0)) as day_of_year, try_cast(null as DATE) as week_start_date, try_cast(null as DATE) as week_end_date, try_cast(null as DATE) as prior_year_week_start_date, try_cast(null as DATE) as prior_year_week_end_date, try_cast(null as NUMBER(38,0)) as week_of_year, try_cast(null as DATE) as iso_week_start_date, try_cast(null as DATE) as iso_week_end_date, try_cast(null as DATE) as prior_year_iso_week_start_date, try_cast(null as DATE) as prior_year_iso_week_end_date, try_cast(null as NUMBER(38,0)) as iso_week_of_year, try_cast(null as NUMBER(38,0)) as prior_year_week_of_year, try_cast(null as NUMBER(38,0)) as prior_year_iso_week_of_year, try_cast(null as NUMBER(38,0)) as month_of_year, try_cast(null as character varying(16777216)) as month_name, try_cast(null as character varying(16777216)) as month_name_short, try_cast(null as DATE) as month_start_date, try_cast(null as DATE) as month_end_date, try_cast(null as DATE) as prior_year_month_start_date, try_cast(null as DATE) as prior_year_month_end_date, try_cast(null as NUMBER(38,0)) as quarter_of_year, try_cast(null as DATE) as quarter_start_date, try_cast(null as DATE) as quarter_end_date, try_cast(null as NUMBER(38,0)) as year_number, try_cast(null as DATE) as year_start_date, try_cast(null as DATE) as year_end_date
union all
select 
    
        try_cast('2020-11-21' as DATE)
     as date_day, try_cast(null as DATE) as prior_date_day, try_cast(null as DATE) as next_date_day, try_cast(null as DATE) as prior_year_date_day, try_cast(null as DATE) as prior_year_over_year_date_day, try_cast(null as NUMBER(3,0)) as day_of_week, try_cast(null as NUMBER(2,0)) as day_of_week_iso, try_cast(null as character varying(16777216)) as day_of_week_name, try_cast(null as character varying(3)) as day_of_week_name_short, try_cast(null as NUMBER(2,0)) as day_of_month, try_cast(null as NUMBER(4,0)) as day_of_year, try_cast(null as DATE) as week_start_date, try_cast(null as DATE) as week_end_date, try_cast(null as DATE) as prior_year_week_start_date, try_cast(null as DATE) as prior_year_week_end_date, try_cast(null as NUMBER(38,0)) as week_of_year, try_cast(null as DATE) as iso_week_start_date, try_cast(null as DATE) as iso_week_end_date, try_cast(null as DATE) as prior_year_iso_week_start_date, try_cast(null as DATE) as prior_year_iso_week_end_date, try_cast(null as NUMBER(38,0)) as iso_week_of_year, try_cast(null as NUMBER(38,0)) as prior_year_week_of_year, try_cast(null as NUMBER(38,0)) as prior_year_iso_week_of_year, try_cast(null as NUMBER(38,0)) as month_of_year, try_cast(null as character varying(16777216)) as month_name, try_cast(null as character varying(16777216)) as month_name_short, try_cast(null as DATE) as month_start_date, try_cast(null as DATE) as month_end_date, try_cast(null as DATE) as prior_year_month_start_date, try_cast(null as DATE) as prior_year_month_end_date, try_cast(null as NUMBER(38,0)) as quarter_of_year, try_cast(null as DATE) as quarter_start_date, try_cast(null as DATE) as quarter_end_date, try_cast(null as NUMBER(38,0)) as year_number, try_cast(null as DATE) as year_start_date, try_cast(null as DATE) as year_end_date
),  __dbt__cte__stg_treasury_revenue_collections as (

-- Fixture for stg_treasury_revenue_collections
select 
    
        try_cast('2020-11-20' as DATE)
     as collection_date, try_cast(null as character varying(16777216)) as channel_type, 
    
        try_cast('\'IRS Tax\'' as character varying(16777216))
     as tax_category, 
    
        try_cast('1000' as NUMBER(19,0))
     as dollars_collected_by_treasury
union all
select 
    
        try_cast('2020-11-20' as DATE)
     as collection_date, try_cast(null as character varying(16777216)) as channel_type, 
    
        try_cast('\'IRS Non Tax\'' as character varying(16777216))
     as tax_category, 
    
        try_cast('2000' as NUMBER(19,0))
     as dollars_collected_by_treasury
union all
select 
    
        try_cast('2020-11-21' as DATE)
     as collection_date, try_cast(null as character varying(16777216)) as channel_type, 
    
        try_cast('\'IRS Tax\'' as character varying(16777216))
     as tax_category, 
    
        try_cast('3000' as NUMBER(19,0))
     as dollars_collected_by_treasury
union all
select 
    
        try_cast('2020-11-21' as DATE)
     as collection_date, try_cast(null as character varying(16777216)) as channel_type, 
    
        try_cast('\'IRS Tax\'' as character varying(16777216))
     as tax_category, 
    
        try_cast('2000' as NUMBER(19,0))
     as dollars_collected_by_treasury
), pol as (
SELECT week_start_date, 
sum(number_of_shares_traded) number_of_shares_traded,
sum(number_of_trades) number_of_trades
from  __dbt__cte__stg_daily_bars a 
JOIN __dbt__cte__dim_date d on d.date_day=a.date
group by 1)

, rev as (
select week_start_date,
sum(dollars_collected_by_treasury) dollars_collected_by_treasury,
sum(case when tax_category = 'IRS Tax' then dollars_collected_by_treasury else 0 end) as irs_tax_dollars_collected_by_treasury, 
sum(case when tax_category = 'Non-Tax' then dollars_collected_by_treasury else 0 end) as non_irs_non_tax_dollars_collected_by_treasury,
sum(case when tax_category = 'IRS Non-Tax' then dollars_collected_by_treasury else 0 end) as irs_non_tax_dollars_collected_by_treasury
FROM __dbt__cte__stg_treasury_revenue_collections a 
JOIN __dbt__cte__dim_date d on d.date_day=a.collection_date
GROUP BY ALL)

select 
    a.week_start_date, 
    a.number_of_shares_traded, 
    a.number_of_trades,
    b.dollars_collected_by_treasury,
    b.irs_tax_dollars_collected_by_treasury

from pol a 
INNER JOIN rev b on b.week_start_date = a.week_start_date
  ) _dbt_internal_unit_test_actual
),
-- Build expected result
dbt_internal_unit_test_expected as (
  select
    week_start_date, number_of_shares_traded, number_of_trades, dollars_collected_by_treasury, irs_tax_dollars_collected_by_treasury, 'expected' as "actual_or_expected"
  from (
    select 
    
        try_cast('2020-11-15' as DATE)
     as week_start_date, 
    
        try_cast('800' as FLOAT)
     as number_of_shares_traded, 
    
        try_cast('8000' as NUMBER(38,0))
     as number_of_trades, 
    
        try_cast('6000' as NUMBER(31,0))
     as dollars_collected_by_treasury, 
    
        try_cast(null as NUMBER(31,0))
     as irs_tax_dollars_collected_by_treasury
  ) _dbt_internal_unit_test_expected
)
-- Union actual and expected results
select * from dbt_internal_unit_test_actual
union all
select * from dbt_internal_unit_test_expected
[0m15:26:21.878802 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 0.230 seconds
[0m15:26:21.885729 [debug] [Thread-1 (]: Applying DROP to: DATAEXPERT_STUDENT.andres.test_amount_sum__dbt_tmp
[0m15:26:21.892257 [debug] [Thread-1 (]: Using snowflake connection "unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum"
[0m15:26:21.892708 [debug] [Thread-1 (]: On unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum"} */
drop table if exists DATAEXPERT_STUDENT.andres.test_amount_sum__dbt_tmp cascade
[0m15:26:22.103933 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 0.211 seconds
[0m15:26:22.109821 [debug] [Thread-1 (]: On unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum: Close
[0m15:26:22.355380 [error] [Thread-1 (]: 4 of 5 FAIL 1 weekly_metrics::test_amount_sum .................................. [[31mFAIL 1[0m in 3.02s]
[0m15:26:22.356055 [debug] [Thread-1 (]: Finished running node unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum
[0m15:26:22.356560 [debug] [Thread-1 (]: Marking all children of 'unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum' to be skipped because of status 'fail'.  Reason: 

[32mactual[0m differs from [31mexpected[0m:

[0;1m@@[0m [33m,[0m[0;1mWEEK_START_DATE[0m[33m,[0m[0;1mNUMBER_OF_SHARES_TRADED[0m[33m,[0m[0;1mNUMBER_OF_TRADES[0m[33m,[0m[0;1mDOLLARS_COLLECTED_BY_TREASURY[0m[33m,[0m[0;1mIRS_TAX_DOLLARS_COLLECTED_BY_TREASURY[0m
[31;1m---[0m[33m,[0m"[31;1mdatetime.date(2020, 11, 15)[0m"     [33m,[0m[31;1mDecimal('800.0')[0m                  [33m,[0m[31;1m8000[0m            [33m,[0m[31;1m6000[0m                         [33m,[0m[31;1mnull[0m

.
[0m15:26:22.357189 [debug] [Thread-1 (]: Began running node model.dbt_capstone_andres.weekly_metrics
[0m15:26:22.357583 [info ] [Thread-1 (]: 5 of 5 SKIP relation andres.weekly_metrics ..................................... [[33mSKIP[0m]
[0m15:26:22.357999 [debug] [Thread-1 (]: Finished running node model.dbt_capstone_andres.weekly_metrics
[0m15:26:22.358367 [debug] [Thread-4 (]: Marking all children of 'model.dbt_capstone_andres.weekly_metrics' to be skipped because of status 'skipped'. 
[0m15:26:22.359394 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:26:22.359710 [debug] [MainThread]: Connection 'unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum' was properly closed.
[0m15:26:22.360080 [info ] [MainThread]: 
[0m15:26:22.360421 [info ] [MainThread]: Finished running 4 table models, 1 unit test in 0 hours 0 minutes and 12.21 seconds (12.21s).
[0m15:26:22.361816 [debug] [MainThread]: Command end result
[0m15:26:22.404489 [info ] [MainThread]: 
[0m15:26:22.404948 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successs, and 0 warnings:[0m
[0m15:26:22.405284 [info ] [MainThread]: 
[0m15:26:22.405673 [error] [MainThread]: [31mFailure in unit_test test_amount_sum (models/marts/_marts.yml)[0m
[0m15:26:22.406064 [error] [MainThread]:   

[32mactual[0m differs from [31mexpected[0m:

[0;1m@@[0m [33m,[0m[0;1mWEEK_START_DATE[0m[33m,[0m[0;1mNUMBER_OF_SHARES_TRADED[0m[33m,[0m[0;1mNUMBER_OF_TRADES[0m[33m,[0m[0;1mDOLLARS_COLLECTED_BY_TREASURY[0m[33m,[0m[0;1mIRS_TAX_DOLLARS_COLLECTED_BY_TREASURY[0m
[31;1m---[0m[33m,[0m"[31;1mdatetime.date(2020, 11, 15)[0m"     [33m,[0m[31;1mDecimal('800.0')[0m                  [33m,[0m[31;1m8000[0m            [33m,[0m[31;1m6000[0m                         [33m,[0m[31;1mnull[0m


[0m15:26:22.406401 [info ] [MainThread]: 
[0m15:26:22.406768 [info ] [MainThread]:   compiled code at target/compiled/dbt_capstone_andres/models/marts/_marts.yml/models/marts/test_amount_sum.sql
[0m15:26:22.407090 [info ] [MainThread]: 
[0m15:26:22.407427 [info ] [MainThread]: Done. PASS=3 WARN=0 ERROR=1 SKIP=1 TOTAL=5
[0m15:26:22.410547 [debug] [MainThread]: Resource report: {"command_name": "build", "command_wall_clock_time": 15.155233, "process_user_time": 3.976878, "process_kernel_time": 0.736211, "process_mem_max_rss": "200085504", "command_success": false, "process_in_blocks": "0", "process_out_blocks": "0"}
[0m15:26:22.411218 [debug] [MainThread]: Command `dbt build` failed at 15:26:22.411114 after 15.16 seconds
[0m15:26:22.411636 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b437fd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b3b8b50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b400b50>]}
[0m15:26:22.412015 [debug] [MainThread]: Flushing usage events
[0m15:26:22.590917 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:32:33.448285 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10bbbfb10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10bc2e4d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10bc2ead0>]}


============================== 15:32:33.454241 | fb5ab773-07de-40e5-8cb6-4c3f5acf86e9 ==============================
[0m15:32:33.454241 [info ] [MainThread]: Running with dbt=1.9.0-b3
[0m15:32:33.454965 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'profiles_dir': '.', 'log_path': 'logs', 'debug': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'invocation_command': 'dbt build', 'introspect': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m15:32:35.125369 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'fb5ab773-07de-40e5-8cb6-4c3f5acf86e9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c411350>]}
[0m15:32:35.181588 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'fb5ab773-07de-40e5-8cb6-4c3f5acf86e9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10bac06d0>]}
[0m15:32:35.183420 [info ] [MainThread]: Registered adapter: snowflake=1.8.4
[0m15:32:35.374713 [debug] [MainThread]: checksum: 5f4fde221ca8627174a0341efc73099aaec05b8546408ceed6f59b1ec0755115, vars: {}, profile: , target: , version: 1.9.0b3
[0m15:32:35.619312 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m15:32:35.620022 [debug] [MainThread]: Partial parsing: updated file: dbt_capstone_andres://models/marts/_marts.yml
[0m15:32:35.758520 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'fb5ab773-07de-40e5-8cb6-4c3f5acf86e9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1121c1550>]}
[0m15:32:35.980292 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'fb5ab773-07de-40e5-8cb6-4c3f5acf86e9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10aeeab90>]}
[0m15:32:35.980980 [info ] [MainThread]: Found 4 models, 2 sources, 862 macros, 1 unit test
[0m15:32:35.983274 [info ] [MainThread]: 
[0m15:32:35.983702 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m15:32:35.984080 [info ] [MainThread]: 
[0m15:32:35.984646 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m15:32:35.989514 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_DATAEXPERT_STUDENT'
[0m15:32:36.003601 [debug] [ThreadPool]: Using snowflake connection "list_DATAEXPERT_STUDENT"
[0m15:32:36.004068 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "connection_name": "list_DATAEXPERT_STUDENT"} */
show terse schemas in database DATAEXPERT_STUDENT
    limit 10000
[0m15:32:36.004429 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:32:36.928944 [debug] [ThreadPool]: SQL status: SUCCESS 73 in 0.924 seconds
[0m15:32:36.931444 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT: Close
[0m15:32:37.162322 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_DATAEXPERT_STUDENT, now list_DATAEXPERT_STUDENT_andres)
[0m15:32:37.173715 [debug] [ThreadPool]: Using snowflake connection "list_DATAEXPERT_STUDENT_andres"
[0m15:32:37.174231 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "connection_name": "list_DATAEXPERT_STUDENT_andres"} */
show objects in DATAEXPERT_STUDENT.andres limit 10000
[0m15:32:37.174654 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:32:37.901937 [debug] [ThreadPool]: SQL status: SUCCESS 25 in 0.727 seconds
[0m15:32:37.905806 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: Close
[0m15:32:38.143355 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'fb5ab773-07de-40e5-8cb6-4c3f5acf86e9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109bb11d0>]}
[0m15:32:38.146360 [debug] [Thread-1 (]: Began running node model.dbt_capstone_andres.dim_date
[0m15:32:38.146952 [info ] [Thread-1 (]: 1 of 5 START sql table model andres.dim_date ................................... [RUN]
[0m15:32:38.147531 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_DATAEXPERT_STUDENT_andres, now model.dbt_capstone_andres.dim_date)
[0m15:32:38.147966 [debug] [Thread-1 (]: Began compiling node model.dbt_capstone_andres.dim_date
[0m15:32:38.189037 [debug] [Thread-1 (]: Using snowflake connection "model.dbt_capstone_andres.dim_date"
[0m15:32:38.189694 [debug] [Thread-1 (]: On model.dbt_capstone_andres.dim_date: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "model.dbt_capstone_andres.dim_date"} */
select datediff(
        day,
        cast('1995-01-01' as timestamp),
        cast('2040-12-31' as timestamp)
        )
[0m15:32:38.190210 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:32:38.837411 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 0.647 seconds
[0m15:32:38.908866 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_capstone_andres.dim_date"
[0m15:32:38.911136 [debug] [Thread-1 (]: Began executing node model.dbt_capstone_andres.dim_date
[0m15:32:38.937822 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_capstone_andres.dim_date"
[0m15:32:38.944488 [debug] [Thread-1 (]: Using snowflake connection "model.dbt_capstone_andres.dim_date"
[0m15:32:38.945136 [debug] [Thread-1 (]: On model.dbt_capstone_andres.dim_date: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "model.dbt_capstone_andres.dim_date"} */
create or replace transient table DATAEXPERT_STUDENT.andres.dim_date
         as
        (

    
with base_dates as (
    
    with date_spine as
(

    





with rawdata as (

    

    

    with p as (
        select 0 as generated_number union all select 1
    ), unioned as (

    select

    
    p0.generated_number * power(2, 0)
     + 
    
    p1.generated_number * power(2, 1)
     + 
    
    p2.generated_number * power(2, 2)
     + 
    
    p3.generated_number * power(2, 3)
     + 
    
    p4.generated_number * power(2, 4)
     + 
    
    p5.generated_number * power(2, 5)
     + 
    
    p6.generated_number * power(2, 6)
     + 
    
    p7.generated_number * power(2, 7)
     + 
    
    p8.generated_number * power(2, 8)
     + 
    
    p9.generated_number * power(2, 9)
     + 
    
    p10.generated_number * power(2, 10)
     + 
    
    p11.generated_number * power(2, 11)
     + 
    
    p12.generated_number * power(2, 12)
     + 
    
    p13.generated_number * power(2, 13)
     + 
    
    p14.generated_number * power(2, 14)
    
    
    + 1
    as generated_number

    from

    
    p as p0
     cross join 
    
    p as p1
     cross join 
    
    p as p2
     cross join 
    
    p as p3
     cross join 
    
    p as p4
     cross join 
    
    p as p5
     cross join 
    
    p as p6
     cross join 
    
    p as p7
     cross join 
    
    p as p8
     cross join 
    
    p as p9
     cross join 
    
    p as p10
     cross join 
    
    p as p11
     cross join 
    
    p as p12
     cross join 
    
    p as p13
     cross join 
    
    p as p14
    
    

    )

    select *
    from unioned
    where generated_number <= 16801
    order by generated_number



),

all_periods as (

    select (
        

    dateadd(
        day,
        (row_number() over (order by 1) - 1),
        cast('1995-01-01' as timestamp)
        )


    ) as date_day
    from rawdata

),

filtered as (

    select *
    from all_periods
    where date_day <= cast('2040-12-31' as timestamp)

)

select * from filtered



)
select
    cast(d.date_day as timestamp) as date_day
from
    date_spine d


),
dates_with_prior_year_dates as (

    select
        cast(d.date_day as date) as date_day,
        cast(

    dateadd(
        year,
        -1,
        d.date_day
        )

 as date) as prior_year_date_day,
        cast(

    dateadd(
        day,
        -364,
        d.date_day
        )

 as date) as prior_year_over_year_date_day
    from
    	base_dates d

)
select
    d.date_day,
    cast(

    dateadd(
        day,
        -1,
        d.date_day
        )

 as date) as prior_date_day,
    cast(

    dateadd(
        day,
        1,
        d.date_day
        )

 as date) as next_date_day,
    d.prior_year_date_day as prior_year_date_day,
    d.prior_year_over_year_date_day,
    case
            when date_part('dayofweek', d.date_day) = 7 then 1
            else date_part('dayofweek', d.date_day) + 1
        end as day_of_week,
    date_part('dayofweekiso', d.date_day) as day_of_week_iso,
    -- long version not implemented on Snowflake so we're doing it manually :/
    case dayname(d.date_day)
        when 'Mon' then 'Monday'
        when 'Tue' then 'Tuesday'
        when 'Wed' then 'Wednesday'
        when 'Thu' then 'Thursday'
        when 'Fri' then 'Friday'
        when 'Sat' then 'Saturday'
        when 'Sun' then 'Sunday'
    end as day_of_week_name,
    dayname(d.date_day) as day_of_week_name_short,
    date_part('day', d.date_day) as day_of_month,
    date_part('dayofyear', d.date_day) as day_of_year,

    
    
    cast(

    dateadd(
        day,
        -1 * (case
            when date_part('dayofweek', d.date_day) = 7 then 1
            else date_part('dayofweek', d.date_day) + 1
        end - 1),
        d.date_day
        )

 as date) as week_start_date,
    cast(

    dateadd(
        day,
        6,
        
    
    cast(

    dateadd(
        day,
        -1 * (case
            when date_part('dayofweek', d.date_day) = 7 then 1
            else date_part('dayofweek', d.date_day) + 1
        end - 1),
        d.date_day
        )

 as date)
        )

 as date) as week_end_date,
    
    
    cast(

    dateadd(
        day,
        -1 * (case
            when date_part('dayofweek', d.prior_year_over_year_date_day) = 7 then 1
            else date_part('dayofweek', d.prior_year_over_year_date_day) + 1
        end - 1),
        d.prior_year_over_year_date_day
        )

 as date) as prior_year_week_start_date,
    cast(

    dateadd(
        day,
        6,
        
    
    cast(

    dateadd(
        day,
        -1 * (case
            when date_part('dayofweek', d.prior_year_over_year_date_day) = 7 then 1
            else date_part('dayofweek', d.prior_year_over_year_date_day) + 1
        end - 1),
        d.prior_year_over_year_date_day
        )

 as date)
        )

 as date) as prior_year_week_end_date,
    cast(date_part('week', d.date_day) as integer) as week_of_year,

    cast(date_trunc('week', d.date_day) as date) as iso_week_start_date,
    cast(

    dateadd(
        day,
        6,
        cast(date_trunc('week', d.date_day) as date)
        )

 as date) as iso_week_end_date,
    cast(date_trunc('week', d.prior_year_over_year_date_day) as date) as prior_year_iso_week_start_date,
    cast(

    dateadd(
        day,
        6,
        cast(date_trunc('week', d.prior_year_over_year_date_day) as date)
        )

 as date) as prior_year_iso_week_end_date,
    cast(date_part('weekiso', d.date_day) as integer) as iso_week_of_year,

    cast(date_part('week', d.prior_year_over_year_date_day) as integer) as prior_year_week_of_year,
    cast(date_part('weekiso', d.prior_year_over_year_date_day) as integer) as prior_year_iso_week_of_year,

    cast(date_part('month', d.date_day) as integer) as month_of_year,
    to_char(d.date_day, 'MMMM')  as month_name,
    to_char(d.date_day, 'MON')  as month_name_short,

    cast(date_trunc('month', d.date_day) as date) as month_start_date,
    cast(cast(
        

    dateadd(
        day,
        -1,
        

    dateadd(
        month,
        1,
        date_trunc('month', d.date_day)
        )


        )


        as date) as date) as month_end_date,

    cast(date_trunc('month', d.prior_year_date_day) as date) as prior_year_month_start_date,
    cast(cast(
        

    dateadd(
        day,
        -1,
        

    dateadd(
        month,
        1,
        date_trunc('month', d.prior_year_date_day)
        )


        )


        as date) as date) as prior_year_month_end_date,

    cast(date_part('quarter', d.date_day) as integer) as quarter_of_year,
    cast(date_trunc('quarter', d.date_day) as date) as quarter_start_date,
    cast(cast(
        

    dateadd(
        day,
        -1,
        

    dateadd(
        quarter,
        1,
        date_trunc('quarter', d.date_day)
        )


        )


        as date) as date) as quarter_end_date,

    cast(date_part('year', d.date_day) as integer) as year_number,
    cast(date_trunc('year', d.date_day) as date) as year_start_date,
    cast(cast(
        

    dateadd(
        day,
        -1,
        

    dateadd(
        year,
        1,
        date_trunc('year', d.date_day)
        )


        )


        as date) as date) as year_end_date
from
    dates_with_prior_year_dates d
order by 1


        );
[0m15:32:40.422637 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 1.477 seconds
[0m15:32:40.443167 [debug] [Thread-1 (]: On model.dbt_capstone_andres.dim_date: Close
[0m15:32:40.665496 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fb5ab773-07de-40e5-8cb6-4c3f5acf86e9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11620e790>]}
[0m15:32:40.666239 [info ] [Thread-1 (]: 1 of 5 OK created sql table model andres.dim_date .............................. [[32mSUCCESS 1[0m in 2.52s]
[0m15:32:40.666888 [debug] [Thread-1 (]: Finished running node model.dbt_capstone_andres.dim_date
[0m15:32:40.667361 [debug] [Thread-1 (]: Began running node model.dbt_capstone_andres.stg_daily_bars
[0m15:32:40.667938 [info ] [Thread-1 (]: 2 of 5 START sql table model andres.stg_daily_bars ............................. [RUN]
[0m15:32:40.668443 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_capstone_andres.dim_date, now model.dbt_capstone_andres.stg_daily_bars)
[0m15:32:40.668851 [debug] [Thread-1 (]: Began compiling node model.dbt_capstone_andres.stg_daily_bars
[0m15:32:40.672477 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_capstone_andres.stg_daily_bars"
[0m15:32:40.673253 [debug] [Thread-1 (]: Began executing node model.dbt_capstone_andres.stg_daily_bars
[0m15:32:40.677241 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_capstone_andres.stg_daily_bars"
[0m15:32:40.678824 [debug] [Thread-1 (]: Using snowflake connection "model.dbt_capstone_andres.stg_daily_bars"
[0m15:32:40.679293 [debug] [Thread-1 (]: On model.dbt_capstone_andres.stg_daily_bars: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "model.dbt_capstone_andres.stg_daily_bars"} */
create or replace transient table DATAEXPERT_STUDENT.andres.stg_daily_bars
         as
        (with source as (

    select * from dataexpert_student.andres.daily_bars

),

renamed as (

    select
        TO_DATE(TO_TIMESTAMP(timestamp / 1000)) as date,
        ticker as ticker,
        v  as number_of_shares_traded,
        n  as number_of_trades,
        round(vw,2) as volume_weighted_avg_price,
        o  as open_price,
        c  as close_price,
        h  as high_price,
        l  as low_price
    from source

)

select * from renamed
        );
[0m15:32:40.679730 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:32:43.036249 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 2.356 seconds
[0m15:32:43.039034 [debug] [Thread-1 (]: On model.dbt_capstone_andres.stg_daily_bars: Close
[0m15:32:43.268616 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fb5ab773-07de-40e5-8cb6-4c3f5acf86e9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11642c990>]}
[0m15:32:43.269670 [info ] [Thread-1 (]: 2 of 5 OK created sql table model andres.stg_daily_bars ........................ [[32mSUCCESS 1[0m in 2.60s]
[0m15:32:43.270568 [debug] [Thread-1 (]: Finished running node model.dbt_capstone_andres.stg_daily_bars
[0m15:32:43.271212 [debug] [Thread-1 (]: Began running node model.dbt_capstone_andres.stg_treasury_revenue_collections
[0m15:32:43.271916 [info ] [Thread-1 (]: 3 of 5 START sql table model andres.stg_treasury_revenue_collections ........... [RUN]
[0m15:32:43.272534 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_capstone_andres.stg_daily_bars, now model.dbt_capstone_andres.stg_treasury_revenue_collections)
[0m15:32:43.273059 [debug] [Thread-1 (]: Began compiling node model.dbt_capstone_andres.stg_treasury_revenue_collections
[0m15:32:43.277404 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_capstone_andres.stg_treasury_revenue_collections"
[0m15:32:43.278263 [debug] [Thread-1 (]: Began executing node model.dbt_capstone_andres.stg_treasury_revenue_collections
[0m15:32:43.282954 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_capstone_andres.stg_treasury_revenue_collections"
[0m15:32:43.284702 [debug] [Thread-1 (]: Using snowflake connection "model.dbt_capstone_andres.stg_treasury_revenue_collections"
[0m15:32:43.285259 [debug] [Thread-1 (]: On model.dbt_capstone_andres.stg_treasury_revenue_collections: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "model.dbt_capstone_andres.stg_treasury_revenue_collections"} */
create or replace transient table DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections
         as
        (with source as (

    select * from dataexpert_student.andres.fiscal_data

),

renamed as (

    select
        record_date as collection_date,
        channel_type_desc as channel_type,
        tax_category_desc as tax_category,
        round(net_collections_amt,0) as dollars_collected_by_treasury

    from source

)

select * from renamed
        );
[0m15:32:43.285779 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:32:44.624883 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 1.339 seconds
[0m15:32:44.627079 [debug] [Thread-1 (]: On model.dbt_capstone_andres.stg_treasury_revenue_collections: Close
[0m15:32:44.851359 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fb5ab773-07de-40e5-8cb6-4c3f5acf86e9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1120bf490>]}
[0m15:32:44.852402 [info ] [Thread-1 (]: 3 of 5 OK created sql table model andres.stg_treasury_revenue_collections ...... [[32mSUCCESS 1[0m in 1.58s]
[0m15:32:44.853322 [debug] [Thread-1 (]: Finished running node model.dbt_capstone_andres.stg_treasury_revenue_collections
[0m15:32:44.854343 [debug] [Thread-1 (]: Began running node unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum
[0m15:32:44.854914 [info ] [Thread-1 (]: 4 of 5 START unit_test weekly_metrics::test_amount_sum ......................... [RUN]
[0m15:32:44.855500 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_capstone_andres.stg_treasury_revenue_collections, now unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum)
[0m15:32:44.855981 [debug] [Thread-1 (]: Began compiling node unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum
[0m15:32:44.856459 [debug] [Thread-1 (]: Began executing node unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum
[0m15:32:44.903508 [debug] [Thread-1 (]: Using snowflake connection "unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum"
[0m15:32:44.903997 [debug] [Thread-1 (]: On unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum"} */
describe table DATAEXPERT_STUDENT.andres.stg_daily_bars
[0m15:32:44.904391 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:32:45.516547 [debug] [Thread-1 (]: SQL status: SUCCESS 9 in 0.612 seconds
[0m15:32:45.550025 [debug] [Thread-1 (]: Writing injected SQL for node "unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum"
[0m15:32:45.557916 [debug] [Thread-1 (]: Using snowflake connection "unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum"
[0m15:32:45.558405 [debug] [Thread-1 (]: On unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum"} */
describe table DATAEXPERT_STUDENT.andres.dim_date
[0m15:32:45.706091 [debug] [Thread-1 (]: SQL status: SUCCESS 36 in 0.147 seconds
[0m15:32:45.719430 [debug] [Thread-1 (]: Writing injected SQL for node "unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum"
[0m15:32:45.727313 [debug] [Thread-1 (]: Using snowflake connection "unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum"
[0m15:32:45.727731 [debug] [Thread-1 (]: On unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum"} */
describe table DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections
[0m15:32:45.908109 [debug] [Thread-1 (]: SQL status: SUCCESS 4 in 0.180 seconds
[0m15:32:45.917653 [debug] [Thread-1 (]: Writing injected SQL for node "unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum"
[0m15:32:45.926464 [debug] [Thread-1 (]: Writing injected SQL for node "unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum"
[0m15:32:45.954136 [debug] [Thread-1 (]: Using snowflake connection "unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum"
[0m15:32:45.954891 [debug] [Thread-1 (]: On unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum"} */
create or replace temporary table DATAEXPERT_STUDENT.andres.test_amount_sum__dbt_tmp
         as
        (select * from (
        with  __dbt__cte__stg_daily_bars as (

-- Fixture for stg_daily_bars
select 
    
        try_cast('2020-11-20' as DATE)
     as date, 
    
        try_cast('\'L\'' as character varying(16777216))
     as ticker, 
    
        try_cast('100' as FLOAT)
     as number_of_shares_traded, 
    
        try_cast('10' as NUMBER(38,0))
     as number_of_trades, try_cast(null as FLOAT) as volume_weighted_avg_price, try_cast(null as FLOAT) as open_price, try_cast(null as FLOAT) as close_price, try_cast(null as FLOAT) as high_price, try_cast(null as FLOAT) as low_price
union all
select 
    
        try_cast('2020-11-20' as DATE)
     as date, 
    
        try_cast('\'S\'' as character varying(16777216))
     as ticker, 
    
        try_cast('200' as FLOAT)
     as number_of_shares_traded, 
    
        try_cast('20' as NUMBER(38,0))
     as number_of_trades, try_cast(null as FLOAT) as volume_weighted_avg_price, try_cast(null as FLOAT) as open_price, try_cast(null as FLOAT) as close_price, try_cast(null as FLOAT) as high_price, try_cast(null as FLOAT) as low_price
union all
select 
    
        try_cast('2020-11-21' as DATE)
     as date, 
    
        try_cast('\'L\'' as character varying(16777216))
     as ticker, 
    
        try_cast('300' as FLOAT)
     as number_of_shares_traded, 
    
        try_cast('40' as NUMBER(38,0))
     as number_of_trades, try_cast(null as FLOAT) as volume_weighted_avg_price, try_cast(null as FLOAT) as open_price, try_cast(null as FLOAT) as close_price, try_cast(null as FLOAT) as high_price, try_cast(null as FLOAT) as low_price
union all
select 
    
        try_cast('2020-11-21' as DATE)
     as date, 
    
        try_cast('\'S\'' as character varying(16777216))
     as ticker, 
    
        try_cast('200' as FLOAT)
     as number_of_shares_traded, 
    
        try_cast('50' as NUMBER(38,0))
     as number_of_trades, try_cast(null as FLOAT) as volume_weighted_avg_price, try_cast(null as FLOAT) as open_price, try_cast(null as FLOAT) as close_price, try_cast(null as FLOAT) as high_price, try_cast(null as FLOAT) as low_price
),  __dbt__cte__dim_date as (

-- Fixture for dim_date
select 
    
        try_cast('2020-11-20' as DATE)
     as date_day, try_cast(null as DATE) as prior_date_day, try_cast(null as DATE) as next_date_day, try_cast(null as DATE) as prior_year_date_day, try_cast(null as DATE) as prior_year_over_year_date_day, try_cast(null as NUMBER(3,0)) as day_of_week, try_cast(null as NUMBER(2,0)) as day_of_week_iso, try_cast(null as character varying(16777216)) as day_of_week_name, try_cast(null as character varying(3)) as day_of_week_name_short, try_cast(null as NUMBER(2,0)) as day_of_month, try_cast(null as NUMBER(4,0)) as day_of_year, try_cast(null as DATE) as week_start_date, try_cast(null as DATE) as week_end_date, try_cast(null as DATE) as prior_year_week_start_date, try_cast(null as DATE) as prior_year_week_end_date, try_cast(null as NUMBER(38,0)) as week_of_year, try_cast(null as DATE) as iso_week_start_date, try_cast(null as DATE) as iso_week_end_date, try_cast(null as DATE) as prior_year_iso_week_start_date, try_cast(null as DATE) as prior_year_iso_week_end_date, try_cast(null as NUMBER(38,0)) as iso_week_of_year, try_cast(null as NUMBER(38,0)) as prior_year_week_of_year, try_cast(null as NUMBER(38,0)) as prior_year_iso_week_of_year, try_cast(null as NUMBER(38,0)) as month_of_year, try_cast(null as character varying(16777216)) as month_name, try_cast(null as character varying(16777216)) as month_name_short, try_cast(null as DATE) as month_start_date, try_cast(null as DATE) as month_end_date, try_cast(null as DATE) as prior_year_month_start_date, try_cast(null as DATE) as prior_year_month_end_date, try_cast(null as NUMBER(38,0)) as quarter_of_year, try_cast(null as DATE) as quarter_start_date, try_cast(null as DATE) as quarter_end_date, try_cast(null as NUMBER(38,0)) as year_number, try_cast(null as DATE) as year_start_date, try_cast(null as DATE) as year_end_date
union all
select 
    
        try_cast('2020-11-21' as DATE)
     as date_day, try_cast(null as DATE) as prior_date_day, try_cast(null as DATE) as next_date_day, try_cast(null as DATE) as prior_year_date_day, try_cast(null as DATE) as prior_year_over_year_date_day, try_cast(null as NUMBER(3,0)) as day_of_week, try_cast(null as NUMBER(2,0)) as day_of_week_iso, try_cast(null as character varying(16777216)) as day_of_week_name, try_cast(null as character varying(3)) as day_of_week_name_short, try_cast(null as NUMBER(2,0)) as day_of_month, try_cast(null as NUMBER(4,0)) as day_of_year, try_cast(null as DATE) as week_start_date, try_cast(null as DATE) as week_end_date, try_cast(null as DATE) as prior_year_week_start_date, try_cast(null as DATE) as prior_year_week_end_date, try_cast(null as NUMBER(38,0)) as week_of_year, try_cast(null as DATE) as iso_week_start_date, try_cast(null as DATE) as iso_week_end_date, try_cast(null as DATE) as prior_year_iso_week_start_date, try_cast(null as DATE) as prior_year_iso_week_end_date, try_cast(null as NUMBER(38,0)) as iso_week_of_year, try_cast(null as NUMBER(38,0)) as prior_year_week_of_year, try_cast(null as NUMBER(38,0)) as prior_year_iso_week_of_year, try_cast(null as NUMBER(38,0)) as month_of_year, try_cast(null as character varying(16777216)) as month_name, try_cast(null as character varying(16777216)) as month_name_short, try_cast(null as DATE) as month_start_date, try_cast(null as DATE) as month_end_date, try_cast(null as DATE) as prior_year_month_start_date, try_cast(null as DATE) as prior_year_month_end_date, try_cast(null as NUMBER(38,0)) as quarter_of_year, try_cast(null as DATE) as quarter_start_date, try_cast(null as DATE) as quarter_end_date, try_cast(null as NUMBER(38,0)) as year_number, try_cast(null as DATE) as year_start_date, try_cast(null as DATE) as year_end_date
),  __dbt__cte__stg_treasury_revenue_collections as (

-- Fixture for stg_treasury_revenue_collections
select 
    
        try_cast('2020-11-20' as DATE)
     as collection_date, try_cast(null as character varying(16777216)) as channel_type, 
    
        try_cast('\'IRS Tax\'' as character varying(16777216))
     as tax_category, 
    
        try_cast('1000' as NUMBER(19,0))
     as dollars_collected_by_treasury
union all
select 
    
        try_cast('2020-11-20' as DATE)
     as collection_date, try_cast(null as character varying(16777216)) as channel_type, 
    
        try_cast('\'IRS Non Tax\'' as character varying(16777216))
     as tax_category, 
    
        try_cast('2000' as NUMBER(19,0))
     as dollars_collected_by_treasury
union all
select 
    
        try_cast('2020-11-21' as DATE)
     as collection_date, try_cast(null as character varying(16777216)) as channel_type, 
    
        try_cast('\'IRS Tax\'' as character varying(16777216))
     as tax_category, 
    
        try_cast('3000' as NUMBER(19,0))
     as dollars_collected_by_treasury
union all
select 
    
        try_cast('2020-11-21' as DATE)
     as collection_date, try_cast(null as character varying(16777216)) as channel_type, 
    
        try_cast('\'IRS Tax\'' as character varying(16777216))
     as tax_category, 
    
        try_cast('2000' as NUMBER(19,0))
     as dollars_collected_by_treasury
), pol as (
SELECT week_start_date, 
sum(number_of_shares_traded) number_of_shares_traded,
sum(number_of_trades) number_of_trades
from  __dbt__cte__stg_daily_bars a 
JOIN __dbt__cte__dim_date d on d.date_day=a.date
group by 1)

, rev as (
select week_start_date,
sum(dollars_collected_by_treasury) dollars_collected_by_treasury,
sum(case when tax_category = 'IRS Tax' then dollars_collected_by_treasury else 0 end) as irs_tax_dollars_collected_by_treasury, 
sum(case when tax_category = 'Non-Tax' then dollars_collected_by_treasury else 0 end) as non_irs_non_tax_dollars_collected_by_treasury,
sum(case when tax_category = 'IRS Non-Tax' then dollars_collected_by_treasury else 0 end) as irs_non_tax_dollars_collected_by_treasury
FROM __dbt__cte__stg_treasury_revenue_collections a 
JOIN __dbt__cte__dim_date d on d.date_day=a.collection_date
GROUP BY ALL)

select 
    a.week_start_date, 
    a.number_of_shares_traded, 
    a.number_of_trades,
    b.dollars_collected_by_treasury,
    b.irs_tax_dollars_collected_by_treasury

from pol a 
INNER JOIN rev b on b.week_start_date = a.week_start_date
    ) as __dbt_sbq
    where false
    limit 0

        );
[0m15:32:46.690929 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 0.735 seconds
[0m15:32:46.696318 [debug] [Thread-1 (]: Using snowflake connection "unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum"
[0m15:32:46.696824 [debug] [Thread-1 (]: On unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum"} */
describe table DATAEXPERT_STUDENT.andres.test_amount_sum__dbt_tmp
[0m15:32:46.845886 [debug] [Thread-1 (]: SQL status: SUCCESS 5 in 0.148 seconds
[0m15:32:46.860365 [debug] [Thread-1 (]: Writing runtime sql for node "unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum"
[0m15:32:46.872922 [debug] [Thread-1 (]: Using snowflake connection "unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum"
[0m15:32:46.873840 [debug] [Thread-1 (]: On unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum"} */
-- Build actual result given inputs
with dbt_internal_unit_test_actual as (
  select
    week_start_date,number_of_shares_traded,number_of_trades,dollars_collected_by_treasury,irs_tax_dollars_collected_by_treasury, 'actual' as "actual_or_expected"
  from (
    with  __dbt__cte__stg_daily_bars as (

-- Fixture for stg_daily_bars
select 
    
        try_cast('2020-11-20' as DATE)
     as date, 
    
        try_cast('\'L\'' as character varying(16777216))
     as ticker, 
    
        try_cast('100' as FLOAT)
     as number_of_shares_traded, 
    
        try_cast('10' as NUMBER(38,0))
     as number_of_trades, try_cast(null as FLOAT) as volume_weighted_avg_price, try_cast(null as FLOAT) as open_price, try_cast(null as FLOAT) as close_price, try_cast(null as FLOAT) as high_price, try_cast(null as FLOAT) as low_price
union all
select 
    
        try_cast('2020-11-20' as DATE)
     as date, 
    
        try_cast('\'S\'' as character varying(16777216))
     as ticker, 
    
        try_cast('200' as FLOAT)
     as number_of_shares_traded, 
    
        try_cast('20' as NUMBER(38,0))
     as number_of_trades, try_cast(null as FLOAT) as volume_weighted_avg_price, try_cast(null as FLOAT) as open_price, try_cast(null as FLOAT) as close_price, try_cast(null as FLOAT) as high_price, try_cast(null as FLOAT) as low_price
union all
select 
    
        try_cast('2020-11-21' as DATE)
     as date, 
    
        try_cast('\'L\'' as character varying(16777216))
     as ticker, 
    
        try_cast('300' as FLOAT)
     as number_of_shares_traded, 
    
        try_cast('40' as NUMBER(38,0))
     as number_of_trades, try_cast(null as FLOAT) as volume_weighted_avg_price, try_cast(null as FLOAT) as open_price, try_cast(null as FLOAT) as close_price, try_cast(null as FLOAT) as high_price, try_cast(null as FLOAT) as low_price
union all
select 
    
        try_cast('2020-11-21' as DATE)
     as date, 
    
        try_cast('\'S\'' as character varying(16777216))
     as ticker, 
    
        try_cast('200' as FLOAT)
     as number_of_shares_traded, 
    
        try_cast('50' as NUMBER(38,0))
     as number_of_trades, try_cast(null as FLOAT) as volume_weighted_avg_price, try_cast(null as FLOAT) as open_price, try_cast(null as FLOAT) as close_price, try_cast(null as FLOAT) as high_price, try_cast(null as FLOAT) as low_price
),  __dbt__cte__dim_date as (

-- Fixture for dim_date
select 
    
        try_cast('2020-11-20' as DATE)
     as date_day, try_cast(null as DATE) as prior_date_day, try_cast(null as DATE) as next_date_day, try_cast(null as DATE) as prior_year_date_day, try_cast(null as DATE) as prior_year_over_year_date_day, try_cast(null as NUMBER(3,0)) as day_of_week, try_cast(null as NUMBER(2,0)) as day_of_week_iso, try_cast(null as character varying(16777216)) as day_of_week_name, try_cast(null as character varying(3)) as day_of_week_name_short, try_cast(null as NUMBER(2,0)) as day_of_month, try_cast(null as NUMBER(4,0)) as day_of_year, try_cast(null as DATE) as week_start_date, try_cast(null as DATE) as week_end_date, try_cast(null as DATE) as prior_year_week_start_date, try_cast(null as DATE) as prior_year_week_end_date, try_cast(null as NUMBER(38,0)) as week_of_year, try_cast(null as DATE) as iso_week_start_date, try_cast(null as DATE) as iso_week_end_date, try_cast(null as DATE) as prior_year_iso_week_start_date, try_cast(null as DATE) as prior_year_iso_week_end_date, try_cast(null as NUMBER(38,0)) as iso_week_of_year, try_cast(null as NUMBER(38,0)) as prior_year_week_of_year, try_cast(null as NUMBER(38,0)) as prior_year_iso_week_of_year, try_cast(null as NUMBER(38,0)) as month_of_year, try_cast(null as character varying(16777216)) as month_name, try_cast(null as character varying(16777216)) as month_name_short, try_cast(null as DATE) as month_start_date, try_cast(null as DATE) as month_end_date, try_cast(null as DATE) as prior_year_month_start_date, try_cast(null as DATE) as prior_year_month_end_date, try_cast(null as NUMBER(38,0)) as quarter_of_year, try_cast(null as DATE) as quarter_start_date, try_cast(null as DATE) as quarter_end_date, try_cast(null as NUMBER(38,0)) as year_number, try_cast(null as DATE) as year_start_date, try_cast(null as DATE) as year_end_date
union all
select 
    
        try_cast('2020-11-21' as DATE)
     as date_day, try_cast(null as DATE) as prior_date_day, try_cast(null as DATE) as next_date_day, try_cast(null as DATE) as prior_year_date_day, try_cast(null as DATE) as prior_year_over_year_date_day, try_cast(null as NUMBER(3,0)) as day_of_week, try_cast(null as NUMBER(2,0)) as day_of_week_iso, try_cast(null as character varying(16777216)) as day_of_week_name, try_cast(null as character varying(3)) as day_of_week_name_short, try_cast(null as NUMBER(2,0)) as day_of_month, try_cast(null as NUMBER(4,0)) as day_of_year, try_cast(null as DATE) as week_start_date, try_cast(null as DATE) as week_end_date, try_cast(null as DATE) as prior_year_week_start_date, try_cast(null as DATE) as prior_year_week_end_date, try_cast(null as NUMBER(38,0)) as week_of_year, try_cast(null as DATE) as iso_week_start_date, try_cast(null as DATE) as iso_week_end_date, try_cast(null as DATE) as prior_year_iso_week_start_date, try_cast(null as DATE) as prior_year_iso_week_end_date, try_cast(null as NUMBER(38,0)) as iso_week_of_year, try_cast(null as NUMBER(38,0)) as prior_year_week_of_year, try_cast(null as NUMBER(38,0)) as prior_year_iso_week_of_year, try_cast(null as NUMBER(38,0)) as month_of_year, try_cast(null as character varying(16777216)) as month_name, try_cast(null as character varying(16777216)) as month_name_short, try_cast(null as DATE) as month_start_date, try_cast(null as DATE) as month_end_date, try_cast(null as DATE) as prior_year_month_start_date, try_cast(null as DATE) as prior_year_month_end_date, try_cast(null as NUMBER(38,0)) as quarter_of_year, try_cast(null as DATE) as quarter_start_date, try_cast(null as DATE) as quarter_end_date, try_cast(null as NUMBER(38,0)) as year_number, try_cast(null as DATE) as year_start_date, try_cast(null as DATE) as year_end_date
),  __dbt__cte__stg_treasury_revenue_collections as (

-- Fixture for stg_treasury_revenue_collections
select 
    
        try_cast('2020-11-20' as DATE)
     as collection_date, try_cast(null as character varying(16777216)) as channel_type, 
    
        try_cast('\'IRS Tax\'' as character varying(16777216))
     as tax_category, 
    
        try_cast('1000' as NUMBER(19,0))
     as dollars_collected_by_treasury
union all
select 
    
        try_cast('2020-11-20' as DATE)
     as collection_date, try_cast(null as character varying(16777216)) as channel_type, 
    
        try_cast('\'IRS Non Tax\'' as character varying(16777216))
     as tax_category, 
    
        try_cast('2000' as NUMBER(19,0))
     as dollars_collected_by_treasury
union all
select 
    
        try_cast('2020-11-21' as DATE)
     as collection_date, try_cast(null as character varying(16777216)) as channel_type, 
    
        try_cast('\'IRS Tax\'' as character varying(16777216))
     as tax_category, 
    
        try_cast('3000' as NUMBER(19,0))
     as dollars_collected_by_treasury
union all
select 
    
        try_cast('2020-11-21' as DATE)
     as collection_date, try_cast(null as character varying(16777216)) as channel_type, 
    
        try_cast('\'IRS Tax\'' as character varying(16777216))
     as tax_category, 
    
        try_cast('2000' as NUMBER(19,0))
     as dollars_collected_by_treasury
), pol as (
SELECT week_start_date, 
sum(number_of_shares_traded) number_of_shares_traded,
sum(number_of_trades) number_of_trades
from  __dbt__cte__stg_daily_bars a 
JOIN __dbt__cte__dim_date d on d.date_day=a.date
group by 1)

, rev as (
select week_start_date,
sum(dollars_collected_by_treasury) dollars_collected_by_treasury,
sum(case when tax_category = 'IRS Tax' then dollars_collected_by_treasury else 0 end) as irs_tax_dollars_collected_by_treasury, 
sum(case when tax_category = 'Non-Tax' then dollars_collected_by_treasury else 0 end) as non_irs_non_tax_dollars_collected_by_treasury,
sum(case when tax_category = 'IRS Non-Tax' then dollars_collected_by_treasury else 0 end) as irs_non_tax_dollars_collected_by_treasury
FROM __dbt__cte__stg_treasury_revenue_collections a 
JOIN __dbt__cte__dim_date d on d.date_day=a.collection_date
GROUP BY ALL)

select 
    a.week_start_date, 
    a.number_of_shares_traded, 
    a.number_of_trades,
    b.dollars_collected_by_treasury,
    b.irs_tax_dollars_collected_by_treasury

from pol a 
INNER JOIN rev b on b.week_start_date = a.week_start_date
  ) _dbt_internal_unit_test_actual
),
-- Build expected result
dbt_internal_unit_test_expected as (
  select
    week_start_date, number_of_shares_traded, number_of_trades, dollars_collected_by_treasury, irs_tax_dollars_collected_by_treasury, 'expected' as "actual_or_expected"
  from (
    select 
    
        try_cast('2020-11-15' as DATE)
     as week_start_date, 
    
        try_cast('800' as FLOAT)
     as number_of_shares_traded, 
    
        try_cast('120' as NUMBER(38,0))
     as number_of_trades, 
    
        try_cast('8000' as NUMBER(31,0))
     as dollars_collected_by_treasury, 
    
        try_cast('6000' as NUMBER(31,0))
     as irs_tax_dollars_collected_by_treasury
  ) _dbt_internal_unit_test_expected
)
-- Union actual and expected results
select * from dbt_internal_unit_test_actual
union all
select * from dbt_internal_unit_test_expected
[0m15:32:47.076564 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 0.202 seconds
[0m15:32:47.089170 [debug] [Thread-1 (]: Applying DROP to: DATAEXPERT_STUDENT.andres.test_amount_sum__dbt_tmp
[0m15:32:47.097295 [debug] [Thread-1 (]: Using snowflake connection "unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum"
[0m15:32:47.097779 [debug] [Thread-1 (]: On unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum"} */
drop table if exists DATAEXPERT_STUDENT.andres.test_amount_sum__dbt_tmp cascade
[0m15:32:47.284363 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 0.186 seconds
[0m15:32:47.294384 [debug] [Thread-1 (]: On unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum: Close
[0m15:32:47.562123 [error] [Thread-1 (]: 4 of 5 FAIL 1 weekly_metrics::test_amount_sum .................................. [[31mFAIL 1[0m in 2.71s]
[0m15:32:47.563056 [debug] [Thread-1 (]: Finished running node unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum
[0m15:32:47.563766 [debug] [Thread-1 (]: Marking all children of 'unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum' to be skipped because of status 'fail'.  Reason: 

[32mactual[0m differs from [31mexpected[0m:

[0;1m@@[0m [33m,[0m[0;1mWEEK_START_DATE[0m[33m,[0m[0;1mNUMBER_OF_SHARES_TRADED[0m[33m,[0m[0;1mNUMBER_OF_TRADES[0m[33m,[0m[0;1mDOLLARS_COLLECTED_BY_TREASURY[0m[33m,[0m[0;1mIRS_TAX_DOLLARS_COLLECTED_BY_TREASURY[0m
[31;1m---[0m[33m,[0m"[31;1mdatetime.date(2020, 11, 15)[0m"     [33m,[0m[31;1mDecimal('800.0')[0m                  [33m,[0m[31;1m120[0m             [33m,[0m[31;1m8000[0m                         [33m,[0m[31;1m6000[0m

.
[0m15:32:47.564596 [debug] [Thread-1 (]: Began running node model.dbt_capstone_andres.weekly_metrics
[0m15:32:47.565134 [info ] [Thread-1 (]: 5 of 5 SKIP relation andres.weekly_metrics ..................................... [[33mSKIP[0m]
[0m15:32:47.565693 [debug] [Thread-1 (]: Finished running node model.dbt_capstone_andres.weekly_metrics
[0m15:32:47.566187 [debug] [Thread-4 (]: Marking all children of 'model.dbt_capstone_andres.weekly_metrics' to be skipped because of status 'skipped'. 
[0m15:32:47.567628 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:32:47.568046 [debug] [MainThread]: Connection 'unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum' was properly closed.
[0m15:32:47.568797 [info ] [MainThread]: 
[0m15:32:47.569300 [info ] [MainThread]: Finished running 4 table models, 1 unit test in 0 hours 0 minutes and 11.58 seconds (11.58s).
[0m15:32:47.570738 [debug] [MainThread]: Command end result
[0m15:32:47.633251 [info ] [MainThread]: 
[0m15:32:47.634400 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successs, and 0 warnings:[0m
[0m15:32:47.635069 [info ] [MainThread]: 
[0m15:32:47.635724 [error] [MainThread]: [31mFailure in unit_test test_amount_sum (models/marts/_marts.yml)[0m
[0m15:32:47.636675 [error] [MainThread]:   

[32mactual[0m differs from [31mexpected[0m:

[0;1m@@[0m [33m,[0m[0;1mWEEK_START_DATE[0m[33m,[0m[0;1mNUMBER_OF_SHARES_TRADED[0m[33m,[0m[0;1mNUMBER_OF_TRADES[0m[33m,[0m[0;1mDOLLARS_COLLECTED_BY_TREASURY[0m[33m,[0m[0;1mIRS_TAX_DOLLARS_COLLECTED_BY_TREASURY[0m
[31;1m---[0m[33m,[0m"[31;1mdatetime.date(2020, 11, 15)[0m"     [33m,[0m[31;1mDecimal('800.0')[0m                  [33m,[0m[31;1m120[0m             [33m,[0m[31;1m8000[0m                         [33m,[0m[31;1m6000[0m


[0m15:32:47.637553 [info ] [MainThread]: 
[0m15:32:47.638617 [info ] [MainThread]:   compiled code at target/compiled/dbt_capstone_andres/models/marts/_marts.yml/models/marts/test_amount_sum.sql
[0m15:32:47.639362 [info ] [MainThread]: 
[0m15:32:47.639989 [info ] [MainThread]: Done. PASS=3 WARN=0 ERROR=1 SKIP=1 TOTAL=5
[0m15:32:47.644791 [debug] [MainThread]: Resource report: {"command_name": "build", "command_wall_clock_time": 14.267639, "process_user_time": 3.793093, "process_kernel_time": 0.758563, "process_mem_max_rss": "196706304", "command_success": false, "process_in_blocks": "0", "process_out_blocks": "0"}
[0m15:32:47.646468 [debug] [MainThread]: Command `dbt build` failed at 15:32:47.646288 after 14.27 seconds
[0m15:32:47.647138 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10bc2e290>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10bc2ee10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107ec3510>]}
[0m15:32:47.647957 [debug] [MainThread]: Flushing usage events
[0m15:32:47.914648 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:34:32.008271 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110edfe90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110f4ea10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110f4f010>]}


============================== 15:34:32.014340 | c6ac6ea3-048f-4b21-a666-01a4991e1bd0 ==============================
[0m15:34:32.014340 [info ] [MainThread]: Running with dbt=1.9.0-b3
[0m15:34:32.015046 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'fail_fast': 'False', 'version_check': 'True', 'log_path': 'logs', 'warn_error': 'None', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'log_format': 'default', 'invocation_command': 'dbt build', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m15:34:33.390604 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'c6ac6ea3-048f-4b21-a666-01a4991e1bd0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110f0c310>]}
[0m15:34:33.447995 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'c6ac6ea3-048f-4b21-a666-01a4991e1bd0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1171c0550>]}
[0m15:34:33.449219 [info ] [MainThread]: Registered adapter: snowflake=1.8.4
[0m15:34:33.637937 [debug] [MainThread]: checksum: 5f4fde221ca8627174a0341efc73099aaec05b8546408ceed6f59b1ec0755115, vars: {}, profile: , target: , version: 1.9.0b3
[0m15:34:33.842547 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m15:34:33.843239 [debug] [MainThread]: Partial parsing: updated file: dbt_capstone_andres://models/marts/_marts.yml
[0m15:34:33.985149 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'c6ac6ea3-048f-4b21-a666-01a4991e1bd0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x117513090>]}
[0m15:34:34.211605 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'c6ac6ea3-048f-4b21-a666-01a4991e1bd0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11710b610>]}
[0m15:34:34.212213 [info ] [MainThread]: Found 4 models, 2 sources, 862 macros, 1 unit test
[0m15:34:34.214495 [info ] [MainThread]: 
[0m15:34:34.214927 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m15:34:34.215301 [info ] [MainThread]: 
[0m15:34:34.215870 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m15:34:34.221012 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_DATAEXPERT_STUDENT'
[0m15:34:34.235586 [debug] [ThreadPool]: Using snowflake connection "list_DATAEXPERT_STUDENT"
[0m15:34:34.236065 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "connection_name": "list_DATAEXPERT_STUDENT"} */
show terse schemas in database DATAEXPERT_STUDENT
    limit 10000
[0m15:34:34.236452 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:34:35.128438 [debug] [ThreadPool]: SQL status: SUCCESS 73 in 0.892 seconds
[0m15:34:35.132547 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT: Close
[0m15:34:35.344921 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_DATAEXPERT_STUDENT, now list_DATAEXPERT_STUDENT_andres)
[0m15:34:35.354685 [debug] [ThreadPool]: Using snowflake connection "list_DATAEXPERT_STUDENT_andres"
[0m15:34:35.355144 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "connection_name": "list_DATAEXPERT_STUDENT_andres"} */
show objects in DATAEXPERT_STUDENT.andres limit 10000
[0m15:34:35.355504 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:34:36.098910 [debug] [ThreadPool]: SQL status: SUCCESS 25 in 0.743 seconds
[0m15:34:36.102779 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: Close
[0m15:34:36.338560 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'c6ac6ea3-048f-4b21-a666-01a4991e1bd0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x116e096d0>]}
[0m15:34:36.342253 [debug] [Thread-1 (]: Began running node model.dbt_capstone_andres.dim_date
[0m15:34:36.342988 [info ] [Thread-1 (]: 1 of 5 START sql table model andres.dim_date ................................... [RUN]
[0m15:34:36.343612 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_DATAEXPERT_STUDENT_andres, now model.dbt_capstone_andres.dim_date)
[0m15:34:36.344106 [debug] [Thread-1 (]: Began compiling node model.dbt_capstone_andres.dim_date
[0m15:34:36.388712 [debug] [Thread-1 (]: Using snowflake connection "model.dbt_capstone_andres.dim_date"
[0m15:34:36.389347 [debug] [Thread-1 (]: On model.dbt_capstone_andres.dim_date: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "model.dbt_capstone_andres.dim_date"} */
select datediff(
        day,
        cast('1995-01-01' as timestamp),
        cast('2040-12-31' as timestamp)
        )
[0m15:34:36.389880 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:34:37.043579 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 0.654 seconds
[0m15:34:37.129783 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_capstone_andres.dim_date"
[0m15:34:37.130649 [debug] [Thread-1 (]: Began executing node model.dbt_capstone_andres.dim_date
[0m15:34:37.160273 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_capstone_andres.dim_date"
[0m15:34:37.167713 [debug] [Thread-1 (]: Using snowflake connection "model.dbt_capstone_andres.dim_date"
[0m15:34:37.168416 [debug] [Thread-1 (]: On model.dbt_capstone_andres.dim_date: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "model.dbt_capstone_andres.dim_date"} */
create or replace transient table DATAEXPERT_STUDENT.andres.dim_date
         as
        (

    
with base_dates as (
    
    with date_spine as
(

    





with rawdata as (

    

    

    with p as (
        select 0 as generated_number union all select 1
    ), unioned as (

    select

    
    p0.generated_number * power(2, 0)
     + 
    
    p1.generated_number * power(2, 1)
     + 
    
    p2.generated_number * power(2, 2)
     + 
    
    p3.generated_number * power(2, 3)
     + 
    
    p4.generated_number * power(2, 4)
     + 
    
    p5.generated_number * power(2, 5)
     + 
    
    p6.generated_number * power(2, 6)
     + 
    
    p7.generated_number * power(2, 7)
     + 
    
    p8.generated_number * power(2, 8)
     + 
    
    p9.generated_number * power(2, 9)
     + 
    
    p10.generated_number * power(2, 10)
     + 
    
    p11.generated_number * power(2, 11)
     + 
    
    p12.generated_number * power(2, 12)
     + 
    
    p13.generated_number * power(2, 13)
     + 
    
    p14.generated_number * power(2, 14)
    
    
    + 1
    as generated_number

    from

    
    p as p0
     cross join 
    
    p as p1
     cross join 
    
    p as p2
     cross join 
    
    p as p3
     cross join 
    
    p as p4
     cross join 
    
    p as p5
     cross join 
    
    p as p6
     cross join 
    
    p as p7
     cross join 
    
    p as p8
     cross join 
    
    p as p9
     cross join 
    
    p as p10
     cross join 
    
    p as p11
     cross join 
    
    p as p12
     cross join 
    
    p as p13
     cross join 
    
    p as p14
    
    

    )

    select *
    from unioned
    where generated_number <= 16801
    order by generated_number



),

all_periods as (

    select (
        

    dateadd(
        day,
        (row_number() over (order by 1) - 1),
        cast('1995-01-01' as timestamp)
        )


    ) as date_day
    from rawdata

),

filtered as (

    select *
    from all_periods
    where date_day <= cast('2040-12-31' as timestamp)

)

select * from filtered



)
select
    cast(d.date_day as timestamp) as date_day
from
    date_spine d


),
dates_with_prior_year_dates as (

    select
        cast(d.date_day as date) as date_day,
        cast(

    dateadd(
        year,
        -1,
        d.date_day
        )

 as date) as prior_year_date_day,
        cast(

    dateadd(
        day,
        -364,
        d.date_day
        )

 as date) as prior_year_over_year_date_day
    from
    	base_dates d

)
select
    d.date_day,
    cast(

    dateadd(
        day,
        -1,
        d.date_day
        )

 as date) as prior_date_day,
    cast(

    dateadd(
        day,
        1,
        d.date_day
        )

 as date) as next_date_day,
    d.prior_year_date_day as prior_year_date_day,
    d.prior_year_over_year_date_day,
    case
            when date_part('dayofweek', d.date_day) = 7 then 1
            else date_part('dayofweek', d.date_day) + 1
        end as day_of_week,
    date_part('dayofweekiso', d.date_day) as day_of_week_iso,
    -- long version not implemented on Snowflake so we're doing it manually :/
    case dayname(d.date_day)
        when 'Mon' then 'Monday'
        when 'Tue' then 'Tuesday'
        when 'Wed' then 'Wednesday'
        when 'Thu' then 'Thursday'
        when 'Fri' then 'Friday'
        when 'Sat' then 'Saturday'
        when 'Sun' then 'Sunday'
    end as day_of_week_name,
    dayname(d.date_day) as day_of_week_name_short,
    date_part('day', d.date_day) as day_of_month,
    date_part('dayofyear', d.date_day) as day_of_year,

    
    
    cast(

    dateadd(
        day,
        -1 * (case
            when date_part('dayofweek', d.date_day) = 7 then 1
            else date_part('dayofweek', d.date_day) + 1
        end - 1),
        d.date_day
        )

 as date) as week_start_date,
    cast(

    dateadd(
        day,
        6,
        
    
    cast(

    dateadd(
        day,
        -1 * (case
            when date_part('dayofweek', d.date_day) = 7 then 1
            else date_part('dayofweek', d.date_day) + 1
        end - 1),
        d.date_day
        )

 as date)
        )

 as date) as week_end_date,
    
    
    cast(

    dateadd(
        day,
        -1 * (case
            when date_part('dayofweek', d.prior_year_over_year_date_day) = 7 then 1
            else date_part('dayofweek', d.prior_year_over_year_date_day) + 1
        end - 1),
        d.prior_year_over_year_date_day
        )

 as date) as prior_year_week_start_date,
    cast(

    dateadd(
        day,
        6,
        
    
    cast(

    dateadd(
        day,
        -1 * (case
            when date_part('dayofweek', d.prior_year_over_year_date_day) = 7 then 1
            else date_part('dayofweek', d.prior_year_over_year_date_day) + 1
        end - 1),
        d.prior_year_over_year_date_day
        )

 as date)
        )

 as date) as prior_year_week_end_date,
    cast(date_part('week', d.date_day) as integer) as week_of_year,

    cast(date_trunc('week', d.date_day) as date) as iso_week_start_date,
    cast(

    dateadd(
        day,
        6,
        cast(date_trunc('week', d.date_day) as date)
        )

 as date) as iso_week_end_date,
    cast(date_trunc('week', d.prior_year_over_year_date_day) as date) as prior_year_iso_week_start_date,
    cast(

    dateadd(
        day,
        6,
        cast(date_trunc('week', d.prior_year_over_year_date_day) as date)
        )

 as date) as prior_year_iso_week_end_date,
    cast(date_part('weekiso', d.date_day) as integer) as iso_week_of_year,

    cast(date_part('week', d.prior_year_over_year_date_day) as integer) as prior_year_week_of_year,
    cast(date_part('weekiso', d.prior_year_over_year_date_day) as integer) as prior_year_iso_week_of_year,

    cast(date_part('month', d.date_day) as integer) as month_of_year,
    to_char(d.date_day, 'MMMM')  as month_name,
    to_char(d.date_day, 'MON')  as month_name_short,

    cast(date_trunc('month', d.date_day) as date) as month_start_date,
    cast(cast(
        

    dateadd(
        day,
        -1,
        

    dateadd(
        month,
        1,
        date_trunc('month', d.date_day)
        )


        )


        as date) as date) as month_end_date,

    cast(date_trunc('month', d.prior_year_date_day) as date) as prior_year_month_start_date,
    cast(cast(
        

    dateadd(
        day,
        -1,
        

    dateadd(
        month,
        1,
        date_trunc('month', d.prior_year_date_day)
        )


        )


        as date) as date) as prior_year_month_end_date,

    cast(date_part('quarter', d.date_day) as integer) as quarter_of_year,
    cast(date_trunc('quarter', d.date_day) as date) as quarter_start_date,
    cast(cast(
        

    dateadd(
        day,
        -1,
        

    dateadd(
        quarter,
        1,
        date_trunc('quarter', d.date_day)
        )


        )


        as date) as date) as quarter_end_date,

    cast(date_part('year', d.date_day) as integer) as year_number,
    cast(date_trunc('year', d.date_day) as date) as year_start_date,
    cast(cast(
        

    dateadd(
        day,
        -1,
        

    dateadd(
        year,
        1,
        date_trunc('year', d.date_day)
        )


        )


        as date) as date) as year_end_date
from
    dates_with_prior_year_dates d
order by 1


        );
[0m15:34:38.665502 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 1.496 seconds
[0m15:34:38.687154 [debug] [Thread-1 (]: On model.dbt_capstone_andres.dim_date: Close
[0m15:34:38.900647 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c6ac6ea3-048f-4b21-a666-01a4991e1bd0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11727d410>]}
[0m15:34:38.901460 [info ] [Thread-1 (]: 1 of 5 OK created sql table model andres.dim_date .............................. [[32mSUCCESS 1[0m in 2.56s]
[0m15:34:38.902229 [debug] [Thread-1 (]: Finished running node model.dbt_capstone_andres.dim_date
[0m15:34:38.902731 [debug] [Thread-1 (]: Began running node model.dbt_capstone_andres.stg_daily_bars
[0m15:34:38.903365 [info ] [Thread-1 (]: 2 of 5 START sql table model andres.stg_daily_bars ............................. [RUN]
[0m15:34:38.903934 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_capstone_andres.dim_date, now model.dbt_capstone_andres.stg_daily_bars)
[0m15:34:38.904365 [debug] [Thread-1 (]: Began compiling node model.dbt_capstone_andres.stg_daily_bars
[0m15:34:38.908640 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_capstone_andres.stg_daily_bars"
[0m15:34:38.909538 [debug] [Thread-1 (]: Began executing node model.dbt_capstone_andres.stg_daily_bars
[0m15:34:38.913639 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_capstone_andres.stg_daily_bars"
[0m15:34:38.915160 [debug] [Thread-1 (]: Using snowflake connection "model.dbt_capstone_andres.stg_daily_bars"
[0m15:34:38.915637 [debug] [Thread-1 (]: On model.dbt_capstone_andres.stg_daily_bars: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "model.dbt_capstone_andres.stg_daily_bars"} */
create or replace transient table DATAEXPERT_STUDENT.andres.stg_daily_bars
         as
        (with source as (

    select * from dataexpert_student.andres.daily_bars

),

renamed as (

    select
        TO_DATE(TO_TIMESTAMP(timestamp / 1000)) as date,
        ticker as ticker,
        v  as number_of_shares_traded,
        n  as number_of_trades,
        round(vw,2) as volume_weighted_avg_price,
        o  as open_price,
        c  as close_price,
        h  as high_price,
        l  as low_price
    from source

)

select * from renamed
        );
[0m15:34:38.916087 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:34:40.581735 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 1.666 seconds
[0m15:34:40.584749 [debug] [Thread-1 (]: On model.dbt_capstone_andres.stg_daily_bars: Close
[0m15:34:40.818558 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c6ac6ea3-048f-4b21-a666-01a4991e1bd0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11b731010>]}
[0m15:34:40.819639 [info ] [Thread-1 (]: 2 of 5 OK created sql table model andres.stg_daily_bars ........................ [[32mSUCCESS 1[0m in 1.91s]
[0m15:34:40.820525 [debug] [Thread-1 (]: Finished running node model.dbt_capstone_andres.stg_daily_bars
[0m15:34:40.821077 [debug] [Thread-1 (]: Began running node model.dbt_capstone_andres.stg_treasury_revenue_collections
[0m15:34:40.821790 [info ] [Thread-1 (]: 3 of 5 START sql table model andres.stg_treasury_revenue_collections ........... [RUN]
[0m15:34:40.822388 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_capstone_andres.stg_daily_bars, now model.dbt_capstone_andres.stg_treasury_revenue_collections)
[0m15:34:40.822890 [debug] [Thread-1 (]: Began compiling node model.dbt_capstone_andres.stg_treasury_revenue_collections
[0m15:34:40.827412 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_capstone_andres.stg_treasury_revenue_collections"
[0m15:34:40.828272 [debug] [Thread-1 (]: Began executing node model.dbt_capstone_andres.stg_treasury_revenue_collections
[0m15:34:40.832263 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_capstone_andres.stg_treasury_revenue_collections"
[0m15:34:40.833733 [debug] [Thread-1 (]: Using snowflake connection "model.dbt_capstone_andres.stg_treasury_revenue_collections"
[0m15:34:40.834255 [debug] [Thread-1 (]: On model.dbt_capstone_andres.stg_treasury_revenue_collections: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "model.dbt_capstone_andres.stg_treasury_revenue_collections"} */
create or replace transient table DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections
         as
        (with source as (

    select * from dataexpert_student.andres.fiscal_data

),

renamed as (

    select
        record_date as collection_date,
        channel_type_desc as channel_type,
        tax_category_desc as tax_category,
        round(net_collections_amt,0) as dollars_collected_by_treasury

    from source

)

select * from renamed
        );
[0m15:34:40.834754 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:34:42.344685 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 1.510 seconds
[0m15:34:42.348083 [debug] [Thread-1 (]: On model.dbt_capstone_andres.stg_treasury_revenue_collections: Close
[0m15:34:42.567528 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c6ac6ea3-048f-4b21-a666-01a4991e1bd0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11b6e5650>]}
[0m15:34:42.568439 [info ] [Thread-1 (]: 3 of 5 OK created sql table model andres.stg_treasury_revenue_collections ...... [[32mSUCCESS 1[0m in 1.75s]
[0m15:34:42.569242 [debug] [Thread-1 (]: Finished running node model.dbt_capstone_andres.stg_treasury_revenue_collections
[0m15:34:42.570268 [debug] [Thread-1 (]: Began running node unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum
[0m15:34:42.570797 [info ] [Thread-1 (]: 4 of 5 START unit_test weekly_metrics::test_amount_sum ......................... [RUN]
[0m15:34:42.571404 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_capstone_andres.stg_treasury_revenue_collections, now unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum)
[0m15:34:42.571893 [debug] [Thread-1 (]: Began compiling node unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum
[0m15:34:42.572378 [debug] [Thread-1 (]: Began executing node unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum
[0m15:34:42.623220 [debug] [Thread-1 (]: Using snowflake connection "unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum"
[0m15:34:42.623844 [debug] [Thread-1 (]: On unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum"} */
describe table DATAEXPERT_STUDENT.andres.stg_daily_bars
[0m15:34:42.624340 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:34:43.240469 [debug] [Thread-1 (]: SQL status: SUCCESS 9 in 0.616 seconds
[0m15:34:43.274380 [debug] [Thread-1 (]: Writing injected SQL for node "unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum"
[0m15:34:43.281019 [debug] [Thread-1 (]: Using snowflake connection "unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum"
[0m15:34:43.281470 [debug] [Thread-1 (]: On unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum"} */
describe table DATAEXPERT_STUDENT.andres.dim_date
[0m15:34:43.413422 [debug] [Thread-1 (]: SQL status: SUCCESS 36 in 0.131 seconds
[0m15:34:43.427490 [debug] [Thread-1 (]: On unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum: Close
[0m15:34:43.642199 [debug] [Thread-1 (]: Compilation Error in model dim_date (models/marts/_marts.yml)
  Invalid column name: ' week_start_date' in unit test fixture for 'dim_date'.
  Accepted columns for 'dim_date' are: ['date_day', 'prior_date_day', 'next_date_day', 'prior_year_date_day', 'prior_year_over_year_date_day', 'day_of_week', 'day_of_week_iso', 'day_of_week_name', 'day_of_week_name_short', 'day_of_month', 'day_of_year', 'week_start_date', 'week_end_date', 'prior_year_week_start_date', 'prior_year_week_end_date', 'week_of_year', 'iso_week_start_date', 'iso_week_end_date', 'prior_year_iso_week_start_date', 'prior_year_iso_week_end_date', 'iso_week_of_year', 'prior_year_week_of_year', 'prior_year_iso_week_of_year', 'month_of_year', 'month_name', 'month_name_short', 'month_start_date', 'month_end_date', 'prior_year_month_start_date', 'prior_year_month_end_date', 'quarter_of_year', 'quarter_start_date', 'quarter_end_date', 'year_number', 'year_start_date', 'year_end_date']
  
  > in macro format_row (macros/unit_test_sql/get_fixture_sql.sql)
  > called by macro get_fixture_sql (macros/unit_test_sql/get_fixture_sql.sql)
  > called by model dim_date (models/marts/_marts.yml)
[0m15:34:43.642897 [error] [Thread-1 (]: 4 of 5 ERROR weekly_metrics::test_amount_sum ................................... [[31mERROR[0m in 1.07s]
[0m15:34:43.643602 [debug] [Thread-1 (]: Finished running node unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum
[0m15:34:43.644210 [debug] [Thread-1 (]: Marking all children of 'unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum' to be skipped because of status 'error'.  Reason: Compilation Error in model dim_date (models/marts/_marts.yml)
  Invalid column name: ' week_start_date' in unit test fixture for 'dim_date'.
  Accepted columns for 'dim_date' are: ['date_day', 'prior_date_day', 'next_date_day', 'prior_year_date_day', 'prior_year_over_year_date_day', 'day_of_week', 'day_of_week_iso', 'day_of_week_name', 'day_of_week_name_short', 'day_of_month', 'day_of_year', 'week_start_date', 'week_end_date', 'prior_year_week_start_date', 'prior_year_week_end_date', 'week_of_year', 'iso_week_start_date', 'iso_week_end_date', 'prior_year_iso_week_start_date', 'prior_year_iso_week_end_date', 'iso_week_of_year', 'prior_year_week_of_year', 'prior_year_iso_week_of_year', 'month_of_year', 'month_name', 'month_name_short', 'month_start_date', 'month_end_date', 'prior_year_month_start_date', 'prior_year_month_end_date', 'quarter_of_year', 'quarter_start_date', 'quarter_end_date', 'year_number', 'year_start_date', 'year_end_date']
  
  > in macro format_row (macros/unit_test_sql/get_fixture_sql.sql)
  > called by macro get_fixture_sql (macros/unit_test_sql/get_fixture_sql.sql)
  > called by model dim_date (models/marts/_marts.yml).
[0m15:34:43.644950 [debug] [Thread-1 (]: Began running node model.dbt_capstone_andres.weekly_metrics
[0m15:34:43.645407 [info ] [Thread-1 (]: 5 of 5 SKIP relation andres.weekly_metrics ..................................... [[33mSKIP[0m]
[0m15:34:43.645891 [debug] [Thread-1 (]: Finished running node model.dbt_capstone_andres.weekly_metrics
[0m15:34:43.646321 [debug] [Thread-4 (]: Marking all children of 'model.dbt_capstone_andres.weekly_metrics' to be skipped because of status 'skipped'. 
[0m15:34:43.647640 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:34:43.648037 [debug] [MainThread]: Connection 'unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum' was properly closed.
[0m15:34:43.648487 [info ] [MainThread]: 
[0m15:34:43.648908 [info ] [MainThread]: Finished running 4 table models, 1 unit test in 0 hours 0 minutes and 9.43 seconds (9.43s).
[0m15:34:43.650536 [debug] [MainThread]: Command end result
[0m15:34:43.707296 [info ] [MainThread]: 
[0m15:34:43.707919 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successs, and 0 warnings:[0m
[0m15:34:43.708380 [info ] [MainThread]: 
[0m15:34:43.708946 [error] [MainThread]:   Compilation Error in model dim_date (models/marts/_marts.yml)
  Invalid column name: ' week_start_date' in unit test fixture for 'dim_date'.
  Accepted columns for 'dim_date' are: ['date_day', 'prior_date_day', 'next_date_day', 'prior_year_date_day', 'prior_year_over_year_date_day', 'day_of_week', 'day_of_week_iso', 'day_of_week_name', 'day_of_week_name_short', 'day_of_month', 'day_of_year', 'week_start_date', 'week_end_date', 'prior_year_week_start_date', 'prior_year_week_end_date', 'week_of_year', 'iso_week_start_date', 'iso_week_end_date', 'prior_year_iso_week_start_date', 'prior_year_iso_week_end_date', 'iso_week_of_year', 'prior_year_week_of_year', 'prior_year_iso_week_of_year', 'month_of_year', 'month_name', 'month_name_short', 'month_start_date', 'month_end_date', 'prior_year_month_start_date', 'prior_year_month_end_date', 'quarter_of_year', 'quarter_start_date', 'quarter_end_date', 'year_number', 'year_start_date', 'year_end_date']
  
  > in macro format_row (macros/unit_test_sql/get_fixture_sql.sql)
  > called by macro get_fixture_sql (macros/unit_test_sql/get_fixture_sql.sql)
  > called by model dim_date (models/marts/_marts.yml)
[0m15:34:43.709703 [info ] [MainThread]: 
[0m15:34:43.710240 [info ] [MainThread]: Done. PASS=3 WARN=0 ERROR=1 SKIP=1 TOTAL=5
[0m15:34:43.713357 [debug] [MainThread]: Resource report: {"command_name": "build", "command_wall_clock_time": 11.7720375, "process_user_time": 3.748925, "process_kernel_time": 0.700501, "process_mem_max_rss": "197013504", "command_success": false, "process_in_blocks": "0", "process_out_blocks": "0"}
[0m15:34:43.714646 [debug] [MainThread]: Command `dbt build` failed at 15:34:43.714474 after 11.77 seconds
[0m15:34:43.715231 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110f181d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110ee4cd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110f63d50>]}
[0m15:34:43.715746 [debug] [MainThread]: Flushing usage events
[0m15:34:44.031666 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:36:44.905438 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10779d7d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10780e790>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10780ed90>]}


============================== 15:36:44.911528 | 8179aca5-040d-4530-bb65-89bd7d505f9a ==============================
[0m15:36:44.911528 [info ] [MainThread]: Running with dbt=1.9.0-b3
[0m15:36:44.912226 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'profiles_dir': '.', 'log_path': 'logs', 'version_check': 'True', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'log_format': 'default', 'static_parser': 'True', 'target_path': 'None', 'invocation_command': 'dbt build', 'send_anonymous_usage_stats': 'True'}
[0m15:36:46.403928 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '8179aca5-040d-4530-bb65-89bd7d505f9a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1077a7b50>]}
[0m15:36:46.460843 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '8179aca5-040d-4530-bb65-89bd7d505f9a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1063bca90>]}
[0m15:36:46.462068 [info ] [MainThread]: Registered adapter: snowflake=1.8.4
[0m15:36:46.654104 [debug] [MainThread]: checksum: 5f4fde221ca8627174a0341efc73099aaec05b8546408ceed6f59b1ec0755115, vars: {}, profile: , target: , version: 1.9.0b3
[0m15:36:46.892995 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m15:36:46.893828 [debug] [MainThread]: Partial parsing: updated file: dbt_capstone_andres://models/marts/_marts.yml
[0m15:36:47.043728 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '8179aca5-040d-4530-bb65-89bd7d505f9a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10dc33f10>]}
[0m15:36:47.275095 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '8179aca5-040d-4530-bb65-89bd7d505f9a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d9e0550>]}
[0m15:36:47.275683 [info ] [MainThread]: Found 4 models, 2 sources, 862 macros, 1 unit test
[0m15:36:47.277748 [info ] [MainThread]: 
[0m15:36:47.278129 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m15:36:47.278487 [info ] [MainThread]: 
[0m15:36:47.279009 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m15:36:47.283866 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_DATAEXPERT_STUDENT'
[0m15:36:47.298173 [debug] [ThreadPool]: Using snowflake connection "list_DATAEXPERT_STUDENT"
[0m15:36:47.298646 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "connection_name": "list_DATAEXPERT_STUDENT"} */
show terse schemas in database DATAEXPERT_STUDENT
    limit 10000
[0m15:36:47.299004 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:36:48.251045 [debug] [ThreadPool]: SQL status: SUCCESS 73 in 0.952 seconds
[0m15:36:48.253921 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT: Close
[0m15:36:48.490516 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_DATAEXPERT_STUDENT, now list_DATAEXPERT_STUDENT_andres)
[0m15:36:48.499775 [debug] [ThreadPool]: Using snowflake connection "list_DATAEXPERT_STUDENT_andres"
[0m15:36:48.500165 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "connection_name": "list_DATAEXPERT_STUDENT_andres"} */
show objects in DATAEXPERT_STUDENT.andres limit 10000
[0m15:36:48.500522 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:36:49.219014 [debug] [ThreadPool]: SQL status: SUCCESS 25 in 0.718 seconds
[0m15:36:49.222859 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: Close
[0m15:36:49.455482 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '8179aca5-040d-4530-bb65-89bd7d505f9a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107e65dd0>]}
[0m15:36:49.458744 [debug] [Thread-1 (]: Began running node model.dbt_capstone_andres.dim_date
[0m15:36:49.459452 [info ] [Thread-1 (]: 1 of 5 START sql table model andres.dim_date ................................... [RUN]
[0m15:36:49.460059 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_DATAEXPERT_STUDENT_andres, now model.dbt_capstone_andres.dim_date)
[0m15:36:49.460538 [debug] [Thread-1 (]: Began compiling node model.dbt_capstone_andres.dim_date
[0m15:36:49.502975 [debug] [Thread-1 (]: Using snowflake connection "model.dbt_capstone_andres.dim_date"
[0m15:36:49.503608 [debug] [Thread-1 (]: On model.dbt_capstone_andres.dim_date: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "model.dbt_capstone_andres.dim_date"} */
select datediff(
        day,
        cast('1995-01-01' as timestamp),
        cast('2040-12-31' as timestamp)
        )
[0m15:36:49.504115 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:36:50.177608 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 0.673 seconds
[0m15:36:50.251829 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_capstone_andres.dim_date"
[0m15:36:50.252562 [debug] [Thread-1 (]: Began executing node model.dbt_capstone_andres.dim_date
[0m15:36:50.278071 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_capstone_andres.dim_date"
[0m15:36:50.284657 [debug] [Thread-1 (]: Using snowflake connection "model.dbt_capstone_andres.dim_date"
[0m15:36:50.285305 [debug] [Thread-1 (]: On model.dbt_capstone_andres.dim_date: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "model.dbt_capstone_andres.dim_date"} */
create or replace transient table DATAEXPERT_STUDENT.andres.dim_date
         as
        (

    
with base_dates as (
    
    with date_spine as
(

    





with rawdata as (

    

    

    with p as (
        select 0 as generated_number union all select 1
    ), unioned as (

    select

    
    p0.generated_number * power(2, 0)
     + 
    
    p1.generated_number * power(2, 1)
     + 
    
    p2.generated_number * power(2, 2)
     + 
    
    p3.generated_number * power(2, 3)
     + 
    
    p4.generated_number * power(2, 4)
     + 
    
    p5.generated_number * power(2, 5)
     + 
    
    p6.generated_number * power(2, 6)
     + 
    
    p7.generated_number * power(2, 7)
     + 
    
    p8.generated_number * power(2, 8)
     + 
    
    p9.generated_number * power(2, 9)
     + 
    
    p10.generated_number * power(2, 10)
     + 
    
    p11.generated_number * power(2, 11)
     + 
    
    p12.generated_number * power(2, 12)
     + 
    
    p13.generated_number * power(2, 13)
     + 
    
    p14.generated_number * power(2, 14)
    
    
    + 1
    as generated_number

    from

    
    p as p0
     cross join 
    
    p as p1
     cross join 
    
    p as p2
     cross join 
    
    p as p3
     cross join 
    
    p as p4
     cross join 
    
    p as p5
     cross join 
    
    p as p6
     cross join 
    
    p as p7
     cross join 
    
    p as p8
     cross join 
    
    p as p9
     cross join 
    
    p as p10
     cross join 
    
    p as p11
     cross join 
    
    p as p12
     cross join 
    
    p as p13
     cross join 
    
    p as p14
    
    

    )

    select *
    from unioned
    where generated_number <= 16801
    order by generated_number



),

all_periods as (

    select (
        

    dateadd(
        day,
        (row_number() over (order by 1) - 1),
        cast('1995-01-01' as timestamp)
        )


    ) as date_day
    from rawdata

),

filtered as (

    select *
    from all_periods
    where date_day <= cast('2040-12-31' as timestamp)

)

select * from filtered



)
select
    cast(d.date_day as timestamp) as date_day
from
    date_spine d


),
dates_with_prior_year_dates as (

    select
        cast(d.date_day as date) as date_day,
        cast(

    dateadd(
        year,
        -1,
        d.date_day
        )

 as date) as prior_year_date_day,
        cast(

    dateadd(
        day,
        -364,
        d.date_day
        )

 as date) as prior_year_over_year_date_day
    from
    	base_dates d

)
select
    d.date_day,
    cast(

    dateadd(
        day,
        -1,
        d.date_day
        )

 as date) as prior_date_day,
    cast(

    dateadd(
        day,
        1,
        d.date_day
        )

 as date) as next_date_day,
    d.prior_year_date_day as prior_year_date_day,
    d.prior_year_over_year_date_day,
    case
            when date_part('dayofweek', d.date_day) = 7 then 1
            else date_part('dayofweek', d.date_day) + 1
        end as day_of_week,
    date_part('dayofweekiso', d.date_day) as day_of_week_iso,
    -- long version not implemented on Snowflake so we're doing it manually :/
    case dayname(d.date_day)
        when 'Mon' then 'Monday'
        when 'Tue' then 'Tuesday'
        when 'Wed' then 'Wednesday'
        when 'Thu' then 'Thursday'
        when 'Fri' then 'Friday'
        when 'Sat' then 'Saturday'
        when 'Sun' then 'Sunday'
    end as day_of_week_name,
    dayname(d.date_day) as day_of_week_name_short,
    date_part('day', d.date_day) as day_of_month,
    date_part('dayofyear', d.date_day) as day_of_year,

    
    
    cast(

    dateadd(
        day,
        -1 * (case
            when date_part('dayofweek', d.date_day) = 7 then 1
            else date_part('dayofweek', d.date_day) + 1
        end - 1),
        d.date_day
        )

 as date) as week_start_date,
    cast(

    dateadd(
        day,
        6,
        
    
    cast(

    dateadd(
        day,
        -1 * (case
            when date_part('dayofweek', d.date_day) = 7 then 1
            else date_part('dayofweek', d.date_day) + 1
        end - 1),
        d.date_day
        )

 as date)
        )

 as date) as week_end_date,
    
    
    cast(

    dateadd(
        day,
        -1 * (case
            when date_part('dayofweek', d.prior_year_over_year_date_day) = 7 then 1
            else date_part('dayofweek', d.prior_year_over_year_date_day) + 1
        end - 1),
        d.prior_year_over_year_date_day
        )

 as date) as prior_year_week_start_date,
    cast(

    dateadd(
        day,
        6,
        
    
    cast(

    dateadd(
        day,
        -1 * (case
            when date_part('dayofweek', d.prior_year_over_year_date_day) = 7 then 1
            else date_part('dayofweek', d.prior_year_over_year_date_day) + 1
        end - 1),
        d.prior_year_over_year_date_day
        )

 as date)
        )

 as date) as prior_year_week_end_date,
    cast(date_part('week', d.date_day) as integer) as week_of_year,

    cast(date_trunc('week', d.date_day) as date) as iso_week_start_date,
    cast(

    dateadd(
        day,
        6,
        cast(date_trunc('week', d.date_day) as date)
        )

 as date) as iso_week_end_date,
    cast(date_trunc('week', d.prior_year_over_year_date_day) as date) as prior_year_iso_week_start_date,
    cast(

    dateadd(
        day,
        6,
        cast(date_trunc('week', d.prior_year_over_year_date_day) as date)
        )

 as date) as prior_year_iso_week_end_date,
    cast(date_part('weekiso', d.date_day) as integer) as iso_week_of_year,

    cast(date_part('week', d.prior_year_over_year_date_day) as integer) as prior_year_week_of_year,
    cast(date_part('weekiso', d.prior_year_over_year_date_day) as integer) as prior_year_iso_week_of_year,

    cast(date_part('month', d.date_day) as integer) as month_of_year,
    to_char(d.date_day, 'MMMM')  as month_name,
    to_char(d.date_day, 'MON')  as month_name_short,

    cast(date_trunc('month', d.date_day) as date) as month_start_date,
    cast(cast(
        

    dateadd(
        day,
        -1,
        

    dateadd(
        month,
        1,
        date_trunc('month', d.date_day)
        )


        )


        as date) as date) as month_end_date,

    cast(date_trunc('month', d.prior_year_date_day) as date) as prior_year_month_start_date,
    cast(cast(
        

    dateadd(
        day,
        -1,
        

    dateadd(
        month,
        1,
        date_trunc('month', d.prior_year_date_day)
        )


        )


        as date) as date) as prior_year_month_end_date,

    cast(date_part('quarter', d.date_day) as integer) as quarter_of_year,
    cast(date_trunc('quarter', d.date_day) as date) as quarter_start_date,
    cast(cast(
        

    dateadd(
        day,
        -1,
        

    dateadd(
        quarter,
        1,
        date_trunc('quarter', d.date_day)
        )


        )


        as date) as date) as quarter_end_date,

    cast(date_part('year', d.date_day) as integer) as year_number,
    cast(date_trunc('year', d.date_day) as date) as year_start_date,
    cast(cast(
        

    dateadd(
        day,
        -1,
        

    dateadd(
        year,
        1,
        date_trunc('year', d.date_day)
        )


        )


        as date) as date) as year_end_date
from
    dates_with_prior_year_dates d
order by 1


        );
[0m15:36:52.120722 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 1.835 seconds
[0m15:36:52.149090 [debug] [Thread-1 (]: On model.dbt_capstone_andres.dim_date: Close
[0m15:36:52.378707 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '8179aca5-040d-4530-bb65-89bd7d505f9a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111e8d590>]}
[0m15:36:52.379825 [info ] [Thread-1 (]: 1 of 5 OK created sql table model andres.dim_date .............................. [[32mSUCCESS 1[0m in 2.92s]
[0m15:36:52.380766 [debug] [Thread-1 (]: Finished running node model.dbt_capstone_andres.dim_date
[0m15:36:52.381418 [debug] [Thread-1 (]: Began running node model.dbt_capstone_andres.stg_daily_bars
[0m15:36:52.382210 [info ] [Thread-1 (]: 2 of 5 START sql table model andres.stg_daily_bars ............................. [RUN]
[0m15:36:52.382864 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_capstone_andres.dim_date, now model.dbt_capstone_andres.stg_daily_bars)
[0m15:36:52.383367 [debug] [Thread-1 (]: Began compiling node model.dbt_capstone_andres.stg_daily_bars
[0m15:36:52.387729 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_capstone_andres.stg_daily_bars"
[0m15:36:52.388673 [debug] [Thread-1 (]: Began executing node model.dbt_capstone_andres.stg_daily_bars
[0m15:36:52.393262 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_capstone_andres.stg_daily_bars"
[0m15:36:52.395042 [debug] [Thread-1 (]: Using snowflake connection "model.dbt_capstone_andres.stg_daily_bars"
[0m15:36:52.395675 [debug] [Thread-1 (]: On model.dbt_capstone_andres.stg_daily_bars: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "model.dbt_capstone_andres.stg_daily_bars"} */
create or replace transient table DATAEXPERT_STUDENT.andres.stg_daily_bars
         as
        (with source as (

    select * from dataexpert_student.andres.daily_bars

),

renamed as (

    select
        TO_DATE(TO_TIMESTAMP(timestamp / 1000)) as date,
        ticker as ticker,
        v  as number_of_shares_traded,
        n  as number_of_trades,
        round(vw,2) as volume_weighted_avg_price,
        o  as open_price,
        c  as close_price,
        h  as high_price,
        l  as low_price
    from source

)

select * from renamed
        );
[0m15:36:52.396265 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:36:54.929466 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 2.533 seconds
[0m15:36:54.931845 [debug] [Thread-1 (]: On model.dbt_capstone_andres.stg_daily_bars: Close
[0m15:36:55.154386 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '8179aca5-040d-4530-bb65-89bd7d505f9a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10dd9f590>]}
[0m15:36:55.155173 [info ] [Thread-1 (]: 2 of 5 OK created sql table model andres.stg_daily_bars ........................ [[32mSUCCESS 1[0m in 2.77s]
[0m15:36:55.155961 [debug] [Thread-1 (]: Finished running node model.dbt_capstone_andres.stg_daily_bars
[0m15:36:55.156355 [debug] [Thread-1 (]: Began running node model.dbt_capstone_andres.stg_treasury_revenue_collections
[0m15:36:55.156861 [info ] [Thread-1 (]: 3 of 5 START sql table model andres.stg_treasury_revenue_collections ........... [RUN]
[0m15:36:55.157465 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_capstone_andres.stg_daily_bars, now model.dbt_capstone_andres.stg_treasury_revenue_collections)
[0m15:36:55.157833 [debug] [Thread-1 (]: Began compiling node model.dbt_capstone_andres.stg_treasury_revenue_collections
[0m15:36:55.160944 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_capstone_andres.stg_treasury_revenue_collections"
[0m15:36:55.162095 [debug] [Thread-1 (]: Began executing node model.dbt_capstone_andres.stg_treasury_revenue_collections
[0m15:36:55.166054 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_capstone_andres.stg_treasury_revenue_collections"
[0m15:36:55.167474 [debug] [Thread-1 (]: Using snowflake connection "model.dbt_capstone_andres.stg_treasury_revenue_collections"
[0m15:36:55.168128 [debug] [Thread-1 (]: On model.dbt_capstone_andres.stg_treasury_revenue_collections: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "model.dbt_capstone_andres.stg_treasury_revenue_collections"} */
create or replace transient table DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections
         as
        (with source as (

    select * from dataexpert_student.andres.fiscal_data

),

renamed as (

    select
        record_date as collection_date,
        channel_type_desc as channel_type,
        tax_category_desc as tax_category,
        round(net_collections_amt,0) as dollars_collected_by_treasury

    from source

)

select * from renamed
        );
[0m15:36:55.168811 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:36:56.479680 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 1.311 seconds
[0m15:36:56.483083 [debug] [Thread-1 (]: On model.dbt_capstone_andres.stg_treasury_revenue_collections: Close
[0m15:36:56.699648 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '8179aca5-040d-4530-bb65-89bd7d505f9a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111f89e90>]}
[0m15:36:56.700672 [info ] [Thread-1 (]: 3 of 5 OK created sql table model andres.stg_treasury_revenue_collections ...... [[32mSUCCESS 1[0m in 1.54s]
[0m15:36:56.701560 [debug] [Thread-1 (]: Finished running node model.dbt_capstone_andres.stg_treasury_revenue_collections
[0m15:36:56.702447 [debug] [Thread-1 (]: Began running node unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum
[0m15:36:56.703018 [info ] [Thread-1 (]: 4 of 5 START unit_test weekly_metrics::test_amount_sum ......................... [RUN]
[0m15:36:56.703733 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_capstone_andres.stg_treasury_revenue_collections, now unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum)
[0m15:36:56.704262 [debug] [Thread-1 (]: Began compiling node unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum
[0m15:36:56.704822 [debug] [Thread-1 (]: Began executing node unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum
[0m15:36:56.749287 [debug] [Thread-1 (]: Using snowflake connection "unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum"
[0m15:36:56.749804 [debug] [Thread-1 (]: On unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum"} */
describe table DATAEXPERT_STUDENT.andres.stg_daily_bars
[0m15:36:56.750206 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:36:57.422499 [debug] [Thread-1 (]: SQL status: SUCCESS 9 in 0.672 seconds
[0m15:36:57.456466 [debug] [Thread-1 (]: Writing injected SQL for node "unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum"
[0m15:36:57.463123 [debug] [Thread-1 (]: Using snowflake connection "unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum"
[0m15:36:57.463594 [debug] [Thread-1 (]: On unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum"} */
describe table DATAEXPERT_STUDENT.andres.dim_date
[0m15:36:57.632623 [debug] [Thread-1 (]: SQL status: SUCCESS 36 in 0.168 seconds
[0m15:36:57.648585 [debug] [Thread-1 (]: Writing injected SQL for node "unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum"
[0m15:36:57.657809 [debug] [Thread-1 (]: Using snowflake connection "unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum"
[0m15:36:57.658311 [debug] [Thread-1 (]: On unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum"} */
describe table DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections
[0m15:36:57.821848 [debug] [Thread-1 (]: SQL status: SUCCESS 4 in 0.163 seconds
[0m15:36:57.833717 [debug] [Thread-1 (]: Writing injected SQL for node "unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum"
[0m15:36:57.843191 [debug] [Thread-1 (]: Writing injected SQL for node "unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum"
[0m15:36:57.873227 [debug] [Thread-1 (]: Using snowflake connection "unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum"
[0m15:36:57.874076 [debug] [Thread-1 (]: On unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum"} */
create or replace temporary table DATAEXPERT_STUDENT.andres.test_amount_sum__dbt_tmp
         as
        (select * from (
        with  __dbt__cte__stg_daily_bars as (

-- Fixture for stg_daily_bars
select 
    
        try_cast('2020-11-20' as DATE)
     as date, 
    
        try_cast('\'L\'' as character varying(16777216))
     as ticker, 
    
        try_cast('100' as FLOAT)
     as number_of_shares_traded, 
    
        try_cast('10' as NUMBER(38,0))
     as number_of_trades, try_cast(null as FLOAT) as volume_weighted_avg_price, try_cast(null as FLOAT) as open_price, try_cast(null as FLOAT) as close_price, try_cast(null as FLOAT) as high_price, try_cast(null as FLOAT) as low_price
union all
select 
    
        try_cast('2020-11-20' as DATE)
     as date, 
    
        try_cast('\'S\'' as character varying(16777216))
     as ticker, 
    
        try_cast('200' as FLOAT)
     as number_of_shares_traded, 
    
        try_cast('20' as NUMBER(38,0))
     as number_of_trades, try_cast(null as FLOAT) as volume_weighted_avg_price, try_cast(null as FLOAT) as open_price, try_cast(null as FLOAT) as close_price, try_cast(null as FLOAT) as high_price, try_cast(null as FLOAT) as low_price
union all
select 
    
        try_cast('2020-11-21' as DATE)
     as date, 
    
        try_cast('\'L\'' as character varying(16777216))
     as ticker, 
    
        try_cast('300' as FLOAT)
     as number_of_shares_traded, 
    
        try_cast('40' as NUMBER(38,0))
     as number_of_trades, try_cast(null as FLOAT) as volume_weighted_avg_price, try_cast(null as FLOAT) as open_price, try_cast(null as FLOAT) as close_price, try_cast(null as FLOAT) as high_price, try_cast(null as FLOAT) as low_price
union all
select 
    
        try_cast('2020-11-21' as DATE)
     as date, 
    
        try_cast('\'S\'' as character varying(16777216))
     as ticker, 
    
        try_cast('200' as FLOAT)
     as number_of_shares_traded, 
    
        try_cast('50' as NUMBER(38,0))
     as number_of_trades, try_cast(null as FLOAT) as volume_weighted_avg_price, try_cast(null as FLOAT) as open_price, try_cast(null as FLOAT) as close_price, try_cast(null as FLOAT) as high_price, try_cast(null as FLOAT) as low_price
),  __dbt__cte__dim_date as (

-- Fixture for dim_date
select 
    
        try_cast('2020-11-20' as DATE)
     as date_day, try_cast(null as DATE) as prior_date_day, try_cast(null as DATE) as next_date_day, try_cast(null as DATE) as prior_year_date_day, try_cast(null as DATE) as prior_year_over_year_date_day, try_cast(null as NUMBER(3,0)) as day_of_week, try_cast(null as NUMBER(2,0)) as day_of_week_iso, try_cast(null as character varying(16777216)) as day_of_week_name, try_cast(null as character varying(3)) as day_of_week_name_short, try_cast(null as NUMBER(2,0)) as day_of_month, try_cast(null as NUMBER(4,0)) as day_of_year, 
    
        try_cast(' 2020-11-15' as DATE)
     as week_start_date, try_cast(null as DATE) as week_end_date, try_cast(null as DATE) as prior_year_week_start_date, try_cast(null as DATE) as prior_year_week_end_date, try_cast(null as NUMBER(38,0)) as week_of_year, try_cast(null as DATE) as iso_week_start_date, try_cast(null as DATE) as iso_week_end_date, try_cast(null as DATE) as prior_year_iso_week_start_date, try_cast(null as DATE) as prior_year_iso_week_end_date, try_cast(null as NUMBER(38,0)) as iso_week_of_year, try_cast(null as NUMBER(38,0)) as prior_year_week_of_year, try_cast(null as NUMBER(38,0)) as prior_year_iso_week_of_year, try_cast(null as NUMBER(38,0)) as month_of_year, try_cast(null as character varying(16777216)) as month_name, try_cast(null as character varying(16777216)) as month_name_short, try_cast(null as DATE) as month_start_date, try_cast(null as DATE) as month_end_date, try_cast(null as DATE) as prior_year_month_start_date, try_cast(null as DATE) as prior_year_month_end_date, try_cast(null as NUMBER(38,0)) as quarter_of_year, try_cast(null as DATE) as quarter_start_date, try_cast(null as DATE) as quarter_end_date, try_cast(null as NUMBER(38,0)) as year_number, try_cast(null as DATE) as year_start_date, try_cast(null as DATE) as year_end_date
union all
select 
    
        try_cast('2020-11-21' as DATE)
     as date_day, try_cast(null as DATE) as prior_date_day, try_cast(null as DATE) as next_date_day, try_cast(null as DATE) as prior_year_date_day, try_cast(null as DATE) as prior_year_over_year_date_day, try_cast(null as NUMBER(3,0)) as day_of_week, try_cast(null as NUMBER(2,0)) as day_of_week_iso, try_cast(null as character varying(16777216)) as day_of_week_name, try_cast(null as character varying(3)) as day_of_week_name_short, try_cast(null as NUMBER(2,0)) as day_of_month, try_cast(null as NUMBER(4,0)) as day_of_year, 
    
        try_cast(' 2020-11-15' as DATE)
     as week_start_date, try_cast(null as DATE) as week_end_date, try_cast(null as DATE) as prior_year_week_start_date, try_cast(null as DATE) as prior_year_week_end_date, try_cast(null as NUMBER(38,0)) as week_of_year, try_cast(null as DATE) as iso_week_start_date, try_cast(null as DATE) as iso_week_end_date, try_cast(null as DATE) as prior_year_iso_week_start_date, try_cast(null as DATE) as prior_year_iso_week_end_date, try_cast(null as NUMBER(38,0)) as iso_week_of_year, try_cast(null as NUMBER(38,0)) as prior_year_week_of_year, try_cast(null as NUMBER(38,0)) as prior_year_iso_week_of_year, try_cast(null as NUMBER(38,0)) as month_of_year, try_cast(null as character varying(16777216)) as month_name, try_cast(null as character varying(16777216)) as month_name_short, try_cast(null as DATE) as month_start_date, try_cast(null as DATE) as month_end_date, try_cast(null as DATE) as prior_year_month_start_date, try_cast(null as DATE) as prior_year_month_end_date, try_cast(null as NUMBER(38,0)) as quarter_of_year, try_cast(null as DATE) as quarter_start_date, try_cast(null as DATE) as quarter_end_date, try_cast(null as NUMBER(38,0)) as year_number, try_cast(null as DATE) as year_start_date, try_cast(null as DATE) as year_end_date
),  __dbt__cte__stg_treasury_revenue_collections as (

-- Fixture for stg_treasury_revenue_collections
select 
    
        try_cast('2020-11-20' as DATE)
     as collection_date, try_cast(null as character varying(16777216)) as channel_type, 
    
        try_cast('\'IRS Tax\'' as character varying(16777216))
     as tax_category, 
    
        try_cast('1000' as NUMBER(19,0))
     as dollars_collected_by_treasury
union all
select 
    
        try_cast('2020-11-20' as DATE)
     as collection_date, try_cast(null as character varying(16777216)) as channel_type, 
    
        try_cast('\'IRS Non Tax\'' as character varying(16777216))
     as tax_category, 
    
        try_cast('2000' as NUMBER(19,0))
     as dollars_collected_by_treasury
union all
select 
    
        try_cast('2020-11-21' as DATE)
     as collection_date, try_cast(null as character varying(16777216)) as channel_type, 
    
        try_cast('\'IRS Tax\'' as character varying(16777216))
     as tax_category, 
    
        try_cast('3000' as NUMBER(19,0))
     as dollars_collected_by_treasury
union all
select 
    
        try_cast('2020-11-21' as DATE)
     as collection_date, try_cast(null as character varying(16777216)) as channel_type, 
    
        try_cast('\'IRS Tax\'' as character varying(16777216))
     as tax_category, 
    
        try_cast('2000' as NUMBER(19,0))
     as dollars_collected_by_treasury
), pol as (
SELECT week_start_date, 
sum(number_of_shares_traded) number_of_shares_traded,
sum(number_of_trades) number_of_trades
from  __dbt__cte__stg_daily_bars a 
JOIN __dbt__cte__dim_date d on d.date_day=a.date
group by 1)

, rev as (
select week_start_date,
sum(dollars_collected_by_treasury) dollars_collected_by_treasury,
sum(case when tax_category = 'IRS Tax' then dollars_collected_by_treasury else 0 end) as irs_tax_dollars_collected_by_treasury, 
sum(case when tax_category = 'Non-Tax' then dollars_collected_by_treasury else 0 end) as non_irs_non_tax_dollars_collected_by_treasury,
sum(case when tax_category = 'IRS Non-Tax' then dollars_collected_by_treasury else 0 end) as irs_non_tax_dollars_collected_by_treasury
FROM __dbt__cte__stg_treasury_revenue_collections a 
JOIN __dbt__cte__dim_date d on d.date_day=a.collection_date
GROUP BY ALL)

select 
    a.week_start_date, 
    a.number_of_shares_traded, 
    a.number_of_trades,
    b.dollars_collected_by_treasury,
    b.irs_tax_dollars_collected_by_treasury

from pol a 
INNER JOIN rev b on b.week_start_date = a.week_start_date
    ) as __dbt_sbq
    where false
    limit 0

        );
[0m15:36:58.584565 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 0.710 seconds
[0m15:36:58.589054 [debug] [Thread-1 (]: Using snowflake connection "unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum"
[0m15:36:58.589569 [debug] [Thread-1 (]: On unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum"} */
describe table DATAEXPERT_STUDENT.andres.test_amount_sum__dbt_tmp
[0m15:36:58.773223 [debug] [Thread-1 (]: SQL status: SUCCESS 5 in 0.183 seconds
[0m15:36:58.784827 [debug] [Thread-1 (]: Writing runtime sql for node "unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum"
[0m15:36:58.795785 [debug] [Thread-1 (]: Using snowflake connection "unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum"
[0m15:36:58.796783 [debug] [Thread-1 (]: On unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum"} */
-- Build actual result given inputs
with dbt_internal_unit_test_actual as (
  select
    week_start_date,number_of_shares_traded,number_of_trades,dollars_collected_by_treasury,irs_tax_dollars_collected_by_treasury, 'actual' as "actual_or_expected"
  from (
    with  __dbt__cte__stg_daily_bars as (

-- Fixture for stg_daily_bars
select 
    
        try_cast('2020-11-20' as DATE)
     as date, 
    
        try_cast('\'L\'' as character varying(16777216))
     as ticker, 
    
        try_cast('100' as FLOAT)
     as number_of_shares_traded, 
    
        try_cast('10' as NUMBER(38,0))
     as number_of_trades, try_cast(null as FLOAT) as volume_weighted_avg_price, try_cast(null as FLOAT) as open_price, try_cast(null as FLOAT) as close_price, try_cast(null as FLOAT) as high_price, try_cast(null as FLOAT) as low_price
union all
select 
    
        try_cast('2020-11-20' as DATE)
     as date, 
    
        try_cast('\'S\'' as character varying(16777216))
     as ticker, 
    
        try_cast('200' as FLOAT)
     as number_of_shares_traded, 
    
        try_cast('20' as NUMBER(38,0))
     as number_of_trades, try_cast(null as FLOAT) as volume_weighted_avg_price, try_cast(null as FLOAT) as open_price, try_cast(null as FLOAT) as close_price, try_cast(null as FLOAT) as high_price, try_cast(null as FLOAT) as low_price
union all
select 
    
        try_cast('2020-11-21' as DATE)
     as date, 
    
        try_cast('\'L\'' as character varying(16777216))
     as ticker, 
    
        try_cast('300' as FLOAT)
     as number_of_shares_traded, 
    
        try_cast('40' as NUMBER(38,0))
     as number_of_trades, try_cast(null as FLOAT) as volume_weighted_avg_price, try_cast(null as FLOAT) as open_price, try_cast(null as FLOAT) as close_price, try_cast(null as FLOAT) as high_price, try_cast(null as FLOAT) as low_price
union all
select 
    
        try_cast('2020-11-21' as DATE)
     as date, 
    
        try_cast('\'S\'' as character varying(16777216))
     as ticker, 
    
        try_cast('200' as FLOAT)
     as number_of_shares_traded, 
    
        try_cast('50' as NUMBER(38,0))
     as number_of_trades, try_cast(null as FLOAT) as volume_weighted_avg_price, try_cast(null as FLOAT) as open_price, try_cast(null as FLOAT) as close_price, try_cast(null as FLOAT) as high_price, try_cast(null as FLOAT) as low_price
),  __dbt__cte__dim_date as (

-- Fixture for dim_date
select 
    
        try_cast('2020-11-20' as DATE)
     as date_day, try_cast(null as DATE) as prior_date_day, try_cast(null as DATE) as next_date_day, try_cast(null as DATE) as prior_year_date_day, try_cast(null as DATE) as prior_year_over_year_date_day, try_cast(null as NUMBER(3,0)) as day_of_week, try_cast(null as NUMBER(2,0)) as day_of_week_iso, try_cast(null as character varying(16777216)) as day_of_week_name, try_cast(null as character varying(3)) as day_of_week_name_short, try_cast(null as NUMBER(2,0)) as day_of_month, try_cast(null as NUMBER(4,0)) as day_of_year, 
    
        try_cast(' 2020-11-15' as DATE)
     as week_start_date, try_cast(null as DATE) as week_end_date, try_cast(null as DATE) as prior_year_week_start_date, try_cast(null as DATE) as prior_year_week_end_date, try_cast(null as NUMBER(38,0)) as week_of_year, try_cast(null as DATE) as iso_week_start_date, try_cast(null as DATE) as iso_week_end_date, try_cast(null as DATE) as prior_year_iso_week_start_date, try_cast(null as DATE) as prior_year_iso_week_end_date, try_cast(null as NUMBER(38,0)) as iso_week_of_year, try_cast(null as NUMBER(38,0)) as prior_year_week_of_year, try_cast(null as NUMBER(38,0)) as prior_year_iso_week_of_year, try_cast(null as NUMBER(38,0)) as month_of_year, try_cast(null as character varying(16777216)) as month_name, try_cast(null as character varying(16777216)) as month_name_short, try_cast(null as DATE) as month_start_date, try_cast(null as DATE) as month_end_date, try_cast(null as DATE) as prior_year_month_start_date, try_cast(null as DATE) as prior_year_month_end_date, try_cast(null as NUMBER(38,0)) as quarter_of_year, try_cast(null as DATE) as quarter_start_date, try_cast(null as DATE) as quarter_end_date, try_cast(null as NUMBER(38,0)) as year_number, try_cast(null as DATE) as year_start_date, try_cast(null as DATE) as year_end_date
union all
select 
    
        try_cast('2020-11-21' as DATE)
     as date_day, try_cast(null as DATE) as prior_date_day, try_cast(null as DATE) as next_date_day, try_cast(null as DATE) as prior_year_date_day, try_cast(null as DATE) as prior_year_over_year_date_day, try_cast(null as NUMBER(3,0)) as day_of_week, try_cast(null as NUMBER(2,0)) as day_of_week_iso, try_cast(null as character varying(16777216)) as day_of_week_name, try_cast(null as character varying(3)) as day_of_week_name_short, try_cast(null as NUMBER(2,0)) as day_of_month, try_cast(null as NUMBER(4,0)) as day_of_year, 
    
        try_cast(' 2020-11-15' as DATE)
     as week_start_date, try_cast(null as DATE) as week_end_date, try_cast(null as DATE) as prior_year_week_start_date, try_cast(null as DATE) as prior_year_week_end_date, try_cast(null as NUMBER(38,0)) as week_of_year, try_cast(null as DATE) as iso_week_start_date, try_cast(null as DATE) as iso_week_end_date, try_cast(null as DATE) as prior_year_iso_week_start_date, try_cast(null as DATE) as prior_year_iso_week_end_date, try_cast(null as NUMBER(38,0)) as iso_week_of_year, try_cast(null as NUMBER(38,0)) as prior_year_week_of_year, try_cast(null as NUMBER(38,0)) as prior_year_iso_week_of_year, try_cast(null as NUMBER(38,0)) as month_of_year, try_cast(null as character varying(16777216)) as month_name, try_cast(null as character varying(16777216)) as month_name_short, try_cast(null as DATE) as month_start_date, try_cast(null as DATE) as month_end_date, try_cast(null as DATE) as prior_year_month_start_date, try_cast(null as DATE) as prior_year_month_end_date, try_cast(null as NUMBER(38,0)) as quarter_of_year, try_cast(null as DATE) as quarter_start_date, try_cast(null as DATE) as quarter_end_date, try_cast(null as NUMBER(38,0)) as year_number, try_cast(null as DATE) as year_start_date, try_cast(null as DATE) as year_end_date
),  __dbt__cte__stg_treasury_revenue_collections as (

-- Fixture for stg_treasury_revenue_collections
select 
    
        try_cast('2020-11-20' as DATE)
     as collection_date, try_cast(null as character varying(16777216)) as channel_type, 
    
        try_cast('\'IRS Tax\'' as character varying(16777216))
     as tax_category, 
    
        try_cast('1000' as NUMBER(19,0))
     as dollars_collected_by_treasury
union all
select 
    
        try_cast('2020-11-20' as DATE)
     as collection_date, try_cast(null as character varying(16777216)) as channel_type, 
    
        try_cast('\'IRS Non Tax\'' as character varying(16777216))
     as tax_category, 
    
        try_cast('2000' as NUMBER(19,0))
     as dollars_collected_by_treasury
union all
select 
    
        try_cast('2020-11-21' as DATE)
     as collection_date, try_cast(null as character varying(16777216)) as channel_type, 
    
        try_cast('\'IRS Tax\'' as character varying(16777216))
     as tax_category, 
    
        try_cast('3000' as NUMBER(19,0))
     as dollars_collected_by_treasury
union all
select 
    
        try_cast('2020-11-21' as DATE)
     as collection_date, try_cast(null as character varying(16777216)) as channel_type, 
    
        try_cast('\'IRS Tax\'' as character varying(16777216))
     as tax_category, 
    
        try_cast('2000' as NUMBER(19,0))
     as dollars_collected_by_treasury
), pol as (
SELECT week_start_date, 
sum(number_of_shares_traded) number_of_shares_traded,
sum(number_of_trades) number_of_trades
from  __dbt__cte__stg_daily_bars a 
JOIN __dbt__cte__dim_date d on d.date_day=a.date
group by 1)

, rev as (
select week_start_date,
sum(dollars_collected_by_treasury) dollars_collected_by_treasury,
sum(case when tax_category = 'IRS Tax' then dollars_collected_by_treasury else 0 end) as irs_tax_dollars_collected_by_treasury, 
sum(case when tax_category = 'Non-Tax' then dollars_collected_by_treasury else 0 end) as non_irs_non_tax_dollars_collected_by_treasury,
sum(case when tax_category = 'IRS Non-Tax' then dollars_collected_by_treasury else 0 end) as irs_non_tax_dollars_collected_by_treasury
FROM __dbt__cte__stg_treasury_revenue_collections a 
JOIN __dbt__cte__dim_date d on d.date_day=a.collection_date
GROUP BY ALL)

select 
    a.week_start_date, 
    a.number_of_shares_traded, 
    a.number_of_trades,
    b.dollars_collected_by_treasury,
    b.irs_tax_dollars_collected_by_treasury

from pol a 
INNER JOIN rev b on b.week_start_date = a.week_start_date
  ) _dbt_internal_unit_test_actual
),
-- Build expected result
dbt_internal_unit_test_expected as (
  select
    week_start_date, number_of_shares_traded, number_of_trades, dollars_collected_by_treasury, irs_tax_dollars_collected_by_treasury, 'expected' as "actual_or_expected"
  from (
    select 
    
        try_cast('2020-11-15' as DATE)
     as week_start_date, 
    
        try_cast('800' as FLOAT)
     as number_of_shares_traded, 
    
        try_cast('120' as NUMBER(38,0))
     as number_of_trades, 
    
        try_cast('8000' as NUMBER(31,0))
     as dollars_collected_by_treasury, 
    
        try_cast('6000' as NUMBER(31,0))
     as irs_tax_dollars_collected_by_treasury
  ) _dbt_internal_unit_test_expected
)
-- Union actual and expected results
select * from dbt_internal_unit_test_actual
union all
select * from dbt_internal_unit_test_expected
[0m15:36:59.043287 [debug] [Thread-1 (]: SQL status: SUCCESS 2 in 0.246 seconds
[0m15:36:59.051120 [debug] [Thread-1 (]: Applying DROP to: DATAEXPERT_STUDENT.andres.test_amount_sum__dbt_tmp
[0m15:36:59.056970 [debug] [Thread-1 (]: Using snowflake connection "unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum"
[0m15:36:59.057432 [debug] [Thread-1 (]: On unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum"} */
drop table if exists DATAEXPERT_STUDENT.andres.test_amount_sum__dbt_tmp cascade
[0m15:36:59.203091 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 0.145 seconds
[0m15:36:59.207142 [debug] [Thread-1 (]: On unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum: Close
[0m15:36:59.441211 [error] [Thread-1 (]: 4 of 5 FAIL 1 weekly_metrics::test_amount_sum .................................. [[31mFAIL 1[0m in 2.74s]
[0m15:36:59.442134 [debug] [Thread-1 (]: Finished running node unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum
[0m15:36:59.442809 [debug] [Thread-1 (]: Marking all children of 'unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum' to be skipped because of status 'fail'.  Reason: 

[32mactual[0m differs from [31mexpected[0m:

[0;1m@@[0m[33m,[0m[0;1mWEEK_START_DATE[0m[33m,[0m[0;1mNUMBER_OF_SHARES_TRADED[0m[33m,[0m[0;1m...[0m[33m,[0m[0;1mDOLLARS_COLLECTED_BY_TREASURY[0m[33m,[0m[0;1mIRS_TAX_DOLLARS_COLLECTED_BY_TREASURY[0m
[31;1m[34;1m→[32;1m[0m [33m,[0m2020-11-15     [33m,[0m800.0                  [33m,[0m...[33m,[0m8000                         [33m,[0m[31;1m6000[34;1m→[32;1m0[0m

.
[0m15:36:59.443619 [debug] [Thread-1 (]: Began running node model.dbt_capstone_andres.weekly_metrics
[0m15:36:59.444148 [info ] [Thread-1 (]: 5 of 5 SKIP relation andres.weekly_metrics ..................................... [[33mSKIP[0m]
[0m15:36:59.444700 [debug] [Thread-1 (]: Finished running node model.dbt_capstone_andres.weekly_metrics
[0m15:36:59.445200 [debug] [Thread-4 (]: Marking all children of 'model.dbt_capstone_andres.weekly_metrics' to be skipped because of status 'skipped'. 
[0m15:36:59.446598 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:36:59.447028 [debug] [MainThread]: Connection 'unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum' was properly closed.
[0m15:36:59.447519 [info ] [MainThread]: 
[0m15:36:59.447966 [info ] [MainThread]: Finished running 4 table models, 1 unit test in 0 hours 0 minutes and 12.17 seconds (12.17s).
[0m15:36:59.449623 [debug] [MainThread]: Command end result
[0m15:36:59.501328 [info ] [MainThread]: 
[0m15:36:59.501931 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successs, and 0 warnings:[0m
[0m15:36:59.502478 [info ] [MainThread]: 
[0m15:36:59.503269 [error] [MainThread]: [31mFailure in unit_test test_amount_sum (models/marts/_marts.yml)[0m
[0m15:36:59.503791 [error] [MainThread]:   

[32mactual[0m differs from [31mexpected[0m:

[0;1m@@[0m[33m,[0m[0;1mWEEK_START_DATE[0m[33m,[0m[0;1mNUMBER_OF_SHARES_TRADED[0m[33m,[0m[0;1m...[0m[33m,[0m[0;1mDOLLARS_COLLECTED_BY_TREASURY[0m[33m,[0m[0;1mIRS_TAX_DOLLARS_COLLECTED_BY_TREASURY[0m
[31;1m[34;1m→[32;1m[0m [33m,[0m2020-11-15     [33m,[0m800.0                  [33m,[0m...[33m,[0m8000                         [33m,[0m[31;1m6000[34;1m→[32;1m0[0m


[0m15:36:59.504233 [info ] [MainThread]: 
[0m15:36:59.504698 [info ] [MainThread]:   compiled code at target/compiled/dbt_capstone_andres/models/marts/_marts.yml/models/marts/test_amount_sum.sql
[0m15:36:59.505073 [info ] [MainThread]: 
[0m15:36:59.505458 [info ] [MainThread]: Done. PASS=3 WARN=0 ERROR=1 SKIP=1 TOTAL=5
[0m15:36:59.508444 [debug] [MainThread]: Resource report: {"command_name": "build", "command_wall_clock_time": 14.6689005, "process_user_time": 3.796706, "process_kernel_time": 0.694323, "process_mem_max_rss": "198152192", "command_success": false, "process_in_blocks": "0", "process_out_blocks": "0"}
[0m15:36:59.509185 [debug] [MainThread]: Command `dbt build` failed at 15:36:59.509066 after 14.67 seconds
[0m15:36:59.509658 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1077a4b50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103b949d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1057799d0>]}
[0m15:36:59.510124 [debug] [MainThread]: Flushing usage events
[0m15:36:59.800804 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:40:21.008596 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d7e8710>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d85a590>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d85ae50>]}


============================== 15:40:21.014738 | 15d9118c-5c6a-4b5e-81e0-dd8220dde405 ==============================
[0m15:40:21.014738 [info ] [MainThread]: Running with dbt=1.9.0-b3
[0m15:40:21.015609 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'fail_fast': 'False', 'log_path': 'logs', 'version_check': 'True', 'profiles_dir': '.', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'False', 'log_format': 'default', 'static_parser': 'True', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'invocation_command': 'dbt build', 'send_anonymous_usage_stats': 'True'}
[0m15:40:22.708549 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '15d9118c-5c6a-4b5e-81e0-dd8220dde405', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d7f3b90>]}
[0m15:40:22.785913 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '15d9118c-5c6a-4b5e-81e0-dd8220dde405', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c48cf50>]}
[0m15:40:22.787769 [info ] [MainThread]: Registered adapter: snowflake=1.8.4
[0m15:40:22.998090 [debug] [MainThread]: checksum: 5f4fde221ca8627174a0341efc73099aaec05b8546408ceed6f59b1ec0755115, vars: {}, profile: , target: , version: 1.9.0b3
[0m15:40:23.313042 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m15:40:23.313723 [debug] [MainThread]: Partial parsing: updated file: dbt_capstone_andres://models/marts/_marts.yml
[0m15:40:23.445129 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '15d9118c-5c6a-4b5e-81e0-dd8220dde405', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x113f1ee90>]}
[0m15:40:23.671510 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '15d9118c-5c6a-4b5e-81e0-dd8220dde405', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d8184d0>]}
[0m15:40:23.672185 [info ] [MainThread]: Found 4 models, 2 sources, 862 macros, 1 unit test
[0m15:40:23.674513 [info ] [MainThread]: 
[0m15:40:23.674925 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m15:40:23.675289 [info ] [MainThread]: 
[0m15:40:23.675846 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m15:40:23.680875 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_DATAEXPERT_STUDENT'
[0m15:40:23.695603 [debug] [ThreadPool]: Using snowflake connection "list_DATAEXPERT_STUDENT"
[0m15:40:23.696089 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "connection_name": "list_DATAEXPERT_STUDENT"} */
show terse schemas in database DATAEXPERT_STUDENT
    limit 10000
[0m15:40:23.696457 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:40:24.693195 [debug] [ThreadPool]: SQL status: SUCCESS 73 in 0.997 seconds
[0m15:40:24.697041 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT: Close
[0m15:40:24.916707 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_DATAEXPERT_STUDENT, now list_DATAEXPERT_STUDENT_andres)
[0m15:40:24.924996 [debug] [ThreadPool]: Using snowflake connection "list_DATAEXPERT_STUDENT_andres"
[0m15:40:24.925384 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "connection_name": "list_DATAEXPERT_STUDENT_andres"} */
show objects in DATAEXPERT_STUDENT.andres limit 10000
[0m15:40:24.925691 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:40:25.591210 [debug] [ThreadPool]: SQL status: SUCCESS 25 in 0.665 seconds
[0m15:40:25.594276 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: Close
[0m15:40:25.831535 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '15d9118c-5c6a-4b5e-81e0-dd8220dde405', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b95ad90>]}
[0m15:40:25.835686 [debug] [Thread-1 (]: Began running node model.dbt_capstone_andres.dim_date
[0m15:40:25.836504 [info ] [Thread-1 (]: 1 of 5 START sql table model andres.dim_date ................................... [RUN]
[0m15:40:25.837111 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_DATAEXPERT_STUDENT_andres, now model.dbt_capstone_andres.dim_date)
[0m15:40:25.837591 [debug] [Thread-1 (]: Began compiling node model.dbt_capstone_andres.dim_date
[0m15:40:25.881236 [debug] [Thread-1 (]: Using snowflake connection "model.dbt_capstone_andres.dim_date"
[0m15:40:25.881891 [debug] [Thread-1 (]: On model.dbt_capstone_andres.dim_date: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "model.dbt_capstone_andres.dim_date"} */
select datediff(
        day,
        cast('1995-01-01' as timestamp),
        cast('2040-12-31' as timestamp)
        )
[0m15:40:25.882404 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:40:26.527729 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 0.645 seconds
[0m15:40:26.606858 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_capstone_andres.dim_date"
[0m15:40:26.609904 [debug] [Thread-1 (]: Began executing node model.dbt_capstone_andres.dim_date
[0m15:40:26.636453 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_capstone_andres.dim_date"
[0m15:40:26.643385 [debug] [Thread-1 (]: Using snowflake connection "model.dbt_capstone_andres.dim_date"
[0m15:40:26.644052 [debug] [Thread-1 (]: On model.dbt_capstone_andres.dim_date: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "model.dbt_capstone_andres.dim_date"} */
create or replace transient table DATAEXPERT_STUDENT.andres.dim_date
         as
        (

    
with base_dates as (
    
    with date_spine as
(

    





with rawdata as (

    

    

    with p as (
        select 0 as generated_number union all select 1
    ), unioned as (

    select

    
    p0.generated_number * power(2, 0)
     + 
    
    p1.generated_number * power(2, 1)
     + 
    
    p2.generated_number * power(2, 2)
     + 
    
    p3.generated_number * power(2, 3)
     + 
    
    p4.generated_number * power(2, 4)
     + 
    
    p5.generated_number * power(2, 5)
     + 
    
    p6.generated_number * power(2, 6)
     + 
    
    p7.generated_number * power(2, 7)
     + 
    
    p8.generated_number * power(2, 8)
     + 
    
    p9.generated_number * power(2, 9)
     + 
    
    p10.generated_number * power(2, 10)
     + 
    
    p11.generated_number * power(2, 11)
     + 
    
    p12.generated_number * power(2, 12)
     + 
    
    p13.generated_number * power(2, 13)
     + 
    
    p14.generated_number * power(2, 14)
    
    
    + 1
    as generated_number

    from

    
    p as p0
     cross join 
    
    p as p1
     cross join 
    
    p as p2
     cross join 
    
    p as p3
     cross join 
    
    p as p4
     cross join 
    
    p as p5
     cross join 
    
    p as p6
     cross join 
    
    p as p7
     cross join 
    
    p as p8
     cross join 
    
    p as p9
     cross join 
    
    p as p10
     cross join 
    
    p as p11
     cross join 
    
    p as p12
     cross join 
    
    p as p13
     cross join 
    
    p as p14
    
    

    )

    select *
    from unioned
    where generated_number <= 16801
    order by generated_number



),

all_periods as (

    select (
        

    dateadd(
        day,
        (row_number() over (order by 1) - 1),
        cast('1995-01-01' as timestamp)
        )


    ) as date_day
    from rawdata

),

filtered as (

    select *
    from all_periods
    where date_day <= cast('2040-12-31' as timestamp)

)

select * from filtered



)
select
    cast(d.date_day as timestamp) as date_day
from
    date_spine d


),
dates_with_prior_year_dates as (

    select
        cast(d.date_day as date) as date_day,
        cast(

    dateadd(
        year,
        -1,
        d.date_day
        )

 as date) as prior_year_date_day,
        cast(

    dateadd(
        day,
        -364,
        d.date_day
        )

 as date) as prior_year_over_year_date_day
    from
    	base_dates d

)
select
    d.date_day,
    cast(

    dateadd(
        day,
        -1,
        d.date_day
        )

 as date) as prior_date_day,
    cast(

    dateadd(
        day,
        1,
        d.date_day
        )

 as date) as next_date_day,
    d.prior_year_date_day as prior_year_date_day,
    d.prior_year_over_year_date_day,
    case
            when date_part('dayofweek', d.date_day) = 7 then 1
            else date_part('dayofweek', d.date_day) + 1
        end as day_of_week,
    date_part('dayofweekiso', d.date_day) as day_of_week_iso,
    -- long version not implemented on Snowflake so we're doing it manually :/
    case dayname(d.date_day)
        when 'Mon' then 'Monday'
        when 'Tue' then 'Tuesday'
        when 'Wed' then 'Wednesday'
        when 'Thu' then 'Thursday'
        when 'Fri' then 'Friday'
        when 'Sat' then 'Saturday'
        when 'Sun' then 'Sunday'
    end as day_of_week_name,
    dayname(d.date_day) as day_of_week_name_short,
    date_part('day', d.date_day) as day_of_month,
    date_part('dayofyear', d.date_day) as day_of_year,

    
    
    cast(

    dateadd(
        day,
        -1 * (case
            when date_part('dayofweek', d.date_day) = 7 then 1
            else date_part('dayofweek', d.date_day) + 1
        end - 1),
        d.date_day
        )

 as date) as week_start_date,
    cast(

    dateadd(
        day,
        6,
        
    
    cast(

    dateadd(
        day,
        -1 * (case
            when date_part('dayofweek', d.date_day) = 7 then 1
            else date_part('dayofweek', d.date_day) + 1
        end - 1),
        d.date_day
        )

 as date)
        )

 as date) as week_end_date,
    
    
    cast(

    dateadd(
        day,
        -1 * (case
            when date_part('dayofweek', d.prior_year_over_year_date_day) = 7 then 1
            else date_part('dayofweek', d.prior_year_over_year_date_day) + 1
        end - 1),
        d.prior_year_over_year_date_day
        )

 as date) as prior_year_week_start_date,
    cast(

    dateadd(
        day,
        6,
        
    
    cast(

    dateadd(
        day,
        -1 * (case
            when date_part('dayofweek', d.prior_year_over_year_date_day) = 7 then 1
            else date_part('dayofweek', d.prior_year_over_year_date_day) + 1
        end - 1),
        d.prior_year_over_year_date_day
        )

 as date)
        )

 as date) as prior_year_week_end_date,
    cast(date_part('week', d.date_day) as integer) as week_of_year,

    cast(date_trunc('week', d.date_day) as date) as iso_week_start_date,
    cast(

    dateadd(
        day,
        6,
        cast(date_trunc('week', d.date_day) as date)
        )

 as date) as iso_week_end_date,
    cast(date_trunc('week', d.prior_year_over_year_date_day) as date) as prior_year_iso_week_start_date,
    cast(

    dateadd(
        day,
        6,
        cast(date_trunc('week', d.prior_year_over_year_date_day) as date)
        )

 as date) as prior_year_iso_week_end_date,
    cast(date_part('weekiso', d.date_day) as integer) as iso_week_of_year,

    cast(date_part('week', d.prior_year_over_year_date_day) as integer) as prior_year_week_of_year,
    cast(date_part('weekiso', d.prior_year_over_year_date_day) as integer) as prior_year_iso_week_of_year,

    cast(date_part('month', d.date_day) as integer) as month_of_year,
    to_char(d.date_day, 'MMMM')  as month_name,
    to_char(d.date_day, 'MON')  as month_name_short,

    cast(date_trunc('month', d.date_day) as date) as month_start_date,
    cast(cast(
        

    dateadd(
        day,
        -1,
        

    dateadd(
        month,
        1,
        date_trunc('month', d.date_day)
        )


        )


        as date) as date) as month_end_date,

    cast(date_trunc('month', d.prior_year_date_day) as date) as prior_year_month_start_date,
    cast(cast(
        

    dateadd(
        day,
        -1,
        

    dateadd(
        month,
        1,
        date_trunc('month', d.prior_year_date_day)
        )


        )


        as date) as date) as prior_year_month_end_date,

    cast(date_part('quarter', d.date_day) as integer) as quarter_of_year,
    cast(date_trunc('quarter', d.date_day) as date) as quarter_start_date,
    cast(cast(
        

    dateadd(
        day,
        -1,
        

    dateadd(
        quarter,
        1,
        date_trunc('quarter', d.date_day)
        )


        )


        as date) as date) as quarter_end_date,

    cast(date_part('year', d.date_day) as integer) as year_number,
    cast(date_trunc('year', d.date_day) as date) as year_start_date,
    cast(cast(
        

    dateadd(
        day,
        -1,
        

    dateadd(
        year,
        1,
        date_trunc('year', d.date_day)
        )


        )


        as date) as date) as year_end_date
from
    dates_with_prior_year_dates d
order by 1


        );
[0m15:40:27.801058 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 1.156 seconds
[0m15:40:27.828526 [debug] [Thread-1 (]: On model.dbt_capstone_andres.dim_date: Close
[0m15:40:28.052833 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '15d9118c-5c6a-4b5e-81e0-dd8220dde405', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x117e60350>]}
[0m15:40:28.053852 [info ] [Thread-1 (]: 1 of 5 OK created sql table model andres.dim_date .............................. [[32mSUCCESS 1[0m in 2.21s]
[0m15:40:28.054772 [debug] [Thread-1 (]: Finished running node model.dbt_capstone_andres.dim_date
[0m15:40:28.055402 [debug] [Thread-1 (]: Began running node model.dbt_capstone_andres.stg_daily_bars
[0m15:40:28.056217 [info ] [Thread-1 (]: 2 of 5 START sql table model andres.stg_daily_bars ............................. [RUN]
[0m15:40:28.056848 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_capstone_andres.dim_date, now model.dbt_capstone_andres.stg_daily_bars)
[0m15:40:28.057365 [debug] [Thread-1 (]: Began compiling node model.dbt_capstone_andres.stg_daily_bars
[0m15:40:28.061676 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_capstone_andres.stg_daily_bars"
[0m15:40:28.062979 [debug] [Thread-1 (]: Began executing node model.dbt_capstone_andres.stg_daily_bars
[0m15:40:28.067481 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_capstone_andres.stg_daily_bars"
[0m15:40:28.069326 [debug] [Thread-1 (]: Using snowflake connection "model.dbt_capstone_andres.stg_daily_bars"
[0m15:40:28.069887 [debug] [Thread-1 (]: On model.dbt_capstone_andres.stg_daily_bars: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "model.dbt_capstone_andres.stg_daily_bars"} */
create or replace transient table DATAEXPERT_STUDENT.andres.stg_daily_bars
         as
        (with source as (

    select * from dataexpert_student.andres.daily_bars

),

renamed as (

    select
        TO_DATE(TO_TIMESTAMP(timestamp / 1000)) as date,
        ticker as ticker,
        v  as number_of_shares_traded,
        n  as number_of_trades,
        round(vw,2) as volume_weighted_avg_price,
        o  as open_price,
        c  as close_price,
        h  as high_price,
        l  as low_price
    from source

)

select * from renamed
        );
[0m15:40:28.070390 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:40:30.291873 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 2.221 seconds
[0m15:40:30.293939 [debug] [Thread-1 (]: On model.dbt_capstone_andres.stg_daily_bars: Close
[0m15:40:30.520828 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '15d9118c-5c6a-4b5e-81e0-dd8220dde405', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x117f5e550>]}
[0m15:40:30.521546 [info ] [Thread-1 (]: 2 of 5 OK created sql table model andres.stg_daily_bars ........................ [[32mSUCCESS 1[0m in 2.46s]
[0m15:40:30.522317 [debug] [Thread-1 (]: Finished running node model.dbt_capstone_andres.stg_daily_bars
[0m15:40:30.522916 [debug] [Thread-1 (]: Began running node model.dbt_capstone_andres.stg_treasury_revenue_collections
[0m15:40:30.523761 [info ] [Thread-1 (]: 3 of 5 START sql table model andres.stg_treasury_revenue_collections ........... [RUN]
[0m15:40:30.524331 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_capstone_andres.stg_daily_bars, now model.dbt_capstone_andres.stg_treasury_revenue_collections)
[0m15:40:30.524938 [debug] [Thread-1 (]: Began compiling node model.dbt_capstone_andres.stg_treasury_revenue_collections
[0m15:40:30.528493 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_capstone_andres.stg_treasury_revenue_collections"
[0m15:40:30.531211 [debug] [Thread-1 (]: Began executing node model.dbt_capstone_andres.stg_treasury_revenue_collections
[0m15:40:30.535363 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_capstone_andres.stg_treasury_revenue_collections"
[0m15:40:30.536781 [debug] [Thread-1 (]: Using snowflake connection "model.dbt_capstone_andres.stg_treasury_revenue_collections"
[0m15:40:30.537225 [debug] [Thread-1 (]: On model.dbt_capstone_andres.stg_treasury_revenue_collections: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "model.dbt_capstone_andres.stg_treasury_revenue_collections"} */
create or replace transient table DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections
         as
        (with source as (

    select * from dataexpert_student.andres.fiscal_data

),

renamed as (

    select
        record_date as collection_date,
        channel_type_desc as channel_type,
        tax_category_desc as tax_category,
        round(net_collections_amt,0) as dollars_collected_by_treasury

    from source

)

select * from renamed
        );
[0m15:40:30.537639 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:40:31.868049 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 1.330 seconds
[0m15:40:31.870046 [debug] [Thread-1 (]: On model.dbt_capstone_andres.stg_treasury_revenue_collections: Close
[0m15:40:32.083452 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '15d9118c-5c6a-4b5e-81e0-dd8220dde405', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x113bb8a10>]}
[0m15:40:32.084146 [info ] [Thread-1 (]: 3 of 5 OK created sql table model andres.stg_treasury_revenue_collections ...... [[32mSUCCESS 1[0m in 1.56s]
[0m15:40:32.084820 [debug] [Thread-1 (]: Finished running node model.dbt_capstone_andres.stg_treasury_revenue_collections
[0m15:40:32.085679 [debug] [Thread-1 (]: Began running node unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum
[0m15:40:32.086104 [info ] [Thread-1 (]: 4 of 5 START unit_test weekly_metrics::test_amount_sum ......................... [RUN]
[0m15:40:32.086592 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_capstone_andres.stg_treasury_revenue_collections, now unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum)
[0m15:40:32.086948 [debug] [Thread-1 (]: Began compiling node unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum
[0m15:40:32.087328 [debug] [Thread-1 (]: Began executing node unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum
[0m15:40:32.133413 [debug] [Thread-1 (]: Using snowflake connection "unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum"
[0m15:40:32.133923 [debug] [Thread-1 (]: On unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum"} */
describe table DATAEXPERT_STUDENT.andres.stg_daily_bars
[0m15:40:32.134322 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:40:32.858548 [debug] [Thread-1 (]: SQL status: SUCCESS 9 in 0.724 seconds
[0m15:40:32.884730 [debug] [Thread-1 (]: Writing injected SQL for node "unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum"
[0m15:40:32.890430 [debug] [Thread-1 (]: Using snowflake connection "unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum"
[0m15:40:32.890897 [debug] [Thread-1 (]: On unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum"} */
describe table DATAEXPERT_STUDENT.andres.dim_date
[0m15:40:33.047889 [debug] [Thread-1 (]: SQL status: SUCCESS 36 in 0.157 seconds
[0m15:40:33.060657 [debug] [Thread-1 (]: Writing injected SQL for node "unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum"
[0m15:40:33.068862 [debug] [Thread-1 (]: Using snowflake connection "unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum"
[0m15:40:33.069372 [debug] [Thread-1 (]: On unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum"} */
describe table DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections
[0m15:40:33.200903 [debug] [Thread-1 (]: SQL status: SUCCESS 4 in 0.131 seconds
[0m15:40:33.210174 [debug] [Thread-1 (]: Writing injected SQL for node "unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum"
[0m15:40:33.218363 [debug] [Thread-1 (]: Writing injected SQL for node "unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum"
[0m15:40:33.248822 [debug] [Thread-1 (]: Using snowflake connection "unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum"
[0m15:40:33.249721 [debug] [Thread-1 (]: On unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum"} */
create or replace temporary table DATAEXPERT_STUDENT.andres.test_amount_sum__dbt_tmp
         as
        (select * from (
        with  __dbt__cte__stg_daily_bars as (

-- Fixture for stg_daily_bars
select 
    
        try_cast('2020-11-20' as DATE)
     as date, 
    
        try_cast('\'L\'' as character varying(16777216))
     as ticker, 
    
        try_cast('100' as FLOAT)
     as number_of_shares_traded, 
    
        try_cast('10' as NUMBER(38,0))
     as number_of_trades, try_cast(null as FLOAT) as volume_weighted_avg_price, try_cast(null as FLOAT) as open_price, try_cast(null as FLOAT) as close_price, try_cast(null as FLOAT) as high_price, try_cast(null as FLOAT) as low_price
union all
select 
    
        try_cast('2020-11-20' as DATE)
     as date, 
    
        try_cast('\'S\'' as character varying(16777216))
     as ticker, 
    
        try_cast('200' as FLOAT)
     as number_of_shares_traded, 
    
        try_cast('20' as NUMBER(38,0))
     as number_of_trades, try_cast(null as FLOAT) as volume_weighted_avg_price, try_cast(null as FLOAT) as open_price, try_cast(null as FLOAT) as close_price, try_cast(null as FLOAT) as high_price, try_cast(null as FLOAT) as low_price
union all
select 
    
        try_cast('2020-11-21' as DATE)
     as date, 
    
        try_cast('\'L\'' as character varying(16777216))
     as ticker, 
    
        try_cast('300' as FLOAT)
     as number_of_shares_traded, 
    
        try_cast('40' as NUMBER(38,0))
     as number_of_trades, try_cast(null as FLOAT) as volume_weighted_avg_price, try_cast(null as FLOAT) as open_price, try_cast(null as FLOAT) as close_price, try_cast(null as FLOAT) as high_price, try_cast(null as FLOAT) as low_price
union all
select 
    
        try_cast('2020-11-21' as DATE)
     as date, 
    
        try_cast('\'S\'' as character varying(16777216))
     as ticker, 
    
        try_cast('200' as FLOAT)
     as number_of_shares_traded, 
    
        try_cast('50' as NUMBER(38,0))
     as number_of_trades, try_cast(null as FLOAT) as volume_weighted_avg_price, try_cast(null as FLOAT) as open_price, try_cast(null as FLOAT) as close_price, try_cast(null as FLOAT) as high_price, try_cast(null as FLOAT) as low_price
),  __dbt__cte__dim_date as (

-- Fixture for dim_date
select 
    
        try_cast('2020-11-20' as DATE)
     as date_day, try_cast(null as DATE) as prior_date_day, try_cast(null as DATE) as next_date_day, try_cast(null as DATE) as prior_year_date_day, try_cast(null as DATE) as prior_year_over_year_date_day, try_cast(null as NUMBER(3,0)) as day_of_week, try_cast(null as NUMBER(2,0)) as day_of_week_iso, try_cast(null as character varying(16777216)) as day_of_week_name, try_cast(null as character varying(3)) as day_of_week_name_short, try_cast(null as NUMBER(2,0)) as day_of_month, try_cast(null as NUMBER(4,0)) as day_of_year, 
    
        try_cast(' 2020-11-15' as DATE)
     as week_start_date, try_cast(null as DATE) as week_end_date, try_cast(null as DATE) as prior_year_week_start_date, try_cast(null as DATE) as prior_year_week_end_date, try_cast(null as NUMBER(38,0)) as week_of_year, try_cast(null as DATE) as iso_week_start_date, try_cast(null as DATE) as iso_week_end_date, try_cast(null as DATE) as prior_year_iso_week_start_date, try_cast(null as DATE) as prior_year_iso_week_end_date, try_cast(null as NUMBER(38,0)) as iso_week_of_year, try_cast(null as NUMBER(38,0)) as prior_year_week_of_year, try_cast(null as NUMBER(38,0)) as prior_year_iso_week_of_year, try_cast(null as NUMBER(38,0)) as month_of_year, try_cast(null as character varying(16777216)) as month_name, try_cast(null as character varying(16777216)) as month_name_short, try_cast(null as DATE) as month_start_date, try_cast(null as DATE) as month_end_date, try_cast(null as DATE) as prior_year_month_start_date, try_cast(null as DATE) as prior_year_month_end_date, try_cast(null as NUMBER(38,0)) as quarter_of_year, try_cast(null as DATE) as quarter_start_date, try_cast(null as DATE) as quarter_end_date, try_cast(null as NUMBER(38,0)) as year_number, try_cast(null as DATE) as year_start_date, try_cast(null as DATE) as year_end_date
union all
select 
    
        try_cast('2020-11-21' as DATE)
     as date_day, try_cast(null as DATE) as prior_date_day, try_cast(null as DATE) as next_date_day, try_cast(null as DATE) as prior_year_date_day, try_cast(null as DATE) as prior_year_over_year_date_day, try_cast(null as NUMBER(3,0)) as day_of_week, try_cast(null as NUMBER(2,0)) as day_of_week_iso, try_cast(null as character varying(16777216)) as day_of_week_name, try_cast(null as character varying(3)) as day_of_week_name_short, try_cast(null as NUMBER(2,0)) as day_of_month, try_cast(null as NUMBER(4,0)) as day_of_year, 
    
        try_cast(' 2020-11-15' as DATE)
     as week_start_date, try_cast(null as DATE) as week_end_date, try_cast(null as DATE) as prior_year_week_start_date, try_cast(null as DATE) as prior_year_week_end_date, try_cast(null as NUMBER(38,0)) as week_of_year, try_cast(null as DATE) as iso_week_start_date, try_cast(null as DATE) as iso_week_end_date, try_cast(null as DATE) as prior_year_iso_week_start_date, try_cast(null as DATE) as prior_year_iso_week_end_date, try_cast(null as NUMBER(38,0)) as iso_week_of_year, try_cast(null as NUMBER(38,0)) as prior_year_week_of_year, try_cast(null as NUMBER(38,0)) as prior_year_iso_week_of_year, try_cast(null as NUMBER(38,0)) as month_of_year, try_cast(null as character varying(16777216)) as month_name, try_cast(null as character varying(16777216)) as month_name_short, try_cast(null as DATE) as month_start_date, try_cast(null as DATE) as month_end_date, try_cast(null as DATE) as prior_year_month_start_date, try_cast(null as DATE) as prior_year_month_end_date, try_cast(null as NUMBER(38,0)) as quarter_of_year, try_cast(null as DATE) as quarter_start_date, try_cast(null as DATE) as quarter_end_date, try_cast(null as NUMBER(38,0)) as year_number, try_cast(null as DATE) as year_start_date, try_cast(null as DATE) as year_end_date
),  __dbt__cte__stg_treasury_revenue_collections as (

-- Fixture for stg_treasury_revenue_collections
select 
    
        try_cast('2020-11-20' as DATE)
     as collection_date, try_cast(null as character varying(16777216)) as channel_type, 
    
        try_cast('IRS Tax' as character varying(16777216))
     as tax_category, 
    
        try_cast('1000' as NUMBER(19,0))
     as dollars_collected_by_treasury
union all
select 
    
        try_cast('2020-11-20' as DATE)
     as collection_date, try_cast(null as character varying(16777216)) as channel_type, 
    
        try_cast('IRS Non Tax' as character varying(16777216))
     as tax_category, 
    
        try_cast('2000' as NUMBER(19,0))
     as dollars_collected_by_treasury
union all
select 
    
        try_cast('2020-11-21' as DATE)
     as collection_date, try_cast(null as character varying(16777216)) as channel_type, 
    
        try_cast('IRS Tax' as character varying(16777216))
     as tax_category, 
    
        try_cast('3000' as NUMBER(19,0))
     as dollars_collected_by_treasury
union all
select 
    
        try_cast('2020-11-21' as DATE)
     as collection_date, try_cast(null as character varying(16777216)) as channel_type, 
    
        try_cast('IRS Tax' as character varying(16777216))
     as tax_category, 
    
        try_cast('2000' as NUMBER(19,0))
     as dollars_collected_by_treasury
), pol as (
SELECT week_start_date, 
sum(number_of_shares_traded) number_of_shares_traded,
sum(number_of_trades) number_of_trades
from  __dbt__cte__stg_daily_bars a 
JOIN __dbt__cte__dim_date d on d.date_day=a.date
group by 1)

, rev as (
select week_start_date,
sum(dollars_collected_by_treasury) dollars_collected_by_treasury,
sum(case when tax_category = 'IRS Tax' then dollars_collected_by_treasury else 0 end) as irs_tax_dollars_collected_by_treasury, 
sum(case when tax_category = 'Non-Tax' then dollars_collected_by_treasury else 0 end) as non_irs_non_tax_dollars_collected_by_treasury,
sum(case when tax_category = 'IRS Non-Tax' then dollars_collected_by_treasury else 0 end) as irs_non_tax_dollars_collected_by_treasury
FROM __dbt__cte__stg_treasury_revenue_collections a 
JOIN __dbt__cte__dim_date d on d.date_day=a.collection_date
GROUP BY ALL)

select 
    a.week_start_date, 
    a.number_of_shares_traded, 
    a.number_of_trades,
    b.dollars_collected_by_treasury,
    b.irs_tax_dollars_collected_by_treasury

from pol a 
INNER JOIN rev b on b.week_start_date = a.week_start_date
    ) as __dbt_sbq
    where false
    limit 0

        );
[0m15:40:33.827265 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 0.577 seconds
[0m15:40:33.830687 [debug] [Thread-1 (]: Using snowflake connection "unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum"
[0m15:40:33.831056 [debug] [Thread-1 (]: On unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum"} */
describe table DATAEXPERT_STUDENT.andres.test_amount_sum__dbt_tmp
[0m15:40:33.984415 [debug] [Thread-1 (]: SQL status: SUCCESS 5 in 0.153 seconds
[0m15:40:33.994881 [debug] [Thread-1 (]: Writing runtime sql for node "unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum"
[0m15:40:34.005357 [debug] [Thread-1 (]: Using snowflake connection "unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum"
[0m15:40:34.006126 [debug] [Thread-1 (]: On unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum"} */
-- Build actual result given inputs
with dbt_internal_unit_test_actual as (
  select
    week_start_date,number_of_shares_traded,number_of_trades,dollars_collected_by_treasury,irs_tax_dollars_collected_by_treasury, 'actual' as "actual_or_expected"
  from (
    with  __dbt__cte__stg_daily_bars as (

-- Fixture for stg_daily_bars
select 
    
        try_cast('2020-11-20' as DATE)
     as date, 
    
        try_cast('\'L\'' as character varying(16777216))
     as ticker, 
    
        try_cast('100' as FLOAT)
     as number_of_shares_traded, 
    
        try_cast('10' as NUMBER(38,0))
     as number_of_trades, try_cast(null as FLOAT) as volume_weighted_avg_price, try_cast(null as FLOAT) as open_price, try_cast(null as FLOAT) as close_price, try_cast(null as FLOAT) as high_price, try_cast(null as FLOAT) as low_price
union all
select 
    
        try_cast('2020-11-20' as DATE)
     as date, 
    
        try_cast('\'S\'' as character varying(16777216))
     as ticker, 
    
        try_cast('200' as FLOAT)
     as number_of_shares_traded, 
    
        try_cast('20' as NUMBER(38,0))
     as number_of_trades, try_cast(null as FLOAT) as volume_weighted_avg_price, try_cast(null as FLOAT) as open_price, try_cast(null as FLOAT) as close_price, try_cast(null as FLOAT) as high_price, try_cast(null as FLOAT) as low_price
union all
select 
    
        try_cast('2020-11-21' as DATE)
     as date, 
    
        try_cast('\'L\'' as character varying(16777216))
     as ticker, 
    
        try_cast('300' as FLOAT)
     as number_of_shares_traded, 
    
        try_cast('40' as NUMBER(38,0))
     as number_of_trades, try_cast(null as FLOAT) as volume_weighted_avg_price, try_cast(null as FLOAT) as open_price, try_cast(null as FLOAT) as close_price, try_cast(null as FLOAT) as high_price, try_cast(null as FLOAT) as low_price
union all
select 
    
        try_cast('2020-11-21' as DATE)
     as date, 
    
        try_cast('\'S\'' as character varying(16777216))
     as ticker, 
    
        try_cast('200' as FLOAT)
     as number_of_shares_traded, 
    
        try_cast('50' as NUMBER(38,0))
     as number_of_trades, try_cast(null as FLOAT) as volume_weighted_avg_price, try_cast(null as FLOAT) as open_price, try_cast(null as FLOAT) as close_price, try_cast(null as FLOAT) as high_price, try_cast(null as FLOAT) as low_price
),  __dbt__cte__dim_date as (

-- Fixture for dim_date
select 
    
        try_cast('2020-11-20' as DATE)
     as date_day, try_cast(null as DATE) as prior_date_day, try_cast(null as DATE) as next_date_day, try_cast(null as DATE) as prior_year_date_day, try_cast(null as DATE) as prior_year_over_year_date_day, try_cast(null as NUMBER(3,0)) as day_of_week, try_cast(null as NUMBER(2,0)) as day_of_week_iso, try_cast(null as character varying(16777216)) as day_of_week_name, try_cast(null as character varying(3)) as day_of_week_name_short, try_cast(null as NUMBER(2,0)) as day_of_month, try_cast(null as NUMBER(4,0)) as day_of_year, 
    
        try_cast(' 2020-11-15' as DATE)
     as week_start_date, try_cast(null as DATE) as week_end_date, try_cast(null as DATE) as prior_year_week_start_date, try_cast(null as DATE) as prior_year_week_end_date, try_cast(null as NUMBER(38,0)) as week_of_year, try_cast(null as DATE) as iso_week_start_date, try_cast(null as DATE) as iso_week_end_date, try_cast(null as DATE) as prior_year_iso_week_start_date, try_cast(null as DATE) as prior_year_iso_week_end_date, try_cast(null as NUMBER(38,0)) as iso_week_of_year, try_cast(null as NUMBER(38,0)) as prior_year_week_of_year, try_cast(null as NUMBER(38,0)) as prior_year_iso_week_of_year, try_cast(null as NUMBER(38,0)) as month_of_year, try_cast(null as character varying(16777216)) as month_name, try_cast(null as character varying(16777216)) as month_name_short, try_cast(null as DATE) as month_start_date, try_cast(null as DATE) as month_end_date, try_cast(null as DATE) as prior_year_month_start_date, try_cast(null as DATE) as prior_year_month_end_date, try_cast(null as NUMBER(38,0)) as quarter_of_year, try_cast(null as DATE) as quarter_start_date, try_cast(null as DATE) as quarter_end_date, try_cast(null as NUMBER(38,0)) as year_number, try_cast(null as DATE) as year_start_date, try_cast(null as DATE) as year_end_date
union all
select 
    
        try_cast('2020-11-21' as DATE)
     as date_day, try_cast(null as DATE) as prior_date_day, try_cast(null as DATE) as next_date_day, try_cast(null as DATE) as prior_year_date_day, try_cast(null as DATE) as prior_year_over_year_date_day, try_cast(null as NUMBER(3,0)) as day_of_week, try_cast(null as NUMBER(2,0)) as day_of_week_iso, try_cast(null as character varying(16777216)) as day_of_week_name, try_cast(null as character varying(3)) as day_of_week_name_short, try_cast(null as NUMBER(2,0)) as day_of_month, try_cast(null as NUMBER(4,0)) as day_of_year, 
    
        try_cast(' 2020-11-15' as DATE)
     as week_start_date, try_cast(null as DATE) as week_end_date, try_cast(null as DATE) as prior_year_week_start_date, try_cast(null as DATE) as prior_year_week_end_date, try_cast(null as NUMBER(38,0)) as week_of_year, try_cast(null as DATE) as iso_week_start_date, try_cast(null as DATE) as iso_week_end_date, try_cast(null as DATE) as prior_year_iso_week_start_date, try_cast(null as DATE) as prior_year_iso_week_end_date, try_cast(null as NUMBER(38,0)) as iso_week_of_year, try_cast(null as NUMBER(38,0)) as prior_year_week_of_year, try_cast(null as NUMBER(38,0)) as prior_year_iso_week_of_year, try_cast(null as NUMBER(38,0)) as month_of_year, try_cast(null as character varying(16777216)) as month_name, try_cast(null as character varying(16777216)) as month_name_short, try_cast(null as DATE) as month_start_date, try_cast(null as DATE) as month_end_date, try_cast(null as DATE) as prior_year_month_start_date, try_cast(null as DATE) as prior_year_month_end_date, try_cast(null as NUMBER(38,0)) as quarter_of_year, try_cast(null as DATE) as quarter_start_date, try_cast(null as DATE) as quarter_end_date, try_cast(null as NUMBER(38,0)) as year_number, try_cast(null as DATE) as year_start_date, try_cast(null as DATE) as year_end_date
),  __dbt__cte__stg_treasury_revenue_collections as (

-- Fixture for stg_treasury_revenue_collections
select 
    
        try_cast('2020-11-20' as DATE)
     as collection_date, try_cast(null as character varying(16777216)) as channel_type, 
    
        try_cast('IRS Tax' as character varying(16777216))
     as tax_category, 
    
        try_cast('1000' as NUMBER(19,0))
     as dollars_collected_by_treasury
union all
select 
    
        try_cast('2020-11-20' as DATE)
     as collection_date, try_cast(null as character varying(16777216)) as channel_type, 
    
        try_cast('IRS Non Tax' as character varying(16777216))
     as tax_category, 
    
        try_cast('2000' as NUMBER(19,0))
     as dollars_collected_by_treasury
union all
select 
    
        try_cast('2020-11-21' as DATE)
     as collection_date, try_cast(null as character varying(16777216)) as channel_type, 
    
        try_cast('IRS Tax' as character varying(16777216))
     as tax_category, 
    
        try_cast('3000' as NUMBER(19,0))
     as dollars_collected_by_treasury
union all
select 
    
        try_cast('2020-11-21' as DATE)
     as collection_date, try_cast(null as character varying(16777216)) as channel_type, 
    
        try_cast('IRS Tax' as character varying(16777216))
     as tax_category, 
    
        try_cast('2000' as NUMBER(19,0))
     as dollars_collected_by_treasury
), pol as (
SELECT week_start_date, 
sum(number_of_shares_traded) number_of_shares_traded,
sum(number_of_trades) number_of_trades
from  __dbt__cte__stg_daily_bars a 
JOIN __dbt__cte__dim_date d on d.date_day=a.date
group by 1)

, rev as (
select week_start_date,
sum(dollars_collected_by_treasury) dollars_collected_by_treasury,
sum(case when tax_category = 'IRS Tax' then dollars_collected_by_treasury else 0 end) as irs_tax_dollars_collected_by_treasury, 
sum(case when tax_category = 'Non-Tax' then dollars_collected_by_treasury else 0 end) as non_irs_non_tax_dollars_collected_by_treasury,
sum(case when tax_category = 'IRS Non-Tax' then dollars_collected_by_treasury else 0 end) as irs_non_tax_dollars_collected_by_treasury
FROM __dbt__cte__stg_treasury_revenue_collections a 
JOIN __dbt__cte__dim_date d on d.date_day=a.collection_date
GROUP BY ALL)

select 
    a.week_start_date, 
    a.number_of_shares_traded, 
    a.number_of_trades,
    b.dollars_collected_by_treasury,
    b.irs_tax_dollars_collected_by_treasury

from pol a 
INNER JOIN rev b on b.week_start_date = a.week_start_date
  ) _dbt_internal_unit_test_actual
),
-- Build expected result
dbt_internal_unit_test_expected as (
  select
    week_start_date, number_of_shares_traded, number_of_trades, dollars_collected_by_treasury, irs_tax_dollars_collected_by_treasury, 'expected' as "actual_or_expected"
  from (
    select 
    
        try_cast('2020-11-15' as DATE)
     as week_start_date, 
    
        try_cast('800' as FLOAT)
     as number_of_shares_traded, 
    
        try_cast('120' as NUMBER(38,0))
     as number_of_trades, 
    
        try_cast('8000' as NUMBER(31,0))
     as dollars_collected_by_treasury, 
    
        try_cast('6000' as NUMBER(31,0))
     as irs_tax_dollars_collected_by_treasury
  ) _dbt_internal_unit_test_expected
)
-- Union actual and expected results
select * from dbt_internal_unit_test_actual
union all
select * from dbt_internal_unit_test_expected
[0m15:40:34.281169 [debug] [Thread-1 (]: SQL status: SUCCESS 2 in 0.274 seconds
[0m15:40:34.292073 [debug] [Thread-1 (]: Applying DROP to: DATAEXPERT_STUDENT.andres.test_amount_sum__dbt_tmp
[0m15:40:34.300176 [debug] [Thread-1 (]: Using snowflake connection "unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum"
[0m15:40:34.300686 [debug] [Thread-1 (]: On unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum"} */
drop table if exists DATAEXPERT_STUDENT.andres.test_amount_sum__dbt_tmp cascade
[0m15:40:34.478864 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 0.178 seconds
[0m15:40:34.483893 [debug] [Thread-1 (]: On unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum: Close
[0m15:40:34.726780 [info ] [Thread-1 (]: 4 of 5 PASS weekly_metrics::test_amount_sum .................................... [[32mPASS[0m in 2.64s]
[0m15:40:34.727819 [debug] [Thread-1 (]: Finished running node unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum
[0m15:40:34.728529 [debug] [Thread-1 (]: Began running node model.dbt_capstone_andres.weekly_metrics
[0m15:40:34.729280 [info ] [Thread-1 (]: 5 of 5 START sql table model andres.weekly_metrics ............................. [RUN]
[0m15:40:34.729899 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly unit_test.dbt_capstone_andres.weekly_metrics.test_amount_sum, now model.dbt_capstone_andres.weekly_metrics)
[0m15:40:34.730384 [debug] [Thread-1 (]: Began compiling node model.dbt_capstone_andres.weekly_metrics
[0m15:40:34.735660 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_capstone_andres.weekly_metrics"
[0m15:40:34.738447 [debug] [Thread-1 (]: Began executing node model.dbt_capstone_andres.weekly_metrics
[0m15:40:34.742830 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_capstone_andres.weekly_metrics"
[0m15:40:34.747071 [debug] [Thread-1 (]: Using snowflake connection "model.dbt_capstone_andres.weekly_metrics"
[0m15:40:34.747719 [debug] [Thread-1 (]: On model.dbt_capstone_andres.weekly_metrics: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "model.dbt_capstone_andres.weekly_metrics"} */
create or replace transient table DATAEXPERT_STUDENT.andres.weekly_metrics
         as
        (with pol as (
SELECT week_start_date, 
sum(number_of_shares_traded) number_of_shares_traded,
sum(number_of_trades) number_of_trades
from  DATAEXPERT_STUDENT.andres.stg_daily_bars a 
JOIN DATAEXPERT_STUDENT.andres.dim_date d on d.date_day=a.date
group by 1)

, rev as (
select week_start_date,
sum(dollars_collected_by_treasury) dollars_collected_by_treasury,
sum(case when tax_category = 'IRS Tax' then dollars_collected_by_treasury else 0 end) as irs_tax_dollars_collected_by_treasury, 
sum(case when tax_category = 'Non-Tax' then dollars_collected_by_treasury else 0 end) as non_irs_non_tax_dollars_collected_by_treasury,
sum(case when tax_category = 'IRS Non-Tax' then dollars_collected_by_treasury else 0 end) as irs_non_tax_dollars_collected_by_treasury
FROM DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections a 
JOIN DATAEXPERT_STUDENT.andres.dim_date d on d.date_day=a.collection_date
GROUP BY ALL)

select 
    a.week_start_date, 
    a.number_of_shares_traded, 
    a.number_of_trades,
    b.dollars_collected_by_treasury,
    b.irs_tax_dollars_collected_by_treasury

from pol a 
INNER JOIN rev b on b.week_start_date = a.week_start_date
        );
[0m15:40:34.748266 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:40:36.752468 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 2.004 seconds
[0m15:40:36.755536 [debug] [Thread-1 (]: On model.dbt_capstone_andres.weekly_metrics: Close
[0m15:40:36.965758 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '15d9118c-5c6a-4b5e-81e0-dd8220dde405', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11e10d350>]}
[0m15:40:36.966720 [info ] [Thread-1 (]: 5 of 5 OK created sql table model andres.weekly_metrics ........................ [[32mSUCCESS 1[0m in 2.24s]
[0m15:40:36.967428 [debug] [Thread-1 (]: Finished running node model.dbt_capstone_andres.weekly_metrics
[0m15:40:36.968762 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:40:36.969180 [debug] [MainThread]: Connection 'model.dbt_capstone_andres.weekly_metrics' was properly closed.
[0m15:40:36.969645 [info ] [MainThread]: 
[0m15:40:36.970243 [info ] [MainThread]: Finished running 4 table models, 1 unit test in 0 hours 0 minutes and 13.29 seconds (13.29s).
[0m15:40:36.971715 [debug] [MainThread]: Command end result
[0m15:40:37.031664 [info ] [MainThread]: 
[0m15:40:37.032568 [info ] [MainThread]: [32mCompleted successfully[0m
[0m15:40:37.033399 [info ] [MainThread]: 
[0m15:40:37.034186 [info ] [MainThread]: Done. PASS=5 WARN=0 ERROR=0 SKIP=0 TOTAL=5
[0m15:40:37.038038 [debug] [MainThread]: Resource report: {"command_name": "build", "command_success": true, "command_wall_clock_time": 16.100153, "process_user_time": 3.819666, "process_kernel_time": 0.765641, "process_mem_max_rss": "197795840", "process_in_blocks": "0", "process_out_blocks": "0"}
[0m15:40:37.038883 [debug] [MainThread]: Command `dbt build` succeeded at 15:40:37.038747 after 16.10 seconds
[0m15:40:37.039416 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d837590>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109b00ad0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109af2010>]}
[0m15:40:37.039930 [debug] [MainThread]: Flushing usage events
[0m15:40:37.239715 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m16:05:31.024968 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1061b31d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106227010>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106227610>]}


============================== 16:05:31.031998 | 508ad0b1-05c4-4750-a2f8-9ee29af46d45 ==============================
[0m16:05:31.031998 [info ] [MainThread]: Running with dbt=1.9.0-b3
[0m16:05:31.032888 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'debug': 'False', 'log_path': 'logs', 'profiles_dir': '.', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt build', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m16:05:32.638002 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '508ad0b1-05c4-4750-a2f8-9ee29af46d45', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1069fcf90>]}
[0m16:05:32.694623 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '508ad0b1-05c4-4750-a2f8-9ee29af46d45', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1060b46d0>]}
[0m16:05:32.695852 [info ] [MainThread]: Registered adapter: snowflake=1.8.4
[0m16:05:32.929912 [debug] [MainThread]: checksum: 5f4fde221ca8627174a0341efc73099aaec05b8546408ceed6f59b1ec0755115, vars: {}, profile: , target: , version: 1.9.0b3
[0m16:05:33.045996 [info ] [MainThread]: Unable to do partial parsing because a project config has changed
[0m16:05:33.046636 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '508ad0b1-05c4-4750-a2f8-9ee29af46d45', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ca37f90>]}
[0m16:05:35.155091 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 2 unused configuration paths:
- models.marts
- models.staging
[0m16:05:35.166458 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '508ad0b1-05c4-4750-a2f8-9ee29af46d45', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10cbcd4d0>]}
[0m16:05:35.314027 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '508ad0b1-05c4-4750-a2f8-9ee29af46d45', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x116ab7310>]}
[0m16:05:35.314609 [info ] [MainThread]: Found 5 models, 2 data tests, 2 sources, 862 macros, 1 unit test
[0m16:05:35.316924 [info ] [MainThread]: 
[0m16:05:35.317334 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m16:05:35.317660 [info ] [MainThread]: 
[0m16:05:35.318129 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m16:05:35.322913 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_DATAEXPERT_STUDENT'
[0m16:05:35.330863 [debug] [ThreadPool]: Using snowflake connection "list_DATAEXPERT_STUDENT"
[0m16:05:35.331285 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "connection_name": "list_DATAEXPERT_STUDENT"} */
show terse schemas in database DATAEXPERT_STUDENT
    limit 10000
[0m16:05:35.331602 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:05:36.630444 [debug] [ThreadPool]: SQL status: SUCCESS 75 in 1.299 seconds
[0m16:05:36.633839 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT: Close
[0m16:05:36.864508 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_DATAEXPERT_STUDENT, now list_DATAEXPERT_STUDENT_andres)
[0m16:05:36.873392 [debug] [ThreadPool]: Using snowflake connection "list_DATAEXPERT_STUDENT_andres"
[0m16:05:36.873799 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "connection_name": "list_DATAEXPERT_STUDENT_andres"} */
show objects in DATAEXPERT_STUDENT.andres limit 10000
[0m16:05:36.874143 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m16:05:37.612100 [debug] [ThreadPool]: SQL status: SUCCESS 28 in 0.738 seconds
[0m16:05:37.616080 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: Close
[0m16:05:37.855062 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '508ad0b1-05c4-4750-a2f8-9ee29af46d45', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110748d90>]}
[0m16:05:37.858160 [debug] [Thread-1 (]: Began running node model.dbt_capstone_andres.dim_date
[0m16:05:37.858777 [info ] [Thread-1 (]: 1 of 8 START sql table model andres.dim_date ................................... [RUN]
[0m16:05:37.859633 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_DATAEXPERT_STUDENT_andres, now model.dbt_capstone_andres.dim_date)
[0m16:05:37.860087 [debug] [Thread-1 (]: Began compiling node model.dbt_capstone_andres.dim_date
[0m16:05:37.874042 [debug] [Thread-1 (]: Using snowflake connection "model.dbt_capstone_andres.dim_date"
[0m16:05:37.874706 [debug] [Thread-1 (]: On model.dbt_capstone_andres.dim_date: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "model.dbt_capstone_andres.dim_date"} */
select datediff(
        day,
        cast('1995-01-01' as timestamp),
        cast('2040-12-31' as timestamp)
        )
[0m16:05:37.875251 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m16:05:38.559841 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 0.685 seconds
[0m16:05:38.578425 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_capstone_andres.dim_date"
[0m16:05:38.581015 [debug] [Thread-1 (]: Began executing node model.dbt_capstone_andres.dim_date
[0m16:05:38.609636 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_capstone_andres.dim_date"
[0m16:05:38.617292 [debug] [Thread-1 (]: Using snowflake connection "model.dbt_capstone_andres.dim_date"
[0m16:05:38.618114 [debug] [Thread-1 (]: On model.dbt_capstone_andres.dim_date: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "model.dbt_capstone_andres.dim_date"} */
create or replace transient table DATAEXPERT_STUDENT.andres.dim_date
         as
        (

    
with base_dates as (
    
    with date_spine as
(

    





with rawdata as (

    

    

    with p as (
        select 0 as generated_number union all select 1
    ), unioned as (

    select

    
    p0.generated_number * power(2, 0)
     + 
    
    p1.generated_number * power(2, 1)
     + 
    
    p2.generated_number * power(2, 2)
     + 
    
    p3.generated_number * power(2, 3)
     + 
    
    p4.generated_number * power(2, 4)
     + 
    
    p5.generated_number * power(2, 5)
     + 
    
    p6.generated_number * power(2, 6)
     + 
    
    p7.generated_number * power(2, 7)
     + 
    
    p8.generated_number * power(2, 8)
     + 
    
    p9.generated_number * power(2, 9)
     + 
    
    p10.generated_number * power(2, 10)
     + 
    
    p11.generated_number * power(2, 11)
     + 
    
    p12.generated_number * power(2, 12)
     + 
    
    p13.generated_number * power(2, 13)
     + 
    
    p14.generated_number * power(2, 14)
    
    
    + 1
    as generated_number

    from

    
    p as p0
     cross join 
    
    p as p1
     cross join 
    
    p as p2
     cross join 
    
    p as p3
     cross join 
    
    p as p4
     cross join 
    
    p as p5
     cross join 
    
    p as p6
     cross join 
    
    p as p7
     cross join 
    
    p as p8
     cross join 
    
    p as p9
     cross join 
    
    p as p10
     cross join 
    
    p as p11
     cross join 
    
    p as p12
     cross join 
    
    p as p13
     cross join 
    
    p as p14
    
    

    )

    select *
    from unioned
    where generated_number <= 16801
    order by generated_number



),

all_periods as (

    select (
        

    dateadd(
        day,
        (row_number() over (order by 1) - 1),
        cast('1995-01-01' as timestamp)
        )


    ) as date_day
    from rawdata

),

filtered as (

    select *
    from all_periods
    where date_day <= cast('2040-12-31' as timestamp)

)

select * from filtered



)
select
    cast(d.date_day as timestamp) as date_day
from
    date_spine d


),
dates_with_prior_year_dates as (

    select
        cast(d.date_day as date) as date_day,
        cast(

    dateadd(
        year,
        -1,
        d.date_day
        )

 as date) as prior_year_date_day,
        cast(

    dateadd(
        day,
        -364,
        d.date_day
        )

 as date) as prior_year_over_year_date_day
    from
    	base_dates d

)
select
    d.date_day,
    cast(

    dateadd(
        day,
        -1,
        d.date_day
        )

 as date) as prior_date_day,
    cast(

    dateadd(
        day,
        1,
        d.date_day
        )

 as date) as next_date_day,
    d.prior_year_date_day as prior_year_date_day,
    d.prior_year_over_year_date_day,
    case
            when date_part('dayofweek', d.date_day) = 7 then 1
            else date_part('dayofweek', d.date_day) + 1
        end as day_of_week,
    date_part('dayofweekiso', d.date_day) as day_of_week_iso,
    -- long version not implemented on Snowflake so we're doing it manually :/
    case dayname(d.date_day)
        when 'Mon' then 'Monday'
        when 'Tue' then 'Tuesday'
        when 'Wed' then 'Wednesday'
        when 'Thu' then 'Thursday'
        when 'Fri' then 'Friday'
        when 'Sat' then 'Saturday'
        when 'Sun' then 'Sunday'
    end as day_of_week_name,
    dayname(d.date_day) as day_of_week_name_short,
    date_part('day', d.date_day) as day_of_month,
    date_part('dayofyear', d.date_day) as day_of_year,

    
    
    cast(

    dateadd(
        day,
        -1 * (case
            when date_part('dayofweek', d.date_day) = 7 then 1
            else date_part('dayofweek', d.date_day) + 1
        end - 1),
        d.date_day
        )

 as date) as week_start_date,
    cast(

    dateadd(
        day,
        6,
        
    
    cast(

    dateadd(
        day,
        -1 * (case
            when date_part('dayofweek', d.date_day) = 7 then 1
            else date_part('dayofweek', d.date_day) + 1
        end - 1),
        d.date_day
        )

 as date)
        )

 as date) as week_end_date,
    
    
    cast(

    dateadd(
        day,
        -1 * (case
            when date_part('dayofweek', d.prior_year_over_year_date_day) = 7 then 1
            else date_part('dayofweek', d.prior_year_over_year_date_day) + 1
        end - 1),
        d.prior_year_over_year_date_day
        )

 as date) as prior_year_week_start_date,
    cast(

    dateadd(
        day,
        6,
        
    
    cast(

    dateadd(
        day,
        -1 * (case
            when date_part('dayofweek', d.prior_year_over_year_date_day) = 7 then 1
            else date_part('dayofweek', d.prior_year_over_year_date_day) + 1
        end - 1),
        d.prior_year_over_year_date_day
        )

 as date)
        )

 as date) as prior_year_week_end_date,
    cast(date_part('week', d.date_day) as integer) as week_of_year,

    cast(date_trunc('week', d.date_day) as date) as iso_week_start_date,
    cast(

    dateadd(
        day,
        6,
        cast(date_trunc('week', d.date_day) as date)
        )

 as date) as iso_week_end_date,
    cast(date_trunc('week', d.prior_year_over_year_date_day) as date) as prior_year_iso_week_start_date,
    cast(

    dateadd(
        day,
        6,
        cast(date_trunc('week', d.prior_year_over_year_date_day) as date)
        )

 as date) as prior_year_iso_week_end_date,
    cast(date_part('weekiso', d.date_day) as integer) as iso_week_of_year,

    cast(date_part('week', d.prior_year_over_year_date_day) as integer) as prior_year_week_of_year,
    cast(date_part('weekiso', d.prior_year_over_year_date_day) as integer) as prior_year_iso_week_of_year,

    cast(date_part('month', d.date_day) as integer) as month_of_year,
    to_char(d.date_day, 'MMMM')  as month_name,
    to_char(d.date_day, 'MON')  as month_name_short,

    cast(date_trunc('month', d.date_day) as date) as month_start_date,
    cast(cast(
        

    dateadd(
        day,
        -1,
        

    dateadd(
        month,
        1,
        date_trunc('month', d.date_day)
        )


        )


        as date) as date) as month_end_date,

    cast(date_trunc('month', d.prior_year_date_day) as date) as prior_year_month_start_date,
    cast(cast(
        

    dateadd(
        day,
        -1,
        

    dateadd(
        month,
        1,
        date_trunc('month', d.prior_year_date_day)
        )


        )


        as date) as date) as prior_year_month_end_date,

    cast(date_part('quarter', d.date_day) as integer) as quarter_of_year,
    cast(date_trunc('quarter', d.date_day) as date) as quarter_start_date,
    cast(cast(
        

    dateadd(
        day,
        -1,
        

    dateadd(
        quarter,
        1,
        date_trunc('quarter', d.date_day)
        )


        )


        as date) as date) as quarter_end_date,

    cast(date_part('year', d.date_day) as integer) as year_number,
    cast(date_trunc('year', d.date_day) as date) as year_start_date,
    cast(cast(
        

    dateadd(
        day,
        -1,
        

    dateadd(
        year,
        1,
        date_trunc('year', d.date_day)
        )


        )


        as date) as date) as year_end_date
from
    dates_with_prior_year_dates d
order by 1


        );
[0m16:05:39.862252 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 1.243 seconds
[0m16:05:39.889765 [debug] [Thread-1 (]: On model.dbt_capstone_andres.dim_date: Close
[0m16:05:40.125185 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '508ad0b1-05c4-4750-a2f8-9ee29af46d45', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c72ecd0>]}
[0m16:05:40.126322 [info ] [Thread-1 (]: 1 of 8 OK created sql table model andres.dim_date .............................. [[32mSUCCESS 1[0m in 2.26s]
[0m16:05:40.127260 [debug] [Thread-1 (]: Finished running node model.dbt_capstone_andres.dim_date
[0m16:05:40.127813 [debug] [Thread-1 (]: Began running node model.dbt_capstone_andres.stg_daily_bars
[0m16:05:40.128510 [info ] [Thread-1 (]: 2 of 8 START sql view model andres.stg_daily_bars .............................. [RUN]
[0m16:05:40.129140 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_capstone_andres.dim_date, now model.dbt_capstone_andres.stg_daily_bars)
[0m16:05:40.129653 [debug] [Thread-1 (]: Began compiling node model.dbt_capstone_andres.stg_daily_bars
[0m16:05:40.134050 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_capstone_andres.stg_daily_bars"
[0m16:05:40.134993 [debug] [Thread-1 (]: Began executing node model.dbt_capstone_andres.stg_daily_bars
[0m16:05:40.161664 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_capstone_andres.stg_daily_bars"
[0m16:05:40.163257 [debug] [Thread-1 (]: Using snowflake connection "model.dbt_capstone_andres.stg_daily_bars"
[0m16:05:40.163755 [debug] [Thread-1 (]: On model.dbt_capstone_andres.stg_daily_bars: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "model.dbt_capstone_andres.stg_daily_bars"} */
create or replace   view DATAEXPERT_STUDENT.andres.stg_daily_bars
  
   as (
    with source as (

    select * from dataexpert_student.andres.daily_bars

),

renamed as (

    select
        TO_DATE(TO_TIMESTAMP(timestamp / 1000)) as date,
        ticker as ticker,
        v  as number_of_shares_traded,
        n  as number_of_trades,
        round(vw,2) as volume_weighted_avg_price,
        o  as open_price,
        c  as close_price,
        h  as high_price,
        l  as low_price
    from source

)

select * from renamed
  );
[0m16:05:40.164236 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m16:05:40.946184 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 0.782 seconds
[0m16:05:40.948508 [debug] [Thread-1 (]: On model.dbt_capstone_andres.stg_daily_bars: Close
[0m16:05:41.162851 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '508ad0b1-05c4-4750-a2f8-9ee29af46d45', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c4e76d0>]}
[0m16:05:41.163632 [info ] [Thread-1 (]: 2 of 8 OK created sql view model andres.stg_daily_bars ......................... [[32mSUCCESS 1[0m in 1.03s]
[0m16:05:41.164264 [debug] [Thread-1 (]: Finished running node model.dbt_capstone_andres.stg_daily_bars
[0m16:05:41.164704 [debug] [Thread-1 (]: Began running node model.dbt_capstone_andres.stg_treasury_revenue_collections
[0m16:05:41.165264 [info ] [Thread-1 (]: 3 of 8 START sql view model andres.stg_treasury_revenue_collections ............ [RUN]
[0m16:05:41.165761 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_capstone_andres.stg_daily_bars, now model.dbt_capstone_andres.stg_treasury_revenue_collections)
[0m16:05:41.166173 [debug] [Thread-1 (]: Began compiling node model.dbt_capstone_andres.stg_treasury_revenue_collections
[0m16:05:41.169529 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_capstone_andres.stg_treasury_revenue_collections"
[0m16:05:41.170539 [debug] [Thread-1 (]: Began executing node model.dbt_capstone_andres.stg_treasury_revenue_collections
[0m16:05:41.174584 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_capstone_andres.stg_treasury_revenue_collections"
[0m16:05:41.176271 [debug] [Thread-1 (]: Using snowflake connection "model.dbt_capstone_andres.stg_treasury_revenue_collections"
[0m16:05:41.177027 [debug] [Thread-1 (]: On model.dbt_capstone_andres.stg_treasury_revenue_collections: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "model.dbt_capstone_andres.stg_treasury_revenue_collections"} */
create or replace   view DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections
  
   as (
    with source as (

    select * from dataexpert_student.andres.fiscal_data

),

renamed as (

    select
        record_date as collection_date,
        channel_type_desc as channel_type,
        tax_category_desc as tax_category,
        round(net_collections_amt,0) as dollars_collected_by_treasury

    from source

)

select * from renamed
  );
[0m16:05:41.177592 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m16:05:42.026685 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 0.849 seconds
[0m16:05:42.030126 [debug] [Thread-1 (]: On model.dbt_capstone_andres.stg_treasury_revenue_collections: Close
[0m16:05:42.249805 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '508ad0b1-05c4-4750-a2f8-9ee29af46d45', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10caa4bd0>]}
[0m16:05:42.250814 [info ] [Thread-1 (]: 3 of 8 OK created sql view model andres.stg_treasury_revenue_collections ....... [[32mSUCCESS 1[0m in 1.08s]
[0m16:05:42.251788 [debug] [Thread-1 (]: Finished running node model.dbt_capstone_andres.stg_treasury_revenue_collections
[0m16:05:42.252683 [debug] [Thread-1 (]: Began running node unit_test.dbt_capstone_andres.audit_weekly_metrics.test_amount_sum
[0m16:05:42.253274 [info ] [Thread-1 (]: 4 of 8 START unit_test audit_weekly_metrics::test_amount_sum ................... [RUN]
[0m16:05:42.253879 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_capstone_andres.stg_treasury_revenue_collections, now unit_test.dbt_capstone_andres.audit_weekly_metrics.test_amount_sum)
[0m16:05:42.254377 [debug] [Thread-1 (]: Began compiling node unit_test.dbt_capstone_andres.audit_weekly_metrics.test_amount_sum
[0m16:05:42.254870 [debug] [Thread-1 (]: Began executing node unit_test.dbt_capstone_andres.audit_weekly_metrics.test_amount_sum
[0m16:05:42.296707 [debug] [Thread-1 (]: Using snowflake connection "unit_test.dbt_capstone_andres.audit_weekly_metrics.test_amount_sum"
[0m16:05:42.297209 [debug] [Thread-1 (]: On unit_test.dbt_capstone_andres.audit_weekly_metrics.test_amount_sum: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "unit_test.dbt_capstone_andres.audit_weekly_metrics.test_amount_sum"} */
describe table DATAEXPERT_STUDENT.andres.stg_daily_bars
[0m16:05:42.297617 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m16:05:42.975124 [debug] [Thread-1 (]: SQL status: SUCCESS 9 in 0.677 seconds
[0m16:05:43.009546 [debug] [Thread-1 (]: Writing injected SQL for node "unit_test.dbt_capstone_andres.audit_weekly_metrics.test_amount_sum"
[0m16:05:43.016112 [debug] [Thread-1 (]: Using snowflake connection "unit_test.dbt_capstone_andres.audit_weekly_metrics.test_amount_sum"
[0m16:05:43.016586 [debug] [Thread-1 (]: On unit_test.dbt_capstone_andres.audit_weekly_metrics.test_amount_sum: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "unit_test.dbt_capstone_andres.audit_weekly_metrics.test_amount_sum"} */
describe table DATAEXPERT_STUDENT.andres.dim_date
[0m16:05:43.194022 [debug] [Thread-1 (]: SQL status: SUCCESS 36 in 0.177 seconds
[0m16:05:43.209190 [debug] [Thread-1 (]: Writing injected SQL for node "unit_test.dbt_capstone_andres.audit_weekly_metrics.test_amount_sum"
[0m16:05:43.217299 [debug] [Thread-1 (]: Using snowflake connection "unit_test.dbt_capstone_andres.audit_weekly_metrics.test_amount_sum"
[0m16:05:43.217822 [debug] [Thread-1 (]: On unit_test.dbt_capstone_andres.audit_weekly_metrics.test_amount_sum: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "unit_test.dbt_capstone_andres.audit_weekly_metrics.test_amount_sum"} */
describe table DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections
[0m16:05:43.363674 [debug] [Thread-1 (]: SQL status: SUCCESS 4 in 0.145 seconds
[0m16:05:43.374837 [debug] [Thread-1 (]: Writing injected SQL for node "unit_test.dbt_capstone_andres.audit_weekly_metrics.test_amount_sum"
[0m16:05:43.383447 [debug] [Thread-1 (]: Writing injected SQL for node "unit_test.dbt_capstone_andres.audit_weekly_metrics.test_amount_sum"
[0m16:05:43.412426 [debug] [Thread-1 (]: Using snowflake connection "unit_test.dbt_capstone_andres.audit_weekly_metrics.test_amount_sum"
[0m16:05:43.413277 [debug] [Thread-1 (]: On unit_test.dbt_capstone_andres.audit_weekly_metrics.test_amount_sum: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "unit_test.dbt_capstone_andres.audit_weekly_metrics.test_amount_sum"} */
create or replace temporary table DATAEXPERT_STUDENT.andres.test_amount_sum__dbt_tmp
         as
        (select * from (
        with  __dbt__cte__stg_daily_bars as (

-- Fixture for stg_daily_bars
select 
    
        try_cast('2020-11-20' as DATE)
     as date, 
    
        try_cast('\'L\'' as character varying(16777216))
     as ticker, 
    
        try_cast('100' as FLOAT)
     as number_of_shares_traded, 
    
        try_cast('10' as NUMBER(38,0))
     as number_of_trades, try_cast(null as FLOAT) as volume_weighted_avg_price, try_cast(null as FLOAT) as open_price, try_cast(null as FLOAT) as close_price, try_cast(null as FLOAT) as high_price, try_cast(null as FLOAT) as low_price
union all
select 
    
        try_cast('2020-11-20' as DATE)
     as date, 
    
        try_cast('\'S\'' as character varying(16777216))
     as ticker, 
    
        try_cast('200' as FLOAT)
     as number_of_shares_traded, 
    
        try_cast('20' as NUMBER(38,0))
     as number_of_trades, try_cast(null as FLOAT) as volume_weighted_avg_price, try_cast(null as FLOAT) as open_price, try_cast(null as FLOAT) as close_price, try_cast(null as FLOAT) as high_price, try_cast(null as FLOAT) as low_price
union all
select 
    
        try_cast('2020-11-21' as DATE)
     as date, 
    
        try_cast('\'L\'' as character varying(16777216))
     as ticker, 
    
        try_cast('300' as FLOAT)
     as number_of_shares_traded, 
    
        try_cast('40' as NUMBER(38,0))
     as number_of_trades, try_cast(null as FLOAT) as volume_weighted_avg_price, try_cast(null as FLOAT) as open_price, try_cast(null as FLOAT) as close_price, try_cast(null as FLOAT) as high_price, try_cast(null as FLOAT) as low_price
union all
select 
    
        try_cast('2020-11-21' as DATE)
     as date, 
    
        try_cast('\'S\'' as character varying(16777216))
     as ticker, 
    
        try_cast('200' as FLOAT)
     as number_of_shares_traded, 
    
        try_cast('50' as NUMBER(38,0))
     as number_of_trades, try_cast(null as FLOAT) as volume_weighted_avg_price, try_cast(null as FLOAT) as open_price, try_cast(null as FLOAT) as close_price, try_cast(null as FLOAT) as high_price, try_cast(null as FLOAT) as low_price
),  __dbt__cte__dim_date as (

-- Fixture for dim_date
select 
    
        try_cast('2020-11-20' as DATE)
     as date_day, try_cast(null as DATE) as prior_date_day, try_cast(null as DATE) as next_date_day, try_cast(null as DATE) as prior_year_date_day, try_cast(null as DATE) as prior_year_over_year_date_day, try_cast(null as NUMBER(3,0)) as day_of_week, try_cast(null as NUMBER(2,0)) as day_of_week_iso, try_cast(null as character varying(16777216)) as day_of_week_name, try_cast(null as character varying(3)) as day_of_week_name_short, try_cast(null as NUMBER(2,0)) as day_of_month, try_cast(null as NUMBER(4,0)) as day_of_year, 
    
        try_cast(' 2020-11-15' as DATE)
     as week_start_date, try_cast(null as DATE) as week_end_date, try_cast(null as DATE) as prior_year_week_start_date, try_cast(null as DATE) as prior_year_week_end_date, try_cast(null as NUMBER(38,0)) as week_of_year, try_cast(null as DATE) as iso_week_start_date, try_cast(null as DATE) as iso_week_end_date, try_cast(null as DATE) as prior_year_iso_week_start_date, try_cast(null as DATE) as prior_year_iso_week_end_date, try_cast(null as NUMBER(38,0)) as iso_week_of_year, try_cast(null as NUMBER(38,0)) as prior_year_week_of_year, try_cast(null as NUMBER(38,0)) as prior_year_iso_week_of_year, try_cast(null as NUMBER(38,0)) as month_of_year, try_cast(null as character varying(16777216)) as month_name, try_cast(null as character varying(16777216)) as month_name_short, try_cast(null as DATE) as month_start_date, try_cast(null as DATE) as month_end_date, try_cast(null as DATE) as prior_year_month_start_date, try_cast(null as DATE) as prior_year_month_end_date, try_cast(null as NUMBER(38,0)) as quarter_of_year, try_cast(null as DATE) as quarter_start_date, try_cast(null as DATE) as quarter_end_date, try_cast(null as NUMBER(38,0)) as year_number, try_cast(null as DATE) as year_start_date, try_cast(null as DATE) as year_end_date
union all
select 
    
        try_cast('2020-11-21' as DATE)
     as date_day, try_cast(null as DATE) as prior_date_day, try_cast(null as DATE) as next_date_day, try_cast(null as DATE) as prior_year_date_day, try_cast(null as DATE) as prior_year_over_year_date_day, try_cast(null as NUMBER(3,0)) as day_of_week, try_cast(null as NUMBER(2,0)) as day_of_week_iso, try_cast(null as character varying(16777216)) as day_of_week_name, try_cast(null as character varying(3)) as day_of_week_name_short, try_cast(null as NUMBER(2,0)) as day_of_month, try_cast(null as NUMBER(4,0)) as day_of_year, 
    
        try_cast(' 2020-11-15' as DATE)
     as week_start_date, try_cast(null as DATE) as week_end_date, try_cast(null as DATE) as prior_year_week_start_date, try_cast(null as DATE) as prior_year_week_end_date, try_cast(null as NUMBER(38,0)) as week_of_year, try_cast(null as DATE) as iso_week_start_date, try_cast(null as DATE) as iso_week_end_date, try_cast(null as DATE) as prior_year_iso_week_start_date, try_cast(null as DATE) as prior_year_iso_week_end_date, try_cast(null as NUMBER(38,0)) as iso_week_of_year, try_cast(null as NUMBER(38,0)) as prior_year_week_of_year, try_cast(null as NUMBER(38,0)) as prior_year_iso_week_of_year, try_cast(null as NUMBER(38,0)) as month_of_year, try_cast(null as character varying(16777216)) as month_name, try_cast(null as character varying(16777216)) as month_name_short, try_cast(null as DATE) as month_start_date, try_cast(null as DATE) as month_end_date, try_cast(null as DATE) as prior_year_month_start_date, try_cast(null as DATE) as prior_year_month_end_date, try_cast(null as NUMBER(38,0)) as quarter_of_year, try_cast(null as DATE) as quarter_start_date, try_cast(null as DATE) as quarter_end_date, try_cast(null as NUMBER(38,0)) as year_number, try_cast(null as DATE) as year_start_date, try_cast(null as DATE) as year_end_date
),  __dbt__cte__stg_treasury_revenue_collections as (

-- Fixture for stg_treasury_revenue_collections
select 
    
        try_cast('2020-11-20' as DATE)
     as collection_date, try_cast(null as character varying(16777216)) as channel_type, 
    
        try_cast('IRS Tax' as character varying(16777216))
     as tax_category, 
    
        try_cast('1000' as NUMBER(19,0))
     as dollars_collected_by_treasury
union all
select 
    
        try_cast('2020-11-20' as DATE)
     as collection_date, try_cast(null as character varying(16777216)) as channel_type, 
    
        try_cast('IRS Non Tax' as character varying(16777216))
     as tax_category, 
    
        try_cast('2000' as NUMBER(19,0))
     as dollars_collected_by_treasury
union all
select 
    
        try_cast('2020-11-21' as DATE)
     as collection_date, try_cast(null as character varying(16777216)) as channel_type, 
    
        try_cast('IRS Tax' as character varying(16777216))
     as tax_category, 
    
        try_cast('3000' as NUMBER(19,0))
     as dollars_collected_by_treasury
union all
select 
    
        try_cast('2020-11-21' as DATE)
     as collection_date, try_cast(null as character varying(16777216)) as channel_type, 
    
        try_cast('IRS Tax' as character varying(16777216))
     as tax_category, 
    
        try_cast('2000' as NUMBER(19,0))
     as dollars_collected_by_treasury
), pol as (
SELECT week_start_date, 
sum(number_of_shares_traded) number_of_shares_traded,
sum(number_of_trades) number_of_trades
from  __dbt__cte__stg_daily_bars a 
JOIN __dbt__cte__dim_date d on d.date_day=a.date
group by 1)

, rev as (
select week_start_date,
sum(dollars_collected_by_treasury) dollars_collected_by_treasury,
sum(case when tax_category = 'IRS Tax' then dollars_collected_by_treasury else 0 end) as irs_tax_dollars_collected_by_treasury, 
sum(case when tax_category = 'Non-Tax' then dollars_collected_by_treasury else 0 end) as non_irs_non_tax_dollars_collected_by_treasury,
sum(case when tax_category = 'IRS Non-Tax' then dollars_collected_by_treasury else 0 end) as irs_non_tax_dollars_collected_by_treasury
FROM __dbt__cte__stg_treasury_revenue_collections a 
JOIN __dbt__cte__dim_date d on d.date_day=a.collection_date
GROUP BY ALL)

select 
    a.week_start_date, 
    a.number_of_shares_traded, 
    a.number_of_trades,
    b.dollars_collected_by_treasury,
    b.irs_tax_dollars_collected_by_treasury

from pol a 
INNER JOIN rev b on b.week_start_date = a.week_start_date
    ) as __dbt_sbq
    where false
    limit 0

        );
[0m16:05:43.981003 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 0.567 seconds
[0m16:05:44.003379 [debug] [Thread-1 (]: Using snowflake connection "unit_test.dbt_capstone_andres.audit_weekly_metrics.test_amount_sum"
[0m16:05:44.004200 [debug] [Thread-1 (]: On unit_test.dbt_capstone_andres.audit_weekly_metrics.test_amount_sum: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "unit_test.dbt_capstone_andres.audit_weekly_metrics.test_amount_sum"} */
describe table DATAEXPERT_STUDENT.andres.test_amount_sum__dbt_tmp
[0m16:05:44.139275 [debug] [Thread-1 (]: SQL status: SUCCESS 5 in 0.134 seconds
[0m16:05:44.154969 [debug] [Thread-1 (]: Writing runtime sql for node "unit_test.dbt_capstone_andres.audit_weekly_metrics.test_amount_sum"
[0m16:05:44.168482 [debug] [Thread-1 (]: Using snowflake connection "unit_test.dbt_capstone_andres.audit_weekly_metrics.test_amount_sum"
[0m16:05:44.169665 [debug] [Thread-1 (]: On unit_test.dbt_capstone_andres.audit_weekly_metrics.test_amount_sum: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "unit_test.dbt_capstone_andres.audit_weekly_metrics.test_amount_sum"} */
-- Build actual result given inputs
with dbt_internal_unit_test_actual as (
  select
    week_start_date,number_of_shares_traded,number_of_trades,dollars_collected_by_treasury,irs_tax_dollars_collected_by_treasury, 'actual' as "actual_or_expected"
  from (
    with  __dbt__cte__stg_daily_bars as (

-- Fixture for stg_daily_bars
select 
    
        try_cast('2020-11-20' as DATE)
     as date, 
    
        try_cast('\'L\'' as character varying(16777216))
     as ticker, 
    
        try_cast('100' as FLOAT)
     as number_of_shares_traded, 
    
        try_cast('10' as NUMBER(38,0))
     as number_of_trades, try_cast(null as FLOAT) as volume_weighted_avg_price, try_cast(null as FLOAT) as open_price, try_cast(null as FLOAT) as close_price, try_cast(null as FLOAT) as high_price, try_cast(null as FLOAT) as low_price
union all
select 
    
        try_cast('2020-11-20' as DATE)
     as date, 
    
        try_cast('\'S\'' as character varying(16777216))
     as ticker, 
    
        try_cast('200' as FLOAT)
     as number_of_shares_traded, 
    
        try_cast('20' as NUMBER(38,0))
     as number_of_trades, try_cast(null as FLOAT) as volume_weighted_avg_price, try_cast(null as FLOAT) as open_price, try_cast(null as FLOAT) as close_price, try_cast(null as FLOAT) as high_price, try_cast(null as FLOAT) as low_price
union all
select 
    
        try_cast('2020-11-21' as DATE)
     as date, 
    
        try_cast('\'L\'' as character varying(16777216))
     as ticker, 
    
        try_cast('300' as FLOAT)
     as number_of_shares_traded, 
    
        try_cast('40' as NUMBER(38,0))
     as number_of_trades, try_cast(null as FLOAT) as volume_weighted_avg_price, try_cast(null as FLOAT) as open_price, try_cast(null as FLOAT) as close_price, try_cast(null as FLOAT) as high_price, try_cast(null as FLOAT) as low_price
union all
select 
    
        try_cast('2020-11-21' as DATE)
     as date, 
    
        try_cast('\'S\'' as character varying(16777216))
     as ticker, 
    
        try_cast('200' as FLOAT)
     as number_of_shares_traded, 
    
        try_cast('50' as NUMBER(38,0))
     as number_of_trades, try_cast(null as FLOAT) as volume_weighted_avg_price, try_cast(null as FLOAT) as open_price, try_cast(null as FLOAT) as close_price, try_cast(null as FLOAT) as high_price, try_cast(null as FLOAT) as low_price
),  __dbt__cte__dim_date as (

-- Fixture for dim_date
select 
    
        try_cast('2020-11-20' as DATE)
     as date_day, try_cast(null as DATE) as prior_date_day, try_cast(null as DATE) as next_date_day, try_cast(null as DATE) as prior_year_date_day, try_cast(null as DATE) as prior_year_over_year_date_day, try_cast(null as NUMBER(3,0)) as day_of_week, try_cast(null as NUMBER(2,0)) as day_of_week_iso, try_cast(null as character varying(16777216)) as day_of_week_name, try_cast(null as character varying(3)) as day_of_week_name_short, try_cast(null as NUMBER(2,0)) as day_of_month, try_cast(null as NUMBER(4,0)) as day_of_year, 
    
        try_cast(' 2020-11-15' as DATE)
     as week_start_date, try_cast(null as DATE) as week_end_date, try_cast(null as DATE) as prior_year_week_start_date, try_cast(null as DATE) as prior_year_week_end_date, try_cast(null as NUMBER(38,0)) as week_of_year, try_cast(null as DATE) as iso_week_start_date, try_cast(null as DATE) as iso_week_end_date, try_cast(null as DATE) as prior_year_iso_week_start_date, try_cast(null as DATE) as prior_year_iso_week_end_date, try_cast(null as NUMBER(38,0)) as iso_week_of_year, try_cast(null as NUMBER(38,0)) as prior_year_week_of_year, try_cast(null as NUMBER(38,0)) as prior_year_iso_week_of_year, try_cast(null as NUMBER(38,0)) as month_of_year, try_cast(null as character varying(16777216)) as month_name, try_cast(null as character varying(16777216)) as month_name_short, try_cast(null as DATE) as month_start_date, try_cast(null as DATE) as month_end_date, try_cast(null as DATE) as prior_year_month_start_date, try_cast(null as DATE) as prior_year_month_end_date, try_cast(null as NUMBER(38,0)) as quarter_of_year, try_cast(null as DATE) as quarter_start_date, try_cast(null as DATE) as quarter_end_date, try_cast(null as NUMBER(38,0)) as year_number, try_cast(null as DATE) as year_start_date, try_cast(null as DATE) as year_end_date
union all
select 
    
        try_cast('2020-11-21' as DATE)
     as date_day, try_cast(null as DATE) as prior_date_day, try_cast(null as DATE) as next_date_day, try_cast(null as DATE) as prior_year_date_day, try_cast(null as DATE) as prior_year_over_year_date_day, try_cast(null as NUMBER(3,0)) as day_of_week, try_cast(null as NUMBER(2,0)) as day_of_week_iso, try_cast(null as character varying(16777216)) as day_of_week_name, try_cast(null as character varying(3)) as day_of_week_name_short, try_cast(null as NUMBER(2,0)) as day_of_month, try_cast(null as NUMBER(4,0)) as day_of_year, 
    
        try_cast(' 2020-11-15' as DATE)
     as week_start_date, try_cast(null as DATE) as week_end_date, try_cast(null as DATE) as prior_year_week_start_date, try_cast(null as DATE) as prior_year_week_end_date, try_cast(null as NUMBER(38,0)) as week_of_year, try_cast(null as DATE) as iso_week_start_date, try_cast(null as DATE) as iso_week_end_date, try_cast(null as DATE) as prior_year_iso_week_start_date, try_cast(null as DATE) as prior_year_iso_week_end_date, try_cast(null as NUMBER(38,0)) as iso_week_of_year, try_cast(null as NUMBER(38,0)) as prior_year_week_of_year, try_cast(null as NUMBER(38,0)) as prior_year_iso_week_of_year, try_cast(null as NUMBER(38,0)) as month_of_year, try_cast(null as character varying(16777216)) as month_name, try_cast(null as character varying(16777216)) as month_name_short, try_cast(null as DATE) as month_start_date, try_cast(null as DATE) as month_end_date, try_cast(null as DATE) as prior_year_month_start_date, try_cast(null as DATE) as prior_year_month_end_date, try_cast(null as NUMBER(38,0)) as quarter_of_year, try_cast(null as DATE) as quarter_start_date, try_cast(null as DATE) as quarter_end_date, try_cast(null as NUMBER(38,0)) as year_number, try_cast(null as DATE) as year_start_date, try_cast(null as DATE) as year_end_date
),  __dbt__cte__stg_treasury_revenue_collections as (

-- Fixture for stg_treasury_revenue_collections
select 
    
        try_cast('2020-11-20' as DATE)
     as collection_date, try_cast(null as character varying(16777216)) as channel_type, 
    
        try_cast('IRS Tax' as character varying(16777216))
     as tax_category, 
    
        try_cast('1000' as NUMBER(19,0))
     as dollars_collected_by_treasury
union all
select 
    
        try_cast('2020-11-20' as DATE)
     as collection_date, try_cast(null as character varying(16777216)) as channel_type, 
    
        try_cast('IRS Non Tax' as character varying(16777216))
     as tax_category, 
    
        try_cast('2000' as NUMBER(19,0))
     as dollars_collected_by_treasury
union all
select 
    
        try_cast('2020-11-21' as DATE)
     as collection_date, try_cast(null as character varying(16777216)) as channel_type, 
    
        try_cast('IRS Tax' as character varying(16777216))
     as tax_category, 
    
        try_cast('3000' as NUMBER(19,0))
     as dollars_collected_by_treasury
union all
select 
    
        try_cast('2020-11-21' as DATE)
     as collection_date, try_cast(null as character varying(16777216)) as channel_type, 
    
        try_cast('IRS Tax' as character varying(16777216))
     as tax_category, 
    
        try_cast('2000' as NUMBER(19,0))
     as dollars_collected_by_treasury
), pol as (
SELECT week_start_date, 
sum(number_of_shares_traded) number_of_shares_traded,
sum(number_of_trades) number_of_trades
from  __dbt__cte__stg_daily_bars a 
JOIN __dbt__cte__dim_date d on d.date_day=a.date
group by 1)

, rev as (
select week_start_date,
sum(dollars_collected_by_treasury) dollars_collected_by_treasury,
sum(case when tax_category = 'IRS Tax' then dollars_collected_by_treasury else 0 end) as irs_tax_dollars_collected_by_treasury, 
sum(case when tax_category = 'Non-Tax' then dollars_collected_by_treasury else 0 end) as non_irs_non_tax_dollars_collected_by_treasury,
sum(case when tax_category = 'IRS Non-Tax' then dollars_collected_by_treasury else 0 end) as irs_non_tax_dollars_collected_by_treasury
FROM __dbt__cte__stg_treasury_revenue_collections a 
JOIN __dbt__cte__dim_date d on d.date_day=a.collection_date
GROUP BY ALL)

select 
    a.week_start_date, 
    a.number_of_shares_traded, 
    a.number_of_trades,
    b.dollars_collected_by_treasury,
    b.irs_tax_dollars_collected_by_treasury

from pol a 
INNER JOIN rev b on b.week_start_date = a.week_start_date
  ) _dbt_internal_unit_test_actual
),
-- Build expected result
dbt_internal_unit_test_expected as (
  select
    week_start_date, number_of_shares_traded, number_of_trades, dollars_collected_by_treasury, irs_tax_dollars_collected_by_treasury, 'expected' as "actual_or_expected"
  from (
    select 
    
        try_cast('2020-11-15' as DATE)
     as week_start_date, 
    
        try_cast('800' as FLOAT)
     as number_of_shares_traded, 
    
        try_cast('120' as NUMBER(38,0))
     as number_of_trades, 
    
        try_cast('8000' as NUMBER(31,0))
     as dollars_collected_by_treasury, 
    
        try_cast('6000' as NUMBER(31,0))
     as irs_tax_dollars_collected_by_treasury
  ) _dbt_internal_unit_test_expected
)
-- Union actual and expected results
select * from dbt_internal_unit_test_actual
union all
select * from dbt_internal_unit_test_expected
[0m16:05:44.388726 [debug] [Thread-1 (]: SQL status: SUCCESS 2 in 0.218 seconds
[0m16:05:44.397991 [debug] [Thread-1 (]: Applying DROP to: DATAEXPERT_STUDENT.andres.test_amount_sum__dbt_tmp
[0m16:05:44.405072 [debug] [Thread-1 (]: Using snowflake connection "unit_test.dbt_capstone_andres.audit_weekly_metrics.test_amount_sum"
[0m16:05:44.405533 [debug] [Thread-1 (]: On unit_test.dbt_capstone_andres.audit_weekly_metrics.test_amount_sum: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "unit_test.dbt_capstone_andres.audit_weekly_metrics.test_amount_sum"} */
drop table if exists DATAEXPERT_STUDENT.andres.test_amount_sum__dbt_tmp cascade
[0m16:05:44.576521 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 0.171 seconds
[0m16:05:44.579365 [debug] [Thread-1 (]: On unit_test.dbt_capstone_andres.audit_weekly_metrics.test_amount_sum: Close
[0m16:05:44.795057 [info ] [Thread-1 (]: 4 of 8 PASS audit_weekly_metrics::test_amount_sum .............................. [[32mPASS[0m in 2.54s]
[0m16:05:44.795768 [debug] [Thread-1 (]: Finished running node unit_test.dbt_capstone_andres.audit_weekly_metrics.test_amount_sum
[0m16:05:44.796239 [debug] [Thread-1 (]: Began running node model.dbt_capstone_andres.audit_weekly_metrics
[0m16:05:44.796760 [info ] [Thread-1 (]: 5 of 8 START sql view model andres.audit_weekly_metrics ........................ [RUN]
[0m16:05:44.797243 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly unit_test.dbt_capstone_andres.audit_weekly_metrics.test_amount_sum, now model.dbt_capstone_andres.audit_weekly_metrics)
[0m16:05:44.797652 [debug] [Thread-1 (]: Began compiling node model.dbt_capstone_andres.audit_weekly_metrics
[0m16:05:44.801755 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_capstone_andres.audit_weekly_metrics"
[0m16:05:44.802495 [debug] [Thread-1 (]: Began executing node model.dbt_capstone_andres.audit_weekly_metrics
[0m16:05:44.808605 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_capstone_andres.audit_weekly_metrics"
[0m16:05:44.811022 [debug] [Thread-1 (]: Using snowflake connection "model.dbt_capstone_andres.audit_weekly_metrics"
[0m16:05:44.811789 [debug] [Thread-1 (]: On model.dbt_capstone_andres.audit_weekly_metrics: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "model.dbt_capstone_andres.audit_weekly_metrics"} */
create or replace   view DATAEXPERT_STUDENT.andres.audit_weekly_metrics
  
   as (
    with pol as (
SELECT week_start_date, 
sum(number_of_shares_traded) number_of_shares_traded,
sum(number_of_trades) number_of_trades
from  DATAEXPERT_STUDENT.andres.stg_daily_bars a 
JOIN DATAEXPERT_STUDENT.andres.dim_date d on d.date_day=a.date
group by 1)

, rev as (
select week_start_date,
sum(dollars_collected_by_treasury) dollars_collected_by_treasury,
sum(case when tax_category = 'IRS Tax' then dollars_collected_by_treasury else 0 end) as irs_tax_dollars_collected_by_treasury, 
sum(case when tax_category = 'Non-Tax' then dollars_collected_by_treasury else 0 end) as non_irs_non_tax_dollars_collected_by_treasury,
sum(case when tax_category = 'IRS Non-Tax' then dollars_collected_by_treasury else 0 end) as irs_non_tax_dollars_collected_by_treasury
FROM DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections a 
JOIN DATAEXPERT_STUDENT.andres.dim_date d on d.date_day=a.collection_date
GROUP BY ALL)

select 
    a.week_start_date, 
    a.number_of_shares_traded, 
    a.number_of_trades,
    b.dollars_collected_by_treasury,
    b.irs_tax_dollars_collected_by_treasury

from pol a 
INNER JOIN rev b on b.week_start_date = a.week_start_date
  );
[0m16:05:44.812441 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m16:05:45.688908 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 0.876 seconds
[0m16:05:45.691807 [debug] [Thread-1 (]: On model.dbt_capstone_andres.audit_weekly_metrics: Close
[0m16:05:45.936936 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '508ad0b1-05c4-4750-a2f8-9ee29af46d45', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x116aef190>]}
[0m16:05:45.937938 [info ] [Thread-1 (]: 5 of 8 OK created sql view model andres.audit_weekly_metrics ................... [[32mSUCCESS 1[0m in 1.14s]
[0m16:05:45.938833 [debug] [Thread-1 (]: Finished running node model.dbt_capstone_andres.audit_weekly_metrics
[0m16:05:45.939746 [debug] [Thread-1 (]: Began running node test.dbt_capstone_andres.not_null_audit_weekly_metrics_week_start_date.51a711f2f6
[0m16:05:45.940392 [info ] [Thread-1 (]: 6 of 8 START test not_null_audit_weekly_metrics_week_start_date ................ [RUN]
[0m16:05:45.941070 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.dbt_capstone_andres.audit_weekly_metrics, now test.dbt_capstone_andres.not_null_audit_weekly_metrics_week_start_date.51a711f2f6)
[0m16:05:45.941607 [debug] [Thread-1 (]: Began compiling node test.dbt_capstone_andres.not_null_audit_weekly_metrics_week_start_date.51a711f2f6
[0m16:05:45.956663 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_capstone_andres.not_null_audit_weekly_metrics_week_start_date.51a711f2f6"
[0m16:05:45.957614 [debug] [Thread-1 (]: Began executing node test.dbt_capstone_andres.not_null_audit_weekly_metrics_week_start_date.51a711f2f6
[0m16:05:45.975883 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_capstone_andres.not_null_audit_weekly_metrics_week_start_date.51a711f2f6"
[0m16:05:45.977135 [debug] [Thread-1 (]: Using snowflake connection "test.dbt_capstone_andres.not_null_audit_weekly_metrics_week_start_date.51a711f2f6"
[0m16:05:45.977680 [debug] [Thread-1 (]: On test.dbt_capstone_andres.not_null_audit_weekly_metrics_week_start_date.51a711f2f6: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "test.dbt_capstone_andres.not_null_audit_weekly_metrics_week_start_date.51a711f2f6"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select week_start_date
from DATAEXPERT_STUDENT.andres.audit_weekly_metrics
where week_start_date is null



      
    ) dbt_internal_test
[0m16:05:45.978128 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m16:05:46.753167 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 0.775 seconds
[0m16:05:46.757725 [debug] [Thread-1 (]: On test.dbt_capstone_andres.not_null_audit_weekly_metrics_week_start_date.51a711f2f6: Close
[0m16:05:47.019895 [info ] [Thread-1 (]: 6 of 8 PASS not_null_audit_weekly_metrics_week_start_date ...................... [[32mPASS[0m in 1.08s]
[0m16:05:47.020765 [debug] [Thread-1 (]: Finished running node test.dbt_capstone_andres.not_null_audit_weekly_metrics_week_start_date.51a711f2f6
[0m16:05:47.021311 [debug] [Thread-1 (]: Began running node test.dbt_capstone_andres.unique_audit_weekly_metrics_week_start_date.f263844fce
[0m16:05:47.021905 [info ] [Thread-1 (]: 7 of 8 START test unique_audit_weekly_metrics_week_start_date .................. [RUN]
[0m16:05:47.022573 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_capstone_andres.not_null_audit_weekly_metrics_week_start_date.51a711f2f6, now test.dbt_capstone_andres.unique_audit_weekly_metrics_week_start_date.f263844fce)
[0m16:05:47.023102 [debug] [Thread-1 (]: Began compiling node test.dbt_capstone_andres.unique_audit_weekly_metrics_week_start_date.f263844fce
[0m16:05:47.031381 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_capstone_andres.unique_audit_weekly_metrics_week_start_date.f263844fce"
[0m16:05:47.032313 [debug] [Thread-1 (]: Began executing node test.dbt_capstone_andres.unique_audit_weekly_metrics_week_start_date.f263844fce
[0m16:05:47.035990 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_capstone_andres.unique_audit_weekly_metrics_week_start_date.f263844fce"
[0m16:05:47.037313 [debug] [Thread-1 (]: Using snowflake connection "test.dbt_capstone_andres.unique_audit_weekly_metrics_week_start_date.f263844fce"
[0m16:05:47.037828 [debug] [Thread-1 (]: On test.dbt_capstone_andres.unique_audit_weekly_metrics_week_start_date.f263844fce: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "test.dbt_capstone_andres.unique_audit_weekly_metrics_week_start_date.f263844fce"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    week_start_date as unique_field,
    count(*) as n_records

from DATAEXPERT_STUDENT.andres.audit_weekly_metrics
where week_start_date is not null
group by week_start_date
having count(*) > 1



      
    ) dbt_internal_test
[0m16:05:47.038315 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m16:05:47.846871 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 0.808 seconds
[0m16:05:47.849573 [debug] [Thread-1 (]: On test.dbt_capstone_andres.unique_audit_weekly_metrics_week_start_date.f263844fce: Close
[0m16:05:48.073813 [info ] [Thread-1 (]: 7 of 8 PASS unique_audit_weekly_metrics_week_start_date ........................ [[32mPASS[0m in 1.05s]
[0m16:05:48.074572 [debug] [Thread-1 (]: Finished running node test.dbt_capstone_andres.unique_audit_weekly_metrics_week_start_date.f263844fce
[0m16:05:48.075318 [debug] [Thread-1 (]: Began running node model.dbt_capstone_andres.weekly_metrics
[0m16:05:48.076036 [info ] [Thread-1 (]: 8 of 8 START sql incremental model andres.weekly_metrics ....................... [RUN]
[0m16:05:48.076689 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_capstone_andres.unique_audit_weekly_metrics_week_start_date.f263844fce, now model.dbt_capstone_andres.weekly_metrics)
[0m16:05:48.077151 [debug] [Thread-1 (]: Began compiling node model.dbt_capstone_andres.weekly_metrics
[0m16:05:48.086668 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbt_capstone_andres.weekly_metrics"
[0m16:05:48.088111 [debug] [Thread-1 (]: Began executing node model.dbt_capstone_andres.weekly_metrics
[0m16:05:48.122667 [debug] [Thread-1 (]: Using snowflake connection "model.dbt_capstone_andres.weekly_metrics"
[0m16:05:48.123257 [debug] [Thread-1 (]: On model.dbt_capstone_andres.weekly_metrics: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "model.dbt_capstone_andres.weekly_metrics"} */
create or replace  temporary view DATAEXPERT_STUDENT.andres.weekly_metrics__dbt_tmp
  
   as (
    

SELECT *
from  DATAEXPERT_STUDENT.andres.audit_weekly_metrics 



where week_start_date >= (select coalesce(max(week_start_date)-7,'1900-01-01') from DATAEXPERT_STUDENT.andres.weekly_metrics )


  );
[0m16:05:48.123715 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m16:05:49.193801 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 1.070 seconds
[0m16:05:49.197355 [debug] [Thread-1 (]: Using snowflake connection "model.dbt_capstone_andres.weekly_metrics"
[0m16:05:49.197820 [debug] [Thread-1 (]: On model.dbt_capstone_andres.weekly_metrics: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "model.dbt_capstone_andres.weekly_metrics"} */
describe table DATAEXPERT_STUDENT.andres.weekly_metrics__dbt_tmp
[0m16:05:49.402731 [debug] [Thread-1 (]: SQL status: SUCCESS 5 in 0.204 seconds
[0m16:05:49.406349 [debug] [Thread-1 (]: Using snowflake connection "model.dbt_capstone_andres.weekly_metrics"
[0m16:05:49.406721 [debug] [Thread-1 (]: On model.dbt_capstone_andres.weekly_metrics: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "model.dbt_capstone_andres.weekly_metrics"} */
describe table DATAEXPERT_STUDENT.andres.weekly_metrics
[0m16:05:49.555587 [debug] [Thread-1 (]: SQL status: SUCCESS 5 in 0.148 seconds
[0m16:05:49.565488 [debug] [Thread-1 (]: Using snowflake connection "model.dbt_capstone_andres.weekly_metrics"
[0m16:05:49.566009 [debug] [Thread-1 (]: On model.dbt_capstone_andres.weekly_metrics: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "model.dbt_capstone_andres.weekly_metrics"} */
describe table "DATAEXPERT_STUDENT"."ANDRES"."WEEKLY_METRICS"
[0m16:05:49.706884 [debug] [Thread-1 (]: SQL status: SUCCESS 5 in 0.140 seconds
[0m16:05:49.742342 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbt_capstone_andres.weekly_metrics"
[0m16:05:49.743938 [debug] [Thread-1 (]: Using snowflake connection "model.dbt_capstone_andres.weekly_metrics"
[0m16:05:49.744381 [debug] [Thread-1 (]: On model.dbt_capstone_andres.weekly_metrics: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "model.dbt_capstone_andres.weekly_metrics"} */
-- back compat for old kwarg name
  
  begin;
[0m16:05:49.966627 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 0.222 seconds
[0m16:05:49.967505 [debug] [Thread-1 (]: Using snowflake connection "model.dbt_capstone_andres.weekly_metrics"
[0m16:05:49.968248 [debug] [Thread-1 (]: On model.dbt_capstone_andres.weekly_metrics: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "model.dbt_capstone_andres.weekly_metrics"} */
merge into DATAEXPERT_STUDENT.andres.weekly_metrics as DBT_INTERNAL_DEST
        using DATAEXPERT_STUDENT.andres.weekly_metrics__dbt_tmp as DBT_INTERNAL_SOURCE
        on (
                DBT_INTERNAL_SOURCE.week_start_date = DBT_INTERNAL_DEST.week_start_date
            )

    
    when matched then update set
        "WEEK_START_DATE" = DBT_INTERNAL_SOURCE."WEEK_START_DATE","NUMBER_OF_SHARES_TRADED" = DBT_INTERNAL_SOURCE."NUMBER_OF_SHARES_TRADED","NUMBER_OF_TRADES" = DBT_INTERNAL_SOURCE."NUMBER_OF_TRADES","DOLLARS_COLLECTED_BY_TREASURY" = DBT_INTERNAL_SOURCE."DOLLARS_COLLECTED_BY_TREASURY","IRS_TAX_DOLLARS_COLLECTED_BY_TREASURY" = DBT_INTERNAL_SOURCE."IRS_TAX_DOLLARS_COLLECTED_BY_TREASURY"
    

    when not matched then insert
        ("WEEK_START_DATE", "NUMBER_OF_SHARES_TRADED", "NUMBER_OF_TRADES", "DOLLARS_COLLECTED_BY_TREASURY", "IRS_TAX_DOLLARS_COLLECTED_BY_TREASURY")
    values
        ("WEEK_START_DATE", "NUMBER_OF_SHARES_TRADED", "NUMBER_OF_TRADES", "DOLLARS_COLLECTED_BY_TREASURY", "IRS_TAX_DOLLARS_COLLECTED_BY_TREASURY")

;
[0m16:05:50.935839 [debug] [Thread-1 (]: SQL status: SUCCESS 2 in 0.967 seconds
[0m16:05:50.936800 [debug] [Thread-1 (]: Using snowflake connection "model.dbt_capstone_andres.weekly_metrics"
[0m16:05:50.937511 [debug] [Thread-1 (]: On model.dbt_capstone_andres.weekly_metrics: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "model.dbt_capstone_andres.weekly_metrics"} */
COMMIT
[0m16:05:51.549349 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 0.611 seconds
[0m16:05:51.556154 [debug] [Thread-1 (]: Applying DROP to: DATAEXPERT_STUDENT.andres.weekly_metrics__dbt_tmp
[0m16:05:51.561413 [debug] [Thread-1 (]: Using snowflake connection "model.dbt_capstone_andres.weekly_metrics"
[0m16:05:51.561930 [debug] [Thread-1 (]: On model.dbt_capstone_andres.weekly_metrics: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "model.dbt_capstone_andres.weekly_metrics"} */
drop view if exists DATAEXPERT_STUDENT.andres.weekly_metrics__dbt_tmp cascade
[0m16:05:51.817425 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 0.255 seconds
[0m16:05:51.819633 [debug] [Thread-1 (]: On model.dbt_capstone_andres.weekly_metrics: Close
[0m16:05:52.108172 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '508ad0b1-05c4-4750-a2f8-9ee29af46d45', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106227a50>]}
[0m16:05:52.110439 [info ] [Thread-1 (]: 8 of 8 OK created sql incremental model andres.weekly_metrics .................. [[32mSUCCESS 2[0m in 4.03s]
[0m16:05:52.122774 [debug] [Thread-1 (]: Finished running node model.dbt_capstone_andres.weekly_metrics
[0m16:05:52.124688 [debug] [MainThread]: Connection 'master' was properly closed.
[0m16:05:52.125352 [debug] [MainThread]: Connection 'model.dbt_capstone_andres.weekly_metrics' was properly closed.
[0m16:05:52.125919 [info ] [MainThread]: 
[0m16:05:52.127100 [info ] [MainThread]: Finished running 1 incremental model, 1 table model, 2 data tests, 1 unit test, 3 view models in 0 hours 0 minutes and 16.81 seconds (16.81s).
[0m16:05:52.131113 [debug] [MainThread]: Command end result
[0m16:05:52.204482 [info ] [MainThread]: 
[0m16:05:52.205030 [info ] [MainThread]: [32mCompleted successfully[0m
[0m16:05:52.205428 [info ] [MainThread]: 
[0m16:05:52.205821 [info ] [MainThread]: Done. PASS=8 WARN=0 ERROR=0 SKIP=0 TOTAL=8
[0m16:05:52.208009 [debug] [MainThread]: Resource report: {"command_name": "build", "command_success": true, "command_wall_clock_time": 21.261889, "process_user_time": 6.103232, "process_kernel_time": 0.836052, "process_mem_max_rss": "204005376", "process_in_blocks": "0", "process_out_blocks": "0"}
[0m16:05:52.208736 [debug] [MainThread]: Command `dbt build` succeeded at 16:05:52.208619 after 21.26 seconds
[0m16:05:52.209216 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1024f0ad0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106237c90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106237b90>]}
[0m16:05:52.210469 [debug] [MainThread]: Flushing usage events
[0m16:05:52.415804 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m16:06:25.348435 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104a7c090>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104b53950>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104cd6a90>]}


============================== 16:06:25.354799 | 123950c2-045d-4863-9934-e01b9c65faea ==============================
[0m16:06:25.354799 [info ] [MainThread]: Running with dbt=1.9.0-b3
[0m16:06:25.355585 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'fail_fast': 'False', 'version_check': 'True', 'log_path': 'logs', 'debug': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'log_format': 'default', 'invocation_command': 'dbt test', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m16:06:26.999924 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '123950c2-045d-4863-9934-e01b9c65faea', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10af68390>]}
[0m16:06:27.062634 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '123950c2-045d-4863-9934-e01b9c65faea', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10394d110>]}
[0m16:06:27.065244 [info ] [MainThread]: Registered adapter: snowflake=1.8.4
[0m16:06:27.264514 [debug] [MainThread]: checksum: 5f4fde221ca8627174a0341efc73099aaec05b8546408ceed6f59b1ec0755115, vars: {}, profile: , target: , version: 1.9.0b3
[0m16:06:27.499563 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m16:06:27.500006 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m16:06:27.505293 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 2 unused configuration paths:
- models.staging
- models.marts
[0m16:06:27.549514 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '123950c2-045d-4863-9934-e01b9c65faea', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b3bd090>]}
[0m16:06:27.773426 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '123950c2-045d-4863-9934-e01b9c65faea', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b030cd0>]}
[0m16:06:27.774178 [info ] [MainThread]: Found 5 models, 2 data tests, 2 sources, 862 macros, 1 unit test
[0m16:06:27.774668 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '123950c2-045d-4863-9934-e01b9c65faea', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b3901d0>]}
[0m16:06:27.777062 [info ] [MainThread]: 
[0m16:06:27.777565 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m16:06:27.778009 [info ] [MainThread]: 
[0m16:06:27.778667 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m16:06:27.787364 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_DATAEXPERT_STUDENT_andres'
[0m16:06:27.809521 [debug] [ThreadPool]: Using snowflake connection "list_DATAEXPERT_STUDENT_andres"
[0m16:06:27.810176 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "connection_name": "list_DATAEXPERT_STUDENT_andres"} */
show objects in DATAEXPERT_STUDENT.andres limit 10000
[0m16:06:27.810642 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:06:28.821454 [debug] [ThreadPool]: SQL status: SUCCESS 28 in 1.011 seconds
[0m16:06:28.824964 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: Close
[0m16:06:29.049750 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '123950c2-045d-4863-9934-e01b9c65faea', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b0831d0>]}
[0m16:06:29.052399 [debug] [Thread-1 (]: Began running node test.dbt_capstone_andres.not_null_audit_weekly_metrics_week_start_date.51a711f2f6
[0m16:06:29.052875 [info ] [Thread-1 (]: 1 of 3 START test not_null_audit_weekly_metrics_week_start_date ................ [RUN]
[0m16:06:29.053402 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_DATAEXPERT_STUDENT_andres, now test.dbt_capstone_andres.not_null_audit_weekly_metrics_week_start_date.51a711f2f6)
[0m16:06:29.053817 [debug] [Thread-1 (]: Began compiling node test.dbt_capstone_andres.not_null_audit_weekly_metrics_week_start_date.51a711f2f6
[0m16:06:29.072490 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_capstone_andres.not_null_audit_weekly_metrics_week_start_date.51a711f2f6"
[0m16:06:29.073353 [debug] [Thread-1 (]: Began executing node test.dbt_capstone_andres.not_null_audit_weekly_metrics_week_start_date.51a711f2f6
[0m16:06:29.097144 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_capstone_andres.not_null_audit_weekly_metrics_week_start_date.51a711f2f6"
[0m16:06:29.098386 [debug] [Thread-1 (]: Using snowflake connection "test.dbt_capstone_andres.not_null_audit_weekly_metrics_week_start_date.51a711f2f6"
[0m16:06:29.098875 [debug] [Thread-1 (]: On test.dbt_capstone_andres.not_null_audit_weekly_metrics_week_start_date.51a711f2f6: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "test.dbt_capstone_andres.not_null_audit_weekly_metrics_week_start_date.51a711f2f6"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select week_start_date
from DATAEXPERT_STUDENT.andres.audit_weekly_metrics
where week_start_date is null



      
    ) dbt_internal_test
[0m16:06:29.099306 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m16:06:30.262554 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 1.163 seconds
[0m16:06:30.272543 [debug] [Thread-1 (]: On test.dbt_capstone_andres.not_null_audit_weekly_metrics_week_start_date.51a711f2f6: Close
[0m16:06:30.495056 [info ] [Thread-1 (]: 1 of 3 PASS not_null_audit_weekly_metrics_week_start_date ...................... [[32mPASS[0m in 1.44s]
[0m16:06:30.495925 [debug] [Thread-1 (]: Finished running node test.dbt_capstone_andres.not_null_audit_weekly_metrics_week_start_date.51a711f2f6
[0m16:06:30.496377 [debug] [Thread-1 (]: Began running node test.dbt_capstone_andres.unique_audit_weekly_metrics_week_start_date.f263844fce
[0m16:06:30.496872 [info ] [Thread-1 (]: 2 of 3 START test unique_audit_weekly_metrics_week_start_date .................. [RUN]
[0m16:06:30.497412 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_capstone_andres.not_null_audit_weekly_metrics_week_start_date.51a711f2f6, now test.dbt_capstone_andres.unique_audit_weekly_metrics_week_start_date.f263844fce)
[0m16:06:30.497837 [debug] [Thread-1 (]: Began compiling node test.dbt_capstone_andres.unique_audit_weekly_metrics_week_start_date.f263844fce
[0m16:06:30.504921 [debug] [Thread-1 (]: Writing injected SQL for node "test.dbt_capstone_andres.unique_audit_weekly_metrics_week_start_date.f263844fce"
[0m16:06:30.505883 [debug] [Thread-1 (]: Began executing node test.dbt_capstone_andres.unique_audit_weekly_metrics_week_start_date.f263844fce
[0m16:06:30.509039 [debug] [Thread-1 (]: Writing runtime sql for node "test.dbt_capstone_andres.unique_audit_weekly_metrics_week_start_date.f263844fce"
[0m16:06:30.510454 [debug] [Thread-1 (]: Using snowflake connection "test.dbt_capstone_andres.unique_audit_weekly_metrics_week_start_date.f263844fce"
[0m16:06:30.510856 [debug] [Thread-1 (]: On test.dbt_capstone_andres.unique_audit_weekly_metrics_week_start_date.f263844fce: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "test.dbt_capstone_andres.unique_audit_weekly_metrics_week_start_date.f263844fce"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    week_start_date as unique_field,
    count(*) as n_records

from DATAEXPERT_STUDENT.andres.audit_weekly_metrics
where week_start_date is not null
group by week_start_date
having count(*) > 1



      
    ) dbt_internal_test
[0m16:06:30.511223 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m16:06:31.286703 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 0.775 seconds
[0m16:06:31.289711 [debug] [Thread-1 (]: On test.dbt_capstone_andres.unique_audit_weekly_metrics_week_start_date.f263844fce: Close
[0m16:06:31.522217 [info ] [Thread-1 (]: 2 of 3 PASS unique_audit_weekly_metrics_week_start_date ........................ [[32mPASS[0m in 1.02s]
[0m16:06:31.523093 [debug] [Thread-1 (]: Finished running node test.dbt_capstone_andres.unique_audit_weekly_metrics_week_start_date.f263844fce
[0m16:06:31.523639 [debug] [Thread-1 (]: Began running node unit_test.dbt_capstone_andres.audit_weekly_metrics.test_amount_sum
[0m16:06:31.524220 [info ] [Thread-1 (]: 3 of 3 START unit_test audit_weekly_metrics::test_amount_sum ................... [RUN]
[0m16:06:31.524843 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.dbt_capstone_andres.unique_audit_weekly_metrics_week_start_date.f263844fce, now unit_test.dbt_capstone_andres.audit_weekly_metrics.test_amount_sum)
[0m16:06:31.525315 [debug] [Thread-1 (]: Began compiling node unit_test.dbt_capstone_andres.audit_weekly_metrics.test_amount_sum
[0m16:06:31.525780 [debug] [Thread-1 (]: Began executing node unit_test.dbt_capstone_andres.audit_weekly_metrics.test_amount_sum
[0m16:06:31.570752 [debug] [Thread-1 (]: Using snowflake connection "unit_test.dbt_capstone_andres.audit_weekly_metrics.test_amount_sum"
[0m16:06:31.571256 [debug] [Thread-1 (]: On unit_test.dbt_capstone_andres.audit_weekly_metrics.test_amount_sum: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "unit_test.dbt_capstone_andres.audit_weekly_metrics.test_amount_sum"} */
describe table DATAEXPERT_STUDENT.andres.stg_daily_bars
[0m16:06:31.571662 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m16:06:32.231525 [debug] [Thread-1 (]: SQL status: SUCCESS 9 in 0.660 seconds
[0m16:06:32.259458 [debug] [Thread-1 (]: Writing injected SQL for node "unit_test.dbt_capstone_andres.audit_weekly_metrics.test_amount_sum"
[0m16:06:32.265438 [debug] [Thread-1 (]: Using snowflake connection "unit_test.dbt_capstone_andres.audit_weekly_metrics.test_amount_sum"
[0m16:06:32.265887 [debug] [Thread-1 (]: On unit_test.dbt_capstone_andres.audit_weekly_metrics.test_amount_sum: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "unit_test.dbt_capstone_andres.audit_weekly_metrics.test_amount_sum"} */
describe table DATAEXPERT_STUDENT.andres.dim_date
[0m16:06:32.439202 [debug] [Thread-1 (]: SQL status: SUCCESS 36 in 0.173 seconds
[0m16:06:32.452080 [debug] [Thread-1 (]: Writing injected SQL for node "unit_test.dbt_capstone_andres.audit_weekly_metrics.test_amount_sum"
[0m16:06:32.460111 [debug] [Thread-1 (]: Using snowflake connection "unit_test.dbt_capstone_andres.audit_weekly_metrics.test_amount_sum"
[0m16:06:32.460574 [debug] [Thread-1 (]: On unit_test.dbt_capstone_andres.audit_weekly_metrics.test_amount_sum: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "unit_test.dbt_capstone_andres.audit_weekly_metrics.test_amount_sum"} */
describe table DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections
[0m16:06:32.599926 [debug] [Thread-1 (]: SQL status: SUCCESS 4 in 0.139 seconds
[0m16:06:32.609185 [debug] [Thread-1 (]: Writing injected SQL for node "unit_test.dbt_capstone_andres.audit_weekly_metrics.test_amount_sum"
[0m16:06:32.617162 [debug] [Thread-1 (]: Writing injected SQL for node "unit_test.dbt_capstone_andres.audit_weekly_metrics.test_amount_sum"
[0m16:06:32.655750 [debug] [Thread-1 (]: Using snowflake connection "unit_test.dbt_capstone_andres.audit_weekly_metrics.test_amount_sum"
[0m16:06:32.656537 [debug] [Thread-1 (]: On unit_test.dbt_capstone_andres.audit_weekly_metrics.test_amount_sum: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "unit_test.dbt_capstone_andres.audit_weekly_metrics.test_amount_sum"} */
create or replace temporary table DATAEXPERT_STUDENT.andres.test_amount_sum__dbt_tmp
         as
        (select * from (
        with  __dbt__cte__stg_daily_bars as (

-- Fixture for stg_daily_bars
select 
    
        try_cast('2020-11-20' as DATE)
     as date, 
    
        try_cast('\'L\'' as character varying(16777216))
     as ticker, 
    
        try_cast('100' as FLOAT)
     as number_of_shares_traded, 
    
        try_cast('10' as NUMBER(38,0))
     as number_of_trades, try_cast(null as FLOAT) as volume_weighted_avg_price, try_cast(null as FLOAT) as open_price, try_cast(null as FLOAT) as close_price, try_cast(null as FLOAT) as high_price, try_cast(null as FLOAT) as low_price
union all
select 
    
        try_cast('2020-11-20' as DATE)
     as date, 
    
        try_cast('\'S\'' as character varying(16777216))
     as ticker, 
    
        try_cast('200' as FLOAT)
     as number_of_shares_traded, 
    
        try_cast('20' as NUMBER(38,0))
     as number_of_trades, try_cast(null as FLOAT) as volume_weighted_avg_price, try_cast(null as FLOAT) as open_price, try_cast(null as FLOAT) as close_price, try_cast(null as FLOAT) as high_price, try_cast(null as FLOAT) as low_price
union all
select 
    
        try_cast('2020-11-21' as DATE)
     as date, 
    
        try_cast('\'L\'' as character varying(16777216))
     as ticker, 
    
        try_cast('300' as FLOAT)
     as number_of_shares_traded, 
    
        try_cast('40' as NUMBER(38,0))
     as number_of_trades, try_cast(null as FLOAT) as volume_weighted_avg_price, try_cast(null as FLOAT) as open_price, try_cast(null as FLOAT) as close_price, try_cast(null as FLOAT) as high_price, try_cast(null as FLOAT) as low_price
union all
select 
    
        try_cast('2020-11-21' as DATE)
     as date, 
    
        try_cast('\'S\'' as character varying(16777216))
     as ticker, 
    
        try_cast('200' as FLOAT)
     as number_of_shares_traded, 
    
        try_cast('50' as NUMBER(38,0))
     as number_of_trades, try_cast(null as FLOAT) as volume_weighted_avg_price, try_cast(null as FLOAT) as open_price, try_cast(null as FLOAT) as close_price, try_cast(null as FLOAT) as high_price, try_cast(null as FLOAT) as low_price
),  __dbt__cte__dim_date as (

-- Fixture for dim_date
select 
    
        try_cast('2020-11-20' as DATE)
     as date_day, try_cast(null as DATE) as prior_date_day, try_cast(null as DATE) as next_date_day, try_cast(null as DATE) as prior_year_date_day, try_cast(null as DATE) as prior_year_over_year_date_day, try_cast(null as NUMBER(3,0)) as day_of_week, try_cast(null as NUMBER(2,0)) as day_of_week_iso, try_cast(null as character varying(16777216)) as day_of_week_name, try_cast(null as character varying(3)) as day_of_week_name_short, try_cast(null as NUMBER(2,0)) as day_of_month, try_cast(null as NUMBER(4,0)) as day_of_year, 
    
        try_cast(' 2020-11-15' as DATE)
     as week_start_date, try_cast(null as DATE) as week_end_date, try_cast(null as DATE) as prior_year_week_start_date, try_cast(null as DATE) as prior_year_week_end_date, try_cast(null as NUMBER(38,0)) as week_of_year, try_cast(null as DATE) as iso_week_start_date, try_cast(null as DATE) as iso_week_end_date, try_cast(null as DATE) as prior_year_iso_week_start_date, try_cast(null as DATE) as prior_year_iso_week_end_date, try_cast(null as NUMBER(38,0)) as iso_week_of_year, try_cast(null as NUMBER(38,0)) as prior_year_week_of_year, try_cast(null as NUMBER(38,0)) as prior_year_iso_week_of_year, try_cast(null as NUMBER(38,0)) as month_of_year, try_cast(null as character varying(16777216)) as month_name, try_cast(null as character varying(16777216)) as month_name_short, try_cast(null as DATE) as month_start_date, try_cast(null as DATE) as month_end_date, try_cast(null as DATE) as prior_year_month_start_date, try_cast(null as DATE) as prior_year_month_end_date, try_cast(null as NUMBER(38,0)) as quarter_of_year, try_cast(null as DATE) as quarter_start_date, try_cast(null as DATE) as quarter_end_date, try_cast(null as NUMBER(38,0)) as year_number, try_cast(null as DATE) as year_start_date, try_cast(null as DATE) as year_end_date
union all
select 
    
        try_cast('2020-11-21' as DATE)
     as date_day, try_cast(null as DATE) as prior_date_day, try_cast(null as DATE) as next_date_day, try_cast(null as DATE) as prior_year_date_day, try_cast(null as DATE) as prior_year_over_year_date_day, try_cast(null as NUMBER(3,0)) as day_of_week, try_cast(null as NUMBER(2,0)) as day_of_week_iso, try_cast(null as character varying(16777216)) as day_of_week_name, try_cast(null as character varying(3)) as day_of_week_name_short, try_cast(null as NUMBER(2,0)) as day_of_month, try_cast(null as NUMBER(4,0)) as day_of_year, 
    
        try_cast(' 2020-11-15' as DATE)
     as week_start_date, try_cast(null as DATE) as week_end_date, try_cast(null as DATE) as prior_year_week_start_date, try_cast(null as DATE) as prior_year_week_end_date, try_cast(null as NUMBER(38,0)) as week_of_year, try_cast(null as DATE) as iso_week_start_date, try_cast(null as DATE) as iso_week_end_date, try_cast(null as DATE) as prior_year_iso_week_start_date, try_cast(null as DATE) as prior_year_iso_week_end_date, try_cast(null as NUMBER(38,0)) as iso_week_of_year, try_cast(null as NUMBER(38,0)) as prior_year_week_of_year, try_cast(null as NUMBER(38,0)) as prior_year_iso_week_of_year, try_cast(null as NUMBER(38,0)) as month_of_year, try_cast(null as character varying(16777216)) as month_name, try_cast(null as character varying(16777216)) as month_name_short, try_cast(null as DATE) as month_start_date, try_cast(null as DATE) as month_end_date, try_cast(null as DATE) as prior_year_month_start_date, try_cast(null as DATE) as prior_year_month_end_date, try_cast(null as NUMBER(38,0)) as quarter_of_year, try_cast(null as DATE) as quarter_start_date, try_cast(null as DATE) as quarter_end_date, try_cast(null as NUMBER(38,0)) as year_number, try_cast(null as DATE) as year_start_date, try_cast(null as DATE) as year_end_date
),  __dbt__cte__stg_treasury_revenue_collections as (

-- Fixture for stg_treasury_revenue_collections
select 
    
        try_cast('2020-11-20' as DATE)
     as collection_date, try_cast(null as character varying(16777216)) as channel_type, 
    
        try_cast('IRS Tax' as character varying(16777216))
     as tax_category, 
    
        try_cast('1000' as NUMBER(19,0))
     as dollars_collected_by_treasury
union all
select 
    
        try_cast('2020-11-20' as DATE)
     as collection_date, try_cast(null as character varying(16777216)) as channel_type, 
    
        try_cast('IRS Non Tax' as character varying(16777216))
     as tax_category, 
    
        try_cast('2000' as NUMBER(19,0))
     as dollars_collected_by_treasury
union all
select 
    
        try_cast('2020-11-21' as DATE)
     as collection_date, try_cast(null as character varying(16777216)) as channel_type, 
    
        try_cast('IRS Tax' as character varying(16777216))
     as tax_category, 
    
        try_cast('3000' as NUMBER(19,0))
     as dollars_collected_by_treasury
union all
select 
    
        try_cast('2020-11-21' as DATE)
     as collection_date, try_cast(null as character varying(16777216)) as channel_type, 
    
        try_cast('IRS Tax' as character varying(16777216))
     as tax_category, 
    
        try_cast('2000' as NUMBER(19,0))
     as dollars_collected_by_treasury
), pol as (
SELECT week_start_date, 
sum(number_of_shares_traded) number_of_shares_traded,
sum(number_of_trades) number_of_trades
from  __dbt__cte__stg_daily_bars a 
JOIN __dbt__cte__dim_date d on d.date_day=a.date
group by 1)

, rev as (
select week_start_date,
sum(dollars_collected_by_treasury) dollars_collected_by_treasury,
sum(case when tax_category = 'IRS Tax' then dollars_collected_by_treasury else 0 end) as irs_tax_dollars_collected_by_treasury, 
sum(case when tax_category = 'Non-Tax' then dollars_collected_by_treasury else 0 end) as non_irs_non_tax_dollars_collected_by_treasury,
sum(case when tax_category = 'IRS Non-Tax' then dollars_collected_by_treasury else 0 end) as irs_non_tax_dollars_collected_by_treasury
FROM __dbt__cte__stg_treasury_revenue_collections a 
JOIN __dbt__cte__dim_date d on d.date_day=a.collection_date
GROUP BY ALL)

select 
    a.week_start_date, 
    a.number_of_shares_traded, 
    a.number_of_trades,
    b.dollars_collected_by_treasury,
    b.irs_tax_dollars_collected_by_treasury

from pol a 
INNER JOIN rev b on b.week_start_date = a.week_start_date
    ) as __dbt_sbq
    where false
    limit 0

        );
[0m16:06:33.324744 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 0.667 seconds
[0m16:06:33.329402 [debug] [Thread-1 (]: Using snowflake connection "unit_test.dbt_capstone_andres.audit_weekly_metrics.test_amount_sum"
[0m16:06:33.329957 [debug] [Thread-1 (]: On unit_test.dbt_capstone_andres.audit_weekly_metrics.test_amount_sum: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "unit_test.dbt_capstone_andres.audit_weekly_metrics.test_amount_sum"} */
describe table DATAEXPERT_STUDENT.andres.test_amount_sum__dbt_tmp
[0m16:06:33.508021 [debug] [Thread-1 (]: SQL status: SUCCESS 5 in 0.177 seconds
[0m16:06:33.524645 [debug] [Thread-1 (]: Writing runtime sql for node "unit_test.dbt_capstone_andres.audit_weekly_metrics.test_amount_sum"
[0m16:06:33.537637 [debug] [Thread-1 (]: Using snowflake connection "unit_test.dbt_capstone_andres.audit_weekly_metrics.test_amount_sum"
[0m16:06:33.538703 [debug] [Thread-1 (]: On unit_test.dbt_capstone_andres.audit_weekly_metrics.test_amount_sum: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "unit_test.dbt_capstone_andres.audit_weekly_metrics.test_amount_sum"} */
-- Build actual result given inputs
with dbt_internal_unit_test_actual as (
  select
    week_start_date,number_of_shares_traded,number_of_trades,dollars_collected_by_treasury,irs_tax_dollars_collected_by_treasury, 'actual' as "actual_or_expected"
  from (
    with  __dbt__cte__stg_daily_bars as (

-- Fixture for stg_daily_bars
select 
    
        try_cast('2020-11-20' as DATE)
     as date, 
    
        try_cast('\'L\'' as character varying(16777216))
     as ticker, 
    
        try_cast('100' as FLOAT)
     as number_of_shares_traded, 
    
        try_cast('10' as NUMBER(38,0))
     as number_of_trades, try_cast(null as FLOAT) as volume_weighted_avg_price, try_cast(null as FLOAT) as open_price, try_cast(null as FLOAT) as close_price, try_cast(null as FLOAT) as high_price, try_cast(null as FLOAT) as low_price
union all
select 
    
        try_cast('2020-11-20' as DATE)
     as date, 
    
        try_cast('\'S\'' as character varying(16777216))
     as ticker, 
    
        try_cast('200' as FLOAT)
     as number_of_shares_traded, 
    
        try_cast('20' as NUMBER(38,0))
     as number_of_trades, try_cast(null as FLOAT) as volume_weighted_avg_price, try_cast(null as FLOAT) as open_price, try_cast(null as FLOAT) as close_price, try_cast(null as FLOAT) as high_price, try_cast(null as FLOAT) as low_price
union all
select 
    
        try_cast('2020-11-21' as DATE)
     as date, 
    
        try_cast('\'L\'' as character varying(16777216))
     as ticker, 
    
        try_cast('300' as FLOAT)
     as number_of_shares_traded, 
    
        try_cast('40' as NUMBER(38,0))
     as number_of_trades, try_cast(null as FLOAT) as volume_weighted_avg_price, try_cast(null as FLOAT) as open_price, try_cast(null as FLOAT) as close_price, try_cast(null as FLOAT) as high_price, try_cast(null as FLOAT) as low_price
union all
select 
    
        try_cast('2020-11-21' as DATE)
     as date, 
    
        try_cast('\'S\'' as character varying(16777216))
     as ticker, 
    
        try_cast('200' as FLOAT)
     as number_of_shares_traded, 
    
        try_cast('50' as NUMBER(38,0))
     as number_of_trades, try_cast(null as FLOAT) as volume_weighted_avg_price, try_cast(null as FLOAT) as open_price, try_cast(null as FLOAT) as close_price, try_cast(null as FLOAT) as high_price, try_cast(null as FLOAT) as low_price
),  __dbt__cte__dim_date as (

-- Fixture for dim_date
select 
    
        try_cast('2020-11-20' as DATE)
     as date_day, try_cast(null as DATE) as prior_date_day, try_cast(null as DATE) as next_date_day, try_cast(null as DATE) as prior_year_date_day, try_cast(null as DATE) as prior_year_over_year_date_day, try_cast(null as NUMBER(3,0)) as day_of_week, try_cast(null as NUMBER(2,0)) as day_of_week_iso, try_cast(null as character varying(16777216)) as day_of_week_name, try_cast(null as character varying(3)) as day_of_week_name_short, try_cast(null as NUMBER(2,0)) as day_of_month, try_cast(null as NUMBER(4,0)) as day_of_year, 
    
        try_cast(' 2020-11-15' as DATE)
     as week_start_date, try_cast(null as DATE) as week_end_date, try_cast(null as DATE) as prior_year_week_start_date, try_cast(null as DATE) as prior_year_week_end_date, try_cast(null as NUMBER(38,0)) as week_of_year, try_cast(null as DATE) as iso_week_start_date, try_cast(null as DATE) as iso_week_end_date, try_cast(null as DATE) as prior_year_iso_week_start_date, try_cast(null as DATE) as prior_year_iso_week_end_date, try_cast(null as NUMBER(38,0)) as iso_week_of_year, try_cast(null as NUMBER(38,0)) as prior_year_week_of_year, try_cast(null as NUMBER(38,0)) as prior_year_iso_week_of_year, try_cast(null as NUMBER(38,0)) as month_of_year, try_cast(null as character varying(16777216)) as month_name, try_cast(null as character varying(16777216)) as month_name_short, try_cast(null as DATE) as month_start_date, try_cast(null as DATE) as month_end_date, try_cast(null as DATE) as prior_year_month_start_date, try_cast(null as DATE) as prior_year_month_end_date, try_cast(null as NUMBER(38,0)) as quarter_of_year, try_cast(null as DATE) as quarter_start_date, try_cast(null as DATE) as quarter_end_date, try_cast(null as NUMBER(38,0)) as year_number, try_cast(null as DATE) as year_start_date, try_cast(null as DATE) as year_end_date
union all
select 
    
        try_cast('2020-11-21' as DATE)
     as date_day, try_cast(null as DATE) as prior_date_day, try_cast(null as DATE) as next_date_day, try_cast(null as DATE) as prior_year_date_day, try_cast(null as DATE) as prior_year_over_year_date_day, try_cast(null as NUMBER(3,0)) as day_of_week, try_cast(null as NUMBER(2,0)) as day_of_week_iso, try_cast(null as character varying(16777216)) as day_of_week_name, try_cast(null as character varying(3)) as day_of_week_name_short, try_cast(null as NUMBER(2,0)) as day_of_month, try_cast(null as NUMBER(4,0)) as day_of_year, 
    
        try_cast(' 2020-11-15' as DATE)
     as week_start_date, try_cast(null as DATE) as week_end_date, try_cast(null as DATE) as prior_year_week_start_date, try_cast(null as DATE) as prior_year_week_end_date, try_cast(null as NUMBER(38,0)) as week_of_year, try_cast(null as DATE) as iso_week_start_date, try_cast(null as DATE) as iso_week_end_date, try_cast(null as DATE) as prior_year_iso_week_start_date, try_cast(null as DATE) as prior_year_iso_week_end_date, try_cast(null as NUMBER(38,0)) as iso_week_of_year, try_cast(null as NUMBER(38,0)) as prior_year_week_of_year, try_cast(null as NUMBER(38,0)) as prior_year_iso_week_of_year, try_cast(null as NUMBER(38,0)) as month_of_year, try_cast(null as character varying(16777216)) as month_name, try_cast(null as character varying(16777216)) as month_name_short, try_cast(null as DATE) as month_start_date, try_cast(null as DATE) as month_end_date, try_cast(null as DATE) as prior_year_month_start_date, try_cast(null as DATE) as prior_year_month_end_date, try_cast(null as NUMBER(38,0)) as quarter_of_year, try_cast(null as DATE) as quarter_start_date, try_cast(null as DATE) as quarter_end_date, try_cast(null as NUMBER(38,0)) as year_number, try_cast(null as DATE) as year_start_date, try_cast(null as DATE) as year_end_date
),  __dbt__cte__stg_treasury_revenue_collections as (

-- Fixture for stg_treasury_revenue_collections
select 
    
        try_cast('2020-11-20' as DATE)
     as collection_date, try_cast(null as character varying(16777216)) as channel_type, 
    
        try_cast('IRS Tax' as character varying(16777216))
     as tax_category, 
    
        try_cast('1000' as NUMBER(19,0))
     as dollars_collected_by_treasury
union all
select 
    
        try_cast('2020-11-20' as DATE)
     as collection_date, try_cast(null as character varying(16777216)) as channel_type, 
    
        try_cast('IRS Non Tax' as character varying(16777216))
     as tax_category, 
    
        try_cast('2000' as NUMBER(19,0))
     as dollars_collected_by_treasury
union all
select 
    
        try_cast('2020-11-21' as DATE)
     as collection_date, try_cast(null as character varying(16777216)) as channel_type, 
    
        try_cast('IRS Tax' as character varying(16777216))
     as tax_category, 
    
        try_cast('3000' as NUMBER(19,0))
     as dollars_collected_by_treasury
union all
select 
    
        try_cast('2020-11-21' as DATE)
     as collection_date, try_cast(null as character varying(16777216)) as channel_type, 
    
        try_cast('IRS Tax' as character varying(16777216))
     as tax_category, 
    
        try_cast('2000' as NUMBER(19,0))
     as dollars_collected_by_treasury
), pol as (
SELECT week_start_date, 
sum(number_of_shares_traded) number_of_shares_traded,
sum(number_of_trades) number_of_trades
from  __dbt__cte__stg_daily_bars a 
JOIN __dbt__cte__dim_date d on d.date_day=a.date
group by 1)

, rev as (
select week_start_date,
sum(dollars_collected_by_treasury) dollars_collected_by_treasury,
sum(case when tax_category = 'IRS Tax' then dollars_collected_by_treasury else 0 end) as irs_tax_dollars_collected_by_treasury, 
sum(case when tax_category = 'Non-Tax' then dollars_collected_by_treasury else 0 end) as non_irs_non_tax_dollars_collected_by_treasury,
sum(case when tax_category = 'IRS Non-Tax' then dollars_collected_by_treasury else 0 end) as irs_non_tax_dollars_collected_by_treasury
FROM __dbt__cte__stg_treasury_revenue_collections a 
JOIN __dbt__cte__dim_date d on d.date_day=a.collection_date
GROUP BY ALL)

select 
    a.week_start_date, 
    a.number_of_shares_traded, 
    a.number_of_trades,
    b.dollars_collected_by_treasury,
    b.irs_tax_dollars_collected_by_treasury

from pol a 
INNER JOIN rev b on b.week_start_date = a.week_start_date
  ) _dbt_internal_unit_test_actual
),
-- Build expected result
dbt_internal_unit_test_expected as (
  select
    week_start_date, number_of_shares_traded, number_of_trades, dollars_collected_by_treasury, irs_tax_dollars_collected_by_treasury, 'expected' as "actual_or_expected"
  from (
    select 
    
        try_cast('2020-11-15' as DATE)
     as week_start_date, 
    
        try_cast('800' as FLOAT)
     as number_of_shares_traded, 
    
        try_cast('120' as NUMBER(38,0))
     as number_of_trades, 
    
        try_cast('8000' as NUMBER(31,0))
     as dollars_collected_by_treasury, 
    
        try_cast('6000' as NUMBER(31,0))
     as irs_tax_dollars_collected_by_treasury
  ) _dbt_internal_unit_test_expected
)
-- Union actual and expected results
select * from dbt_internal_unit_test_actual
union all
select * from dbt_internal_unit_test_expected
[0m16:06:33.797631 [debug] [Thread-1 (]: SQL status: SUCCESS 2 in 0.258 seconds
[0m16:06:33.808042 [debug] [Thread-1 (]: Applying DROP to: DATAEXPERT_STUDENT.andres.test_amount_sum__dbt_tmp
[0m16:06:33.816105 [debug] [Thread-1 (]: Using snowflake connection "unit_test.dbt_capstone_andres.audit_weekly_metrics.test_amount_sum"
[0m16:06:33.816602 [debug] [Thread-1 (]: On unit_test.dbt_capstone_andres.audit_weekly_metrics.test_amount_sum: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "unit_test.dbt_capstone_andres.audit_weekly_metrics.test_amount_sum"} */
drop table if exists DATAEXPERT_STUDENT.andres.test_amount_sum__dbt_tmp cascade
[0m16:06:33.993884 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 0.177 seconds
[0m16:06:33.997340 [debug] [Thread-1 (]: On unit_test.dbt_capstone_andres.audit_weekly_metrics.test_amount_sum: Close
[0m16:06:34.221626 [info ] [Thread-1 (]: 3 of 3 PASS audit_weekly_metrics::test_amount_sum .............................. [[32mPASS[0m in 2.70s]
[0m16:06:34.222365 [debug] [Thread-1 (]: Finished running node unit_test.dbt_capstone_andres.audit_weekly_metrics.test_amount_sum
[0m16:06:34.223573 [debug] [MainThread]: Connection 'master' was properly closed.
[0m16:06:34.223941 [debug] [MainThread]: Connection 'unit_test.dbt_capstone_andres.audit_weekly_metrics.test_amount_sum' was properly closed.
[0m16:06:34.224325 [info ] [MainThread]: 
[0m16:06:34.224708 [info ] [MainThread]: Finished running 2 data tests, 1 unit test in 0 hours 0 minutes and 6.45 seconds (6.45s).
[0m16:06:34.225705 [debug] [MainThread]: Command end result
[0m16:06:34.276772 [info ] [MainThread]: 
[0m16:06:34.277305 [info ] [MainThread]: [32mCompleted successfully[0m
[0m16:06:34.277679 [info ] [MainThread]: 
[0m16:06:34.278055 [info ] [MainThread]: Done. PASS=3 WARN=0 ERROR=0 SKIP=0 TOTAL=3
[0m16:06:34.280215 [debug] [MainThread]: Resource report: {"command_name": "test", "command_success": true, "command_wall_clock_time": 9.006685, "process_user_time": 3.45406, "process_kernel_time": 0.725731, "process_mem_max_rss": "194957312", "process_in_blocks": "0", "process_out_blocks": "0"}
[0m16:06:34.280855 [debug] [MainThread]: Command `dbt test` succeeded at 16:06:34.280752 after 9.01 seconds
[0m16:06:34.281339 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104ab2b10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1010d4b90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x101058ad0>]}
[0m16:06:34.282162 [debug] [MainThread]: Flushing usage events
[0m16:06:34.454208 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m16:06:56.188306 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10eae4810>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10eb52710>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10eb52d10>]}


============================== 16:06:56.192268 | 156dec71-a05a-4a1f-b02a-878ad13b99a1 ==============================
[0m16:06:56.192268 [info ] [MainThread]: Running with dbt=1.9.0-b3
[0m16:06:56.192862 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '.', 'debug': 'False', 'warn_error': 'None', 'log_path': 'logs', 'fail_fast': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'invocation_command': 'dbt source freshness', 'introspect': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m16:06:57.124940 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '156dec71-a05a-4a1f-b02a-878ad13b99a1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114dc15d0>]}
[0m16:06:57.182881 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '156dec71-a05a-4a1f-b02a-878ad13b99a1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e9e0810>]}
[0m16:06:57.183709 [info ] [MainThread]: Registered adapter: snowflake=1.8.4
[0m16:06:57.372179 [debug] [MainThread]: checksum: 5f4fde221ca8627174a0341efc73099aaec05b8546408ceed6f59b1ec0755115, vars: {}, profile: , target: , version: 1.9.0b3
[0m16:06:57.578633 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m16:06:57.579070 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m16:06:57.584106 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 2 unused configuration paths:
- models.staging
- models.marts
[0m16:06:57.625619 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '156dec71-a05a-4a1f-b02a-878ad13b99a1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1151d1190>]}
[0m16:06:57.770925 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '156dec71-a05a-4a1f-b02a-878ad13b99a1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1150c5050>]}
[0m16:06:57.771570 [info ] [MainThread]: Found 5 models, 2 data tests, 2 sources, 862 macros, 1 unit test
[0m16:06:57.772014 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '156dec71-a05a-4a1f-b02a-878ad13b99a1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11509cfd0>]}
[0m16:06:57.774161 [info ] [MainThread]: 
[0m16:06:57.774677 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m16:06:57.775055 [info ] [MainThread]: 
[0m16:06:57.775847 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m16:06:57.782825 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_DATAEXPERT_STUDENT_andres'
[0m16:06:57.802494 [debug] [ThreadPool]: Using snowflake connection "list_DATAEXPERT_STUDENT_andres"
[0m16:06:57.803006 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "connection_name": "list_DATAEXPERT_STUDENT_andres"} */
show objects in DATAEXPERT_STUDENT.andres limit 10000
[0m16:06:57.803383 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:06:58.662349 [debug] [ThreadPool]: SQL status: SUCCESS 28 in 0.859 seconds
[0m16:06:58.666032 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: Close
[0m16:06:58.927022 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '156dec71-a05a-4a1f-b02a-878ad13b99a1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114e69a10>]}
[0m16:06:58.927842 [info ] [MainThread]: Pulling freshness from warehouse metadata tables for 0 sources
[0m16:06:58.930343 [debug] [Thread-1 (]: Began running node source.dbt_capstone_andres.treasury.fiscal_data
[0m16:06:58.930882 [info ] [Thread-1 (]: 1 of 1 START freshness of treasury.fiscal_data ................................. [RUN]
[0m16:06:58.931473 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_DATAEXPERT_STUDENT_andres, now source.dbt_capstone_andres.treasury.fiscal_data)
[0m16:06:58.931960 [debug] [Thread-1 (]: Began compiling node source.dbt_capstone_andres.treasury.fiscal_data
[0m16:06:58.932447 [debug] [Thread-1 (]: Began executing node source.dbt_capstone_andres.treasury.fiscal_data
[0m16:06:58.942848 [debug] [Thread-1 (]: Using snowflake connection "source.dbt_capstone_andres.treasury.fiscal_data"
[0m16:06:58.943566 [debug] [Thread-1 (]: On source.dbt_capstone_andres.treasury.fiscal_data: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "source.dbt_capstone_andres.treasury.fiscal_data"} */
select
      max(record_date) as max_loaded_at,
      convert_timezone('UTC', current_timestamp()) as snapshotted_at
    from dataexpert_student.andres.fiscal_data
[0m16:06:58.944073 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m16:06:59.684802 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 0.741 seconds
[0m16:06:59.687234 [debug] [Thread-1 (]: On source.dbt_capstone_andres.treasury.fiscal_data: Close
[0m16:06:59.922280 [debug] [Thread-1 (]: Database Error in source fiscal_data (models/staging/_sources.yml)
  Expected a timestamp value when querying field 'record_date' of table dataexpert_student.andres.fiscal_data but received value of type 'date' instead
[0m16:06:59.923161 [error] [Thread-1 (]: 1 of 1 ERROR freshness of treasury.fiscal_data ................................. [[31mERROR[0m in 0.99s]
[0m16:06:59.924081 [debug] [Thread-1 (]: Finished running node source.dbt_capstone_andres.treasury.fiscal_data
[0m16:06:59.925576 [debug] [MainThread]: Connection 'master' was properly closed.
[0m16:06:59.925994 [debug] [MainThread]: Connection 'source.dbt_capstone_andres.treasury.fiscal_data' was properly closed.
[0m16:06:59.926414 [info ] [MainThread]: 
[0m16:06:59.926864 [info ] [MainThread]: Finished running 1 source in 0 hours 0 minutes and 2.15 seconds (2.15s).
[0m16:06:59.973989 [info ] [MainThread]: 
[0m16:06:59.974496 [error] [MainThread]:   Database Error in source fiscal_data (models/staging/_sources.yml)
  Expected a timestamp value when querying field 'record_date' of table dataexpert_student.andres.fiscal_data but received value of type 'date' instead
[0m16:06:59.974861 [info ] [MainThread]: Done.
[0m16:06:59.976611 [debug] [MainThread]: Resource report: {"command_name": "freshness", "command_wall_clock_time": 3.850664, "process_user_time": 2.914864, "process_kernel_time": 0.501662, "process_mem_max_rss": "189337600", "command_success": false, "process_in_blocks": "0", "process_out_blocks": "0"}
[0m16:06:59.977389 [debug] [MainThread]: Command `dbt source freshness` failed at 16:06:59.977244 after 3.85 seconds
[0m16:06:59.977848 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10eae4110>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ae7cbd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10e529d90>]}
[0m16:06:59.978293 [debug] [MainThread]: Flushing usage events
[0m16:07:00.153583 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m21:18:08.928773 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4b5ee05290>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4b5f138ad0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4b5ee05a10>]}
[0m21:18:08.928873 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8cc8ee43d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8cc8ee6d90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8cc8e92a90>]}


============================== 21:18:08.936596 | 0fd9f22f-0114-42df-8d7c-005eea4e5938 ==============================
[0m21:18:08.936596 [info ] [MainThread]: Running with dbt=1.9.0-b3


============================== 21:18:08.937944 | e772b2e8-60dd-4019-80dc-fe8babebec55 ==============================
[0m21:18:08.937944 [info ] [MainThread]: Running with dbt=1.9.0-b3
[0m21:18:08.937803 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/usr/local/airflow/dags/capstone/dbt_capstone_andres', 'debug': 'False', 'version_check': 'True', 'log_path': '/usr/local/airflow/dags/capstone/dbt_capstone_andres/logs', 'warn_error': 'None', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt source freshness', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m21:18:08.939213 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'profiles_dir': '/usr/local/airflow/dags/capstone/dbt_capstone_andres', 'log_path': '/usr/local/airflow/dags/capstone/dbt_capstone_andres/logs', 'debug': 'False', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'static_parser': 'True', 'log_format': 'default', 'target_path': 'None', 'invocation_command': 'dbt source freshness', 'send_anonymous_usage_stats': 'True'}
[0m21:18:08.941013 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc2e3e7c810>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc2e3e76690>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc2e3e7c990>]}


============================== 21:18:08.944932 | 88aa81b0-2e97-41c9-b5ab-d8b52fd7cfa6 ==============================
[0m21:18:08.944932 [info ] [MainThread]: Running with dbt=1.9.0-b3
[0m21:18:08.946577 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'profiles_dir': '/usr/local/airflow/dags/capstone/dbt_capstone_andres', 'log_path': '/usr/local/airflow/dags/capstone/dbt_capstone_andres/logs', 'fail_fast': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'static_parser': 'True', 'invocation_command': 'dbt source freshness', 'introspect': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m21:18:08.950426 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f67abbe61d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f67ac8b4290>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f67abbe6050>]}


============================== 21:18:08.954652 | 854f5af8-ad16-4b74-8d05-36e9bd90bc8d ==============================
[0m21:18:08.954652 [info ] [MainThread]: Running with dbt=1.9.0-b3
[0m21:18:08.956553 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'profiles_dir': '/usr/local/airflow/dags/capstone/dbt_capstone_andres', 'log_path': '/usr/local/airflow/dags/capstone/dbt_capstone_andres/logs', 'debug': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'log_format': 'default', 'static_parser': 'True', 'target_path': 'None', 'invocation_command': 'dbt source freshness', 'send_anonymous_usage_stats': 'True'}
[0m21:18:08.989685 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7083595190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7083897890>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7083594dd0>]}


============================== 21:18:08.993586 | 6850da95-05de-4349-b848-2a8812439725 ==============================
[0m21:18:08.993586 [info ] [MainThread]: Running with dbt=1.9.0-b3
[0m21:18:08.994648 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'debug': 'False', 'log_path': '/usr/local/airflow/dags/capstone/dbt_capstone_andres/logs', 'profiles_dir': '/usr/local/airflow/dags/capstone/dbt_capstone_andres', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'log_format': 'default', 'invocation_command': 'dbt source freshness', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m21:18:10.542256 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '6850da95-05de-4349-b848-2a8812439725', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f705c2dc710>]}
[0m21:18:10.545818 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '854f5af8-ad16-4b74-8d05-36e9bd90bc8d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f67abbed310>]}
[0m21:18:10.550146 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'e772b2e8-60dd-4019-80dc-fe8babebec55', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8ca19c0290>]}
[0m21:18:10.552250 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '88aa81b0-2e97-41c9-b5ab-d8b52fd7cfa6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc2e2781890>]}
[0m21:18:10.560062 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '0fd9f22f-0114-42df-8d7c-005eea4e5938', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4b339d9cd0>]}
[0m21:18:10.630729 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '6850da95-05de-4349-b848-2a8812439725', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f70836955d0>]}
[0m21:18:10.632578 [info ] [MainThread]: Registered adapter: snowflake=1.8.4
[0m21:18:10.640494 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '88aa81b0-2e97-41c9-b5ab-d8b52fd7cfa6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc2e526f410>]}
[0m21:18:10.640898 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'e772b2e8-60dd-4019-80dc-fe8babebec55', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8cca28b650>]}
[0m21:18:10.642493 [info ] [MainThread]: Registered adapter: snowflake=1.8.4
[0m21:18:10.644095 [info ] [MainThread]: Registered adapter: snowflake=1.8.4
[0m21:18:10.648396 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '854f5af8-ad16-4b74-8d05-36e9bd90bc8d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f67abce5610>]}
[0m21:18:10.650555 [info ] [MainThread]: Registered adapter: snowflake=1.8.4
[0m21:18:10.657956 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '0fd9f22f-0114-42df-8d7c-005eea4e5938', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4b5ef05510>]}
[0m21:18:10.659876 [info ] [MainThread]: Registered adapter: snowflake=1.8.4
[0m21:18:10.737101 [debug] [MainThread]: checksum: 5f4fde221ca8627174a0341efc73099aaec05b8546408ceed6f59b1ec0755115, vars: {}, profile: , target: , version: 1.9.0b3
[0m21:18:10.741357 [debug] [MainThread]: checksum: 5f4fde221ca8627174a0341efc73099aaec05b8546408ceed6f59b1ec0755115, vars: {}, profile: , target: , version: 1.9.0b3
[0m21:18:10.743426 [debug] [MainThread]: checksum: 5f4fde221ca8627174a0341efc73099aaec05b8546408ceed6f59b1ec0755115, vars: {}, profile: , target: , version: 1.9.0b3
[0m21:18:10.751788 [debug] [MainThread]: checksum: 5f4fde221ca8627174a0341efc73099aaec05b8546408ceed6f59b1ec0755115, vars: {}, profile: , target: , version: 1.9.0b3
[0m21:18:10.757558 [debug] [MainThread]: checksum: 5f4fde221ca8627174a0341efc73099aaec05b8546408ceed6f59b1ec0755115, vars: {}, profile: , target: , version: 1.9.0b3
[0m21:18:11.509211 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m21:18:11.510336 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m21:18:11.519172 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m21:18:11.519178 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 2 unused configuration paths:
- models.marts
- models.staging
[0m21:18:11.520451 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m21:18:11.530472 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m21:18:11.531883 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m21:18:11.534920 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 2 unused configuration paths:
- models.marts
- models.staging
[0m21:18:11.540757 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 2 unused configuration paths:
- models.staging
- models.marts
[0m21:18:11.543660 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m21:18:11.544967 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m21:18:11.551597 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m21:18:11.552699 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m21:18:11.555011 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 2 unused configuration paths:
- models.staging
- models.marts
[0m21:18:11.561244 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 2 unused configuration paths:
- models.marts
- models.staging
[0m21:18:11.594970 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'e772b2e8-60dd-4019-80dc-fe8babebec55', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8cc7daaf50>]}
[0m21:18:11.606989 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '88aa81b0-2e97-41c9-b5ab-d8b52fd7cfa6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc2bc618910>]}
[0m21:18:11.612774 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '6850da95-05de-4349-b848-2a8812439725', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7057d4cfd0>]}
[0m21:18:11.625675 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '0fd9f22f-0114-42df-8d7c-005eea4e5938', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4b335c4790>]}
[0m21:18:11.638307 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '854f5af8-ad16-4b74-8d05-36e9bd90bc8d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f678486c490>]}
[0m21:18:11.947296 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'e772b2e8-60dd-4019-80dc-fe8babebec55', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8ca16a5e10>]}
[0m21:18:11.948419 [info ] [MainThread]: Found 5 models, 2 data tests, 2 sources, 862 macros, 1 unit test
[0m21:18:11.949418 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e772b2e8-60dd-4019-80dc-fe8babebec55', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8ca1972150>]}
[0m21:18:11.951866 [info ] [MainThread]: 
[0m21:18:11.952820 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m21:18:11.953631 [info ] [MainThread]: 
[0m21:18:11.954725 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m21:18:11.962188 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_DATAEXPERT_STUDENT_andres'
[0m21:18:11.987621 [debug] [ThreadPool]: Using snowflake connection "list_DATAEXPERT_STUDENT_andres"
[0m21:18:11.988515 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "connection_name": "list_DATAEXPERT_STUDENT_andres"} */
show objects in DATAEXPERT_STUDENT.andres limit 10000
[0m21:18:11.989236 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:18:12.014392 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '88aa81b0-2e97-41c9-b5ab-d8b52fd7cfa6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc2bc8d5dd0>]}
[0m21:18:12.015320 [info ] [MainThread]: Found 5 models, 2 data tests, 2 sources, 862 macros, 1 unit test
[0m21:18:12.016180 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '88aa81b0-2e97-41c9-b5ab-d8b52fd7cfa6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc2e4295310>]}
[0m21:18:12.018306 [info ] [MainThread]: 
[0m21:18:12.019326 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m21:18:12.020144 [info ] [MainThread]: 
[0m21:18:12.021103 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m21:18:12.027205 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_DATAEXPERT_STUDENT_andres'
[0m21:18:12.048884 [debug] [ThreadPool]: Using snowflake connection "list_DATAEXPERT_STUDENT_andres"
[0m21:18:12.049711 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "connection_name": "list_DATAEXPERT_STUDENT_andres"} */
show objects in DATAEXPERT_STUDENT.andres limit 10000
[0m21:18:12.050322 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:18:12.085771 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '854f5af8-ad16-4b74-8d05-36e9bd90bc8d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6784436190>]}
[0m21:18:12.088051 [info ] [MainThread]: Found 5 models, 2 data tests, 2 sources, 862 macros, 1 unit test
[0m21:18:12.089611 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '854f5af8-ad16-4b74-8d05-36e9bd90bc8d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f678441a150>]}
[0m21:18:12.093395 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '0fd9f22f-0114-42df-8d7c-005eea4e5938', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4b3384a5d0>]}
[0m21:18:12.093480 [info ] [MainThread]: 
[0m21:18:12.094816 [info ] [MainThread]: Found 5 models, 2 data tests, 2 sources, 862 macros, 1 unit test
[0m21:18:12.095619 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m21:18:12.096642 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '0fd9f22f-0114-42df-8d7c-005eea4e5938', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4b33832410>]}
[0m21:18:12.097489 [info ] [MainThread]: 
[0m21:18:12.100234 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '6850da95-05de-4349-b848-2a8812439725', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f705c0fa050>]}
[0m21:18:12.100445 [info ] [MainThread]: 
[0m21:18:12.100692 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m21:18:12.102217 [info ] [MainThread]: Found 5 models, 2 data tests, 2 sources, 862 macros, 1 unit test
[0m21:18:12.102152 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m21:18:12.103581 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6850da95-05de-4349-b848-2a8812439725', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f705c0de3d0>]}
[0m21:18:12.105436 [info ] [MainThread]: 
[0m21:18:12.107758 [info ] [MainThread]: 
[0m21:18:12.107775 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m21:18:12.109210 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m21:18:12.110215 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_DATAEXPERT_STUDENT_andres'
[0m21:18:12.110585 [info ] [MainThread]: 
[0m21:18:12.112675 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m21:18:12.118499 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_DATAEXPERT_STUDENT_andres'
[0m21:18:12.121863 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_DATAEXPERT_STUDENT_andres'
[0m21:18:12.139031 [debug] [ThreadPool]: Using snowflake connection "list_DATAEXPERT_STUDENT_andres"
[0m21:18:12.143471 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "connection_name": "list_DATAEXPERT_STUDENT_andres"} */
show objects in DATAEXPERT_STUDENT.andres limit 10000
[0m21:18:12.144375 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:18:12.144518 [debug] [ThreadPool]: Using snowflake connection "list_DATAEXPERT_STUDENT_andres"
[0m21:18:12.145442 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "connection_name": "list_DATAEXPERT_STUDENT_andres"} */
show objects in DATAEXPERT_STUDENT.andres limit 10000
[0m21:18:12.146246 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:18:12.147208 [debug] [ThreadPool]: Using snowflake connection "list_DATAEXPERT_STUDENT_andres"
[0m21:18:12.148168 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "connection_name": "list_DATAEXPERT_STUDENT_andres"} */
show objects in DATAEXPERT_STUDENT.andres limit 10000
[0m21:18:12.149340 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:18:12.828005 [debug] [ThreadPool]: SQL status: SUCCESS 28 in 0.839 seconds
[0m21:18:12.831798 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: Close
[0m21:18:12.889235 [debug] [ThreadPool]: SQL status: SUCCESS 28 in 0.839 seconds
[0m21:18:12.892679 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: Close
[0m21:18:12.914322 [debug] [ThreadPool]: SQL status: SUCCESS 28 in 0.770 seconds
[0m21:18:12.915092 [debug] [ThreadPool]: SQL status: SUCCESS 28 in 0.766 seconds
[0m21:18:12.917975 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: Close
[0m21:18:12.918865 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: Close
[0m21:18:12.933221 [debug] [ThreadPool]: SQL status: SUCCESS 28 in 0.787 seconds
[0m21:18:12.936939 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: Close
[0m21:18:13.130442 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e772b2e8-60dd-4019-80dc-fe8babebec55', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8ca1b86f50>]}
[0m21:18:13.130794 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '88aa81b0-2e97-41c9-b5ab-d8b52fd7cfa6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc2bc8d7890>]}
[0m21:18:13.131746 [info ] [MainThread]: Pulling freshness from warehouse metadata tables for 0 sources
[0m21:18:13.131897 [info ] [MainThread]: Pulling freshness from warehouse metadata tables for 0 sources
[0m21:18:13.138784 [debug] [Thread-1 (]: Began running node source.dbt_capstone_andres.treasury.fiscal_data
[0m21:18:13.139134 [debug] [Thread-1 (]: Began running node source.dbt_capstone_andres.treasury.fiscal_data
[0m21:18:13.140169 [info ] [Thread-1 (]: 1 of 1 START freshness of treasury.fiscal_data ................................. [RUN]
[0m21:18:13.140335 [info ] [Thread-1 (]: 1 of 1 START freshness of treasury.fiscal_data ................................. [RUN]
[0m21:18:13.142089 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_DATAEXPERT_STUDENT_andres, now source.dbt_capstone_andres.treasury.fiscal_data)
[0m21:18:13.142535 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_DATAEXPERT_STUDENT_andres, now source.dbt_capstone_andres.treasury.fiscal_data)
[0m21:18:13.143241 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6850da95-05de-4349-b848-2a8812439725', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7057d4cbd0>]}
[0m21:18:13.143713 [debug] [Thread-1 (]: Began compiling node source.dbt_capstone_andres.treasury.fiscal_data
[0m21:18:13.144264 [debug] [Thread-1 (]: Began compiling node source.dbt_capstone_andres.treasury.fiscal_data
[0m21:18:13.145051 [info ] [MainThread]: Pulling freshness from warehouse metadata tables for 0 sources
[0m21:18:13.145078 [debug] [Thread-1 (]: Began executing node source.dbt_capstone_andres.treasury.fiscal_data
[0m21:18:13.145622 [debug] [Thread-1 (]: Began executing node source.dbt_capstone_andres.treasury.fiscal_data
[0m21:18:13.147758 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '854f5af8-ad16-4b74-8d05-36e9bd90bc8d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6784474610>]}
[0m21:18:13.149028 [info ] [MainThread]: Pulling freshness from warehouse metadata tables for 0 sources
[0m21:18:13.151697 [debug] [Thread-1 (]: Began running node source.dbt_capstone_andres.treasury.fiscal_data
[0m21:18:13.152857 [info ] [Thread-1 (]: 1 of 1 START freshness of treasury.fiscal_data ................................. [RUN]
[0m21:18:13.154928 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_DATAEXPERT_STUDENT_andres, now source.dbt_capstone_andres.treasury.fiscal_data)
[0m21:18:13.154881 [debug] [Thread-1 (]: Began running node source.dbt_capstone_andres.treasury.fiscal_data
[0m21:18:13.156043 [debug] [Thread-1 (]: Began compiling node source.dbt_capstone_andres.treasury.fiscal_data
[0m21:18:13.156357 [info ] [Thread-1 (]: 1 of 1 START freshness of treasury.fiscal_data ................................. [RUN]
[0m21:18:13.157739 [debug] [Thread-1 (]: Began executing node source.dbt_capstone_andres.treasury.fiscal_data
[0m21:18:13.158370 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_DATAEXPERT_STUDENT_andres, now source.dbt_capstone_andres.treasury.fiscal_data)
[0m21:18:13.159282 [debug] [Thread-1 (]: Using snowflake connection "source.dbt_capstone_andres.treasury.fiscal_data"
[0m21:18:13.159605 [debug] [Thread-1 (]: Began compiling node source.dbt_capstone_andres.treasury.fiscal_data
[0m21:18:13.160062 [debug] [Thread-1 (]: Using snowflake connection "source.dbt_capstone_andres.treasury.fiscal_data"
[0m21:18:13.160425 [debug] [Thread-1 (]: On source.dbt_capstone_andres.treasury.fiscal_data: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "source.dbt_capstone_andres.treasury.fiscal_data"} */
select
      max(record_date) as max_loaded_at,
      convert_timezone('UTC', current_timestamp()) as snapshotted_at
    from dataexpert_student.andres.fiscal_data
[0m21:18:13.160974 [debug] [Thread-1 (]: Began executing node source.dbt_capstone_andres.treasury.fiscal_data
[0m21:18:13.161774 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m21:18:13.161247 [debug] [Thread-1 (]: On source.dbt_capstone_andres.treasury.fiscal_data: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "source.dbt_capstone_andres.treasury.fiscal_data"} */
select
      max(record_date) as max_loaded_at,
      convert_timezone('UTC', current_timestamp()) as snapshotted_at
    from dataexpert_student.andres.fiscal_data
[0m21:18:13.162993 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m21:18:13.164018 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '0fd9f22f-0114-42df-8d7c-005eea4e5938', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4b5fbd1750>]}
[0m21:18:13.166241 [info ] [MainThread]: Pulling freshness from warehouse metadata tables for 0 sources
[0m21:18:13.172875 [debug] [Thread-1 (]: Began running node source.dbt_capstone_andres.treasury.fiscal_data
[0m21:18:13.173567 [debug] [Thread-1 (]: Using snowflake connection "source.dbt_capstone_andres.treasury.fiscal_data"
[0m21:18:13.174618 [info ] [Thread-1 (]: 1 of 1 START freshness of treasury.fiscal_data ................................. [RUN]
[0m21:18:13.175004 [debug] [Thread-1 (]: On source.dbt_capstone_andres.treasury.fiscal_data: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "source.dbt_capstone_andres.treasury.fiscal_data"} */
select
      max(record_date) as max_loaded_at,
      convert_timezone('UTC', current_timestamp()) as snapshotted_at
    from dataexpert_student.andres.fiscal_data
[0m21:18:13.176139 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_DATAEXPERT_STUDENT_andres, now source.dbt_capstone_andres.treasury.fiscal_data)
[0m21:18:13.176376 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m21:18:13.177060 [debug] [Thread-1 (]: Began compiling node source.dbt_capstone_andres.treasury.fiscal_data
[0m21:18:13.177926 [debug] [Thread-1 (]: Using snowflake connection "source.dbt_capstone_andres.treasury.fiscal_data"
[0m21:18:13.177983 [debug] [Thread-1 (]: Began executing node source.dbt_capstone_andres.treasury.fiscal_data
[0m21:18:13.179002 [debug] [Thread-1 (]: On source.dbt_capstone_andres.treasury.fiscal_data: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "source.dbt_capstone_andres.treasury.fiscal_data"} */
select
      max(record_date) as max_loaded_at,
      convert_timezone('UTC', current_timestamp()) as snapshotted_at
    from dataexpert_student.andres.fiscal_data
[0m21:18:13.180332 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m21:18:13.193037 [debug] [Thread-1 (]: Using snowflake connection "source.dbt_capstone_andres.treasury.fiscal_data"
[0m21:18:13.193997 [debug] [Thread-1 (]: On source.dbt_capstone_andres.treasury.fiscal_data: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "source.dbt_capstone_andres.treasury.fiscal_data"} */
select
      max(record_date) as max_loaded_at,
      convert_timezone('UTC', current_timestamp()) as snapshotted_at
    from dataexpert_student.andres.fiscal_data
[0m21:18:13.194839 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m21:18:13.839596 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 0.677 seconds
[0m21:18:13.839526 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 0.678 seconds
[0m21:18:13.844627 [debug] [Thread-1 (]: On source.dbt_capstone_andres.treasury.fiscal_data: Close
[0m21:18:13.844630 [debug] [Thread-1 (]: On source.dbt_capstone_andres.treasury.fiscal_data: Close
[0m21:18:13.861662 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 0.685 seconds
[0m21:18:13.861881 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 0.682 seconds
[0m21:18:13.863846 [debug] [Thread-1 (]: On source.dbt_capstone_andres.treasury.fiscal_data: Close
[0m21:18:13.864003 [debug] [Thread-1 (]: On source.dbt_capstone_andres.treasury.fiscal_data: Close
[0m21:18:13.917121 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 0.722 seconds
[0m21:18:13.922817 [debug] [Thread-1 (]: On source.dbt_capstone_andres.treasury.fiscal_data: Close
[0m21:18:14.070113 [debug] [Thread-1 (]: Database Error in source fiscal_data (models/staging/_sources.yml)
  Expected a timestamp value when querying field 'record_date' of table dataexpert_student.andres.fiscal_data but received value of type 'date' instead
[0m21:18:14.071505 [error] [Thread-1 (]: 1 of 1 ERROR freshness of treasury.fiscal_data ................................. [[31mERROR[0m in 0.93s]
[0m21:18:14.073087 [debug] [Thread-1 (]: Finished running node source.dbt_capstone_andres.treasury.fiscal_data
[0m21:18:14.077166 [debug] [MainThread]: Connection 'master' was properly closed.
[0m21:18:14.078697 [debug] [MainThread]: Connection 'source.dbt_capstone_andres.treasury.fiscal_data' was properly closed.
[0m21:18:14.079616 [info ] [MainThread]: 
[0m21:18:14.080729 [info ] [MainThread]: Finished running 1 source in 0 hours 0 minutes and 2.06 seconds (2.06s).
[0m21:18:14.092959 [debug] [Thread-1 (]: Database Error in source fiscal_data (models/staging/_sources.yml)
  Expected a timestamp value when querying field 'record_date' of table dataexpert_student.andres.fiscal_data but received value of type 'date' instead
[0m21:18:14.094419 [error] [Thread-1 (]: 1 of 1 ERROR freshness of treasury.fiscal_data ................................. [[31mERROR[0m in 0.95s]
[0m21:18:14.096543 [debug] [Thread-1 (]: Finished running node source.dbt_capstone_andres.treasury.fiscal_data
[0m21:18:14.099909 [debug] [MainThread]: Connection 'master' was properly closed.
[0m21:18:14.101091 [debug] [MainThread]: Connection 'source.dbt_capstone_andres.treasury.fiscal_data' was properly closed.
[0m21:18:14.102347 [info ] [MainThread]: 
[0m21:18:14.103498 [info ] [MainThread]: Finished running 1 source in 0 hours 0 minutes and 2.15 seconds (2.15s).
[0m21:18:14.127855 [debug] [Thread-1 (]: Database Error in source fiscal_data (models/staging/_sources.yml)
  Expected a timestamp value when querying field 'record_date' of table dataexpert_student.andres.fiscal_data but received value of type 'date' instead
[0m21:18:14.127884 [debug] [Thread-1 (]: Database Error in source fiscal_data (models/staging/_sources.yml)
  Expected a timestamp value when querying field 'record_date' of table dataexpert_student.andres.fiscal_data but received value of type 'date' instead
[0m21:18:14.132465 [error] [Thread-1 (]: 1 of 1 ERROR freshness of treasury.fiscal_data ................................. [[31mERROR[0m in 0.97s]
[0m21:18:14.132588 [error] [Thread-1 (]: 1 of 1 ERROR freshness of treasury.fiscal_data ................................. [[31mERROR[0m in 0.98s]
[0m21:18:14.134592 [debug] [Thread-1 (]: Finished running node source.dbt_capstone_andres.treasury.fiscal_data
[0m21:18:14.134696 [debug] [Thread-1 (]: Finished running node source.dbt_capstone_andres.treasury.fiscal_data
[0m21:18:14.134785 [debug] [Thread-1 (]: Database Error in source fiscal_data (models/staging/_sources.yml)
  Expected a timestamp value when querying field 'record_date' of table dataexpert_student.andres.fiscal_data but received value of type 'date' instead
[0m21:18:14.136776 [error] [Thread-1 (]: 1 of 1 ERROR freshness of treasury.fiscal_data ................................. [[31mERROR[0m in 0.96s]
[0m21:18:14.138910 [debug] [Thread-1 (]: Finished running node source.dbt_capstone_andres.treasury.fiscal_data
[0m21:18:14.138504 [debug] [MainThread]: Connection 'master' was properly closed.
[0m21:18:14.138852 [debug] [MainThread]: Connection 'master' was properly closed.
[0m21:18:14.140194 [debug] [MainThread]: Connection 'source.dbt_capstone_andres.treasury.fiscal_data' was properly closed.
[0m21:18:14.140379 [debug] [MainThread]: Connection 'source.dbt_capstone_andres.treasury.fiscal_data' was properly closed.
[0m21:18:14.141931 [debug] [MainThread]: Connection 'master' was properly closed.
[0m21:18:14.142569 [info ] [MainThread]: 
[0m21:18:14.142848 [info ] [MainThread]: 
[0m21:18:14.144256 [info ] [MainThread]: Finished running 1 source in 0 hours 0 minutes and 2.03 seconds (2.03s).
[0m21:18:14.144048 [info ] [MainThread]: Finished running 1 source in 0 hours 0 minutes and 2.04 seconds (2.04s).
[0m21:18:14.143469 [debug] [MainThread]: Connection 'source.dbt_capstone_andres.treasury.fiscal_data' was properly closed.
[0m21:18:14.149344 [info ] [MainThread]: 
[0m21:18:14.150762 [info ] [MainThread]: Finished running 1 source in 0 hours 0 minutes and 2.04 seconds (2.04s).
[0m21:18:14.309776 [info ] [MainThread]: 
[0m21:18:14.310673 [error] [MainThread]:   Database Error in source fiscal_data (models/staging/_sources.yml)
  Expected a timestamp value when querying field 'record_date' of table dataexpert_student.andres.fiscal_data but received value of type 'date' instead
[0m21:18:14.311450 [info ] [MainThread]: Done.
[0m21:18:14.313260 [debug] [MainThread]: Resource report: {"command_name": "freshness", "command_wall_clock_time": 5.4493184, "process_user_time": 4.687945, "process_kernel_time": 0.967024, "process_mem_max_rss": "202080", "process_in_blocks": "592", "process_out_blocks": "3684", "command_success": false}
[0m21:18:14.314902 [debug] [MainThread]: Command `dbt source freshness` failed at 21:18:14.314730 after 5.45 seconds
[0m21:18:14.316312 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc2e4183ad0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc2e3e7e210>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc2e7bf4f50>]}
[0m21:18:14.317270 [debug] [MainThread]: Flushing usage events
[0m21:18:14.366627 [info ] [MainThread]: 
[0m21:18:14.367597 [error] [MainThread]:   Database Error in source fiscal_data (models/staging/_sources.yml)
  Expected a timestamp value when querying field 'record_date' of table dataexpert_student.andres.fiscal_data but received value of type 'date' instead
[0m21:18:14.368305 [info ] [MainThread]: Done.
[0m21:18:14.369725 [debug] [MainThread]: Resource report: {"command_name": "freshness", "command_wall_clock_time": 5.5125103, "process_user_time": 4.765696, "process_kernel_time": 1.13626, "process_mem_max_rss": "201668", "process_in_blocks": "632", "process_out_blocks": "3684", "command_success": false}
[0m21:18:14.370137 [info ] [MainThread]: 
[0m21:18:14.370843 [debug] [MainThread]: Command `dbt source freshness` failed at 21:18:14.370705 after 5.51 seconds
[0m21:18:14.371327 [error] [MainThread]:   Database Error in source fiscal_data (models/staging/_sources.yml)
  Expected a timestamp value when querying field 'record_date' of table dataexpert_student.andres.fiscal_data but received value of type 'date' instead
[0m21:18:14.371816 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4b5ee73ad0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4b5f103910>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4b62c19550>]}
[0m21:18:14.372121 [info ] [MainThread]: Done.
[0m21:18:14.373235 [debug] [MainThread]: Flushing usage events
[0m21:18:14.373986 [debug] [MainThread]: Resource report: {"command_name": "freshness", "command_wall_clock_time": 5.516364, "process_user_time": 4.686134, "process_kernel_time": 1.097503, "process_mem_max_rss": "201648", "process_in_blocks": "1000", "process_out_blocks": "3684", "command_success": false}
[0m21:18:14.375644 [debug] [MainThread]: Command `dbt source freshness` failed at 21:18:14.375402 after 5.52 seconds
[0m21:18:14.377259 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8cc90a5690>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8cc90a5a50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8cccbf4b90>]}
[0m21:18:14.378580 [debug] [MainThread]: Flushing usage events
[0m21:18:14.404443 [info ] [MainThread]: 
[0m21:18:14.406240 [error] [MainThread]:   Database Error in source fiscal_data (models/staging/_sources.yml)
  Expected a timestamp value when querying field 'record_date' of table dataexpert_student.andres.fiscal_data but received value of type 'date' instead
[0m21:18:14.407302 [info ] [MainThread]: Done.
[0m21:18:14.409588 [debug] [MainThread]: Resource report: {"command_name": "freshness", "command_wall_clock_time": 5.5413103, "process_user_time": 4.668682, "process_kernel_time": 0.929595, "process_mem_max_rss": "201652", "process_in_blocks": "512", "process_out_blocks": "3684", "command_success": false}
[0m21:18:14.411402 [debug] [MainThread]: Command `dbt source freshness` failed at 21:18:14.411215 after 5.54 seconds
[0m21:18:14.412638 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f67aba57dd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f67aba57cd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f67af988cd0>]}
[0m21:18:14.413826 [debug] [MainThread]: Flushing usage events
[0m21:18:14.437185 [info ] [MainThread]: 
[0m21:18:14.438460 [error] [MainThread]:   Database Error in source fiscal_data (models/staging/_sources.yml)
  Expected a timestamp value when querying field 'record_date' of table dataexpert_student.andres.fiscal_data but received value of type 'date' instead
[0m21:18:14.439661 [info ] [MainThread]: Done.
[0m21:18:14.441429 [debug] [MainThread]: Resource report: {"command_name": "freshness", "command_wall_clock_time": 5.522208, "process_user_time": 4.610054, "process_kernel_time": 1.065692, "process_mem_max_rss": "201528", "process_in_blocks": "472", "process_out_blocks": "3684", "command_success": false}
[0m21:18:14.442728 [debug] [MainThread]: Command `dbt source freshness` failed at 21:18:14.442581 after 5.52 seconds
[0m21:18:14.443637 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7084268790>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7087344bd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f708358cad0>]}
[0m21:18:14.444676 [debug] [MainThread]: Flushing usage events
[0m21:18:14.517009 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m21:18:14.570362 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m21:18:14.570881 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m21:18:14.619258 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m21:18:14.631491 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m21:23:23.397908 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4128d1190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4129b4810>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb412917c10>]}


============================== 21:23:23.401041 | ffbfe39b-e31e-4333-9af4-3cb558e4e7d8 ==============================
[0m21:23:23.401041 [info ] [MainThread]: Running with dbt=1.9.0-b3
[0m21:23:23.402120 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'version_check': 'True', 'log_path': '/usr/local/airflow/dags/capstone/dbt_capstone_andres/logs', 'debug': 'False', 'profiles_dir': '/usr/local/airflow/dags/capstone/dbt_capstone_andres', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'log_format': 'default', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'target_path': 'None', 'invocation_command': 'dbt source freshness', 'send_anonymous_usage_stats': 'True'}
[0m21:23:23.406985 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6c697ddad0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6c6982bb10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6c6cbde290>]}


============================== 21:23:23.412833 | 74dba7a4-8136-41d9-acf9-79f0c9e9d8b3 ==============================
[0m21:23:23.412833 [info ] [MainThread]: Running with dbt=1.9.0-b3
[0m21:23:23.414212 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/usr/local/airflow/dags/capstone/dbt_capstone_andres', 'version_check': 'True', 'warn_error': 'None', 'log_path': '/usr/local/airflow/dags/capstone/dbt_capstone_andres/logs', 'fail_fast': 'False', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'log_format': 'default', 'static_parser': 'True', 'target_path': 'None', 'invocation_command': 'dbt source freshness', 'send_anonymous_usage_stats': 'True'}
[0m21:23:23.411775 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f79af5e0dd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f79b09e1690>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f79af5e9dd0>]}


============================== 21:23:23.415886 | 939cb7b1-1436-4a61-bb26-035a91fdff00 ==============================
[0m21:23:23.415886 [info ] [MainThread]: Running with dbt=1.9.0-b3
[0m21:23:23.418096 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'profiles_dir': '/usr/local/airflow/dags/capstone/dbt_capstone_andres', 'log_path': '/usr/local/airflow/dags/capstone/dbt_capstone_andres/logs', 'debug': 'False', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'log_format': 'default', 'introspect': 'True', 'target_path': 'None', 'invocation_command': 'dbt source freshness', 'send_anonymous_usage_stats': 'True'}
[0m21:23:23.426884 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc004116150>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc004448c90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc004116490>]}


============================== 21:23:23.431538 | b9009c94-9591-443f-83d7-086375b4bcc6 ==============================
[0m21:23:23.431538 [info ] [MainThread]: Running with dbt=1.9.0-b3
[0m21:23:23.435309 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'profiles_dir': '/usr/local/airflow/dags/capstone/dbt_capstone_andres', 'log_path': '/usr/local/airflow/dags/capstone/dbt_capstone_andres/logs', 'version_check': 'True', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt source freshness', 'static_parser': 'True', 'log_format': 'default', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m21:23:23.441569 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f63fd50d090>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f63fd506690>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f63fd50cf90>]}


============================== 21:23:23.445268 | ef289ee3-b22d-4e74-b3ed-5f721d49e42e ==============================
[0m21:23:23.445268 [info ] [MainThread]: Running with dbt=1.9.0-b3
[0m21:23:23.447082 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/usr/local/airflow/dags/capstone/dbt_capstone_andres', 'fail_fast': 'False', 'version_check': 'True', 'log_path': '/usr/local/airflow/dags/capstone/dbt_capstone_andres/logs', 'debug': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt source freshness', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m21:23:25.203425 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'b9009c94-9591-443f-83d7-086375b4bcc6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbfdcc917d0>]}
[0m21:23:25.204105 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '939cb7b1-1436-4a61-bb26-035a91fdff00', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f798832ee90>]}
[0m21:23:25.223041 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'ffbfe39b-e31e-4333-9af4-3cb558e4e7d8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb3e74ba090>]}
[0m21:23:25.226106 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'ef289ee3-b22d-4e74-b3ed-5f721d49e42e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f63fd624710>]}
[0m21:23:25.226173 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '74dba7a4-8136-41d9-acf9-79f0c9e9d8b3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6c42309710>]}
[0m21:23:25.277164 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'b9009c94-9591-443f-83d7-086375b4bcc6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc004215710>]}
[0m21:23:25.278655 [info ] [MainThread]: Registered adapter: snowflake=1.8.4
[0m21:23:25.281960 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '939cb7b1-1436-4a61-bb26-035a91fdff00', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f79b09fb3d0>]}
[0m21:23:25.283826 [info ] [MainThread]: Registered adapter: snowflake=1.8.4
[0m21:23:25.305784 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '74dba7a4-8136-41d9-acf9-79f0c9e9d8b3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6c6abd77d0>]}
[0m21:23:25.306651 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'ef289ee3-b22d-4e74-b3ed-5f721d49e42e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f63f09321d0>]}
[0m21:23:25.307223 [info ] [MainThread]: Registered adapter: snowflake=1.8.4
[0m21:23:25.308511 [info ] [MainThread]: Registered adapter: snowflake=1.8.4
[0m21:23:25.311031 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'ffbfe39b-e31e-4333-9af4-3cb558e4e7d8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb4129d5650>]}
[0m21:23:25.314296 [info ] [MainThread]: Registered adapter: snowflake=1.8.4
[0m21:23:25.364929 [debug] [MainThread]: checksum: 5f4fde221ca8627174a0341efc73099aaec05b8546408ceed6f59b1ec0755115, vars: {}, profile: , target: , version: 1.9.0b3
[0m21:23:25.369743 [debug] [MainThread]: checksum: 5f4fde221ca8627174a0341efc73099aaec05b8546408ceed6f59b1ec0755115, vars: {}, profile: , target: , version: 1.9.0b3
[0m21:23:25.390864 [debug] [MainThread]: checksum: 5f4fde221ca8627174a0341efc73099aaec05b8546408ceed6f59b1ec0755115, vars: {}, profile: , target: , version: 1.9.0b3
[0m21:23:25.394289 [debug] [MainThread]: checksum: 5f4fde221ca8627174a0341efc73099aaec05b8546408ceed6f59b1ec0755115, vars: {}, profile: , target: , version: 1.9.0b3
[0m21:23:25.396500 [debug] [MainThread]: checksum: 5f4fde221ca8627174a0341efc73099aaec05b8546408ceed6f59b1ec0755115, vars: {}, profile: , target: , version: 1.9.0b3
[0m21:23:26.004626 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m21:23:26.006614 [debug] [MainThread]: Partial parsing: updated file: dbt_capstone_andres://models/staging/_sources.yml
[0m21:23:26.012715 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m21:23:26.014431 [debug] [MainThread]: Partial parsing: updated file: dbt_capstone_andres://models/staging/_sources.yml
[0m21:23:26.019233 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m21:23:26.021281 [debug] [MainThread]: Partial parsing: updated file: dbt_capstone_andres://models/staging/_sources.yml
[0m21:23:26.036931 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m21:23:26.037949 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m21:23:26.038892 [debug] [MainThread]: Partial parsing: updated file: dbt_capstone_andres://models/staging/_sources.yml
[0m21:23:26.039778 [debug] [MainThread]: Partial parsing: updated file: dbt_capstone_andres://models/staging/_sources.yml
[0m21:23:26.674984 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 2 unused configuration paths:
- models.marts
- models.staging
[0m21:23:26.685792 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 2 unused configuration paths:
- models.marts
- models.staging
[0m21:23:26.691502 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 2 unused configuration paths:
- models.marts
- models.staging
[0m21:23:26.692979 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'b9009c94-9591-443f-83d7-086375b4bcc6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbfdcc20ed0>]}
[0m21:23:26.699129 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 2 unused configuration paths:
- models.staging
- models.marts
[0m21:23:26.700906 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 2 unused configuration paths:
- models.marts
- models.staging
[0m21:23:26.703877 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '939cb7b1-1436-4a61-bb26-035a91fdff00', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7983b1d610>]}
[0m21:23:26.707333 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '74dba7a4-8136-41d9-acf9-79f0c9e9d8b3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6c4193b590>]}
[0m21:23:26.715779 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'ef289ee3-b22d-4e74-b3ed-5f721d49e42e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f63d5c71910>]}
[0m21:23:26.717180 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'ffbfe39b-e31e-4333-9af4-3cb558e4e7d8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb3e6b4ce10>]}
[0m21:23:26.914496 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '74dba7a4-8136-41d9-acf9-79f0c9e9d8b3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6c4221b350>]}
[0m21:23:26.915303 [info ] [MainThread]: Found 5 models, 2 data tests, 2 sources, 862 macros, 1 unit test
[0m21:23:26.915979 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '74dba7a4-8136-41d9-acf9-79f0c9e9d8b3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6c41c4e010>]}
[0m21:23:26.917824 [info ] [MainThread]: 
[0m21:23:26.918462 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m21:23:26.919068 [info ] [MainThread]: 
[0m21:23:26.919883 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m21:23:26.925522 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_DATAEXPERT_STUDENT_andres'
[0m21:23:26.943661 [debug] [ThreadPool]: Using snowflake connection "list_DATAEXPERT_STUDENT_andres"
[0m21:23:26.944450 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "connection_name": "list_DATAEXPERT_STUDENT_andres"} */
show objects in DATAEXPERT_STUDENT.andres limit 10000
[0m21:23:26.945193 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:23:27.012530 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'b9009c94-9591-443f-83d7-086375b4bcc6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc00614dc10>]}
[0m21:23:27.013705 [info ] [MainThread]: Found 5 models, 2 data tests, 2 sources, 862 macros, 1 unit test
[0m21:23:27.014518 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b9009c94-9591-443f-83d7-086375b4bcc6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbfdc21d610>]}
[0m21:23:27.016599 [info ] [MainThread]: 
[0m21:23:27.017303 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m21:23:27.018127 [info ] [MainThread]: 
[0m21:23:27.019050 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m21:23:27.024875 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_DATAEXPERT_STUDENT_andres'
[0m21:23:27.029331 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '939cb7b1-1436-4a61-bb26-035a91fdff00', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7988271c10>]}
[0m21:23:27.030190 [info ] [MainThread]: Found 5 models, 2 data tests, 2 sources, 862 macros, 1 unit test
[0m21:23:27.030932 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '939cb7b1-1436-4a61-bb26-035a91fdff00', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f79af5ebb10>]}
[0m21:23:27.032961 [info ] [MainThread]: 
[0m21:23:27.033621 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m21:23:27.034269 [info ] [MainThread]: 
[0m21:23:27.035200 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m21:23:27.041821 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_DATAEXPERT_STUDENT_andres'
[0m21:23:27.050802 [debug] [ThreadPool]: Using snowflake connection "list_DATAEXPERT_STUDENT_andres"
[0m21:23:27.051643 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "connection_name": "list_DATAEXPERT_STUDENT_andres"} */
show objects in DATAEXPERT_STUDENT.andres limit 10000
[0m21:23:27.053016 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:23:27.065729 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'ef289ee3-b22d-4e74-b3ed-5f721d49e42e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f63d6167010>]}
[0m21:23:27.066061 [debug] [ThreadPool]: Using snowflake connection "list_DATAEXPERT_STUDENT_andres"
[0m21:23:27.066745 [info ] [MainThread]: Found 5 models, 2 data tests, 2 sources, 862 macros, 1 unit test
[0m21:23:27.066959 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "connection_name": "list_DATAEXPERT_STUDENT_andres"} */
show objects in DATAEXPERT_STUDENT.andres limit 10000
[0m21:23:27.067555 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ef289ee3-b22d-4e74-b3ed-5f721d49e42e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f63d5fc4050>]}
[0m21:23:27.067861 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:23:27.069823 [info ] [MainThread]: 
[0m21:23:27.070632 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m21:23:27.071410 [info ] [MainThread]: 
[0m21:23:27.072299 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m21:23:27.079599 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_DATAEXPERT_STUDENT_andres'
[0m21:23:27.092464 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'ffbfe39b-e31e-4333-9af4-3cb558e4e7d8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb3e709cad0>]}
[0m21:23:27.093359 [info ] [MainThread]: Found 5 models, 2 data tests, 2 sources, 862 macros, 1 unit test
[0m21:23:27.094107 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ffbfe39b-e31e-4333-9af4-3cb558e4e7d8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb3e6f62a90>]}
[0m21:23:27.097173 [info ] [MainThread]: 
[0m21:23:27.097976 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m21:23:27.098761 [info ] [MainThread]: 
[0m21:23:27.099709 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m21:23:27.104853 [debug] [ThreadPool]: Using snowflake connection "list_DATAEXPERT_STUDENT_andres"
[0m21:23:27.105699 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "connection_name": "list_DATAEXPERT_STUDENT_andres"} */
show objects in DATAEXPERT_STUDENT.andres limit 10000
[0m21:23:27.106434 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:23:27.106610 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_DATAEXPERT_STUDENT_andres'
[0m21:23:27.127922 [debug] [ThreadPool]: Using snowflake connection "list_DATAEXPERT_STUDENT_andres"
[0m21:23:27.128699 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "connection_name": "list_DATAEXPERT_STUDENT_andres"} */
show objects in DATAEXPERT_STUDENT.andres limit 10000
[0m21:23:27.129443 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:23:27.886452 [debug] [ThreadPool]: SQL status: SUCCESS 28 in 0.833 seconds
[0m21:23:27.890394 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: Close
[0m21:23:27.910525 [debug] [ThreadPool]: SQL status: SUCCESS 28 in 0.843 seconds
[0m21:23:27.913549 [debug] [ThreadPool]: SQL status: SUCCESS 28 in 0.968 seconds
[0m21:23:27.914655 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: Close
[0m21:23:27.917465 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: Close
[0m21:23:27.994160 [debug] [ThreadPool]: SQL status: SUCCESS 28 in 0.888 seconds
[0m21:23:27.998022 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: Close
[0m21:23:28.059221 [debug] [ThreadPool]: SQL status: SUCCESS 28 in 0.930 seconds
[0m21:23:28.064111 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: Close
[0m21:23:28.112547 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b9009c94-9591-443f-83d7-086375b4bcc6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc004efd090>]}
[0m21:23:28.113775 [info ] [MainThread]: Pulling freshness from warehouse metadata tables for 0 sources
[0m21:23:28.117793 [debug] [Thread-1 (]: Began running node source.dbt_capstone_andres.treasury.fiscal_data
[0m21:23:28.119082 [info ] [Thread-1 (]: 1 of 1 START freshness of treasury.fiscal_data ................................. [RUN]
[0m21:23:28.120105 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_DATAEXPERT_STUDENT_andres, now source.dbt_capstone_andres.treasury.fiscal_data)
[0m21:23:28.121023 [debug] [Thread-1 (]: Began compiling node source.dbt_capstone_andres.treasury.fiscal_data
[0m21:23:28.121949 [debug] [Thread-1 (]: Began executing node source.dbt_capstone_andres.treasury.fiscal_data
[0m21:23:28.133519 [debug] [Thread-1 (]: Using snowflake connection "source.dbt_capstone_andres.treasury.fiscal_data"
[0m21:23:28.134516 [debug] [Thread-1 (]: On source.dbt_capstone_andres.treasury.fiscal_data: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "source.dbt_capstone_andres.treasury.fiscal_data"} */
select
      max(record_date) as max_loaded_at,
      convert_timezone('UTC', current_timestamp()) as snapshotted_at
    from dataexpert_student.andres.fiscal_data
[0m21:23:28.135641 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m21:23:28.137601 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '939cb7b1-1436-4a61-bb26-035a91fdff00', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7982f04890>]}
[0m21:23:28.138663 [info ] [MainThread]: Pulling freshness from warehouse metadata tables for 0 sources
[0m21:23:28.142739 [debug] [Thread-1 (]: Began running node source.dbt_capstone_andres.treasury.fiscal_data
[0m21:23:28.144315 [info ] [Thread-1 (]: 1 of 1 START freshness of treasury.fiscal_data ................................. [RUN]
[0m21:23:28.145666 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_DATAEXPERT_STUDENT_andres, now source.dbt_capstone_andres.treasury.fiscal_data)
[0m21:23:28.146547 [debug] [Thread-1 (]: Began compiling node source.dbt_capstone_andres.treasury.fiscal_data
[0m21:23:28.147329 [debug] [Thread-1 (]: Began executing node source.dbt_capstone_andres.treasury.fiscal_data
[0m21:23:28.159517 [debug] [Thread-1 (]: Using snowflake connection "source.dbt_capstone_andres.treasury.fiscal_data"
[0m21:23:28.160590 [debug] [Thread-1 (]: On source.dbt_capstone_andres.treasury.fiscal_data: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "source.dbt_capstone_andres.treasury.fiscal_data"} */
select
      max(record_date) as max_loaded_at,
      convert_timezone('UTC', current_timestamp()) as snapshotted_at
    from dataexpert_student.andres.fiscal_data
[0m21:23:28.160777 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '74dba7a4-8136-41d9-acf9-79f0c9e9d8b3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6c415f5c90>]}
[0m21:23:28.161559 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m21:23:28.161956 [info ] [MainThread]: Pulling freshness from warehouse metadata tables for 0 sources
[0m21:23:28.166083 [debug] [Thread-1 (]: Began running node source.dbt_capstone_andres.treasury.fiscal_data
[0m21:23:28.167612 [info ] [Thread-1 (]: 1 of 1 START freshness of treasury.fiscal_data ................................. [RUN]
[0m21:23:28.168688 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_DATAEXPERT_STUDENT_andres, now source.dbt_capstone_andres.treasury.fiscal_data)
[0m21:23:28.169815 [debug] [Thread-1 (]: Began compiling node source.dbt_capstone_andres.treasury.fiscal_data
[0m21:23:28.171031 [debug] [Thread-1 (]: Began executing node source.dbt_capstone_andres.treasury.fiscal_data
[0m21:23:28.182916 [debug] [Thread-1 (]: Using snowflake connection "source.dbt_capstone_andres.treasury.fiscal_data"
[0m21:23:28.183854 [debug] [Thread-1 (]: On source.dbt_capstone_andres.treasury.fiscal_data: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "source.dbt_capstone_andres.treasury.fiscal_data"} */
select
      max(record_date) as max_loaded_at,
      convert_timezone('UTC', current_timestamp()) as snapshotted_at
    from dataexpert_student.andres.fiscal_data
[0m21:23:28.184666 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m21:23:28.232718 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ef289ee3-b22d-4e74-b3ed-5f721d49e42e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f63d60ef490>]}
[0m21:23:28.233987 [info ] [MainThread]: Pulling freshness from warehouse metadata tables for 0 sources
[0m21:23:28.239288 [debug] [Thread-1 (]: Began running node source.dbt_capstone_andres.treasury.fiscal_data
[0m21:23:28.240727 [info ] [Thread-1 (]: 1 of 1 START freshness of treasury.fiscal_data ................................. [RUN]
[0m21:23:28.241729 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_DATAEXPERT_STUDENT_andres, now source.dbt_capstone_andres.treasury.fiscal_data)
[0m21:23:28.242529 [debug] [Thread-1 (]: Began compiling node source.dbt_capstone_andres.treasury.fiscal_data
[0m21:23:28.243424 [debug] [Thread-1 (]: Began executing node source.dbt_capstone_andres.treasury.fiscal_data
[0m21:23:28.254897 [debug] [Thread-1 (]: Using snowflake connection "source.dbt_capstone_andres.treasury.fiscal_data"
[0m21:23:28.256100 [debug] [Thread-1 (]: On source.dbt_capstone_andres.treasury.fiscal_data: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "source.dbt_capstone_andres.treasury.fiscal_data"} */
select
      max(record_date) as max_loaded_at,
      convert_timezone('UTC', current_timestamp()) as snapshotted_at
    from dataexpert_student.andres.fiscal_data
[0m21:23:28.258945 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m21:23:28.289130 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ffbfe39b-e31e-4333-9af4-3cb558e4e7d8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb414899390>]}
[0m21:23:28.290740 [info ] [MainThread]: Pulling freshness from warehouse metadata tables for 0 sources
[0m21:23:28.294872 [debug] [Thread-1 (]: Began running node source.dbt_capstone_andres.treasury.fiscal_data
[0m21:23:28.296080 [info ] [Thread-1 (]: 1 of 1 START freshness of treasury.fiscal_data ................................. [RUN]
[0m21:23:28.297139 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_DATAEXPERT_STUDENT_andres, now source.dbt_capstone_andres.treasury.fiscal_data)
[0m21:23:28.298044 [debug] [Thread-1 (]: Began compiling node source.dbt_capstone_andres.treasury.fiscal_data
[0m21:23:28.298897 [debug] [Thread-1 (]: Began executing node source.dbt_capstone_andres.treasury.fiscal_data
[0m21:23:28.309300 [debug] [Thread-1 (]: Using snowflake connection "source.dbt_capstone_andres.treasury.fiscal_data"
[0m21:23:28.310497 [debug] [Thread-1 (]: On source.dbt_capstone_andres.treasury.fiscal_data: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "source.dbt_capstone_andres.treasury.fiscal_data"} */
select
      max(record_date) as max_loaded_at,
      convert_timezone('UTC', current_timestamp()) as snapshotted_at
    from dataexpert_student.andres.fiscal_data
[0m21:23:28.311356 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m21:23:29.177404 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 0.993 seconds
[0m21:23:29.179891 [debug] [Thread-1 (]: On source.dbt_capstone_andres.treasury.fiscal_data: Close
[0m21:23:29.180772 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 0.922 seconds
[0m21:23:29.182749 [debug] [Thread-1 (]: On source.dbt_capstone_andres.treasury.fiscal_data: Close
[0m21:23:29.243674 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 0.932 seconds
[0m21:23:29.244069 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 1.108 seconds
[0m21:23:29.247748 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 1.086 seconds
[0m21:23:29.246391 [debug] [Thread-1 (]: On source.dbt_capstone_andres.treasury.fiscal_data: Close
[0m21:23:29.247403 [debug] [Thread-1 (]: On source.dbt_capstone_andres.treasury.fiscal_data: Close
[0m21:23:29.250053 [debug] [Thread-1 (]: On source.dbt_capstone_andres.treasury.fiscal_data: Close
[0m21:23:29.417706 [debug] [Thread-1 (]: Database Error in source fiscal_data (models/staging/_sources.yml)
  Expected a timestamp value when querying field 'record_date' of table dataexpert_student.andres.fiscal_data but received value of type 'date' instead
[0m21:23:29.419249 [error] [Thread-1 (]: 1 of 1 ERROR freshness of treasury.fiscal_data ................................. [[31mERROR[0m in 1.25s]
[0m21:23:29.421891 [debug] [Thread-1 (]: Finished running node source.dbt_capstone_andres.treasury.fiscal_data
[0m21:23:29.425032 [debug] [Thread-1 (]: Database Error in source fiscal_data (models/staging/_sources.yml)
  Expected a timestamp value when querying field 'record_date' of table dataexpert_student.andres.fiscal_data but received value of type 'date' instead
[0m21:23:29.426827 [debug] [MainThread]: Connection 'master' was properly closed.
[0m21:23:29.427142 [error] [Thread-1 (]: 1 of 1 ERROR freshness of treasury.fiscal_data ................................. [[31mERROR[0m in 1.19s]
[0m21:23:29.427990 [debug] [MainThread]: Connection 'source.dbt_capstone_andres.treasury.fiscal_data' was properly closed.
[0m21:23:29.429675 [debug] [Thread-1 (]: Finished running node source.dbt_capstone_andres.treasury.fiscal_data
[0m21:23:29.429594 [info ] [MainThread]: 
[0m21:23:29.431223 [info ] [MainThread]: Finished running 1 source in 0 hours 0 minutes and 2.51 seconds (2.51s).
[0m21:23:29.434337 [debug] [MainThread]: Connection 'master' was properly closed.
[0m21:23:29.435569 [debug] [MainThread]: Connection 'source.dbt_capstone_andres.treasury.fiscal_data' was properly closed.
[0m21:23:29.436563 [info ] [MainThread]: 
[0m21:23:29.437667 [info ] [MainThread]: Finished running 1 source in 0 hours 0 minutes and 2.36 seconds (2.36s).
[0m21:23:29.493674 [debug] [Thread-1 (]: Database Error in source fiscal_data (models/staging/_sources.yml)
  Expected a timestamp value when querying field 'record_date' of table dataexpert_student.andres.fiscal_data but received value of type 'date' instead
[0m21:23:29.494881 [error] [Thread-1 (]: 1 of 1 ERROR freshness of treasury.fiscal_data ................................. [[31mERROR[0m in 1.35s]
[0m21:23:29.496073 [debug] [Thread-1 (]: Finished running node source.dbt_capstone_andres.treasury.fiscal_data
[0m21:23:29.498209 [debug] [Thread-1 (]: Database Error in source fiscal_data (models/staging/_sources.yml)
  Expected a timestamp value when querying field 'record_date' of table dataexpert_student.andres.fiscal_data but received value of type 'date' instead
[0m21:23:29.499268 [debug] [MainThread]: Connection 'master' was properly closed.
[0m21:23:29.499559 [error] [Thread-1 (]: 1 of 1 ERROR freshness of treasury.fiscal_data ................................. [[31mERROR[0m in 1.38s]
[0m21:23:29.500214 [debug] [MainThread]: Connection 'source.dbt_capstone_andres.treasury.fiscal_data' was properly closed.
[0m21:23:29.500816 [debug] [Thread-1 (]: Finished running node source.dbt_capstone_andres.treasury.fiscal_data
[0m21:23:29.501283 [info ] [MainThread]: 
[0m21:23:29.502609 [info ] [MainThread]: Finished running 1 source in 0 hours 0 minutes and 2.47 seconds (2.47s).
[0m21:23:29.503668 [debug] [MainThread]: Connection 'master' was properly closed.
[0m21:23:29.504812 [debug] [MainThread]: Connection 'source.dbt_capstone_andres.treasury.fiscal_data' was properly closed.
[0m21:23:29.505580 [debug] [Thread-1 (]: Database Error in source fiscal_data (models/staging/_sources.yml)
  Expected a timestamp value when querying field 'record_date' of table dataexpert_student.andres.fiscal_data but received value of type 'date' instead
[0m21:23:29.505790 [info ] [MainThread]: 
[0m21:23:29.507297 [error] [Thread-1 (]: 1 of 1 ERROR freshness of treasury.fiscal_data ................................. [[31mERROR[0m in 1.21s]
[0m21:23:29.507406 [info ] [MainThread]: Finished running 1 source in 0 hours 0 minutes and 2.49 seconds (2.49s).
[0m21:23:29.508872 [debug] [Thread-1 (]: Finished running node source.dbt_capstone_andres.treasury.fiscal_data
[0m21:23:29.512540 [debug] [MainThread]: Connection 'master' was properly closed.
[0m21:23:29.513947 [debug] [MainThread]: Connection 'source.dbt_capstone_andres.treasury.fiscal_data' was properly closed.
[0m21:23:29.515058 [info ] [MainThread]: 
[0m21:23:29.515953 [info ] [MainThread]: Finished running 1 source in 0 hours 0 minutes and 2.42 seconds (2.42s).
[0m21:23:29.602796 [info ] [MainThread]: 
[0m21:23:29.603766 [error] [MainThread]:   Database Error in source fiscal_data (models/staging/_sources.yml)
  Expected a timestamp value when querying field 'record_date' of table dataexpert_student.andres.fiscal_data but received value of type 'date' instead
[0m21:23:29.604518 [info ] [MainThread]: Done.
[0m21:23:29.605936 [debug] [MainThread]: Resource report: {"command_name": "freshness", "command_wall_clock_time": 6.2519627, "process_user_time": 5.574417, "process_kernel_time": 1.561473, "process_mem_max_rss": "210200", "process_out_blocks": "5583", "command_success": false, "process_in_blocks": "0"}
[0m21:23:29.607037 [debug] [MainThread]: Command `dbt source freshness` failed at 21:23:29.606902 after 6.25 seconds
[0m21:23:29.608182 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f63fd540c10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f63fd540350>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f640132d4d0>]}
[0m21:23:29.609331 [debug] [MainThread]: Flushing usage events
[0m21:23:29.662177 [info ] [MainThread]: 
[0m21:23:29.663659 [error] [MainThread]:   Database Error in source fiscal_data (models/staging/_sources.yml)
  Expected a timestamp value when querying field 'record_date' of table dataexpert_student.andres.fiscal_data but received value of type 'date' instead
[0m21:23:29.664583 [info ] [MainThread]: Done.
[0m21:23:29.666186 [debug] [MainThread]: Resource report: {"command_name": "freshness", "command_wall_clock_time": 6.355096, "process_user_time": 5.490187, "process_kernel_time": 1.426078, "process_mem_max_rss": "210272", "process_out_blocks": "5583", "command_success": false, "process_in_blocks": "0"}
[0m21:23:29.667513 [debug] [MainThread]: Command `dbt source freshness` failed at 21:23:29.667222 after 6.36 seconds
[0m21:23:29.668925 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6c69657c10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6c69657910>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6c6d609550>]}
[0m21:23:29.669807 [debug] [MainThread]: Flushing usage events
[0m21:23:29.717635 [info ] [MainThread]: 
[0m21:23:29.718772 [error] [MainThread]:   Database Error in source fiscal_data (models/staging/_sources.yml)
  Expected a timestamp value when querying field 'record_date' of table dataexpert_student.andres.fiscal_data but received value of type 'date' instead
[0m21:23:29.719875 [info ] [MainThread]: Done.
[0m21:23:29.721725 [debug] [MainThread]: Resource report: {"command_name": "freshness", "command_wall_clock_time": 6.388548, "process_user_time": 5.596867, "process_kernel_time": 1.583858, "process_mem_max_rss": "210056", "process_out_blocks": "5583", "command_success": false, "process_in_blocks": "0"}
[0m21:23:29.723069 [debug] [MainThread]: Command `dbt source freshness` failed at 21:23:29.722898 after 6.39 seconds
[0m21:23:29.724210 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc00415dd10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc00415d050>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc007eacb50>]}
[0m21:23:29.725466 [debug] [MainThread]: Flushing usage events
[0m21:23:29.751605 [info ] [MainThread]: 
[0m21:23:29.752765 [error] [MainThread]:   Database Error in source fiscal_data (models/staging/_sources.yml)
  Expected a timestamp value when querying field 'record_date' of table dataexpert_student.andres.fiscal_data but received value of type 'date' instead
[0m21:23:29.753908 [info ] [MainThread]: Done.
[0m21:23:29.756341 [debug] [MainThread]: Resource report: {"command_name": "freshness", "command_wall_clock_time": 6.441989, "process_user_time": 5.494858, "process_kernel_time": 1.423213, "process_mem_max_rss": "210356", "process_out_blocks": "5583", "command_success": false, "process_in_blocks": "0"}
[0m21:23:29.758651 [debug] [MainThread]: Command `dbt source freshness` failed at 21:23:29.758312 after 6.44 seconds
[0m21:23:29.760260 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb41293f9d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb41668cb90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb412c10cd0>]}
[0m21:23:29.761731 [debug] [MainThread]: Flushing usage events
[0m21:23:29.785506 [info ] [MainThread]: 
[0m21:23:29.787098 [error] [MainThread]:   Database Error in source fiscal_data (models/staging/_sources.yml)
  Expected a timestamp value when querying field 'record_date' of table dataexpert_student.andres.fiscal_data but received value of type 'date' instead
[0m21:23:29.788242 [info ] [MainThread]: Done.
[0m21:23:29.790103 [debug] [MainThread]: Resource report: {"command_name": "freshness", "command_wall_clock_time": 6.4643645, "process_user_time": 5.539999, "process_kernel_time": 1.396459, "process_mem_max_rss": "211996", "process_out_blocks": "5583", "command_success": false, "process_in_blocks": "0"}
[0m21:23:29.791263 [debug] [MainThread]: Command `dbt source freshness` failed at 21:23:29.791127 after 6.47 seconds
[0m21:23:29.792348 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f79af65bad0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f79af65b7d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f79b3398b50>]}
[0m21:23:29.793370 [debug] [MainThread]: Flushing usage events
[0m21:23:29.959284 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m21:23:29.969752 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m21:23:29.969724 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m21:23:29.978007 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m21:23:30.005061 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m21:25:21.188617 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9ef7d2cd50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9ef7c05f10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9ef7c05dd0>]}


============================== 21:25:21.191838 | b877def6-808f-4a54-8d87-60ea053962bc ==============================
[0m21:25:21.191838 [info ] [MainThread]: Running with dbt=1.9.0-b3
[0m21:25:21.192745 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/usr/local/airflow/dags/capstone/dbt_capstone_andres', 'debug': 'False', 'warn_error': 'None', 'log_path': '/usr/local/airflow/dags/capstone/dbt_capstone_andres/logs', 'fail_fast': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'invocation_command': 'dbt source freshness', 'send_anonymous_usage_stats': 'True'}
[0m21:25:22.217948 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'b877def6-808f-4a54-8d87-60ea053962bc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9ed05ff7d0>]}
[0m21:25:22.278195 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'b877def6-808f-4a54-8d87-60ea053962bc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9ef7af95d0>]}
[0m21:25:22.279489 [info ] [MainThread]: Registered adapter: snowflake=1.8.4
[0m21:25:22.325973 [debug] [MainThread]: checksum: 5f4fde221ca8627174a0341efc73099aaec05b8546408ceed6f59b1ec0755115, vars: {}, profile: , target: , version: 1.9.0b3
[0m21:25:23.146147 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m21:25:23.146973 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m21:25:23.153471 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 2 unused configuration paths:
- models.staging
- models.marts
[0m21:25:23.197501 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'b877def6-808f-4a54-8d87-60ea053962bc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9ed020bb50>]}
[0m21:25:23.370905 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'b877def6-808f-4a54-8d87-60ea053962bc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9ed02a1f90>]}
[0m21:25:23.371767 [info ] [MainThread]: Found 5 models, 2 data tests, 2 sources, 862 macros, 1 unit test
[0m21:25:23.372447 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b877def6-808f-4a54-8d87-60ea053962bc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9ed0510c50>]}
[0m21:25:23.374386 [info ] [MainThread]: 
[0m21:25:23.375142 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m21:25:23.375877 [info ] [MainThread]: 
[0m21:25:23.376724 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m21:25:23.382624 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_DATAEXPERT_STUDENT_andres'
[0m21:25:23.404180 [debug] [ThreadPool]: Using snowflake connection "list_DATAEXPERT_STUDENT_andres"
[0m21:25:23.405099 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "connection_name": "list_DATAEXPERT_STUDENT_andres"} */
show objects in DATAEXPERT_STUDENT.andres limit 10000
[0m21:25:23.405811 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:25:24.335671 [debug] [ThreadPool]: SQL status: SUCCESS 28 in 0.930 seconds
[0m21:25:24.340548 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: Close
[0m21:25:24.608836 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b877def6-808f-4a54-8d87-60ea053962bc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9ef86b7e10>]}
[0m21:25:24.610097 [info ] [MainThread]: Pulling freshness from warehouse metadata tables for 0 sources
[0m21:25:24.617752 [debug] [Thread-1 (]: Began running node source.dbt_capstone_andres.treasury.fiscal_data
[0m21:25:24.618991 [info ] [Thread-1 (]: 1 of 1 START freshness of treasury.fiscal_data ................................. [RUN]
[0m21:25:24.620206 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_DATAEXPERT_STUDENT_andres, now source.dbt_capstone_andres.treasury.fiscal_data)
[0m21:25:24.621389 [debug] [Thread-1 (]: Began compiling node source.dbt_capstone_andres.treasury.fiscal_data
[0m21:25:24.622451 [debug] [Thread-1 (]: Began executing node source.dbt_capstone_andres.treasury.fiscal_data
[0m21:25:24.634501 [debug] [Thread-1 (]: Using snowflake connection "source.dbt_capstone_andres.treasury.fiscal_data"
[0m21:25:24.635505 [debug] [Thread-1 (]: On source.dbt_capstone_andres.treasury.fiscal_data: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "source.dbt_capstone_andres.treasury.fiscal_data"} */
select
      max(record_date) as max_loaded_at,
      convert_timezone('UTC', current_timestamp()) as snapshotted_at
    from dataexpert_student.andres.fiscal_data
[0m21:25:24.636567 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m21:25:25.633478 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 0.997 seconds
[0m21:25:25.635912 [debug] [Thread-1 (]: On source.dbt_capstone_andres.treasury.fiscal_data: Close
[0m21:25:25.866546 [debug] [Thread-1 (]: Database Error in source fiscal_data (models/staging/_sources.yml)
  Expected a timestamp value when querying field 'record_date' of table dataexpert_student.andres.fiscal_data but received value of type 'date' instead
[0m21:25:25.867700 [error] [Thread-1 (]: 1 of 1 ERROR freshness of treasury.fiscal_data ................................. [[31mERROR[0m in 1.25s]
[0m21:25:25.868908 [debug] [Thread-1 (]: Finished running node source.dbt_capstone_andres.treasury.fiscal_data
[0m21:25:25.871624 [debug] [MainThread]: Connection 'master' was properly closed.
[0m21:25:25.872397 [debug] [MainThread]: Connection 'source.dbt_capstone_andres.treasury.fiscal_data' was properly closed.
[0m21:25:25.873168 [info ] [MainThread]: 
[0m21:25:25.874014 [info ] [MainThread]: Finished running 1 source in 0 hours 0 minutes and 2.50 seconds (2.50s).
[0m21:25:25.976072 [info ] [MainThread]: 
[0m21:25:25.977007 [error] [MainThread]:   Database Error in source fiscal_data (models/staging/_sources.yml)
  Expected a timestamp value when querying field 'record_date' of table dataexpert_student.andres.fiscal_data but received value of type 'date' instead
[0m21:25:25.977786 [info ] [MainThread]: Done.
[0m21:25:25.979101 [debug] [MainThread]: Resource report: {"command_name": "freshness", "command_wall_clock_time": 4.845723, "process_user_time": 3.916114, "process_kernel_time": 1.106631, "process_mem_max_rss": "202396", "process_out_blocks": "3684", "command_success": false, "process_in_blocks": "0"}
[0m21:25:25.980201 [debug] [MainThread]: Command `dbt source freshness` failed at 21:25:25.980025 after 4.85 seconds
[0m21:25:25.981141 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9ef9a16d90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9ef79f8290>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9efb798b90>]}
[0m21:25:25.982159 [debug] [MainThread]: Flushing usage events
[0m21:25:26.255121 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m21:28:34.203514 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2228994110>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f22289c8790>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2228994150>]}


============================== 21:28:34.207124 | 478bef3a-f3f8-4ea2-bb66-ec2756f65ade ==============================
[0m21:28:34.207124 [info ] [MainThread]: Running with dbt=1.9.0-b3
[0m21:28:34.208318 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/usr/local/airflow/dags/capstone/dbt_capstone_andres', 'debug': 'False', 'fail_fast': 'False', 'log_path': '/usr/local/airflow/dags/capstone/dbt_capstone_andres/logs', 'version_check': 'True', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'invocation_command': 'dbt source freshness', 'static_parser': 'True', 'introspect': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m21:28:36.674754 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '478bef3a-f3f8-4ea2-bb66-ec2756f65ade', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f22016cf690>]}
[0m21:28:36.791124 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '478bef3a-f3f8-4ea2-bb66-ec2756f65ade', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2229d87390>]}
[0m21:28:36.793140 [info ] [MainThread]: Registered adapter: snowflake=1.8.4
[0m21:28:36.866469 [debug] [MainThread]: checksum: 5f4fde221ca8627174a0341efc73099aaec05b8546408ceed6f59b1ec0755115, vars: {}, profile: , target: , version: 1.9.0b3
[0m21:28:37.448172 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m21:28:37.449634 [debug] [MainThread]: Partial parsing: updated file: dbt_capstone_andres://models/staging/_sources.yml
[0m21:28:38.094478 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 2 unused configuration paths:
- models.staging
- models.marts
[0m21:28:38.109577 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '478bef3a-f3f8-4ea2-bb66-ec2756f65ade', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f22015abd50>]}
[0m21:28:38.299947 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '478bef3a-f3f8-4ea2-bb66-ec2756f65ade', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f220159cfd0>]}
[0m21:28:38.301027 [info ] [MainThread]: Found 5 models, 2 data tests, 2 sources, 862 macros, 1 unit test
[0m21:28:38.301868 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '478bef3a-f3f8-4ea2-bb66-ec2756f65ade', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2229a96a10>]}
[0m21:28:38.304285 [info ] [MainThread]: 
[0m21:28:38.305068 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m21:28:38.305739 [info ] [MainThread]: 
[0m21:28:38.306721 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m21:28:38.313044 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_DATAEXPERT_STUDENT_andres'
[0m21:28:38.333148 [debug] [ThreadPool]: Using snowflake connection "list_DATAEXPERT_STUDENT_andres"
[0m21:28:38.334137 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "connection_name": "list_DATAEXPERT_STUDENT_andres"} */
show objects in DATAEXPERT_STUDENT.andres limit 10000
[0m21:28:38.334828 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:28:39.265701 [debug] [ThreadPool]: SQL status: SUCCESS 28 in 0.931 seconds
[0m21:28:39.269487 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: Close
[0m21:28:39.507996 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '478bef3a-f3f8-4ea2-bb66-ec2756f65ade', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2200b24610>]}
[0m21:28:39.509229 [info ] [MainThread]: Pulling freshness from warehouse metadata tables for 0 sources
[0m21:28:39.517790 [debug] [Thread-1 (]: Began running node source.dbt_capstone_andres.treasury.fiscal_data
[0m21:28:39.519160 [info ] [Thread-1 (]: 1 of 1 START freshness of treasury.fiscal_data ................................. [RUN]
[0m21:28:39.520277 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_DATAEXPERT_STUDENT_andres, now source.dbt_capstone_andres.treasury.fiscal_data)
[0m21:28:39.521255 [debug] [Thread-1 (]: Began compiling node source.dbt_capstone_andres.treasury.fiscal_data
[0m21:28:39.522247 [debug] [Thread-1 (]: Began executing node source.dbt_capstone_andres.treasury.fiscal_data
[0m21:28:39.533880 [debug] [Thread-1 (]: Using snowflake connection "source.dbt_capstone_andres.treasury.fiscal_data"
[0m21:28:39.534957 [debug] [Thread-1 (]: On source.dbt_capstone_andres.treasury.fiscal_data: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "source.dbt_capstone_andres.treasury.fiscal_data"} */
select
      max(record_date::timestamp) as max_loaded_at,
      convert_timezone('UTC', current_timestamp()) as snapshotted_at
    from dataexpert_student.andres.fiscal_data
[0m21:28:39.535820 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m21:28:40.743904 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 1.208 seconds
[0m21:28:40.747770 [debug] [Thread-1 (]: On source.dbt_capstone_andres.treasury.fiscal_data: Close
[0m21:28:40.968422 [info ] [Thread-1 (]: 1 of 1 PASS freshness of treasury.fiscal_data .................................. [[32mPASS[0m in 1.45s]
[0m21:28:40.969817 [debug] [Thread-1 (]: Finished running node source.dbt_capstone_andres.treasury.fiscal_data
[0m21:28:40.973164 [debug] [MainThread]: Connection 'master' was properly closed.
[0m21:28:40.974058 [debug] [MainThread]: Connection 'source.dbt_capstone_andres.treasury.fiscal_data' was properly closed.
[0m21:28:40.974781 [info ] [MainThread]: 
[0m21:28:40.975604 [info ] [MainThread]: Finished running 1 source in 0 hours 0 minutes and 2.67 seconds (2.67s).
[0m21:28:41.065144 [info ] [MainThread]: Done.
[0m21:28:41.066605 [debug] [MainThread]: Resource report: {"command_name": "freshness", "command_success": true, "command_wall_clock_time": 6.9326544, "process_user_time": 6.13137, "process_kernel_time": 1.310682, "process_mem_max_rss": "212060", "process_out_blocks": "5584", "process_in_blocks": "0"}
[0m21:28:41.067848 [debug] [MainThread]: Command `dbt source freshness` succeeded at 21:28:41.067656 after 6.93 seconds
[0m21:28:41.069215 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f222880b990>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2229d6d810>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f222c73ff50>]}
[0m21:28:41.070644 [debug] [MainThread]: Flushing usage events
[0m21:28:41.261819 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m22:41:22.057654 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbf5a795f90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbf5bb6d590>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbf5ab6cd90>]}


============================== 22:41:22.061577 | 22050e17-54d1-4147-8fd6-fb8c3baa4fe3 ==============================
[0m22:41:22.061577 [info ] [MainThread]: Running with dbt=1.9.0-b3
[0m22:41:22.062500 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/usr/local/airflow/dags/capstone/dbt_capstone_andres', 'fail_fast': 'False', 'version_check': 'True', 'log_path': '/usr/local/airflow/dags/capstone/dbt_capstone_andres/logs', 'debug': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'log_format': 'default', 'invocation_command': 'dbt source freshness', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m22:41:23.087661 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '22050e17-54d1-4147-8fd6-fb8c3baa4fe3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbf2f355f90>]}
[0m22:41:23.146339 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '22050e17-54d1-4147-8fd6-fb8c3baa4fe3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbf5b522f90>]}
[0m22:41:23.147519 [info ] [MainThread]: Registered adapter: snowflake=1.8.4
[0m22:41:23.187068 [debug] [MainThread]: checksum: 5f4fde221ca8627174a0341efc73099aaec05b8546408ceed6f59b1ec0755115, vars: {}, profile: , target: , version: 1.9.0b3
[0m22:41:23.568924 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m22:41:23.569648 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m22:41:23.576596 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 2 unused configuration paths:
- models.marts
- models.staging
[0m22:41:23.625579 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '22050e17-54d1-4147-8fd6-fb8c3baa4fe3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbf2ef5f710>]}
[0m22:41:23.819827 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '22050e17-54d1-4147-8fd6-fb8c3baa4fe3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbf2f2e6810>]}
[0m22:41:23.820674 [info ] [MainThread]: Found 5 models, 2 data tests, 2 sources, 862 macros, 1 unit test
[0m22:41:23.821352 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '22050e17-54d1-4147-8fd6-fb8c3baa4fe3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbf2f269110>]}
[0m22:41:23.823282 [info ] [MainThread]: 
[0m22:41:23.823908 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m22:41:23.824590 [info ] [MainThread]: 
[0m22:41:23.825395 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m22:41:23.830330 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_DATAEXPERT_STUDENT_andres'
[0m22:41:23.849379 [debug] [ThreadPool]: Using snowflake connection "list_DATAEXPERT_STUDENT_andres"
[0m22:41:23.850160 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "connection_name": "list_DATAEXPERT_STUDENT_andres"} */
show objects in DATAEXPERT_STUDENT.andres limit 10000
[0m22:41:23.850818 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:41:24.810850 [debug] [ThreadPool]: SQL status: SUCCESS 28 in 0.960 seconds
[0m22:41:24.814948 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: Close
[0m22:41:25.058484 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '22050e17-54d1-4147-8fd6-fb8c3baa4fe3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbf2ee5f110>]}
[0m22:41:25.059618 [info ] [MainThread]: Pulling freshness from warehouse metadata tables for 0 sources
[0m22:41:25.064318 [debug] [Thread-1 (]: Began running node source.dbt_capstone_andres.treasury.fiscal_data
[0m22:41:25.065205 [info ] [Thread-1 (]: 1 of 1 START freshness of treasury.fiscal_data ................................. [RUN]
[0m22:41:25.066196 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_DATAEXPERT_STUDENT_andres, now source.dbt_capstone_andres.treasury.fiscal_data)
[0m22:41:25.067100 [debug] [Thread-1 (]: Began compiling node source.dbt_capstone_andres.treasury.fiscal_data
[0m22:41:25.067964 [debug] [Thread-1 (]: Began executing node source.dbt_capstone_andres.treasury.fiscal_data
[0m22:41:25.078607 [debug] [Thread-1 (]: Using snowflake connection "source.dbt_capstone_andres.treasury.fiscal_data"
[0m22:41:25.079580 [debug] [Thread-1 (]: On source.dbt_capstone_andres.treasury.fiscal_data: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "source.dbt_capstone_andres.treasury.fiscal_data"} */
select
      max(record_date::timestamp) as max_loaded_at,
      convert_timezone('UTC', current_timestamp()) as snapshotted_at
    from dataexpert_student.andres.fiscal_data
[0m22:41:25.080568 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m22:41:25.868930 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 0.788 seconds
[0m22:41:25.872629 [debug] [Thread-1 (]: On source.dbt_capstone_andres.treasury.fiscal_data: Close
[0m22:41:26.123642 [info ] [Thread-1 (]: 1 of 1 PASS freshness of treasury.fiscal_data .................................. [[32mPASS[0m in 1.06s]
[0m22:41:26.124610 [debug] [Thread-1 (]: Finished running node source.dbt_capstone_andres.treasury.fiscal_data
[0m22:41:26.126782 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:41:26.127533 [debug] [MainThread]: Connection 'source.dbt_capstone_andres.treasury.fiscal_data' was properly closed.
[0m22:41:26.128224 [info ] [MainThread]: 
[0m22:41:26.129051 [info ] [MainThread]: Finished running 1 source in 0 hours 0 minutes and 2.30 seconds (2.30s).
[0m22:41:26.214109 [info ] [MainThread]: Done.
[0m22:41:26.215829 [debug] [MainThread]: Resource report: {"command_name": "freshness", "command_success": true, "command_wall_clock_time": 4.2100573, "process_user_time": 3.542497, "process_kernel_time": 1.005598, "process_mem_max_rss": "202496", "process_in_blocks": "33504", "process_out_blocks": "3685"}
[0m22:41:26.217160 [debug] [MainThread]: Command `dbt source freshness` succeeded at 22:41:26.216923 after 4.21 seconds
[0m22:41:26.218136 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbf5a803650>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbf5b46c350>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbf5aac8a50>]}
[0m22:41:26.218931 [debug] [MainThread]: Flushing usage events
[0m22:41:26.494235 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m22:45:24.907812 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe403a5b010>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe403a5af90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe403a58450>]}


============================== 22:45:24.910580 | 2b30a0a6-e3a8-41b8-bbc7-ab636af1157d ==============================
[0m22:45:24.910580 [info ] [MainThread]: Running with dbt=1.9.0-b3
[0m22:45:24.911497 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/usr/local/airflow/dags/capstone/dbt_capstone_andres', 'debug': 'False', 'warn_error': 'None', 'log_path': '/usr/local/airflow/dags/capstone/dbt_capstone_andres/logs', 'fail_fast': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'log_format': 'default', 'invocation_command': 'dbt source freshness', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m22:45:25.812255 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '2b30a0a6-e3a8-41b8-bbc7-ab636af1157d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe3dc62be10>]}
[0m22:45:25.869054 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '2b30a0a6-e3a8-41b8-bbc7-ab636af1157d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe404e4b590>]}
[0m22:45:25.870329 [info ] [MainThread]: Registered adapter: snowflake=1.8.4
[0m22:45:25.911049 [debug] [MainThread]: checksum: 5f4fde221ca8627174a0341efc73099aaec05b8546408ceed6f59b1ec0755115, vars: {}, profile: , target: , version: 1.9.0b3
[0m22:45:26.284993 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m22:45:26.285661 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m22:45:26.291127 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 2 unused configuration paths:
- models.marts
- models.staging
[0m22:45:26.332719 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '2b30a0a6-e3a8-41b8-bbc7-ab636af1157d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe403f79550>]}
[0m22:45:26.497269 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '2b30a0a6-e3a8-41b8-bbc7-ab636af1157d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe3dc4be3d0>]}
[0m22:45:26.498077 [info ] [MainThread]: Found 5 models, 2 data tests, 2 sources, 862 macros, 1 unit test
[0m22:45:26.498830 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2b30a0a6-e3a8-41b8-bbc7-ab636af1157d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe3dc43d010>]}
[0m22:45:26.500813 [info ] [MainThread]: 
[0m22:45:26.501488 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m22:45:26.502135 [info ] [MainThread]: 
[0m22:45:26.502944 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m22:45:26.508964 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_DATAEXPERT_STUDENT_andres'
[0m22:45:26.527472 [debug] [ThreadPool]: Using snowflake connection "list_DATAEXPERT_STUDENT_andres"
[0m22:45:26.528372 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "connection_name": "list_DATAEXPERT_STUDENT_andres"} */
show objects in DATAEXPERT_STUDENT.andres limit 10000
[0m22:45:26.529150 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:45:27.450092 [debug] [ThreadPool]: SQL status: SUCCESS 28 in 0.921 seconds
[0m22:45:27.454602 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: Close
[0m22:45:27.675506 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2b30a0a6-e3a8-41b8-bbc7-ab636af1157d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe3dc4f5010>]}
[0m22:45:27.676947 [info ] [MainThread]: Pulling freshness from warehouse metadata tables for 0 sources
[0m22:45:27.682356 [debug] [Thread-1 (]: Began running node source.dbt_capstone_andres.treasury.fiscal_data
[0m22:45:27.683418 [info ] [Thread-1 (]: 1 of 1 START freshness of treasury.fiscal_data ................................. [RUN]
[0m22:45:27.684725 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_DATAEXPERT_STUDENT_andres, now source.dbt_capstone_andres.treasury.fiscal_data)
[0m22:45:27.685822 [debug] [Thread-1 (]: Began compiling node source.dbt_capstone_andres.treasury.fiscal_data
[0m22:45:27.687011 [debug] [Thread-1 (]: Began executing node source.dbt_capstone_andres.treasury.fiscal_data
[0m22:45:27.699506 [debug] [Thread-1 (]: Using snowflake connection "source.dbt_capstone_andres.treasury.fiscal_data"
[0m22:45:27.700570 [debug] [Thread-1 (]: On source.dbt_capstone_andres.treasury.fiscal_data: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "source.dbt_capstone_andres.treasury.fiscal_data"} */
select
      max(record_date::timestamp) as max_loaded_at,
      convert_timezone('UTC', current_timestamp()) as snapshotted_at
    from dataexpert_student.andres.fiscal_data
[0m22:45:27.701815 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m22:45:28.618947 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 0.917 seconds
[0m22:45:28.620972 [debug] [Thread-1 (]: On source.dbt_capstone_andres.treasury.fiscal_data: Close
[0m22:45:28.844986 [info ] [Thread-1 (]: 1 of 1 PASS freshness of treasury.fiscal_data .................................. [[32mPASS[0m in 1.16s]
[0m22:45:28.846173 [debug] [Thread-1 (]: Finished running node source.dbt_capstone_andres.treasury.fiscal_data
[0m22:45:28.848732 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:45:28.849422 [debug] [MainThread]: Connection 'source.dbt_capstone_andres.treasury.fiscal_data' was properly closed.
[0m22:45:28.850120 [info ] [MainThread]: 
[0m22:45:28.850839 [info ] [MainThread]: Finished running 1 source in 0 hours 0 minutes and 2.35 seconds (2.35s).
[0m22:45:28.936326 [info ] [MainThread]: Done.
[0m22:45:28.937686 [debug] [MainThread]: Resource report: {"command_name": "freshness", "command_success": true, "command_wall_clock_time": 4.0780516, "process_user_time": 3.455659, "process_kernel_time": 0.883182, "process_mem_max_rss": "204340", "process_out_blocks": "3685", "process_in_blocks": "0"}
[0m22:45:28.938655 [debug] [MainThread]: Command `dbt source freshness` succeeded at 22:45:28.938490 after 4.08 seconds
[0m22:45:28.939321 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe403ac7b50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe407875590>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe4077f4b90>]}
[0m22:45:28.940148 [debug] [MainThread]: Flushing usage events
[0m22:45:29.225083 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m22:45:47.049857 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6697121610>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f66974646d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f669732d990>]}


============================== 22:45:47.053175 | 38b7292f-38fa-4d07-a0eb-bf60b910e30d ==============================
[0m22:45:47.053175 [info ] [MainThread]: Running with dbt=1.9.0-b3
[0m22:45:47.054311 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/usr/local/airflow/dags/capstone/dbt_capstone_andres', 'version_check': 'True', 'debug': 'False', 'log_path': '/usr/local/airflow/dags/capstone/dbt_capstone_andres/logs', 'warn_error': 'None', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt source freshness', 'log_format': 'default', 'static_parser': 'True', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m22:45:48.177173 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '38b7292f-38fa-4d07-a0eb-bf60b910e30d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6697d69c10>]}
[0m22:45:48.247115 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '38b7292f-38fa-4d07-a0eb-bf60b910e30d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f666bd4c390>]}
[0m22:45:48.248998 [info ] [MainThread]: Registered adapter: snowflake=1.8.4
[0m22:45:48.315559 [debug] [MainThread]: checksum: 5f4fde221ca8627174a0341efc73099aaec05b8546408ceed6f59b1ec0755115, vars: {}, profile: , target: , version: 1.9.0b3
[0m22:45:48.788572 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m22:45:48.789231 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m22:45:48.795881 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 2 unused configuration paths:
- models.marts
- models.staging
[0m22:45:48.842426 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '38b7292f-38fa-4d07-a0eb-bf60b910e30d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f666b8e1810>]}
[0m22:45:49.088930 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '38b7292f-38fa-4d07-a0eb-bf60b910e30d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f666bc6e310>]}
[0m22:45:49.089970 [info ] [MainThread]: Found 5 models, 2 data tests, 2 sources, 862 macros, 1 unit test
[0m22:45:49.091006 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '38b7292f-38fa-4d07-a0eb-bf60b910e30d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f666bbe8f90>]}
[0m22:45:49.094237 [info ] [MainThread]: 
[0m22:45:49.095217 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m22:45:49.096201 [info ] [MainThread]: 
[0m22:45:49.097904 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m22:45:49.103956 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_DATAEXPERT_STUDENT_andres'
[0m22:45:49.124437 [debug] [ThreadPool]: Using snowflake connection "list_DATAEXPERT_STUDENT_andres"
[0m22:45:49.125158 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "connection_name": "list_DATAEXPERT_STUDENT_andres"} */
show objects in DATAEXPERT_STUDENT.andres limit 10000
[0m22:45:49.125859 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:45:50.008352 [debug] [ThreadPool]: SQL status: SUCCESS 28 in 0.882 seconds
[0m22:45:50.011610 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: Close
[0m22:45:50.274856 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '38b7292f-38fa-4d07-a0eb-bf60b910e30d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f666b923850>]}
[0m22:45:50.275836 [info ] [MainThread]: Pulling freshness from warehouse metadata tables for 0 sources
[0m22:45:50.280101 [debug] [Thread-1 (]: Began running node source.dbt_capstone_andres.treasury.fiscal_data
[0m22:45:50.280996 [info ] [Thread-1 (]: 1 of 1 START freshness of treasury.fiscal_data ................................. [RUN]
[0m22:45:50.281954 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_DATAEXPERT_STUDENT_andres, now source.dbt_capstone_andres.treasury.fiscal_data)
[0m22:45:50.282693 [debug] [Thread-1 (]: Began compiling node source.dbt_capstone_andres.treasury.fiscal_data
[0m22:45:50.283429 [debug] [Thread-1 (]: Began executing node source.dbt_capstone_andres.treasury.fiscal_data
[0m22:45:50.293050 [debug] [Thread-1 (]: Using snowflake connection "source.dbt_capstone_andres.treasury.fiscal_data"
[0m22:45:50.293910 [debug] [Thread-1 (]: On source.dbt_capstone_andres.treasury.fiscal_data: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "source.dbt_capstone_andres.treasury.fiscal_data"} */
select
      max(record_date::timestamp) as max_loaded_at,
      convert_timezone('UTC', current_timestamp()) as snapshotted_at
    from dataexpert_student.andres.fiscal_data
[0m22:45:50.294686 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m22:45:51.052527 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 0.758 seconds
[0m22:45:51.054982 [debug] [Thread-1 (]: On source.dbt_capstone_andres.treasury.fiscal_data: Close
[0m22:45:51.289663 [info ] [Thread-1 (]: 1 of 1 PASS freshness of treasury.fiscal_data .................................. [[32mPASS[0m in 1.01s]
[0m22:45:51.290826 [debug] [Thread-1 (]: Finished running node source.dbt_capstone_andres.treasury.fiscal_data
[0m22:45:51.293582 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:45:51.294232 [debug] [MainThread]: Connection 'source.dbt_capstone_andres.treasury.fiscal_data' was properly closed.
[0m22:45:51.294930 [info ] [MainThread]: 
[0m22:45:51.295597 [info ] [MainThread]: Finished running 1 source in 0 hours 0 minutes and 2.20 seconds (2.20s).
[0m22:45:51.403869 [info ] [MainThread]: Done.
[0m22:45:51.405742 [debug] [MainThread]: Resource report: {"command_name": "freshness", "command_success": true, "command_wall_clock_time": 4.429304, "process_user_time": 3.959537, "process_kernel_time": 0.883908, "process_mem_max_rss": "206104", "process_out_blocks": "3685", "process_in_blocks": "0"}
[0m22:45:51.407024 [debug] [MainThread]: Command `dbt source freshness` succeeded at 22:45:51.406883 after 4.43 seconds
[0m22:45:51.407861 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6696f93710>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f669732d750>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f669ae9ffd0>]}
[0m22:45:51.408658 [debug] [MainThread]: Flushing usage events
[0m22:45:51.609608 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m22:46:09.134625 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f953a60c690>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f953a60ca10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f953a60cd90>]}


============================== 22:46:09.138992 | 1789651f-8af9-4e0b-9071-0ea583d8db96 ==============================
[0m22:46:09.138992 [info ] [MainThread]: Running with dbt=1.9.0-b3
[0m22:46:09.140294 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/usr/local/airflow/dags/capstone/dbt_capstone_andres', 'debug': 'False', 'fail_fast': 'False', 'log_path': '/usr/local/airflow/dags/capstone/dbt_capstone_andres/logs', 'version_check': 'True', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'log_format': 'default', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'target_path': 'None', 'invocation_command': 'dbt source freshness', 'send_anonymous_usage_stats': 'True'}
[0m22:46:10.397036 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '1789651f-8af9-4e0b-9071-0ea583d8db96', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f950f1fff10>]}
[0m22:46:10.479619 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '1789651f-8af9-4e0b-9071-0ea583d8db96', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f953b9ff6d0>]}
[0m22:46:10.480996 [info ] [MainThread]: Registered adapter: snowflake=1.8.4
[0m22:46:10.546892 [debug] [MainThread]: checksum: 5f4fde221ca8627174a0341efc73099aaec05b8546408ceed6f59b1ec0755115, vars: {}, profile: , target: , version: 1.9.0b3
[0m22:46:11.268321 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m22:46:11.269499 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m22:46:11.276894 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 2 unused configuration paths:
- models.staging
- models.marts
[0m22:46:11.329739 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '1789651f-8af9-4e0b-9071-0ea583d8db96', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f953a60c710>]}
[0m22:46:11.842044 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '1789651f-8af9-4e0b-9071-0ea583d8db96', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f950ee96850>]}
[0m22:46:11.843296 [info ] [MainThread]: Found 5 models, 2 data tests, 2 sources, 862 macros, 1 unit test
[0m22:46:11.844454 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '1789651f-8af9-4e0b-9071-0ea583d8db96', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f953b29c0d0>]}
[0m22:46:11.847544 [info ] [MainThread]: 
[0m22:46:11.848777 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m22:46:11.849702 [info ] [MainThread]: 
[0m22:46:11.851108 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m22:46:11.858870 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_DATAEXPERT_STUDENT_andres'
[0m22:46:11.885600 [debug] [ThreadPool]: Using snowflake connection "list_DATAEXPERT_STUDENT_andres"
[0m22:46:11.887041 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "connection_name": "list_DATAEXPERT_STUDENT_andres"} */
show objects in DATAEXPERT_STUDENT.andres limit 10000
[0m22:46:11.887920 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:46:12.823449 [debug] [ThreadPool]: SQL status: SUCCESS 28 in 0.935 seconds
[0m22:46:12.828387 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: Close
[0m22:46:13.060746 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '1789651f-8af9-4e0b-9071-0ea583d8db96', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f950f3acbd0>]}
[0m22:46:13.061884 [info ] [MainThread]: Pulling freshness from warehouse metadata tables for 0 sources
[0m22:46:13.066888 [debug] [Thread-1 (]: Began running node source.dbt_capstone_andres.treasury.fiscal_data
[0m22:46:13.068133 [info ] [Thread-1 (]: 1 of 1 START freshness of treasury.fiscal_data ................................. [RUN]
[0m22:46:13.070266 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_DATAEXPERT_STUDENT_andres, now source.dbt_capstone_andres.treasury.fiscal_data)
[0m22:46:13.072181 [debug] [Thread-1 (]: Began compiling node source.dbt_capstone_andres.treasury.fiscal_data
[0m22:46:13.073469 [debug] [Thread-1 (]: Began executing node source.dbt_capstone_andres.treasury.fiscal_data
[0m22:46:13.088941 [debug] [Thread-1 (]: Using snowflake connection "source.dbt_capstone_andres.treasury.fiscal_data"
[0m22:46:13.089939 [debug] [Thread-1 (]: On source.dbt_capstone_andres.treasury.fiscal_data: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "source.dbt_capstone_andres.treasury.fiscal_data"} */
select
      max(record_date::timestamp) as max_loaded_at,
      convert_timezone('UTC', current_timestamp()) as snapshotted_at
    from dataexpert_student.andres.fiscal_data
[0m22:46:13.090826 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m22:46:13.819695 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 0.729 seconds
[0m22:46:13.821773 [debug] [Thread-1 (]: On source.dbt_capstone_andres.treasury.fiscal_data: Close
[0m22:46:14.158033 [info ] [Thread-1 (]: 1 of 1 PASS freshness of treasury.fiscal_data .................................. [[32mPASS[0m in 1.09s]
[0m22:46:14.159371 [debug] [Thread-1 (]: Finished running node source.dbt_capstone_andres.treasury.fiscal_data
[0m22:46:14.162296 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:46:14.162883 [debug] [MainThread]: Connection 'source.dbt_capstone_andres.treasury.fiscal_data' was properly closed.
[0m22:46:14.163487 [info ] [MainThread]: 
[0m22:46:14.164221 [info ] [MainThread]: Finished running 1 source in 0 hours 0 minutes and 2.31 seconds (2.31s).
[0m22:46:14.278700 [info ] [MainThread]: Done.
[0m22:46:14.280132 [debug] [MainThread]: Resource report: {"command_name": "freshness", "command_success": true, "command_wall_clock_time": 5.2098584, "process_user_time": 5.118108, "process_kernel_time": 0.974877, "process_mem_max_rss": "206076", "process_out_blocks": "3685", "process_in_blocks": "0"}
[0m22:46:14.281198 [debug] [MainThread]: Command `dbt source freshness` succeeded at 22:46:14.281086 after 5.21 seconds
[0m22:46:14.282097 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f953b2e01d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f953e3affd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f953e3b8f50>]}
[0m22:46:14.282889 [debug] [MainThread]: Flushing usage events
[0m22:46:14.470266 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m22:46:33.484389 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe38385da50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe38385d210>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe3845740d0>]}


============================== 22:46:33.488264 | 0c174557-9aa0-46d8-8ca5-b1940ad65cd4 ==============================
[0m22:46:33.488264 [info ] [MainThread]: Running with dbt=1.9.0-b3
[0m22:46:33.489626 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/usr/local/airflow/dags/capstone/dbt_capstone_andres', 'fail_fast': 'False', 'version_check': 'True', 'log_path': '/usr/local/airflow/dags/capstone/dbt_capstone_andres/logs', 'warn_error': 'None', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'invocation_command': 'dbt source freshness', 'log_format': 'default', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m22:46:34.930395 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '0c174557-9aa0-46d8-8ca5-b1940ad65cd4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe3836c4710>]}
[0m22:46:35.039140 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '0c174557-9aa0-46d8-8ca5-b1940ad65cd4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe384a635d0>]}
[0m22:46:35.040739 [info ] [MainThread]: Registered adapter: snowflake=1.8.4
[0m22:46:35.128311 [debug] [MainThread]: checksum: 5f4fde221ca8627174a0341efc73099aaec05b8546408ceed6f59b1ec0755115, vars: {}, profile: , target: , version: 1.9.0b3
[0m22:46:35.760664 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m22:46:35.761486 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m22:46:35.767384 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 2 unused configuration paths:
- models.staging
- models.marts
[0m22:46:35.817608 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '0c174557-9aa0-46d8-8ca5-b1940ad65cd4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe357fc4410>]}
[0m22:46:36.027752 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '0c174557-9aa0-46d8-8ca5-b1940ad65cd4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe35c071f90>]}
[0m22:46:36.028592 [info ] [MainThread]: Found 5 models, 2 data tests, 2 sources, 862 macros, 1 unit test
[0m22:46:36.029275 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '0c174557-9aa0-46d8-8ca5-b1940ad65cd4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe35c2c0f90>]}
[0m22:46:36.031304 [info ] [MainThread]: 
[0m22:46:36.031945 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m22:46:36.032598 [info ] [MainThread]: 
[0m22:46:36.033403 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m22:46:36.038829 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_DATAEXPERT_STUDENT_andres'
[0m22:46:36.059841 [debug] [ThreadPool]: Using snowflake connection "list_DATAEXPERT_STUDENT_andres"
[0m22:46:36.060684 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "connection_name": "list_DATAEXPERT_STUDENT_andres"} */
show objects in DATAEXPERT_STUDENT.andres limit 10000
[0m22:46:36.061375 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:46:36.907463 [debug] [ThreadPool]: SQL status: SUCCESS 28 in 0.846 seconds
[0m22:46:36.910812 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: Close
[0m22:46:37.154620 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '0c174557-9aa0-46d8-8ca5-b1940ad65cd4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe38564d690>]}
[0m22:46:37.155528 [info ] [MainThread]: Pulling freshness from warehouse metadata tables for 0 sources
[0m22:46:37.163764 [debug] [Thread-1 (]: Began running node source.dbt_capstone_andres.treasury.fiscal_data
[0m22:46:37.164772 [info ] [Thread-1 (]: 1 of 1 START freshness of treasury.fiscal_data ................................. [RUN]
[0m22:46:37.165760 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_DATAEXPERT_STUDENT_andres, now source.dbt_capstone_andres.treasury.fiscal_data)
[0m22:46:37.167478 [debug] [Thread-1 (]: Began compiling node source.dbt_capstone_andres.treasury.fiscal_data
[0m22:46:37.168541 [debug] [Thread-1 (]: Began executing node source.dbt_capstone_andres.treasury.fiscal_data
[0m22:46:37.181566 [debug] [Thread-1 (]: Using snowflake connection "source.dbt_capstone_andres.treasury.fiscal_data"
[0m22:46:37.182461 [debug] [Thread-1 (]: On source.dbt_capstone_andres.treasury.fiscal_data: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "source.dbt_capstone_andres.treasury.fiscal_data"} */
select
      max(record_date::timestamp) as max_loaded_at,
      convert_timezone('UTC', current_timestamp()) as snapshotted_at
    from dataexpert_student.andres.fiscal_data
[0m22:46:37.183178 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m22:46:37.938339 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 0.755 seconds
[0m22:46:37.943765 [debug] [Thread-1 (]: On source.dbt_capstone_andres.treasury.fiscal_data: Close
[0m22:46:38.233515 [info ] [Thread-1 (]: 1 of 1 PASS freshness of treasury.fiscal_data .................................. [[32mPASS[0m in 1.07s]
[0m22:46:38.235515 [debug] [Thread-1 (]: Finished running node source.dbt_capstone_andres.treasury.fiscal_data
[0m22:46:38.240297 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:46:38.241462 [debug] [MainThread]: Connection 'source.dbt_capstone_andres.treasury.fiscal_data' was properly closed.
[0m22:46:38.242608 [info ] [MainThread]: 
[0m22:46:38.243905 [info ] [MainThread]: Finished running 1 source in 0 hours 0 minutes and 2.21 seconds (2.21s).
[0m22:46:38.380864 [info ] [MainThread]: Done.
[0m22:46:38.382826 [debug] [MainThread]: Resource report: {"command_name": "freshness", "command_success": true, "command_wall_clock_time": 4.9808154, "process_user_time": 5.011159, "process_kernel_time": 0.830382, "process_mem_max_rss": "203936", "process_out_blocks": "3685", "process_in_blocks": "0"}
[0m22:46:38.384561 [debug] [MainThread]: Command `dbt source freshness` succeeded at 22:46:38.384398 after 4.98 seconds
[0m22:46:38.387417 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe383988910>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe387475590>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe3873f4b90>]}
[0m22:46:38.388509 [debug] [MainThread]: Flushing usage events
[0m22:46:38.587116 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m22:46:57.826731 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa523414a90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa523127c50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa5230e0e50>]}


============================== 22:46:57.830838 | 16df97ca-fe16-4ac1-afda-780ca97ddb70 ==============================
[0m22:46:57.830838 [info ] [MainThread]: Running with dbt=1.9.0-b3
[0m22:46:57.833274 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/usr/local/airflow/dags/capstone/dbt_capstone_andres', 'fail_fast': 'False', 'version_check': 'True', 'log_path': '/usr/local/airflow/dags/capstone/dbt_capstone_andres/logs', 'debug': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'invocation_command': 'dbt source freshness', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m22:46:59.263279 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '16df97ca-fe16-4ac1-afda-780ca97ddb70', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa4f7de3150>]}
[0m22:46:59.344726 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '16df97ca-fe16-4ac1-afda-780ca97ddb70', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa5244f3350>]}
[0m22:46:59.346306 [info ] [MainThread]: Registered adapter: snowflake=1.8.4
[0m22:46:59.442470 [debug] [MainThread]: checksum: 5f4fde221ca8627174a0341efc73099aaec05b8546408ceed6f59b1ec0755115, vars: {}, profile: , target: , version: 1.9.0b3
[0m22:47:00.094967 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m22:47:00.096062 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m22:47:00.106686 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 2 unused configuration paths:
- models.marts
- models.staging
[0m22:47:00.181581 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '16df97ca-fe16-4ac1-afda-780ca97ddb70', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa4f78a8a90>]}
[0m22:47:00.431999 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '16df97ca-fe16-4ac1-afda-780ca97ddb70', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa4f7c56490>]}
[0m22:47:00.433330 [info ] [MainThread]: Found 5 models, 2 data tests, 2 sources, 862 macros, 1 unit test
[0m22:47:00.434593 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '16df97ca-fe16-4ac1-afda-780ca97ddb70', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa4f7bd1510>]}
[0m22:47:00.437968 [info ] [MainThread]: 
[0m22:47:00.439433 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m22:47:00.440553 [info ] [MainThread]: 
[0m22:47:00.444967 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m22:47:00.451981 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_DATAEXPERT_STUDENT_andres'
[0m22:47:00.477930 [debug] [ThreadPool]: Using snowflake connection "list_DATAEXPERT_STUDENT_andres"
[0m22:47:00.478774 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "connection_name": "list_DATAEXPERT_STUDENT_andres"} */
show objects in DATAEXPERT_STUDENT.andres limit 10000
[0m22:47:00.479711 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:47:01.328610 [debug] [ThreadPool]: SQL status: SUCCESS 28 in 0.849 seconds
[0m22:47:01.331765 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: Close
[0m22:47:01.551346 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '16df97ca-fe16-4ac1-afda-780ca97ddb70', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa523f08e90>]}
[0m22:47:01.555841 [info ] [MainThread]: Pulling freshness from warehouse metadata tables for 0 sources
[0m22:47:01.562685 [debug] [Thread-1 (]: Began running node source.dbt_capstone_andres.treasury.fiscal_data
[0m22:47:01.563817 [info ] [Thread-1 (]: 1 of 1 START freshness of treasury.fiscal_data ................................. [RUN]
[0m22:47:01.564949 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_DATAEXPERT_STUDENT_andres, now source.dbt_capstone_andres.treasury.fiscal_data)
[0m22:47:01.565920 [debug] [Thread-1 (]: Began compiling node source.dbt_capstone_andres.treasury.fiscal_data
[0m22:47:01.566882 [debug] [Thread-1 (]: Began executing node source.dbt_capstone_andres.treasury.fiscal_data
[0m22:47:01.577871 [debug] [Thread-1 (]: Using snowflake connection "source.dbt_capstone_andres.treasury.fiscal_data"
[0m22:47:01.578865 [debug] [Thread-1 (]: On source.dbt_capstone_andres.treasury.fiscal_data: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "source.dbt_capstone_andres.treasury.fiscal_data"} */
select
      max(record_date::timestamp) as max_loaded_at,
      convert_timezone('UTC', current_timestamp()) as snapshotted_at
    from dataexpert_student.andres.fiscal_data
[0m22:47:01.579946 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m22:47:02.391327 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 0.811 seconds
[0m22:47:02.393364 [debug] [Thread-1 (]: On source.dbt_capstone_andres.treasury.fiscal_data: Close
[0m22:47:02.621474 [info ] [Thread-1 (]: 1 of 1 PASS freshness of treasury.fiscal_data .................................. [[32mPASS[0m in 1.06s]
[0m22:47:02.622486 [debug] [Thread-1 (]: Finished running node source.dbt_capstone_andres.treasury.fiscal_data
[0m22:47:02.624324 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:47:02.624923 [debug] [MainThread]: Connection 'source.dbt_capstone_andres.treasury.fiscal_data' was properly closed.
[0m22:47:02.625546 [info ] [MainThread]: 
[0m22:47:02.626157 [info ] [MainThread]: Finished running 1 source in 0 hours 0 minutes and 2.18 seconds (2.18s).
[0m22:47:02.713883 [info ] [MainThread]: Done.
[0m22:47:02.715352 [debug] [MainThread]: Resource report: {"command_name": "freshness", "command_success": true, "command_wall_clock_time": 4.9807787, "process_user_time": 4.535334, "process_kernel_time": 0.793063, "process_mem_max_rss": "209832", "process_out_blocks": "3685", "process_in_blocks": "0"}
[0m22:47:02.716410 [debug] [MainThread]: Command `dbt source freshness` succeeded at 22:47:02.716290 after 4.98 seconds
[0m22:47:02.717182 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa52314fc50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa52314fa90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa526e9ffd0>]}
[0m22:47:02.717865 [debug] [MainThread]: Flushing usage events
[0m22:47:02.906853 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m22:47:15.071762 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8e85a28b10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8e85a5c7d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8e85cf8510>]}


============================== 22:47:15.076900 | 49ba852b-c2d4-48f3-8434-0fdf243c2ef0 ==============================
[0m22:47:15.076900 [info ] [MainThread]: Running with dbt=1.9.0-b3
[0m22:47:15.078322 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'fail_fast': 'False', 'log_path': '/usr/local/airflow/dags/capstone/dbt_capstone_andres/logs', 'profiles_dir': '/usr/local/airflow/dags/capstone/dbt_capstone_andres', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'invocation_command': 'dbt source freshness', 'static_parser': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m22:47:16.761998 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '49ba852b-c2d4-48f3-8434-0fdf243c2ef0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8e5e7daf10>]}
[0m22:47:16.927118 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '49ba852b-c2d4-48f3-8434-0fdf243c2ef0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8e85b2d210>]}
[0m22:47:16.930697 [info ] [MainThread]: Registered adapter: snowflake=1.8.4
[0m22:47:17.000710 [debug] [MainThread]: checksum: 5f4fde221ca8627174a0341efc73099aaec05b8546408ceed6f59b1ec0755115, vars: {}, profile: , target: , version: 1.9.0b3
[0m22:47:17.934860 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m22:47:17.937073 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m22:47:17.951184 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 2 unused configuration paths:
- models.staging
- models.marts
[0m22:47:18.039749 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '49ba852b-c2d4-48f3-8434-0fdf243c2ef0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8e5e36fd10>]}
[0m22:47:18.399524 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '49ba852b-c2d4-48f3-8434-0fdf243c2ef0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8e5e56e450>]}
[0m22:47:18.401179 [info ] [MainThread]: Found 5 models, 2 data tests, 2 sources, 862 macros, 1 unit test
[0m22:47:18.402551 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '49ba852b-c2d4-48f3-8434-0fdf243c2ef0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8e5e4d4f10>]}
[0m22:47:18.406406 [info ] [MainThread]: 
[0m22:47:18.407985 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m22:47:18.409167 [info ] [MainThread]: 
[0m22:47:18.410855 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m22:47:18.420655 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_DATAEXPERT_STUDENT_andres'
[0m22:47:18.456102 [debug] [ThreadPool]: Using snowflake connection "list_DATAEXPERT_STUDENT_andres"
[0m22:47:18.457424 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "connection_name": "list_DATAEXPERT_STUDENT_andres"} */
show objects in DATAEXPERT_STUDENT.andres limit 10000
[0m22:47:18.458779 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:47:19.409275 [debug] [ThreadPool]: SQL status: SUCCESS 28 in 0.950 seconds
[0m22:47:19.421980 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: Close
[0m22:47:19.663329 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '49ba852b-c2d4-48f3-8434-0fdf243c2ef0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8e85897990>]}
[0m22:47:19.665508 [info ] [MainThread]: Pulling freshness from warehouse metadata tables for 0 sources
[0m22:47:19.675228 [debug] [Thread-1 (]: Began running node source.dbt_capstone_andres.treasury.fiscal_data
[0m22:47:19.676892 [info ] [Thread-1 (]: 1 of 1 START freshness of treasury.fiscal_data ................................. [RUN]
[0m22:47:19.678827 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_DATAEXPERT_STUDENT_andres, now source.dbt_capstone_andres.treasury.fiscal_data)
[0m22:47:19.680290 [debug] [Thread-1 (]: Began compiling node source.dbt_capstone_andres.treasury.fiscal_data
[0m22:47:19.681984 [debug] [Thread-1 (]: Began executing node source.dbt_capstone_andres.treasury.fiscal_data
[0m22:47:19.705560 [debug] [Thread-1 (]: Using snowflake connection "source.dbt_capstone_andres.treasury.fiscal_data"
[0m22:47:19.707431 [debug] [Thread-1 (]: On source.dbt_capstone_andres.treasury.fiscal_data: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "source.dbt_capstone_andres.treasury.fiscal_data"} */
select
      max(record_date::timestamp) as max_loaded_at,
      convert_timezone('UTC', current_timestamp()) as snapshotted_at
    from dataexpert_student.andres.fiscal_data
[0m22:47:19.709206 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m22:47:20.504137 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 0.795 seconds
[0m22:47:20.506155 [debug] [Thread-1 (]: On source.dbt_capstone_andres.treasury.fiscal_data: Close
[0m22:47:20.714551 [info ] [Thread-1 (]: 1 of 1 PASS freshness of treasury.fiscal_data .................................. [[32mPASS[0m in 1.04s]
[0m22:47:20.716030 [debug] [Thread-1 (]: Finished running node source.dbt_capstone_andres.treasury.fiscal_data
[0m22:47:20.719351 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:47:20.720860 [debug] [MainThread]: Connection 'source.dbt_capstone_andres.treasury.fiscal_data' was properly closed.
[0m22:47:20.721991 [info ] [MainThread]: 
[0m22:47:20.723146 [info ] [MainThread]: Finished running 1 source in 0 hours 0 minutes and 2.31 seconds (2.31s).
[0m22:47:20.915630 [info ] [MainThread]: Done.
[0m22:47:20.918487 [debug] [MainThread]: Resource report: {"command_name": "freshness", "command_success": true, "command_wall_clock_time": 5.936695, "process_user_time": 5.252618, "process_kernel_time": 0.798568, "process_mem_max_rss": "202416", "process_out_blocks": "3685", "process_in_blocks": "0"}
[0m22:47:20.922341 [debug] [MainThread]: Command `dbt source freshness` succeeded at 22:47:20.922121 after 5.94 seconds
[0m22:47:20.924062 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8e85a75dd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8e858979d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8e8978ffd0>]}
[0m22:47:20.927953 [debug] [MainThread]: Flushing usage events
[0m22:47:21.121285 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m22:47:32.466164 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3413790a10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f34139958d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f341378ac50>]}


============================== 22:47:32.469170 | bdfba754-308e-429d-8136-58d1c42956a8 ==============================
[0m22:47:32.469170 [info ] [MainThread]: Running with dbt=1.9.0-b3
[0m22:47:32.470974 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/usr/local/airflow/dags/capstone/dbt_capstone_andres', 'fail_fast': 'False', 'version_check': 'True', 'log_path': '/usr/local/airflow/dags/capstone/dbt_capstone_andres/logs', 'debug': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'static_parser': 'True', 'log_format': 'default', 'target_path': 'None', 'invocation_command': 'dbt source freshness', 'send_anonymous_usage_stats': 'True'}
[0m22:47:33.732949 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'bdfba754-308e-429d-8136-58d1c42956a8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3413610150>]}
[0m22:47:33.810108 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'bdfba754-308e-429d-8136-58d1c42956a8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f341388d610>]}
[0m22:47:33.811490 [info ] [MainThread]: Registered adapter: snowflake=1.8.4
[0m22:47:33.864169 [debug] [MainThread]: checksum: 5f4fde221ca8627174a0341efc73099aaec05b8546408ceed6f59b1ec0755115, vars: {}, profile: , target: , version: 1.9.0b3
[0m22:47:34.453719 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m22:47:34.454517 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m22:47:34.461819 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 2 unused configuration paths:
- models.marts
- models.staging
[0m22:47:34.514330 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'bdfba754-308e-429d-8136-58d1c42956a8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f33e7f50d90>]}
[0m22:47:34.762362 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'bdfba754-308e-429d-8136-58d1c42956a8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f33ec01a3d0>]}
[0m22:47:34.763479 [info ] [MainThread]: Found 5 models, 2 data tests, 2 sources, 862 macros, 1 unit test
[0m22:47:34.764530 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'bdfba754-308e-429d-8136-58d1c42956a8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f33ec288b50>]}
[0m22:47:34.767081 [info ] [MainThread]: 
[0m22:47:34.768025 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m22:47:34.768855 [info ] [MainThread]: 
[0m22:47:34.769941 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m22:47:34.776003 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_DATAEXPERT_STUDENT_andres'
[0m22:47:34.797373 [debug] [ThreadPool]: Using snowflake connection "list_DATAEXPERT_STUDENT_andres"
[0m22:47:34.798184 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "connection_name": "list_DATAEXPERT_STUDENT_andres"} */
show objects in DATAEXPERT_STUDENT.andres limit 10000
[0m22:47:34.798903 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:47:35.790718 [debug] [ThreadPool]: SQL status: SUCCESS 28 in 0.992 seconds
[0m22:47:35.794552 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: Close
[0m22:47:36.051225 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'bdfba754-308e-429d-8136-58d1c42956a8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3414528ed0>]}
[0m22:47:36.052538 [info ] [MainThread]: Pulling freshness from warehouse metadata tables for 0 sources
[0m22:47:36.056570 [debug] [Thread-1 (]: Began running node source.dbt_capstone_andres.treasury.fiscal_data
[0m22:47:36.057493 [info ] [Thread-1 (]: 1 of 1 START freshness of treasury.fiscal_data ................................. [RUN]
[0m22:47:36.058454 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_DATAEXPERT_STUDENT_andres, now source.dbt_capstone_andres.treasury.fiscal_data)
[0m22:47:36.059276 [debug] [Thread-1 (]: Began compiling node source.dbt_capstone_andres.treasury.fiscal_data
[0m22:47:36.060219 [debug] [Thread-1 (]: Began executing node source.dbt_capstone_andres.treasury.fiscal_data
[0m22:47:36.072166 [debug] [Thread-1 (]: Using snowflake connection "source.dbt_capstone_andres.treasury.fiscal_data"
[0m22:47:36.073111 [debug] [Thread-1 (]: On source.dbt_capstone_andres.treasury.fiscal_data: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "source.dbt_capstone_andres.treasury.fiscal_data"} */
select
      max(record_date::timestamp) as max_loaded_at,
      convert_timezone('UTC', current_timestamp()) as snapshotted_at
    from dataexpert_student.andres.fiscal_data
[0m22:47:36.073977 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m22:47:36.854624 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 0.781 seconds
[0m22:47:36.856613 [debug] [Thread-1 (]: On source.dbt_capstone_andres.treasury.fiscal_data: Close
[0m22:47:37.098037 [info ] [Thread-1 (]: 1 of 1 PASS freshness of treasury.fiscal_data .................................. [[32mPASS[0m in 1.04s]
[0m22:47:37.099105 [debug] [Thread-1 (]: Finished running node source.dbt_capstone_andres.treasury.fiscal_data
[0m22:47:37.101595 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:47:37.102306 [debug] [MainThread]: Connection 'source.dbt_capstone_andres.treasury.fiscal_data' was properly closed.
[0m22:47:37.103162 [info ] [MainThread]: 
[0m22:47:37.103939 [info ] [MainThread]: Finished running 1 source in 0 hours 0 minutes and 2.33 seconds (2.33s).
[0m22:47:37.205070 [info ] [MainThread]: Done.
[0m22:47:37.206622 [debug] [MainThread]: Resource report: {"command_name": "freshness", "command_success": true, "command_wall_clock_time": 4.801327, "process_user_time": 4.08317, "process_kernel_time": 0.959529, "process_mem_max_rss": "203880", "process_out_blocks": "3685", "process_in_blocks": "0"}
[0m22:47:37.207727 [debug] [MainThread]: Command `dbt source freshness` succeeded at 22:47:37.207608 after 4.80 seconds
[0m22:47:37.208481 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3413603c10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3413603a90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f341750bfd0>]}
[0m22:47:37.209273 [debug] [MainThread]: Flushing usage events
[0m22:47:37.398048 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m01:10:44.128087 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa9369aa850>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa936b69390>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa9369a9c90>]}


============================== 01:10:44.149364 | abe080fc-e5c7-4dcb-933f-3cb51ce08db9 ==============================
[0m01:10:44.149364 [info ] [MainThread]: Running with dbt=1.9.0-b3
[0m01:10:44.155119 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'debug': 'False', 'log_path': '/usr/local/airflow/dags/capstone/dbt_capstone_andres/logs', 'fail_fast': 'False', 'profiles_dir': '/usr/local/airflow/dags/capstone/dbt_capstone_andres', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'introspect': 'True', 'invocation_command': 'dbt source freshness', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m01:10:52.785528 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'abe080fc-e5c7-4dcb-933f-3cb51ce08db9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa90b547c90>]}
[0m01:10:53.306041 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'abe080fc-e5c7-4dcb-933f-3cb51ce08db9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa936a5d7d0>]}
[0m01:10:53.312183 [info ] [MainThread]: Registered adapter: snowflake=1.8.4
[0m01:10:53.596171 [debug] [MainThread]: checksum: 5f4fde221ca8627174a0341efc73099aaec05b8546408ceed6f59b1ec0755115, vars: {}, profile: , target: , version: 1.9.0b3
[0m01:10:55.962763 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m01:10:55.966915 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m01:10:56.001051 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 2 unused configuration paths:
- models.marts
- models.staging
[0m01:10:56.273007 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'abe080fc-e5c7-4dcb-933f-3cb51ce08db9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa90b61ad90>]}
[0m01:10:57.454992 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'abe080fc-e5c7-4dcb-933f-3cb51ce08db9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa90b3da890>]}
[0m01:10:57.460146 [info ] [MainThread]: Found 5 models, 2 data tests, 2 sources, 862 macros, 1 unit test
[0m01:10:57.464449 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'abe080fc-e5c7-4dcb-933f-3cb51ce08db9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa90b359550>]}
[0m01:10:57.476597 [info ] [MainThread]: 
[0m01:10:57.480897 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m01:10:57.484805 [info ] [MainThread]: 
[0m01:10:57.489840 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m01:10:57.520850 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_DATAEXPERT_STUDENT_andres'
[0m01:10:57.633932 [debug] [ThreadPool]: Using snowflake connection "list_DATAEXPERT_STUDENT_andres"
[0m01:10:57.637885 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "connection_name": "list_DATAEXPERT_STUDENT_andres"} */
show objects in DATAEXPERT_STUDENT.andres limit 10000
[0m01:10:57.641640 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m01:10:59.397993 [debug] [ThreadPool]: SQL status: SUCCESS 28 in 1.756 seconds
[0m01:10:59.415797 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: Close
[0m01:11:00.223655 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'abe080fc-e5c7-4dcb-933f-3cb51ce08db9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa90b546f90>]}
[0m01:11:00.228950 [info ] [MainThread]: Pulling freshness from warehouse metadata tables for 0 sources
[0m01:11:00.249696 [debug] [Thread-1 (]: Began running node source.dbt_capstone_andres.treasury.fiscal_data
[0m01:11:00.253928 [info ] [Thread-1 (]: 1 of 1 START freshness of treasury.fiscal_data ................................. [RUN]
[0m01:11:00.259303 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_DATAEXPERT_STUDENT_andres, now source.dbt_capstone_andres.treasury.fiscal_data)
[0m01:11:00.263785 [debug] [Thread-1 (]: Began compiling node source.dbt_capstone_andres.treasury.fiscal_data
[0m01:11:00.268853 [debug] [Thread-1 (]: Began executing node source.dbt_capstone_andres.treasury.fiscal_data
[0m01:11:00.321168 [debug] [Thread-1 (]: Using snowflake connection "source.dbt_capstone_andres.treasury.fiscal_data"
[0m01:11:00.325875 [debug] [Thread-1 (]: On source.dbt_capstone_andres.treasury.fiscal_data: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "source.dbt_capstone_andres.treasury.fiscal_data"} */
select
      max(record_date::timestamp) as max_loaded_at,
      convert_timezone('UTC', current_timestamp()) as snapshotted_at
    from dataexpert_student.andres.fiscal_data
[0m01:11:00.329968 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m01:11:01.627186 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 1.297 seconds
[0m01:11:01.637725 [debug] [Thread-1 (]: On source.dbt_capstone_andres.treasury.fiscal_data: Close
[0m01:11:01.912559 [info ] [Thread-1 (]: 1 of 1 PASS freshness of treasury.fiscal_data .................................. [[32mPASS[0m in 1.65s]
[0m01:11:01.918467 [debug] [Thread-1 (]: Finished running node source.dbt_capstone_andres.treasury.fiscal_data
[0m01:11:01.929751 [debug] [MainThread]: Connection 'master' was properly closed.
[0m01:11:01.933489 [debug] [MainThread]: Connection 'source.dbt_capstone_andres.treasury.fiscal_data' was properly closed.
[0m01:11:01.937049 [info ] [MainThread]: 
[0m01:11:01.941810 [info ] [MainThread]: Finished running 1 source in 0 hours 0 minutes and 4.45 seconds (4.45s).
[0m01:11:02.445034 [info ] [MainThread]: Done.
[0m01:11:02.452033 [debug] [MainThread]: Resource report: {"command_name": "freshness", "command_success": true, "command_wall_clock_time": 18.636877, "process_user_time": 22.556536, "process_kernel_time": 3.471428, "process_mem_max_rss": "210116", "process_out_blocks": "3685", "process_in_blocks": "0"}
[0m01:11:02.458083 [debug] [MainThread]: Command `dbt source freshness` succeeded at 01:11:02.457009 after 18.64 seconds
[0m01:11:02.462280 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa9369c7dd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa93763c210>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa93a723fd0>]}
[0m01:11:02.469505 [debug] [MainThread]: Flushing usage events
[0m01:11:02.937562 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:53:59.233754 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc44f915ed0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc44fb21890>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc44f916490>]}


============================== 14:53:59.239227 | f3ec6d56-663c-42c7-acd6-fe66f2b0f027 ==============================
[0m14:53:59.239227 [info ] [MainThread]: Running with dbt=1.9.0-b3
[0m14:53:59.240315 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/usr/local/airflow/dags/capstone/dbt_capstone_andres', 'fail_fast': 'False', 'version_check': 'True', 'log_path': '/usr/local/airflow/dags/capstone/dbt_capstone_andres/logs', 'debug': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'log_format': 'default', 'invocation_command': 'dbt source freshness', 'static_parser': 'True', 'introspect': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m14:54:00.253570 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'f3ec6d56-663c-42c7-acd6-fe66f2b0f027', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc4284ec4d0>]}
[0m14:54:00.315639 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'f3ec6d56-663c-42c7-acd6-fe66f2b0f027', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc450d275d0>]}
[0m14:54:00.316848 [info ] [MainThread]: Registered adapter: snowflake=1.8.4
[0m14:54:00.357705 [debug] [MainThread]: checksum: 5f4fde221ca8627174a0341efc73099aaec05b8546408ceed6f59b1ec0755115, vars: {}, profile: , target: , version: 1.9.0b3
[0m14:54:00.760400 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:54:00.761131 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:54:00.766869 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 2 unused configuration paths:
- models.staging
- models.marts
[0m14:54:00.808972 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'f3ec6d56-663c-42c7-acd6-fe66f2b0f027', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc4280eb110>]}
[0m14:54:00.983452 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'f3ec6d56-663c-42c7-acd6-fe66f2b0f027', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc42846dcd0>]}
[0m14:54:00.984270 [info ] [MainThread]: Found 5 models, 2 data tests, 2 sources, 862 macros, 1 unit test
[0m14:54:00.984978 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f3ec6d56-663c-42c7-acd6-fe66f2b0f027', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc4283f0c90>]}
[0m14:54:00.986884 [info ] [MainThread]: 
[0m14:54:00.987537 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:54:00.988203 [info ] [MainThread]: 
[0m14:54:00.989149 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m14:54:00.994941 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_DATAEXPERT_STUDENT_andres'
[0m14:54:01.030411 [debug] [ThreadPool]: Using snowflake connection "list_DATAEXPERT_STUDENT_andres"
[0m14:54:01.031746 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "connection_name": "list_DATAEXPERT_STUDENT_andres"} */
show objects in DATAEXPERT_STUDENT.andres limit 10000
[0m14:54:01.032624 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:54:02.398753 [debug] [ThreadPool]: SQL status: SUCCESS 28 in 1.366 seconds
[0m14:54:02.404239 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: Close
[0m14:54:02.723559 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f3ec6d56-663c-42c7-acd6-fe66f2b0f027', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc4280d5b10>]}
[0m14:54:02.725699 [info ] [MainThread]: Pulling freshness from warehouse metadata tables for 0 sources
[0m14:54:02.733298 [debug] [Thread-1 (]: Began running node source.dbt_capstone_andres.treasury.fiscal_data
[0m14:54:02.734736 [info ] [Thread-1 (]: 1 of 1 START freshness of treasury.fiscal_data ................................. [RUN]
[0m14:54:02.736212 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_DATAEXPERT_STUDENT_andres, now source.dbt_capstone_andres.treasury.fiscal_data)
[0m14:54:02.737594 [debug] [Thread-1 (]: Began compiling node source.dbt_capstone_andres.treasury.fiscal_data
[0m14:54:02.738693 [debug] [Thread-1 (]: Began executing node source.dbt_capstone_andres.treasury.fiscal_data
[0m14:54:02.757115 [debug] [Thread-1 (]: Using snowflake connection "source.dbt_capstone_andres.treasury.fiscal_data"
[0m14:54:02.760952 [debug] [Thread-1 (]: On source.dbt_capstone_andres.treasury.fiscal_data: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "source.dbt_capstone_andres.treasury.fiscal_data"} */
select
      max(record_date::timestamp) as max_loaded_at,
      convert_timezone('UTC', current_timestamp()) as snapshotted_at
    from dataexpert_student.andres.fiscal_data
[0m14:54:02.763941 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:54:03.816898 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 1.054 seconds
[0m14:54:03.823032 [debug] [Thread-1 (]: On source.dbt_capstone_andres.treasury.fiscal_data: Close
[0m14:54:04.083186 [info ] [Thread-1 (]: 1 of 1 PASS freshness of treasury.fiscal_data .................................. [[32mPASS[0m in 1.35s]
[0m14:54:04.084759 [debug] [Thread-1 (]: Finished running node source.dbt_capstone_andres.treasury.fiscal_data
[0m14:54:04.087995 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:54:04.089129 [debug] [MainThread]: Connection 'source.dbt_capstone_andres.treasury.fiscal_data' was properly closed.
[0m14:54:04.089964 [info ] [MainThread]: 
[0m14:54:04.090712 [info ] [MainThread]: Finished running 1 source in 0 hours 0 minutes and 3.10 seconds (3.10s).
[0m14:54:04.183211 [info ] [MainThread]: Done.
[0m14:54:04.185135 [debug] [MainThread]: Resource report: {"command_name": "freshness", "command_success": true, "command_wall_clock_time": 5.0040097, "process_user_time": 3.914919, "process_kernel_time": 1.14605, "process_mem_max_rss": "202324", "process_out_blocks": "3685", "process_in_blocks": "0"}
[0m14:54:04.187124 [debug] [MainThread]: Command `dbt source freshness` succeeded at 14:54:04.186855 after 5.01 seconds
[0m14:54:04.188535 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc44f963110>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc44f962c50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc4536bffd0>]}
[0m14:54:04.189344 [debug] [MainThread]: Flushing usage events
[0m14:54:04.507811 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m01:00:25.159352 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb909fe1250>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb90a320890>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb909fe0890>]}


============================== 01:00:25.168913 | fa040ce4-5b73-4a7c-acd6-cafc3a08e0b5 ==============================
[0m01:00:25.168913 [info ] [MainThread]: Running with dbt=1.9.0-b3
[0m01:00:25.169915 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'profiles_dir': '/usr/local/airflow/dags/capstone/dbt_capstone_andres', 'log_path': '/usr/local/airflow/dags/capstone/dbt_capstone_andres/logs', 'version_check': 'True', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'log_format': 'default', 'static_parser': 'True', 'invocation_command': 'dbt source freshness', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m01:00:26.416951 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'fa040ce4-5b73-4a7c-acd6-cafc3a08e0b5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb8dec16450>]}
[0m01:00:26.480306 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'fa040ce4-5b73-4a7c-acd6-cafc3a08e0b5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb90b3fb450>]}
[0m01:00:26.483683 [info ] [MainThread]: Registered adapter: snowflake=1.8.4
[0m01:00:26.529392 [debug] [MainThread]: checksum: 5f4fde221ca8627174a0341efc73099aaec05b8546408ceed6f59b1ec0755115, vars: {}, profile: , target: , version: 1.9.0b3
[0m01:00:26.947008 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m01:00:26.949215 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m01:00:26.954901 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 2 unused configuration paths:
- models.marts
- models.staging
[0m01:00:27.003649 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'fa040ce4-5b73-4a7c-acd6-cafc3a08e0b5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb8ded952d0>]}
[0m01:00:27.261597 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'fa040ce4-5b73-4a7c-acd6-cafc3a08e0b5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb8dead6390>]}
[0m01:00:27.262716 [info ] [MainThread]: Found 5 models, 2 data tests, 2 sources, 862 macros, 1 unit test
[0m01:00:27.264170 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'fa040ce4-5b73-4a7c-acd6-cafc3a08e0b5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb8dea50190>]}
[0m01:00:27.267136 [info ] [MainThread]: 
[0m01:00:27.268715 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m01:00:27.270391 [info ] [MainThread]: 
[0m01:00:27.271443 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m01:00:27.277539 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_DATAEXPERT_STUDENT_andres'
[0m01:00:27.300106 [debug] [ThreadPool]: Using snowflake connection "list_DATAEXPERT_STUDENT_andres"
[0m01:00:27.301499 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "connection_name": "list_DATAEXPERT_STUDENT_andres"} */
show objects in DATAEXPERT_STUDENT.andres limit 10000
[0m01:00:27.303324 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m01:00:28.456072 [debug] [ThreadPool]: SQL status: SUCCESS 28 in 1.153 seconds
[0m01:00:28.462731 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: Close
[0m01:00:28.872010 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'fa040ce4-5b73-4a7c-acd6-cafc3a08e0b5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb8de750310>]}
[0m01:00:28.872918 [info ] [MainThread]: Pulling freshness from warehouse metadata tables for 0 sources
[0m01:00:28.877471 [debug] [Thread-1 (]: Began running node source.dbt_capstone_andres.treasury.fiscal_data
[0m01:00:28.878388 [info ] [Thread-1 (]: 1 of 1 START freshness of treasury.fiscal_data ................................. [RUN]
[0m01:00:28.879339 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_DATAEXPERT_STUDENT_andres, now source.dbt_capstone_andres.treasury.fiscal_data)
[0m01:00:28.879997 [debug] [Thread-1 (]: Began compiling node source.dbt_capstone_andres.treasury.fiscal_data
[0m01:00:28.880656 [debug] [Thread-1 (]: Began executing node source.dbt_capstone_andres.treasury.fiscal_data
[0m01:00:28.891946 [debug] [Thread-1 (]: Using snowflake connection "source.dbt_capstone_andres.treasury.fiscal_data"
[0m01:00:28.892682 [debug] [Thread-1 (]: On source.dbt_capstone_andres.treasury.fiscal_data: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "source.dbt_capstone_andres.treasury.fiscal_data"} */
select
      max(record_date::timestamp) as max_loaded_at,
      convert_timezone('UTC', current_timestamp()) as snapshotted_at
    from dataexpert_student.andres.fiscal_data
[0m01:00:28.893434 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m01:00:29.825160 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 0.932 seconds
[0m01:00:29.828598 [debug] [Thread-1 (]: On source.dbt_capstone_andres.treasury.fiscal_data: Close
[0m01:00:30.047747 [info ] [Thread-1 (]: 1 of 1 PASS freshness of treasury.fiscal_data .................................. [[32mPASS[0m in 1.17s]
[0m01:00:30.049092 [debug] [Thread-1 (]: Finished running node source.dbt_capstone_andres.treasury.fiscal_data
[0m01:00:30.051444 [debug] [MainThread]: Connection 'master' was properly closed.
[0m01:00:30.052104 [debug] [MainThread]: Connection 'source.dbt_capstone_andres.treasury.fiscal_data' was properly closed.
[0m01:00:30.052799 [info ] [MainThread]: 
[0m01:00:30.053531 [info ] [MainThread]: Finished running 1 source in 0 hours 0 minutes and 2.78 seconds (2.78s).
[0m01:00:30.144515 [info ] [MainThread]: Done.
[0m01:00:30.146579 [debug] [MainThread]: Resource report: {"command_name": "freshness", "command_success": true, "command_wall_clock_time": 5.036192, "process_user_time": 3.959142, "process_kernel_time": 1.68315, "process_mem_max_rss": "210576", "process_out_blocks": "3685", "process_in_blocks": "0"}
[0m01:00:30.147595 [debug] [MainThread]: Command `dbt source freshness` succeeded at 01:00:30.147476 after 5.04 seconds
[0m01:00:30.148377 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb90a04ff10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb90a04fe10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb90dd8ffd0>]}
[0m01:00:30.149080 [debug] [MainThread]: Flushing usage events
[0m01:00:30.392686 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m01:00:31.823458 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f07a8858b10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f07a8a65e10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f07a8b285d0>]}


============================== 01:00:31.854134 | ee10ce1e-fa75-4bd0-b8a0-14d5749a6988 ==============================
[0m01:00:31.854134 [info ] [MainThread]: Running with dbt=1.9.0-b3
[0m01:00:31.855950 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'profiles_dir': '/usr/local/airflow/dags/capstone/dbt_capstone_andres', 'log_path': '/usr/local/airflow/dags/capstone/dbt_capstone_andres/logs', 'fail_fast': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt source freshness', 'introspect': 'True', 'log_format': 'default', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m01:00:32.978415 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'ee10ce1e-fa75-4bd0-b8a0-14d5749a6988', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0781410310>]}
[0m01:00:33.046295 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'ee10ce1e-fa75-4bd0-b8a0-14d5749a6988', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f07a9c6b550>]}
[0m01:00:33.047771 [info ] [MainThread]: Registered adapter: snowflake=1.8.4
[0m01:00:33.095216 [debug] [MainThread]: checksum: 5f4fde221ca8627174a0341efc73099aaec05b8546408ceed6f59b1ec0755115, vars: {}, profile: , target: , version: 1.9.0b3
[0m01:00:33.493143 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m01:00:33.493827 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m01:00:33.499562 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 2 unused configuration paths:
- models.marts
- models.staging
[0m01:00:33.540746 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'ee10ce1e-fa75-4bd0-b8a0-14d5749a6988', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f078103dad0>]}
[0m01:00:33.716378 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'ee10ce1e-fa75-4bd0-b8a0-14d5749a6988', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f07813b65d0>]}
[0m01:00:33.717180 [info ] [MainThread]: Found 5 models, 2 data tests, 2 sources, 862 macros, 1 unit test
[0m01:00:33.717859 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ee10ce1e-fa75-4bd0-b8a0-14d5749a6988', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f078131d0d0>]}
[0m01:00:33.719898 [info ] [MainThread]: 
[0m01:00:33.720606 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m01:00:33.721215 [info ] [MainThread]: 
[0m01:00:33.721982 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m01:00:33.727763 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_DATAEXPERT_STUDENT_andres'
[0m01:00:33.746310 [debug] [ThreadPool]: Using snowflake connection "list_DATAEXPERT_STUDENT_andres"
[0m01:00:33.747130 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "connection_name": "list_DATAEXPERT_STUDENT_andres"} */
show objects in DATAEXPERT_STUDENT.andres limit 10000
[0m01:00:33.747830 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m01:00:34.791015 [debug] [ThreadPool]: SQL status: SUCCESS 28 in 1.043 seconds
[0m01:00:34.795941 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: Close
[0m01:00:35.019847 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ee10ce1e-fa75-4bd0-b8a0-14d5749a6988', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f07a89178d0>]}
[0m01:00:35.021268 [info ] [MainThread]: Pulling freshness from warehouse metadata tables for 0 sources
[0m01:00:35.027753 [debug] [Thread-1 (]: Began running node source.dbt_capstone_andres.treasury.fiscal_data
[0m01:00:35.028879 [info ] [Thread-1 (]: 1 of 1 START freshness of treasury.fiscal_data ................................. [RUN]
[0m01:00:35.030144 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_DATAEXPERT_STUDENT_andres, now source.dbt_capstone_andres.treasury.fiscal_data)
[0m01:00:35.031192 [debug] [Thread-1 (]: Began compiling node source.dbt_capstone_andres.treasury.fiscal_data
[0m01:00:35.032218 [debug] [Thread-1 (]: Began executing node source.dbt_capstone_andres.treasury.fiscal_data
[0m01:00:35.043758 [debug] [Thread-1 (]: Using snowflake connection "source.dbt_capstone_andres.treasury.fiscal_data"
[0m01:00:35.044724 [debug] [Thread-1 (]: On source.dbt_capstone_andres.treasury.fiscal_data: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "source.dbt_capstone_andres.treasury.fiscal_data"} */
select
      max(record_date::timestamp) as max_loaded_at,
      convert_timezone('UTC', current_timestamp()) as snapshotted_at
    from dataexpert_student.andres.fiscal_data
[0m01:00:35.045700 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m01:00:35.974306 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 0.928 seconds
[0m01:00:35.980088 [debug] [Thread-1 (]: On source.dbt_capstone_andres.treasury.fiscal_data: Close
[0m01:00:36.215432 [info ] [Thread-1 (]: 1 of 1 PASS freshness of treasury.fiscal_data .................................. [[32mPASS[0m in 1.19s]
[0m01:00:36.216885 [debug] [Thread-1 (]: Finished running node source.dbt_capstone_andres.treasury.fiscal_data
[0m01:00:36.219878 [debug] [MainThread]: Connection 'master' was properly closed.
[0m01:00:36.220810 [debug] [MainThread]: Connection 'source.dbt_capstone_andres.treasury.fiscal_data' was properly closed.
[0m01:00:36.221764 [info ] [MainThread]: 
[0m01:00:36.222719 [info ] [MainThread]: Finished running 1 source in 0 hours 0 minutes and 2.50 seconds (2.50s).
[0m01:00:36.307134 [info ] [MainThread]: Done.
[0m01:00:36.308504 [debug] [MainThread]: Resource report: {"command_name": "freshness", "command_success": true, "command_wall_clock_time": 4.534546, "process_user_time": 3.714612, "process_kernel_time": 1.687522, "process_mem_max_rss": "210620", "process_out_blocks": "3685", "process_in_blocks": "0"}
[0m01:00:36.309497 [debug] [MainThread]: Command `dbt source freshness` succeeded at 01:00:36.309392 after 4.54 seconds
[0m01:00:36.310245 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f07a88a6290>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f07a88a6550>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f07ac5ebfd0>]}
[0m01:00:36.311161 [debug] [MainThread]: Flushing usage events
[0m01:00:36.728994 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m04:41:38.495102 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa7de4d4750>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa7de4d4b90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa7de4d5410>]}


============================== 04:41:38.501046 | 88505e2c-3c3d-4629-ab91-e845fbe3692e ==============================
[0m04:41:38.501046 [info ] [MainThread]: Running with dbt=1.9.0-b3
[0m04:41:38.502109 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/usr/local/airflow/dags/capstone/dbt_capstone_andres', 'fail_fast': 'False', 'debug': 'False', 'log_path': '/usr/local/airflow/dags/capstone/dbt_capstone_andres/logs', 'warn_error': 'None', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'static_parser': 'True', 'invocation_command': 'dbt source freshness', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m04:41:39.515419 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '88505e2c-3c3d-4629-ab91-e845fbe3692e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa7b31e01d0>]}
[0m04:41:39.573601 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '88505e2c-3c3d-4629-ab91-e845fbe3692e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa7df8c7710>]}
[0m04:41:39.574721 [info ] [MainThread]: Registered adapter: snowflake=1.8.4
[0m04:41:39.617048 [debug] [MainThread]: checksum: 5f4fde221ca8627174a0341efc73099aaec05b8546408ceed6f59b1ec0755115, vars: {}, profile: , target: , version: 1.9.0b3
[0m04:41:40.001473 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m04:41:40.002169 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m04:41:40.007912 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 2 unused configuration paths:
- models.staging
- models.marts
[0m04:41:40.049812 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '88505e2c-3c3d-4629-ab91-e845fbe3692e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa7de4d6ed0>]}
[0m04:41:40.221383 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '88505e2c-3c3d-4629-ab91-e845fbe3692e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa7b2ffe1d0>]}
[0m04:41:40.222284 [info ] [MainThread]: Found 5 models, 2 data tests, 2 sources, 862 macros, 1 unit test
[0m04:41:40.223185 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '88505e2c-3c3d-4629-ab91-e845fbe3692e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa7b2f7cf10>]}
[0m04:41:40.225493 [info ] [MainThread]: 
[0m04:41:40.226407 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m04:41:40.227319 [info ] [MainThread]: 
[0m04:41:40.228257 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m04:41:40.234059 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_DATAEXPERT_STUDENT_andres'
[0m04:41:40.252825 [debug] [ThreadPool]: Using snowflake connection "list_DATAEXPERT_STUDENT_andres"
[0m04:41:40.253597 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "connection_name": "list_DATAEXPERT_STUDENT_andres"} */
show objects in DATAEXPERT_STUDENT.andres limit 10000
[0m04:41:40.254297 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m04:41:49.739680 [debug] [ThreadPool]: SQL status: SUCCESS 28 in 9.483 seconds
[0m04:41:49.765419 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: Close
[0m04:41:50.004093 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '88505e2c-3c3d-4629-ab91-e845fbe3692e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa7de593b90>]}
[0m04:41:50.008684 [info ] [MainThread]: Pulling freshness from warehouse metadata tables for 0 sources
[0m04:41:50.022031 [debug] [Thread-1 (]: Began running node source.dbt_capstone_andres.treasury.fiscal_data
[0m04:41:50.023521 [info ] [Thread-1 (]: 1 of 1 START freshness of treasury.fiscal_data ................................. [RUN]
[0m04:41:50.024801 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_DATAEXPERT_STUDENT_andres, now source.dbt_capstone_andres.treasury.fiscal_data)
[0m04:41:50.026388 [debug] [Thread-1 (]: Began compiling node source.dbt_capstone_andres.treasury.fiscal_data
[0m04:41:50.027468 [debug] [Thread-1 (]: Began executing node source.dbt_capstone_andres.treasury.fiscal_data
[0m04:41:50.052474 [debug] [Thread-1 (]: Using snowflake connection "source.dbt_capstone_andres.treasury.fiscal_data"
[0m04:41:50.053400 [debug] [Thread-1 (]: On source.dbt_capstone_andres.treasury.fiscal_data: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "source.dbt_capstone_andres.treasury.fiscal_data"} */
select
      max(record_date::timestamp) as max_loaded_at,
      convert_timezone('UTC', current_timestamp()) as snapshotted_at
    from dataexpert_student.andres.fiscal_data
[0m04:41:50.054299 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m04:41:52.570249 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 2.516 seconds
[0m04:41:52.575624 [debug] [Thread-1 (]: On source.dbt_capstone_andres.treasury.fiscal_data: Close
[0m04:41:52.800141 [info ] [Thread-1 (]: 1 of 1 PASS freshness of treasury.fiscal_data .................................. [[32mPASS[0m in 2.78s]
[0m04:41:52.802003 [debug] [Thread-1 (]: Finished running node source.dbt_capstone_andres.treasury.fiscal_data
[0m04:41:52.806211 [debug] [MainThread]: Connection 'master' was properly closed.
[0m04:41:52.807140 [debug] [MainThread]: Connection 'source.dbt_capstone_andres.treasury.fiscal_data' was properly closed.
[0m04:41:52.808274 [info ] [MainThread]: 
[0m04:41:52.809480 [info ] [MainThread]: Finished running 1 source in 0 hours 0 minutes and 12.58 seconds (12.58s).
[0m04:41:52.946520 [info ] [MainThread]: Done.
[0m04:41:52.950473 [debug] [MainThread]: Resource report: {"command_name": "freshness", "command_success": true, "command_wall_clock_time": 14.50941, "process_user_time": 4.626517, "process_kernel_time": 1.465506, "process_mem_max_rss": "207112", "process_out_blocks": "3685", "process_in_blocks": "0"}
[0m04:41:52.953096 [debug] [MainThread]: Command `dbt source freshness` succeeded at 04:41:52.952882 after 14.51 seconds
[0m04:41:52.954291 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa7de343f50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa7de343e50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa7e2273fd0>]}
[0m04:41:52.955037 [debug] [MainThread]: Flushing usage events
[0m04:41:53.331481 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m01:00:26.987461 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0baf49c450>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0baf49c910>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0baf49ce50>]}


============================== 01:00:26.992285 | 416f5df5-86ff-4854-914c-57d06a547395 ==============================
[0m01:00:26.992285 [info ] [MainThread]: Running with dbt=1.9.0-b3
[0m01:00:26.993361 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'fail_fast': 'False', 'log_path': '/usr/local/airflow/dags/capstone/dbt_capstone_andres/logs', 'debug': 'False', 'profiles_dir': '/usr/local/airflow/dags/capstone/dbt_capstone_andres', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt source freshness', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m01:00:27.956211 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '416f5df5-86ff-4854-914c-57d06a547395', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0b881da610>]}
[0m01:00:28.015232 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '416f5df5-86ff-4854-914c-57d06a547395', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0baf59d4d0>]}
[0m01:00:28.016385 [info ] [MainThread]: Registered adapter: snowflake=1.8.4
[0m01:00:28.054955 [debug] [MainThread]: checksum: 5f4fde221ca8627174a0341efc73099aaec05b8546408ceed6f59b1ec0755115, vars: {}, profile: , target: , version: 1.9.0b3
[0m01:00:28.455126 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m01:00:28.455981 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m01:00:28.461803 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 2 unused configuration paths:
- models.staging
- models.marts
[0m01:00:28.505112 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '416f5df5-86ff-4854-914c-57d06a547395', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0baf57ccd0>]}
[0m01:00:28.672992 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '416f5df5-86ff-4854-914c-57d06a547395', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0b83ff6210>]}
[0m01:00:28.673821 [info ] [MainThread]: Found 5 models, 2 data tests, 2 sources, 862 macros, 1 unit test
[0m01:00:28.674537 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '416f5df5-86ff-4854-914c-57d06a547395', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0b83f70f10>]}
[0m01:00:28.676567 [info ] [MainThread]: 
[0m01:00:28.677316 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m01:00:28.677970 [info ] [MainThread]: 
[0m01:00:28.678831 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m01:00:28.684335 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_DATAEXPERT_STUDENT_andres'
[0m01:00:28.707995 [debug] [ThreadPool]: Using snowflake connection "list_DATAEXPERT_STUDENT_andres"
[0m01:00:28.708872 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "connection_name": "list_DATAEXPERT_STUDENT_andres"} */
show objects in DATAEXPERT_STUDENT.andres limit 10000
[0m01:00:28.709554 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m01:00:29.617579 [debug] [ThreadPool]: SQL status: SUCCESS 28 in 0.908 seconds
[0m01:00:29.622554 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: Close
[0m01:00:29.866631 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '416f5df5-86ff-4854-914c-57d06a547395', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0b83c7bb90>]}
[0m01:00:29.867992 [info ] [MainThread]: Pulling freshness from warehouse metadata tables for 0 sources
[0m01:00:29.875074 [debug] [Thread-1 (]: Began running node source.dbt_capstone_andres.treasury.fiscal_data
[0m01:00:29.876099 [info ] [Thread-1 (]: 1 of 1 START freshness of treasury.fiscal_data ................................. [RUN]
[0m01:00:29.877117 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_DATAEXPERT_STUDENT_andres, now source.dbt_capstone_andres.treasury.fiscal_data)
[0m01:00:29.878003 [debug] [Thread-1 (]: Began compiling node source.dbt_capstone_andres.treasury.fiscal_data
[0m01:00:29.878820 [debug] [Thread-1 (]: Began executing node source.dbt_capstone_andres.treasury.fiscal_data
[0m01:00:29.889860 [debug] [Thread-1 (]: Using snowflake connection "source.dbt_capstone_andres.treasury.fiscal_data"
[0m01:00:29.890782 [debug] [Thread-1 (]: On source.dbt_capstone_andres.treasury.fiscal_data: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "source.dbt_capstone_andres.treasury.fiscal_data"} */
select
      max(record_date::timestamp) as max_loaded_at,
      convert_timezone('UTC', current_timestamp()) as snapshotted_at
    from dataexpert_student.andres.fiscal_data
[0m01:00:29.891676 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m01:00:30.820985 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 0.929 seconds
[0m01:00:30.824254 [debug] [Thread-1 (]: On source.dbt_capstone_andres.treasury.fiscal_data: Close
[0m01:00:31.045667 [info ] [Thread-1 (]: 1 of 1 PASS freshness of treasury.fiscal_data .................................. [[32mPASS[0m in 1.17s]
[0m01:00:31.047280 [debug] [Thread-1 (]: Finished running node source.dbt_capstone_andres.treasury.fiscal_data
[0m01:00:31.049902 [debug] [MainThread]: Connection 'master' was properly closed.
[0m01:00:31.050940 [debug] [MainThread]: Connection 'source.dbt_capstone_andres.treasury.fiscal_data' was properly closed.
[0m01:00:31.051895 [info ] [MainThread]: 
[0m01:00:31.052929 [info ] [MainThread]: Finished running 1 source in 0 hours 0 minutes and 2.37 seconds (2.37s).
[0m01:00:31.142445 [info ] [MainThread]: Done.
[0m01:00:31.143837 [debug] [MainThread]: Resource report: {"command_name": "freshness", "command_success": true, "command_wall_clock_time": 4.2178373, "process_user_time": 3.630441, "process_kernel_time": 0.954506, "process_mem_max_rss": "210696", "process_out_blocks": "3685", "process_in_blocks": "0"}
[0m01:00:31.145088 [debug] [MainThread]: Command `dbt source freshness` succeeded at 01:00:31.144867 after 4.22 seconds
[0m01:00:31.146029 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0baf50fbd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0baf50fc50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0bb31ebfd0>]}
[0m01:00:31.146781 [debug] [MainThread]: Flushing usage events
[0m01:00:31.523483 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m02:23:57.577539 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1e2a398b10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1e2b1d5850>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1e2a398f90>]}


============================== 02:23:57.582150 | df95b786-fe86-4d34-9a96-828f753906ed ==============================
[0m02:23:57.582150 [info ] [MainThread]: Running with dbt=1.9.0-b3
[0m02:23:57.583092 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'fail_fast': 'False', 'log_path': '/usr/local/airflow/dags/capstone/dbt_capstone_andres/logs', 'version_check': 'True', 'profiles_dir': '/usr/local/airflow/dags/capstone/dbt_capstone_andres', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'introspect': 'True', 'invocation_command': 'dbt source freshness', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m02:23:58.641090 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'df95b786-fe86-4d34-9a96-828f753906ed', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1dff0ed750>]}
[0m02:23:58.697946 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'df95b786-fe86-4d34-9a96-828f753906ed', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1e2a4995d0>]}
[0m02:23:58.700284 [info ] [MainThread]: Registered adapter: snowflake=1.8.4
[0m02:23:58.741003 [debug] [MainThread]: checksum: 5f4fde221ca8627174a0341efc73099aaec05b8546408ceed6f59b1ec0755115, vars: {}, profile: , target: , version: 1.9.0b3
[0m02:23:59.137483 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m02:23:59.138190 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m02:23:59.144495 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 2 unused configuration paths:
- models.marts
- models.staging
[0m02:23:59.187626 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'df95b786-fe86-4d34-9a96-828f753906ed', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1e2b130f10>]}
[0m02:23:59.362706 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'df95b786-fe86-4d34-9a96-828f753906ed', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1dfeee5f10>]}
[0m02:23:59.363466 [info ] [MainThread]: Found 5 models, 2 data tests, 2 sources, 862 macros, 1 unit test
[0m02:23:59.364096 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'df95b786-fe86-4d34-9a96-828f753906ed', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1dfee65290>]}
[0m02:23:59.365970 [info ] [MainThread]: 
[0m02:23:59.366732 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m02:23:59.367340 [info ] [MainThread]: 
[0m02:23:59.368115 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m02:23:59.374274 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_DATAEXPERT_STUDENT_andres'
[0m02:23:59.393601 [debug] [ThreadPool]: Using snowflake connection "list_DATAEXPERT_STUDENT_andres"
[0m02:23:59.394403 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "connection_name": "list_DATAEXPERT_STUDENT_andres"} */
show objects in DATAEXPERT_STUDENT.andres limit 10000
[0m02:23:59.395060 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m02:24:00.413445 [debug] [ThreadPool]: SQL status: SUCCESS 28 in 1.018 seconds
[0m02:24:00.418650 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: Close
[0m02:24:00.648241 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'df95b786-fe86-4d34-9a96-828f753906ed', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1e2b7abc90>]}
[0m02:24:00.649643 [info ] [MainThread]: Pulling freshness from warehouse metadata tables for 0 sources
[0m02:24:00.654549 [debug] [Thread-1 (]: Began running node source.dbt_capstone_andres.treasury.fiscal_data
[0m02:24:00.655585 [info ] [Thread-1 (]: 1 of 1 START freshness of treasury.fiscal_data ................................. [RUN]
[0m02:24:00.656947 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_DATAEXPERT_STUDENT_andres, now source.dbt_capstone_andres.treasury.fiscal_data)
[0m02:24:00.657966 [debug] [Thread-1 (]: Began compiling node source.dbt_capstone_andres.treasury.fiscal_data
[0m02:24:00.658999 [debug] [Thread-1 (]: Began executing node source.dbt_capstone_andres.treasury.fiscal_data
[0m02:24:00.671437 [debug] [Thread-1 (]: Using snowflake connection "source.dbt_capstone_andres.treasury.fiscal_data"
[0m02:24:00.672401 [debug] [Thread-1 (]: On source.dbt_capstone_andres.treasury.fiscal_data: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "source.dbt_capstone_andres.treasury.fiscal_data"} */
select
      max(record_date::timestamp) as max_loaded_at,
      convert_timezone('UTC', current_timestamp()) as snapshotted_at
    from dataexpert_student.andres.fiscal_data
[0m02:24:00.673278 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m02:24:02.930087 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 2.256 seconds
[0m02:24:02.947533 [debug] [Thread-1 (]: On source.dbt_capstone_andres.treasury.fiscal_data: Close
[0m02:24:03.225833 [info ] [Thread-1 (]: 1 of 1 PASS freshness of treasury.fiscal_data .................................. [[32mPASS[0m in 2.57s]
[0m02:24:03.231688 [debug] [Thread-1 (]: Finished running node source.dbt_capstone_andres.treasury.fiscal_data
[0m02:24:03.243072 [debug] [MainThread]: Connection 'master' was properly closed.
[0m02:24:03.246883 [debug] [MainThread]: Connection 'source.dbt_capstone_andres.treasury.fiscal_data' was properly closed.
[0m02:24:03.250654 [info ] [MainThread]: 
[0m02:24:03.254722 [info ] [MainThread]: Finished running 1 source in 0 hours 0 minutes and 3.88 seconds (3.88s).
[0m02:37:59.470511 [info ] [MainThread]: Done.
[0m02:37:59.559503 [debug] [MainThread]: Resource report: {"command_name": "freshness", "command_success": true, "command_wall_clock_time": 842.10223, "process_user_time": 4.268319, "process_kernel_time": 1.620853, "process_mem_max_rss": "210440", "process_out_blocks": "3685", "process_in_blocks": "0"}
[0m02:37:59.585165 [debug] [MainThread]: Command `dbt source freshness` succeeded at 02:37:59.584672 after 842.13 seconds
[0m02:37:59.587823 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1e2a40bad0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1e2a40b8d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1e2e13ffd0>]}
[0m02:37:59.590040 [debug] [MainThread]: Flushing usage events
[0m02:38:00.647940 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m01:06:34.250879 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f273bed9c50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f273c0e5e10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f273c0e5390>]}


============================== 01:06:34.267599 | f6ae129e-1621-4487-96a0-93cd633d27aa ==============================
[0m01:06:34.267599 [info ] [MainThread]: Running with dbt=1.9.0-b3
[0m01:06:34.273277 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/usr/local/airflow/dags/capstone/dbt_capstone_andres', 'version_check': 'True', 'fail_fast': 'False', 'log_path': '/usr/local/airflow/dags/capstone/dbt_capstone_andres/logs', 'debug': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt source freshness', 'static_parser': 'True', 'log_format': 'default', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m01:06:41.140517 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'f6ae129e-1621-4487-96a0-93cd633d27aa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2714c48210>]}
[0m01:06:41.548612 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'f6ae129e-1621-4487-96a0-93cd633d27aa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f273bfd93d0>]}
[0m01:06:41.554081 [info ] [MainThread]: Registered adapter: snowflake=1.8.4
[0m01:06:41.879620 [debug] [MainThread]: checksum: 5f4fde221ca8627174a0341efc73099aaec05b8546408ceed6f59b1ec0755115, vars: {}, profile: , target: , version: 1.9.0b3
[0m01:06:45.364749 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m01:06:45.371428 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m01:06:45.426289 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 2 unused configuration paths:
- models.marts
- models.staging
[0m01:06:46.353488 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'f6ae129e-1621-4487-96a0-93cd633d27aa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2714694250>]}
[0m01:06:47.856900 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'f6ae129e-1621-4487-96a0-93cd633d27aa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f27149e22d0>]}
[0m01:06:47.863342 [info ] [MainThread]: Found 5 models, 2 data tests, 2 sources, 862 macros, 1 unit test
[0m01:06:47.868756 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f6ae129e-1621-4487-96a0-93cd633d27aa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2714960f10>]}
[0m01:06:47.885786 [info ] [MainThread]: 
[0m01:06:47.890168 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m01:06:47.894024 [info ] [MainThread]: 
[0m01:06:47.899477 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m01:06:47.961072 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_DATAEXPERT_STUDENT_andres'
[0m01:06:48.121785 [debug] [ThreadPool]: Using snowflake connection "list_DATAEXPERT_STUDENT_andres"
[0m01:06:48.126006 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "connection_name": "list_DATAEXPERT_STUDENT_andres"} */
show objects in DATAEXPERT_STUDENT.andres limit 10000
[0m01:06:48.129606 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m01:06:50.595079 [debug] [ThreadPool]: SQL status: SUCCESS 28 in 2.465 seconds
[0m01:06:50.626403 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: Close
[0m01:06:50.933306 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f6ae129e-1621-4487-96a0-93cd633d27aa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2714654410>]}
[0m01:06:50.937656 [info ] [MainThread]: Pulling freshness from warehouse metadata tables for 0 sources
[0m01:06:50.955875 [debug] [Thread-1 (]: Began running node source.dbt_capstone_andres.treasury.fiscal_data
[0m01:06:50.960872 [info ] [Thread-1 (]: 1 of 1 START freshness of treasury.fiscal_data ................................. [RUN]
[0m01:06:50.967772 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_DATAEXPERT_STUDENT_andres, now source.dbt_capstone_andres.treasury.fiscal_data)
[0m01:06:50.973348 [debug] [Thread-1 (]: Began compiling node source.dbt_capstone_andres.treasury.fiscal_data
[0m01:06:50.977882 [debug] [Thread-1 (]: Began executing node source.dbt_capstone_andres.treasury.fiscal_data
[0m01:06:51.056177 [debug] [Thread-1 (]: Using snowflake connection "source.dbt_capstone_andres.treasury.fiscal_data"
[0m01:06:51.061446 [debug] [Thread-1 (]: On source.dbt_capstone_andres.treasury.fiscal_data: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "source.dbt_capstone_andres.treasury.fiscal_data"} */
select
      max(record_date::timestamp) as max_loaded_at,
      convert_timezone('UTC', current_timestamp()) as snapshotted_at
    from dataexpert_student.andres.fiscal_data
[0m01:06:51.066064 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m01:06:53.767072 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 2.700 seconds
[0m01:06:53.783356 [debug] [Thread-1 (]: On source.dbt_capstone_andres.treasury.fiscal_data: Close
[0m01:06:54.081983 [info ] [Thread-1 (]: 1 of 1 PASS freshness of treasury.fiscal_data .................................. [[32mPASS[0m in 3.11s]
[0m01:06:54.090816 [debug] [Thread-1 (]: Finished running node source.dbt_capstone_andres.treasury.fiscal_data
[0m01:06:54.107836 [debug] [MainThread]: Connection 'master' was properly closed.
[0m01:06:54.113221 [debug] [MainThread]: Connection 'source.dbt_capstone_andres.treasury.fiscal_data' was properly closed.
[0m01:06:54.117387 [info ] [MainThread]: 
[0m01:06:54.121662 [info ] [MainThread]: Finished running 1 source in 0 hours 0 minutes and 6.22 seconds (6.22s).
[0m01:06:55.000186 [info ] [MainThread]: Done.
[0m01:06:55.007986 [debug] [MainThread]: Resource report: {"command_name": "freshness", "command_success": true, "command_wall_clock_time": 21.081, "process_user_time": 21.500626, "process_kernel_time": 4.372424, "process_mem_max_rss": "210104", "process_out_blocks": "3685", "process_in_blocks": "0"}
[0m01:06:55.016604 [debug] [MainThread]: Command `dbt source freshness` succeeded at 01:06:55.014973 after 21.09 seconds
[0m01:06:55.022908 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f273bd4fcd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f273bd4fb90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f273fbebfd0>]}
[0m01:06:55.029942 [debug] [MainThread]: Flushing usage events
[0m01:06:55.473052 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m01:08:53.490805 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd203a1df50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd205ea40d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd203c29750>]}


============================== 01:08:53.493715 | b849a572-bf76-4c60-a6a5-f6daee8f2f48 ==============================
[0m01:08:53.493715 [info ] [MainThread]: Running with dbt=1.9.0-b3
[0m01:08:53.494668 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'fail_fast': 'False', 'log_path': '/usr/local/airflow/dags/capstone/dbt_capstone_andres/logs', 'version_check': 'True', 'profiles_dir': '/usr/local/airflow/dags/capstone/dbt_capstone_andres', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'introspect': 'True', 'invocation_command': 'dbt source freshness', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m01:08:54.448960 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'b849a572-bf76-4c60-a6a5-f6daee8f2f48', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd1dc6083d0>]}
[0m01:08:54.510100 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'b849a572-bf76-4c60-a6a5-f6daee8f2f48', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd204e0f510>]}
[0m01:08:54.511361 [info ] [MainThread]: Registered adapter: snowflake=1.8.4
[0m01:08:54.552648 [debug] [MainThread]: checksum: 5f4fde221ca8627174a0341efc73099aaec05b8546408ceed6f59b1ec0755115, vars: {}, profile: , target: , version: 1.9.0b3
[0m01:08:54.960391 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m01:08:54.961260 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m01:08:54.967559 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 2 unused configuration paths:
- models.marts
- models.staging
[0m01:08:55.013501 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'b849a572-bf76-4c60-a6a5-f6daee8f2f48', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd203a15650>]}
[0m01:08:55.196573 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'b849a572-bf76-4c60-a6a5-f6daee8f2f48', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd1dc29e6d0>]}
[0m01:08:55.198441 [info ] [MainThread]: Found 5 models, 2 data tests, 2 sources, 862 macros, 1 unit test
[0m01:08:55.199721 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b849a572-bf76-4c60-a6a5-f6daee8f2f48', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd2047ad390>]}
[0m01:08:55.203901 [info ] [MainThread]: 
[0m01:08:55.205764 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m01:08:55.207032 [info ] [MainThread]: 
[0m01:08:55.208540 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m01:08:55.219845 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_DATAEXPERT_STUDENT_andres'
[0m01:08:55.263788 [debug] [ThreadPool]: Using snowflake connection "list_DATAEXPERT_STUDENT_andres"
[0m01:08:55.265383 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "connection_name": "list_DATAEXPERT_STUDENT_andres"} */
show objects in DATAEXPERT_STUDENT.andres limit 10000
[0m01:08:55.266658 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m01:08:56.190000 [debug] [ThreadPool]: SQL status: SUCCESS 28 in 0.923 seconds
[0m01:08:56.194319 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: Close
[0m01:08:56.437591 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b849a572-bf76-4c60-a6a5-f6daee8f2f48', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd1dc1d7210>]}
[0m01:08:56.439000 [info ] [MainThread]: Pulling freshness from warehouse metadata tables for 0 sources
[0m01:08:56.446947 [debug] [Thread-1 (]: Began running node source.dbt_capstone_andres.treasury.fiscal_data
[0m01:08:56.448081 [info ] [Thread-1 (]: 1 of 1 START freshness of treasury.fiscal_data ................................. [RUN]
[0m01:08:56.449311 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_DATAEXPERT_STUDENT_andres, now source.dbt_capstone_andres.treasury.fiscal_data)
[0m01:08:56.450201 [debug] [Thread-1 (]: Began compiling node source.dbt_capstone_andres.treasury.fiscal_data
[0m01:08:56.451189 [debug] [Thread-1 (]: Began executing node source.dbt_capstone_andres.treasury.fiscal_data
[0m01:08:56.464123 [debug] [Thread-1 (]: Using snowflake connection "source.dbt_capstone_andres.treasury.fiscal_data"
[0m01:08:56.465600 [debug] [Thread-1 (]: On source.dbt_capstone_andres.treasury.fiscal_data: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "source.dbt_capstone_andres.treasury.fiscal_data"} */
select
      max(record_date::timestamp) as max_loaded_at,
      convert_timezone('UTC', current_timestamp()) as snapshotted_at
    from dataexpert_student.andres.fiscal_data
[0m01:08:56.466733 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m01:08:57.223571 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 0.757 seconds
[0m01:08:57.226048 [debug] [Thread-1 (]: On source.dbt_capstone_andres.treasury.fiscal_data: Close
[0m01:08:57.461003 [info ] [Thread-1 (]: 1 of 1 PASS freshness of treasury.fiscal_data .................................. [[32mPASS[0m in 1.01s]
[0m01:08:57.462475 [debug] [Thread-1 (]: Finished running node source.dbt_capstone_andres.treasury.fiscal_data
[0m01:08:57.465202 [debug] [MainThread]: Connection 'master' was properly closed.
[0m01:08:57.465878 [debug] [MainThread]: Connection 'source.dbt_capstone_andres.treasury.fiscal_data' was properly closed.
[0m01:08:57.466827 [info ] [MainThread]: 
[0m01:08:57.467686 [info ] [MainThread]: Finished running 1 source in 0 hours 0 minutes and 2.26 seconds (2.26s).
[0m01:08:57.557053 [info ] [MainThread]: Done.
[0m01:08:57.558772 [debug] [MainThread]: Resource report: {"command_name": "freshness", "command_success": true, "command_wall_clock_time": 4.1198587, "process_user_time": 3.69927, "process_kernel_time": 1.208559, "process_mem_max_rss": "210524", "process_out_blocks": "3685", "process_in_blocks": "0"}
[0m01:08:57.559903 [debug] [MainThread]: Command `dbt source freshness` succeeded at 01:08:57.559777 after 4.12 seconds
[0m01:08:57.560794 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd20388fa10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd207839410>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd2077affd0>]}
[0m01:08:57.561834 [debug] [MainThread]: Flushing usage events
[0m01:08:57.898819 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m01:22:17.817562 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb1e0454610>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb1e1287210>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb1e0661290>]}


============================== 01:22:17.824011 | 9c847eaa-a174-4090-a87f-a8bf117dfa9a ==============================
[0m01:22:17.824011 [info ] [MainThread]: Running with dbt=1.9.0-b3
[0m01:22:17.844424 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'debug': 'False', 'log_path': '/usr/local/airflow/dags/capstone/dbt_capstone_andres/logs', 'fail_fast': 'False', 'profiles_dir': '/usr/local/airflow/dags/capstone/dbt_capstone_andres', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt source freshness', 'static_parser': 'True', 'introspect': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m01:22:18.825750 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '9c847eaa-a174-4090-a87f-a8bf117dfa9a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb1b9192150>]}
[0m01:22:18.886657 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '9c847eaa-a174-4090-a87f-a8bf117dfa9a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb1e05556d0>]}
[0m01:22:18.887923 [info ] [MainThread]: Registered adapter: snowflake=1.8.4
[0m01:22:18.930382 [debug] [MainThread]: checksum: 5f4fde221ca8627174a0341efc73099aaec05b8546408ceed6f59b1ec0755115, vars: {}, profile: , target: , version: 1.9.0b3
[0m01:22:19.340705 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m01:22:19.341467 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m01:22:19.347175 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 2 unused configuration paths:
- models.marts
- models.staging
[0m01:22:19.390363 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '9c847eaa-a174-4090-a87f-a8bf117dfa9a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb1b8db4410>]}
[0m01:22:19.579571 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '9c847eaa-a174-4090-a87f-a8bf117dfa9a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb1b8ea9d90>]}
[0m01:22:19.580542 [info ] [MainThread]: Found 5 models, 2 data tests, 2 sources, 862 macros, 1 unit test
[0m01:22:19.581273 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9c847eaa-a174-4090-a87f-a8bf117dfa9a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb1b8e11110>]}
[0m01:22:19.583333 [info ] [MainThread]: 
[0m01:22:19.584038 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m01:22:19.584662 [info ] [MainThread]: 
[0m01:22:19.585586 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m01:22:19.591150 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_DATAEXPERT_STUDENT_andres'
[0m01:22:19.614315 [debug] [ThreadPool]: Using snowflake connection "list_DATAEXPERT_STUDENT_andres"
[0m01:22:19.615230 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "connection_name": "list_DATAEXPERT_STUDENT_andres"} */
show objects in DATAEXPERT_STUDENT.andres limit 10000
[0m01:22:19.616013 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m01:22:20.632723 [debug] [ThreadPool]: SQL status: SUCCESS 28 in 1.017 seconds
[0m01:22:20.637713 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: Close
[0m01:22:20.861230 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9c847eaa-a174-4090-a87f-a8bf117dfa9a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb1e05353d0>]}
[0m01:22:20.862648 [info ] [MainThread]: Pulling freshness from warehouse metadata tables for 0 sources
[0m01:22:20.868156 [debug] [Thread-1 (]: Began running node source.dbt_capstone_andres.treasury.fiscal_data
[0m01:22:20.869232 [info ] [Thread-1 (]: 1 of 1 START freshness of treasury.fiscal_data ................................. [RUN]
[0m01:22:20.870397 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_DATAEXPERT_STUDENT_andres, now source.dbt_capstone_andres.treasury.fiscal_data)
[0m01:22:20.871557 [debug] [Thread-1 (]: Began compiling node source.dbt_capstone_andres.treasury.fiscal_data
[0m01:22:20.872587 [debug] [Thread-1 (]: Began executing node source.dbt_capstone_andres.treasury.fiscal_data
[0m01:22:20.883705 [debug] [Thread-1 (]: Using snowflake connection "source.dbt_capstone_andres.treasury.fiscal_data"
[0m01:22:20.884623 [debug] [Thread-1 (]: On source.dbt_capstone_andres.treasury.fiscal_data: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "source.dbt_capstone_andres.treasury.fiscal_data"} */
select
      max(record_date::timestamp) as max_loaded_at,
      convert_timezone('UTC', current_timestamp()) as snapshotted_at
    from dataexpert_student.andres.fiscal_data
[0m01:22:20.885724 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m01:22:21.741899 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 0.856 seconds
[0m01:22:21.744226 [debug] [Thread-1 (]: On source.dbt_capstone_andres.treasury.fiscal_data: Close
[0m01:22:21.979283 [info ] [Thread-1 (]: 1 of 1 PASS freshness of treasury.fiscal_data .................................. [[32mPASS[0m in 1.11s]
[0m01:22:21.981716 [debug] [Thread-1 (]: Finished running node source.dbt_capstone_andres.treasury.fiscal_data
[0m01:22:21.984963 [debug] [MainThread]: Connection 'master' was properly closed.
[0m01:22:21.985843 [debug] [MainThread]: Connection 'source.dbt_capstone_andres.treasury.fiscal_data' was properly closed.
[0m01:22:21.987100 [info ] [MainThread]: 
[0m01:22:21.988073 [info ] [MainThread]: Finished running 1 source in 0 hours 0 minutes and 2.40 seconds (2.40s).
[0m01:22:22.075797 [info ] [MainThread]: Done.
[0m01:22:22.077854 [debug] [MainThread]: Resource report: {"command_name": "freshness", "command_success": true, "command_wall_clock_time": 4.328578, "process_user_time": 3.908976, "process_kernel_time": 1.047809, "process_mem_max_rss": "210400", "process_out_blocks": "3685", "process_in_blocks": "0"}
[0m01:22:22.079191 [debug] [MainThread]: Command `dbt source freshness` succeeded at 01:22:22.079067 after 4.33 seconds
[0m01:22:22.080069 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb1e04c1450>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb1e049bcd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb1e41ebfd0>]}
[0m01:22:22.080907 [debug] [MainThread]: Flushing usage events
[0m01:22:22.401937 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m01:05:53.498875 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1cce160550>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1cce1607d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1cce160d10>]}


============================== 01:05:53.515610 | 1c51130e-1ac8-4611-b42d-51e16284b351 ==============================
[0m01:05:53.515610 [info ] [MainThread]: Running with dbt=1.9.0-b3
[0m01:05:53.521251 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/usr/local/airflow/dags/capstone/dbt_capstone_andres', 'fail_fast': 'False', 'version_check': 'True', 'log_path': '/usr/local/airflow/dags/capstone/dbt_capstone_andres/logs', 'warn_error': 'None', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'invocation_command': 'dbt source freshness', 'log_format': 'default', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m01:05:59.960781 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '1c51130e-1ac8-4611-b42d-51e16284b351', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1ca2d04390>]}
[0m01:06:00.384134 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '1c51130e-1ac8-4611-b42d-51e16284b351', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1ccf573450>]}
[0m01:06:00.390747 [info ] [MainThread]: Registered adapter: snowflake=1.8.4
[0m01:06:00.663980 [debug] [MainThread]: checksum: 5f4fde221ca8627174a0341efc73099aaec05b8546408ceed6f59b1ec0755115, vars: {}, profile: , target: , version: 1.9.0b3
[0m01:06:03.275543 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m01:06:03.279738 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m01:06:03.318219 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 2 unused configuration paths:
- models.staging
- models.marts
[0m01:06:03.624746 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '1c51130e-1ac8-4611-b42d-51e16284b351', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1ca29151d0>]}
[0m01:06:05.112155 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '1c51130e-1ac8-4611-b42d-51e16284b351', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1ca2bf6390>]}
[0m01:06:05.118801 [info ] [MainThread]: Found 5 models, 2 data tests, 2 sources, 862 macros, 1 unit test
[0m01:06:05.125794 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '1c51130e-1ac8-4611-b42d-51e16284b351', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1ca2b70c50>]}
[0m01:06:05.142381 [info ] [MainThread]: 
[0m01:06:05.146668 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m01:06:05.151300 [info ] [MainThread]: 
[0m01:06:05.156598 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m01:06:05.193704 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_DATAEXPERT_STUDENT_andres'
[0m01:06:05.340220 [debug] [ThreadPool]: Using snowflake connection "list_DATAEXPERT_STUDENT_andres"
[0m01:06:05.345320 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "connection_name": "list_DATAEXPERT_STUDENT_andres"} */
show objects in DATAEXPERT_STUDENT.andres limit 10000
[0m01:06:05.349884 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m01:06:07.563484 [debug] [ThreadPool]: SQL status: SUCCESS 28 in 2.214 seconds
[0m01:06:07.586047 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: Close
[0m01:06:07.892336 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '1c51130e-1ac8-4611-b42d-51e16284b351', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1ca284fc90>]}
[0m01:06:07.899512 [info ] [MainThread]: Pulling freshness from warehouse metadata tables for 0 sources
[0m01:06:07.926442 [debug] [Thread-1 (]: Began running node source.dbt_capstone_andres.treasury.fiscal_data
[0m01:06:07.932628 [info ] [Thread-1 (]: 1 of 1 START freshness of treasury.fiscal_data ................................. [RUN]
[0m01:06:07.938409 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_DATAEXPERT_STUDENT_andres, now source.dbt_capstone_andres.treasury.fiscal_data)
[0m01:06:07.943962 [debug] [Thread-1 (]: Began compiling node source.dbt_capstone_andres.treasury.fiscal_data
[0m01:06:07.949568 [debug] [Thread-1 (]: Began executing node source.dbt_capstone_andres.treasury.fiscal_data
[0m01:06:08.020517 [debug] [Thread-1 (]: Using snowflake connection "source.dbt_capstone_andres.treasury.fiscal_data"
[0m01:06:08.027526 [debug] [Thread-1 (]: On source.dbt_capstone_andres.treasury.fiscal_data: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "source.dbt_capstone_andres.treasury.fiscal_data"} */
select
      max(record_date::timestamp) as max_loaded_at,
      convert_timezone('UTC', current_timestamp()) as snapshotted_at
    from dataexpert_student.andres.fiscal_data
[0m01:06:08.034327 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m01:06:09.056546 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 1.022 seconds
[0m01:06:09.075093 [debug] [Thread-1 (]: On source.dbt_capstone_andres.treasury.fiscal_data: Close
[0m01:06:09.344252 [info ] [Thread-1 (]: 1 of 1 PASS freshness of treasury.fiscal_data .................................. [[32mPASS[0m in 1.41s]
[0m01:06:09.351802 [debug] [Thread-1 (]: Finished running node source.dbt_capstone_andres.treasury.fiscal_data
[0m01:06:09.364508 [debug] [MainThread]: Connection 'master' was properly closed.
[0m01:06:09.368825 [debug] [MainThread]: Connection 'source.dbt_capstone_andres.treasury.fiscal_data' was properly closed.
[0m01:06:09.372573 [info ] [MainThread]: 
[0m01:06:09.377331 [info ] [MainThread]: Finished running 1 source in 0 hours 0 minutes and 4.22 seconds (4.22s).
[0m01:06:09.882075 [info ] [MainThread]: Done.
[0m01:06:09.891021 [debug] [MainThread]: Resource report: {"command_name": "freshness", "command_success": true, "command_wall_clock_time": 16.761808, "process_user_time": 21.822657, "process_kernel_time": 4.513631, "process_mem_max_rss": "210540", "process_out_blocks": "3685", "process_in_blocks": "0"}
[0m01:06:09.897418 [debug] [MainThread]: Command `dbt source freshness` succeeded at 01:06:09.896655 after 16.77 seconds
[0m01:06:09.901557 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1cce1cfcd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1cce1cf8d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1cd1f0ff50>]}
[0m01:06:09.905521 [debug] [MainThread]: Flushing usage events
[0m01:06:10.309804 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m04:17:22.029483 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f33a5da0710>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f33a5fad790>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f33a5fad610>]}


============================== 04:17:22.038024 | ab0a7377-a84b-4646-9f72-0e00d76d7c81 ==============================
[0m04:17:22.038024 [info ] [MainThread]: Running with dbt=1.9.0-b3
[0m04:17:22.039043 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/usr/local/airflow/dags/capstone/dbt_capstone_andres', 'fail_fast': 'False', 'version_check': 'True', 'log_path': '/usr/local/airflow/dags/capstone/dbt_capstone_andres/logs', 'debug': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt source freshness', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m04:17:23.729735 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'ab0a7377-a84b-4646-9f72-0e00d76d7c81', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f337a92b850>]}
[0m04:17:23.793270 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'ab0a7377-a84b-4646-9f72-0e00d76d7c81', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f33a5ea14d0>]}
[0m04:17:23.795337 [info ] [MainThread]: Registered adapter: snowflake=1.8.4
[0m04:17:23.846307 [debug] [MainThread]: checksum: 5f4fde221ca8627174a0341efc73099aaec05b8546408ceed6f59b1ec0755115, vars: {}, profile: , target: , version: 1.9.0b3
[0m04:17:24.317480 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m04:17:24.318143 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m04:17:24.323625 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 2 unused configuration paths:
- models.staging
- models.marts
[0m04:17:24.365908 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'ab0a7377-a84b-4646-9f72-0e00d76d7c81', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f337aa3ca90>]}
[0m04:17:24.552624 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'ab0a7377-a84b-4646-9f72-0e00d76d7c81', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f337a7ca490>]}
[0m04:17:24.553532 [info ] [MainThread]: Found 5 models, 2 data tests, 2 sources, 862 macros, 1 unit test
[0m04:17:24.554257 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ab0a7377-a84b-4646-9f72-0e00d76d7c81', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f337a74d110>]}
[0m04:17:24.556292 [info ] [MainThread]: 
[0m04:17:24.556997 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m04:17:24.557631 [info ] [MainThread]: 
[0m04:17:24.558699 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m04:17:24.566092 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_DATAEXPERT_STUDENT_andres'
[0m04:17:24.586898 [debug] [ThreadPool]: Using snowflake connection "list_DATAEXPERT_STUDENT_andres"
[0m04:17:24.587616 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "connection_name": "list_DATAEXPERT_STUDENT_andres"} */
show objects in DATAEXPERT_STUDENT.andres limit 10000
[0m04:17:24.588281 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m04:17:25.608470 [debug] [ThreadPool]: SQL status: SUCCESS 28 in 1.020 seconds
[0m04:17:25.613693 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: Close
[0m04:17:25.850214 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ab0a7377-a84b-4646-9f72-0e00d76d7c81', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f337a534890>]}
[0m04:17:25.851462 [info ] [MainThread]: Pulling freshness from warehouse metadata tables for 0 sources
[0m04:17:25.860468 [debug] [Thread-1 (]: Began running node source.dbt_capstone_andres.treasury.fiscal_data
[0m04:17:25.861787 [info ] [Thread-1 (]: 1 of 1 START freshness of treasury.fiscal_data ................................. [RUN]
[0m04:17:25.862936 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_DATAEXPERT_STUDENT_andres, now source.dbt_capstone_andres.treasury.fiscal_data)
[0m04:17:25.863880 [debug] [Thread-1 (]: Began compiling node source.dbt_capstone_andres.treasury.fiscal_data
[0m04:17:25.864875 [debug] [Thread-1 (]: Began executing node source.dbt_capstone_andres.treasury.fiscal_data
[0m04:17:25.877616 [debug] [Thread-1 (]: Using snowflake connection "source.dbt_capstone_andres.treasury.fiscal_data"
[0m04:17:25.878666 [debug] [Thread-1 (]: On source.dbt_capstone_andres.treasury.fiscal_data: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "source.dbt_capstone_andres.treasury.fiscal_data"} */
select
      max(record_date::timestamp) as max_loaded_at,
      convert_timezone('UTC', current_timestamp()) as snapshotted_at
    from dataexpert_student.andres.fiscal_data
[0m04:17:25.879669 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m04:45:20.894403 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 7.710 seconds
[0m04:45:21.181817 [debug] [Thread-1 (]: On source.dbt_capstone_andres.treasury.fiscal_data: Close
[0m04:45:22.628702 [info ] [Thread-1 (]: 1 of 1 PASS freshness of treasury.fiscal_data .................................. [[32mPASS[0m in 1676.76s]
[0m04:45:22.632252 [debug] [Thread-1 (]: Finished running node source.dbt_capstone_andres.treasury.fiscal_data
[0m04:45:22.646764 [debug] [MainThread]: Connection 'master' was properly closed.
[0m04:45:22.647957 [debug] [MainThread]: Connection 'source.dbt_capstone_andres.treasury.fiscal_data' was properly closed.
[0m04:45:22.649538 [info ] [MainThread]: 
[0m04:45:22.651797 [info ] [MainThread]: Finished running 1 source in 0 hours 27 minutes and 58.09 seconds (1678.09s).
[0m04:45:22.864411 [info ] [MainThread]: Done.
[0m04:45:22.883756 [debug] [MainThread]: Resource report: {"command_name": "freshness", "command_success": true, "command_wall_clock_time": 13.645922, "process_user_time": 4.561407, "process_kernel_time": 3.0574, "process_mem_max_rss": "211120", "process_out_blocks": "3685", "process_in_blocks": "0"}
[0m04:45:22.888818 [debug] [MainThread]: Command `dbt source freshness` succeeded at 04:45:22.888439 after 13.65 seconds
[0m04:45:22.896944 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f33a9be9550>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f33a9b5ffd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f33a9b68b90>]}
[0m04:45:22.901392 [debug] [MainThread]: Flushing usage events
[0m04:45:23.286536 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m01:06:49.447992 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f229d9e4810>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f229daa7490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f229da2bbd0>]}


============================== 01:06:49.476929 | f3136f9e-b0ab-47d1-8084-e03c4b4690d3 ==============================
[0m01:06:49.476929 [info ] [MainThread]: Running with dbt=1.9.0-b3
[0m01:06:49.482290 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/usr/local/airflow/dags/capstone/dbt_capstone_andres', 'fail_fast': 'False', 'version_check': 'True', 'log_path': '/usr/local/airflow/dags/capstone/dbt_capstone_andres/logs', 'warn_error': 'None', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'log_format': 'default', 'introspect': 'True', 'invocation_command': 'dbt source freshness', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m01:06:55.190300 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'f3136f9e-b0ab-47d1-8084-e03c4b4690d3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2276574c90>]}
[0m01:06:55.531172 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'f3136f9e-b0ab-47d1-8084-e03c4b4690d3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f229dae5550>]}
[0m01:06:55.537022 [info ] [MainThread]: Registered adapter: snowflake=1.8.4
[0m01:06:55.771979 [debug] [MainThread]: checksum: 5f4fde221ca8627174a0341efc73099aaec05b8546408ceed6f59b1ec0755115, vars: {}, profile: , target: , version: 1.9.0b3
[0m01:06:58.145893 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m01:06:58.150036 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m01:06:58.184128 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 2 unused configuration paths:
- models.marts
- models.staging
[0m01:06:58.445994 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'f3136f9e-b0ab-47d1-8084-e03c4b4690d3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f229db0cfd0>]}
[0m01:06:59.498971 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'f3136f9e-b0ab-47d1-8084-e03c4b4690d3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f227621a2d0>]}
[0m01:06:59.508642 [info ] [MainThread]: Found 5 models, 2 data tests, 2 sources, 862 macros, 1 unit test
[0m01:06:59.516935 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f3136f9e-b0ab-47d1-8084-e03c4b4690d3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f229e775410>]}
[0m01:06:59.535737 [info ] [MainThread]: 
[0m01:06:59.542359 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m01:06:59.550996 [info ] [MainThread]: 
[0m01:06:59.557722 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m01:06:59.593474 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_DATAEXPERT_STUDENT_andres'
[0m01:06:59.731103 [debug] [ThreadPool]: Using snowflake connection "list_DATAEXPERT_STUDENT_andres"
[0m01:06:59.735685 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "connection_name": "list_DATAEXPERT_STUDENT_andres"} */
show objects in DATAEXPERT_STUDENT.andres limit 10000
[0m01:06:59.739723 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m01:07:01.418283 [debug] [ThreadPool]: SQL status: SUCCESS 28 in 1.679 seconds
[0m01:07:01.437020 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: Close
[0m01:07:01.739902 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f3136f9e-b0ab-47d1-8084-e03c4b4690d3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2276257290>]}
[0m01:07:01.744903 [info ] [MainThread]: Pulling freshness from warehouse metadata tables for 0 sources
[0m01:07:01.775103 [debug] [Thread-1 (]: Began running node source.dbt_capstone_andres.treasury.fiscal_data
[0m01:07:01.781613 [info ] [Thread-1 (]: 1 of 1 START freshness of treasury.fiscal_data ................................. [RUN]
[0m01:07:01.789100 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_DATAEXPERT_STUDENT_andres, now source.dbt_capstone_andres.treasury.fiscal_data)
[0m01:07:01.794799 [debug] [Thread-1 (]: Began compiling node source.dbt_capstone_andres.treasury.fiscal_data
[0m01:07:01.799933 [debug] [Thread-1 (]: Began executing node source.dbt_capstone_andres.treasury.fiscal_data
[0m01:07:01.872435 [debug] [Thread-1 (]: Using snowflake connection "source.dbt_capstone_andres.treasury.fiscal_data"
[0m01:07:01.880152 [debug] [Thread-1 (]: On source.dbt_capstone_andres.treasury.fiscal_data: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "source.dbt_capstone_andres.treasury.fiscal_data"} */
select
      max(record_date::timestamp) as max_loaded_at,
      convert_timezone('UTC', current_timestamp()) as snapshotted_at
    from dataexpert_student.andres.fiscal_data
[0m01:07:01.894663 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m01:07:03.180898 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 1.288 seconds
[0m01:07:03.191713 [debug] [Thread-1 (]: On source.dbt_capstone_andres.treasury.fiscal_data: Close
[0m01:07:03.440727 [info ] [Thread-1 (]: 1 of 1 PASS freshness of treasury.fiscal_data .................................. [[32mPASS[0m in 1.65s]
[0m01:07:03.447064 [debug] [Thread-1 (]: Finished running node source.dbt_capstone_andres.treasury.fiscal_data
[0m01:07:03.458713 [debug] [MainThread]: Connection 'master' was properly closed.
[0m01:07:03.462447 [debug] [MainThread]: Connection 'source.dbt_capstone_andres.treasury.fiscal_data' was properly closed.
[0m01:07:03.466005 [info ] [MainThread]: 
[0m01:07:03.470105 [info ] [MainThread]: Finished running 1 source in 0 hours 0 minutes and 3.91 seconds (3.91s).
[0m01:07:04.062758 [info ] [MainThread]: Done.
[0m01:07:04.099152 [debug] [MainThread]: Resource report: {"command_name": "freshness", "command_success": true, "command_wall_clock_time": 14.952723, "process_user_time": 19.875164, "process_kernel_time": 4.889584, "process_mem_max_rss": "210136", "process_out_blocks": "3685", "process_in_blocks": "0"}
[0m01:07:04.111944 [debug] [MainThread]: Command `dbt source freshness` succeeded at 01:07:04.109985 after 14.97 seconds
[0m01:07:04.119213 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f22a1815590>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f22a178bf50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f22a1794bd0>]}
[0m01:07:04.123717 [debug] [MainThread]: Flushing usage events
[0m01:07:04.506380 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m01:23:00.926827 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f590e914a90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f590e915410>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f590e916010>]}


============================== 01:23:00.930187 | f9637cd8-3636-4158-9363-810f040794c9 ==============================
[0m01:23:00.930187 [info ] [MainThread]: Running with dbt=1.9.0-b3
[0m01:23:00.931369 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/usr/local/airflow/dags/capstone/dbt_capstone_andres', 'fail_fast': 'False', 'version_check': 'True', 'log_path': '/usr/local/airflow/dags/capstone/dbt_capstone_andres/logs', 'debug': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'log_format': 'default', 'invocation_command': 'dbt source freshness', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m01:23:02.000985 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'f9637cd8-3636-4158-9363-810f040794c9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f58e82d7d90>]}
[0m01:23:02.073114 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'f9637cd8-3636-4158-9363-810f040794c9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f58e3698550>]}
[0m01:23:02.074578 [info ] [MainThread]: Registered adapter: snowflake=1.8.4
[0m01:23:02.124280 [debug] [MainThread]: checksum: 5f4fde221ca8627174a0341efc73099aaec05b8546408ceed6f59b1ec0755115, vars: {}, profile: , target: , version: 1.9.0b3
[0m01:23:02.576115 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m01:23:02.576860 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m01:23:02.582583 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 2 unused configuration paths:
- models.staging
- models.marts
[0m01:23:02.627228 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'f9637cd8-3636-4158-9363-810f040794c9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f590e937ed0>]}
[0m01:23:02.834631 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'f9637cd8-3636-4158-9363-810f040794c9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f58e34761d0>]}
[0m01:23:02.835567 [info ] [MainThread]: Found 5 models, 2 data tests, 2 sources, 862 macros, 1 unit test
[0m01:23:02.836317 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f9637cd8-3636-4158-9363-810f040794c9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f58e33f0f10>]}
[0m01:23:02.838707 [info ] [MainThread]: 
[0m01:23:02.839525 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m01:23:02.840245 [info ] [MainThread]: 
[0m01:23:02.841288 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m01:23:02.847834 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_DATAEXPERT_STUDENT_andres'
[0m01:23:02.870968 [debug] [ThreadPool]: Using snowflake connection "list_DATAEXPERT_STUDENT_andres"
[0m01:23:02.871992 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "connection_name": "list_DATAEXPERT_STUDENT_andres"} */
show objects in DATAEXPERT_STUDENT.andres limit 10000
[0m01:23:02.872859 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m01:23:03.853408 [debug] [ThreadPool]: SQL status: SUCCESS 28 in 0.980 seconds
[0m01:23:03.856871 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: Close
[0m01:23:04.073926 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f9637cd8-3636-4158-9363-810f040794c9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f590ec93650>]}
[0m01:23:04.075262 [info ] [MainThread]: Pulling freshness from warehouse metadata tables for 0 sources
[0m01:23:04.078501 [debug] [Thread-1 (]: Began running node source.dbt_capstone_andres.treasury.fiscal_data
[0m01:23:04.079302 [info ] [Thread-1 (]: 1 of 1 START freshness of treasury.fiscal_data ................................. [RUN]
[0m01:23:04.080323 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_DATAEXPERT_STUDENT_andres, now source.dbt_capstone_andres.treasury.fiscal_data)
[0m01:23:04.081009 [debug] [Thread-1 (]: Began compiling node source.dbt_capstone_andres.treasury.fiscal_data
[0m01:23:04.081868 [debug] [Thread-1 (]: Began executing node source.dbt_capstone_andres.treasury.fiscal_data
[0m01:23:04.092263 [debug] [Thread-1 (]: Using snowflake connection "source.dbt_capstone_andres.treasury.fiscal_data"
[0m01:23:04.093135 [debug] [Thread-1 (]: On source.dbt_capstone_andres.treasury.fiscal_data: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "source.dbt_capstone_andres.treasury.fiscal_data"} */
select
      max(record_date::timestamp) as max_loaded_at,
      convert_timezone('UTC', current_timestamp()) as snapshotted_at
    from dataexpert_student.andres.fiscal_data
[0m01:23:04.093943 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m01:23:06.288797 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 2.195 seconds
[0m01:23:06.294215 [debug] [Thread-1 (]: On source.dbt_capstone_andres.treasury.fiscal_data: Close
[0m01:23:06.543085 [info ] [Thread-1 (]: 1 of 1 PASS freshness of treasury.fiscal_data .................................. [[32mPASS[0m in 2.46s]
[0m01:23:06.544945 [debug] [Thread-1 (]: Finished running node source.dbt_capstone_andres.treasury.fiscal_data
[0m01:23:06.548180 [debug] [MainThread]: Connection 'master' was properly closed.
[0m01:23:06.549131 [debug] [MainThread]: Connection 'source.dbt_capstone_andres.treasury.fiscal_data' was properly closed.
[0m01:23:06.550231 [info ] [MainThread]: 
[0m01:23:06.551140 [info ] [MainThread]: Finished running 1 source in 0 hours 0 minutes and 3.71 seconds (3.71s).
[0m01:23:06.638350 [info ] [MainThread]: Done.
[0m01:23:06.642051 [debug] [MainThread]: Resource report: {"command_name": "freshness", "command_success": true, "command_wall_clock_time": 5.789247, "process_user_time": 2060.2998, "process_kernel_time": 550.00745, "process_mem_max_rss": "210236", "process_out_blocks": "3685", "process_in_blocks": "0"}
[0m01:23:06.643913 [debug] [MainThread]: Command `dbt source freshness` succeeded at 01:23:06.643736 after 5.79 seconds
[0m01:23:06.645074 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f590e783c90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f590e783e90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f59126a3fd0>]}
[0m01:23:06.646446 [debug] [MainThread]: Flushing usage events
[0m01:23:06.874796 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m16:13:12.998865 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3d40204c90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3d40411750>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3d40206310>]}


============================== 16:13:13.006058 | fffa4505-7770-4d39-b2eb-11ee0b8a2cd2 ==============================
[0m16:13:13.006058 [info ] [MainThread]: Running with dbt=1.9.0-b3
[0m16:13:13.008401 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'profiles_dir': '/usr/local/airflow/dags/capstone/dbt_capstone_andres', 'log_path': '/usr/local/airflow/dags/capstone/dbt_capstone_andres/logs', 'version_check': 'True', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt source freshness', 'static_parser': 'True', 'introspect': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m16:13:14.102458 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'fffa4505-7770-4d39-b2eb-11ee0b8a2cd2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3d18e51e90>]}
[0m16:13:14.159695 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'fffa4505-7770-4d39-b2eb-11ee0b8a2cd2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3d41617690>]}
[0m16:13:14.161657 [info ] [MainThread]: Registered adapter: snowflake=1.8.4
[0m16:13:14.208710 [debug] [MainThread]: checksum: 5f4fde221ca8627174a0341efc73099aaec05b8546408ceed6f59b1ec0755115, vars: {}, profile: , target: , version: 1.9.0b3
[0m16:13:14.643307 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m16:13:14.644025 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m16:13:14.650372 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 2 unused configuration paths:
- models.staging
- models.marts
[0m16:13:14.695531 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'fffa4505-7770-4d39-b2eb-11ee0b8a2cd2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3d18a44f10>]}
[0m16:13:14.866930 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'fffa4505-7770-4d39-b2eb-11ee0b8a2cd2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3d18de2690>]}
[0m16:13:14.867856 [info ] [MainThread]: Found 5 models, 2 data tests, 2 sources, 862 macros, 1 unit test
[0m16:13:14.868573 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'fffa4505-7770-4d39-b2eb-11ee0b8a2cd2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3d18d65350>]}
[0m16:13:14.870421 [info ] [MainThread]: 
[0m16:13:14.871171 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m16:13:14.871846 [info ] [MainThread]: 
[0m16:13:14.872697 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m16:13:14.878941 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_DATAEXPERT_STUDENT_andres'
[0m16:13:14.899548 [debug] [ThreadPool]: Using snowflake connection "list_DATAEXPERT_STUDENT_andres"
[0m16:13:14.900368 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "connection_name": "list_DATAEXPERT_STUDENT_andres"} */
show objects in DATAEXPERT_STUDENT.andres limit 10000
[0m16:13:14.901064 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:13:15.891500 [debug] [ThreadPool]: SQL status: SUCCESS 28 in 0.990 seconds
[0m16:13:15.896658 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: Close
[0m16:13:16.142872 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'fffa4505-7770-4d39-b2eb-11ee0b8a2cd2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3d1901d810>]}
[0m16:13:16.143862 [info ] [MainThread]: Pulling freshness from warehouse metadata tables for 0 sources
[0m16:13:16.147701 [debug] [Thread-1 (]: Began running node source.dbt_capstone_andres.treasury.fiscal_data
[0m16:13:16.148475 [info ] [Thread-1 (]: 1 of 1 START freshness of treasury.fiscal_data ................................. [RUN]
[0m16:13:16.149376 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_DATAEXPERT_STUDENT_andres, now source.dbt_capstone_andres.treasury.fiscal_data)
[0m16:13:16.150204 [debug] [Thread-1 (]: Began compiling node source.dbt_capstone_andres.treasury.fiscal_data
[0m16:13:16.150990 [debug] [Thread-1 (]: Began executing node source.dbt_capstone_andres.treasury.fiscal_data
[0m16:13:16.161153 [debug] [Thread-1 (]: Using snowflake connection "source.dbt_capstone_andres.treasury.fiscal_data"
[0m16:13:16.162143 [debug] [Thread-1 (]: On source.dbt_capstone_andres.treasury.fiscal_data: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "source.dbt_capstone_andres.treasury.fiscal_data"} */
select
      max(record_date::timestamp) as max_loaded_at,
      convert_timezone('UTC', current_timestamp()) as snapshotted_at
    from dataexpert_student.andres.fiscal_data
[0m16:13:16.163006 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m16:13:17.103444 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 0.940 seconds
[0m16:13:17.107402 [debug] [Thread-1 (]: On source.dbt_capstone_andres.treasury.fiscal_data: Close
[0m16:13:17.341126 [info ] [Thread-1 (]: 1 of 1 PASS freshness of treasury.fiscal_data .................................. [[32mPASS[0m in 1.19s]
[0m16:13:17.342906 [debug] [Thread-1 (]: Finished running node source.dbt_capstone_andres.treasury.fiscal_data
[0m16:13:17.346461 [debug] [MainThread]: Connection 'master' was properly closed.
[0m16:13:17.347678 [debug] [MainThread]: Connection 'source.dbt_capstone_andres.treasury.fiscal_data' was properly closed.
[0m16:13:17.348687 [info ] [MainThread]: 
[0m16:13:17.349756 [info ] [MainThread]: Finished running 1 source in 0 hours 0 minutes and 2.48 seconds (2.48s).
[0m16:13:17.497622 [info ] [MainThread]: Done.
[0m16:13:17.501084 [debug] [MainThread]: Resource report: {"command_name": "freshness", "command_success": true, "command_wall_clock_time": 4.554966, "process_user_time": 3.672688, "process_kernel_time": 1.273986, "process_mem_max_rss": "210084", "process_in_blocks": "30504", "process_out_blocks": "3685"}
[0m16:13:17.502496 [debug] [MainThread]: Command `dbt source freshness` succeeded at 16:13:17.502334 after 4.56 seconds
[0m16:13:17.503809 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3d40273c90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3d40273bd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3d43f8ffd0>]}
[0m16:13:17.504977 [debug] [MainThread]: Flushing usage events
[0m16:13:17.895210 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m16:13:42.311141 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fadeec8cf10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fadeee99690>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fadeee99cd0>]}


============================== 16:13:42.314078 | 6c55f1f8-7d8a-491d-9657-f5d4c1a8643f ==============================
[0m16:13:42.314078 [info ] [MainThread]: Running with dbt=1.9.0-b3
[0m16:13:42.315131 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'debug': 'False', 'log_path': '/usr/local/airflow/dags/capstone/dbt_capstone_andres/logs', 'fail_fast': 'False', 'profiles_dir': '/usr/local/airflow/dags/capstone/dbt_capstone_andres', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt source freshness', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m16:13:43.352758 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '6c55f1f8-7d8a-491d-9657-f5d4c1a8643f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fadc37af750>]}
[0m16:13:43.417583 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '6c55f1f8-7d8a-491d-9657-f5d4c1a8643f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fadeed8d4d0>]}
[0m16:13:43.418643 [info ] [MainThread]: Registered adapter: snowflake=1.8.4
[0m16:13:43.458053 [debug] [MainThread]: checksum: 5f4fde221ca8627174a0341efc73099aaec05b8546408ceed6f59b1ec0755115, vars: {}, profile: , target: , version: 1.9.0b3
[0m16:13:43.835963 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m16:13:43.836793 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m16:13:43.843693 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 2 unused configuration paths:
- models.staging
- models.marts
[0m16:13:43.893966 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '6c55f1f8-7d8a-491d-9657-f5d4c1a8643f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fadc3585e90>]}
[0m16:13:44.074024 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '6c55f1f8-7d8a-491d-9657-f5d4c1a8643f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fadc36521d0>]}
[0m16:13:44.074878 [info ] [MainThread]: Found 5 models, 2 data tests, 2 sources, 862 macros, 1 unit test
[0m16:13:44.075746 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6c55f1f8-7d8a-491d-9657-f5d4c1a8643f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fadc35d0ed0>]}
[0m16:13:44.077682 [info ] [MainThread]: 
[0m16:13:44.078464 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m16:13:44.079082 [info ] [MainThread]: 
[0m16:13:44.079873 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m16:13:44.085165 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_DATAEXPERT_STUDENT_andres'
[0m16:13:44.104177 [debug] [ThreadPool]: Using snowflake connection "list_DATAEXPERT_STUDENT_andres"
[0m16:13:44.105012 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "connection_name": "list_DATAEXPERT_STUDENT_andres"} */
show objects in DATAEXPERT_STUDENT.andres limit 10000
[0m16:13:44.105726 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:13:44.894419 [debug] [ThreadPool]: SQL status: SUCCESS 28 in 0.789 seconds
[0m16:13:44.898533 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: Close
[0m16:13:45.124659 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6c55f1f8-7d8a-491d-9657-f5d4c1a8643f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fadeecd0390>]}
[0m16:13:45.125820 [info ] [MainThread]: Pulling freshness from warehouse metadata tables for 0 sources
[0m16:13:45.131552 [debug] [Thread-1 (]: Began running node source.dbt_capstone_andres.treasury.fiscal_data
[0m16:13:45.132588 [info ] [Thread-1 (]: 1 of 1 START freshness of treasury.fiscal_data ................................. [RUN]
[0m16:13:45.133696 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_DATAEXPERT_STUDENT_andres, now source.dbt_capstone_andres.treasury.fiscal_data)
[0m16:13:45.134657 [debug] [Thread-1 (]: Began compiling node source.dbt_capstone_andres.treasury.fiscal_data
[0m16:13:45.135601 [debug] [Thread-1 (]: Began executing node source.dbt_capstone_andres.treasury.fiscal_data
[0m16:13:45.147460 [debug] [Thread-1 (]: Using snowflake connection "source.dbt_capstone_andres.treasury.fiscal_data"
[0m16:13:45.148437 [debug] [Thread-1 (]: On source.dbt_capstone_andres.treasury.fiscal_data: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "source.dbt_capstone_andres.treasury.fiscal_data"} */
select
      max(record_date::timestamp) as max_loaded_at,
      convert_timezone('UTC', current_timestamp()) as snapshotted_at
    from dataexpert_student.andres.fiscal_data
[0m16:13:45.149424 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m16:13:45.920500 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 0.771 seconds
[0m16:13:45.922815 [debug] [Thread-1 (]: On source.dbt_capstone_andres.treasury.fiscal_data: Close
[0m16:13:46.165376 [info ] [Thread-1 (]: 1 of 1 PASS freshness of treasury.fiscal_data .................................. [[32mPASS[0m in 1.03s]
[0m16:13:46.166495 [debug] [Thread-1 (]: Finished running node source.dbt_capstone_andres.treasury.fiscal_data
[0m16:13:46.168533 [debug] [MainThread]: Connection 'master' was properly closed.
[0m16:13:46.169610 [debug] [MainThread]: Connection 'source.dbt_capstone_andres.treasury.fiscal_data' was properly closed.
[0m16:13:46.170213 [info ] [MainThread]: 
[0m16:13:46.170874 [info ] [MainThread]: Finished running 1 source in 0 hours 0 minutes and 2.09 seconds (2.09s).
[0m16:13:46.252104 [info ] [MainThread]: Done.
[0m16:13:46.253407 [debug] [MainThread]: Resource report: {"command_name": "freshness", "command_success": true, "command_wall_clock_time": 4.000395, "process_user_time": 3.603131, "process_kernel_time": 0.97099, "process_mem_max_rss": "210000", "process_out_blocks": "3685", "process_in_blocks": "0"}
[0m16:13:46.254375 [debug] [MainThread]: Command `dbt source freshness` succeeded at 16:13:46.254267 after 4.00 seconds
[0m16:13:46.255136 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fadeecfbc10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fadeefc0910>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fadf29ebfd0>]}
[0m16:13:46.256377 [debug] [MainThread]: Flushing usage events
[0m16:13:46.468530 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m16:14:09.897906 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f893d58b010>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f893d588910>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f893d589350>]}


============================== 16:14:09.901167 | b204084f-cfbb-4e5f-aa01-0410fe449bf8 ==============================
[0m16:14:09.901167 [info ] [MainThread]: Running with dbt=1.9.0-b3
[0m16:14:09.902542 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'version_check': 'True', 'log_path': '/usr/local/airflow/dags/capstone/dbt_capstone_andres/logs', 'fail_fast': 'False', 'profiles_dir': '/usr/local/airflow/dags/capstone/dbt_capstone_andres', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'log_format': 'default', 'static_parser': 'True', 'invocation_command': 'dbt source freshness', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m16:14:11.807113 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'b204084f-cfbb-4e5f-aa01-0410fe449bf8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f89163b0f10>]}
[0m16:14:11.998311 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'b204084f-cfbb-4e5f-aa01-0410fe449bf8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8917592750>]}
[0m16:14:12.000643 [info ] [MainThread]: Registered adapter: snowflake=1.8.4
[0m16:14:12.143896 [debug] [MainThread]: checksum: 5f4fde221ca8627174a0341efc73099aaec05b8546408ceed6f59b1ec0755115, vars: {}, profile: , target: , version: 1.9.0b3
[0m16:14:13.020846 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m16:14:13.022914 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m16:14:13.039291 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 2 unused configuration paths:
- models.marts
- models.staging
[0m16:14:13.191501 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'b204084f-cfbb-4e5f-aa01-0410fe449bf8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8915ef62d0>]}
[0m16:14:13.521809 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'b204084f-cfbb-4e5f-aa01-0410fe449bf8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8915fa63d0>]}
[0m16:14:13.523433 [info ] [MainThread]: Found 5 models, 2 data tests, 2 sources, 862 macros, 1 unit test
[0m16:14:13.524968 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b204084f-cfbb-4e5f-aa01-0410fe449bf8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8915f28f90>]}
[0m16:14:13.529310 [info ] [MainThread]: 
[0m16:14:13.530623 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m16:14:13.531772 [info ] [MainThread]: 
[0m16:14:13.533139 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m16:14:13.540590 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_DATAEXPERT_STUDENT_andres'
[0m16:14:13.594823 [debug] [ThreadPool]: Using snowflake connection "list_DATAEXPERT_STUDENT_andres"
[0m16:14:13.596072 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "connection_name": "list_DATAEXPERT_STUDENT_andres"} */
show objects in DATAEXPERT_STUDENT.andres limit 10000
[0m16:14:13.597208 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:14:14.662961 [debug] [ThreadPool]: SQL status: SUCCESS 28 in 1.065 seconds
[0m16:14:14.672020 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: Close
[0m16:14:14.932189 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b204084f-cfbb-4e5f-aa01-0410fe449bf8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f893e1e7fd0>]}
[0m16:14:14.934634 [info ] [MainThread]: Pulling freshness from warehouse metadata tables for 0 sources
[0m16:14:14.944776 [debug] [Thread-1 (]: Began running node source.dbt_capstone_andres.treasury.fiscal_data
[0m16:14:14.947094 [info ] [Thread-1 (]: 1 of 1 START freshness of treasury.fiscal_data ................................. [RUN]
[0m16:14:14.949623 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_DATAEXPERT_STUDENT_andres, now source.dbt_capstone_andres.treasury.fiscal_data)
[0m16:14:14.951413 [debug] [Thread-1 (]: Began compiling node source.dbt_capstone_andres.treasury.fiscal_data
[0m16:14:14.953376 [debug] [Thread-1 (]: Began executing node source.dbt_capstone_andres.treasury.fiscal_data
[0m16:14:14.987576 [debug] [Thread-1 (]: Using snowflake connection "source.dbt_capstone_andres.treasury.fiscal_data"
[0m16:14:14.989645 [debug] [Thread-1 (]: On source.dbt_capstone_andres.treasury.fiscal_data: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "source.dbt_capstone_andres.treasury.fiscal_data"} */
select
      max(record_date::timestamp) as max_loaded_at,
      convert_timezone('UTC', current_timestamp()) as snapshotted_at
    from dataexpert_student.andres.fiscal_data
[0m16:14:14.991517 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m16:14:15.751700 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 0.760 seconds
[0m16:14:15.754497 [debug] [Thread-1 (]: On source.dbt_capstone_andres.treasury.fiscal_data: Close
[0m16:14:16.029697 [info ] [Thread-1 (]: 1 of 1 PASS freshness of treasury.fiscal_data .................................. [[32mPASS[0m in 1.08s]
[0m16:14:16.030836 [debug] [Thread-1 (]: Finished running node source.dbt_capstone_andres.treasury.fiscal_data
[0m16:14:16.033367 [debug] [MainThread]: Connection 'master' was properly closed.
[0m16:14:16.034222 [debug] [MainThread]: Connection 'source.dbt_capstone_andres.treasury.fiscal_data' was properly closed.
[0m16:14:16.034937 [info ] [MainThread]: 
[0m16:14:16.036324 [info ] [MainThread]: Finished running 1 source in 0 hours 0 minutes and 2.50 seconds (2.50s).
[0m16:14:16.164671 [info ] [MainThread]: Done.
[0m16:14:16.166472 [debug] [MainThread]: Resource report: {"command_name": "freshness", "command_success": true, "command_wall_clock_time": 6.350972, "process_user_time": 6.853764, "process_kernel_time": 0.974595, "process_mem_max_rss": "209904", "process_out_blocks": "3685", "process_in_blocks": "0"}
[0m16:14:16.169304 [debug] [MainThread]: Command `dbt source freshness` succeeded at 16:14:16.169076 after 6.35 seconds
[0m16:14:16.170358 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f893d588b90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f893d58ac50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f893e2343d0>]}
[0m16:14:16.171308 [debug] [MainThread]: Flushing usage events
[0m16:14:16.388228 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m16:14:29.334117 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbb4d398c10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbb4d398550>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbb4d39a090>]}


============================== 16:14:29.340866 | 9848a264-6ebd-45e7-aadf-58999c3d402c ==============================
[0m16:14:29.340866 [info ] [MainThread]: Running with dbt=1.9.0-b3
[0m16:14:29.345564 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/usr/local/airflow/dags/capstone/dbt_capstone_andres', 'debug': 'False', 'fail_fast': 'False', 'log_path': '/usr/local/airflow/dags/capstone/dbt_capstone_andres/logs', 'version_check': 'True', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'invocation_command': 'dbt source freshness', 'log_format': 'default', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m16:14:30.881096 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '9848a264-6ebd-45e7-aadf-58999c3d402c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbb260aff90>]}
[0m16:14:31.000102 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '9848a264-6ebd-45e7-aadf-58999c3d402c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbb4e78b6d0>]}
[0m16:14:31.003654 [info ] [MainThread]: Registered adapter: snowflake=1.8.4
[0m16:14:31.076354 [debug] [MainThread]: checksum: 5f4fde221ca8627174a0341efc73099aaec05b8546408ceed6f59b1ec0755115, vars: {}, profile: , target: , version: 1.9.0b3
[0m16:14:31.841219 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m16:14:31.843387 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m16:14:31.857860 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 2 unused configuration paths:
- models.staging
- models.marts
[0m16:14:31.924772 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '9848a264-6ebd-45e7-aadf-58999c3d402c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbb26057f90>]}
[0m16:14:32.273304 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '9848a264-6ebd-45e7-aadf-58999c3d402c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbb25ed9f50>]}
[0m16:14:32.275136 [info ] [MainThread]: Found 5 models, 2 data tests, 2 sources, 862 macros, 1 unit test
[0m16:14:32.276426 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9848a264-6ebd-45e7-aadf-58999c3d402c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbb25e58c90>]}
[0m16:14:32.280700 [info ] [MainThread]: 
[0m16:14:32.282393 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m16:14:32.283750 [info ] [MainThread]: 
[0m16:14:32.285643 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m16:14:32.297187 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_DATAEXPERT_STUDENT_andres'
[0m16:14:32.337436 [debug] [ThreadPool]: Using snowflake connection "list_DATAEXPERT_STUDENT_andres"
[0m16:14:32.338976 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "connection_name": "list_DATAEXPERT_STUDENT_andres"} */
show objects in DATAEXPERT_STUDENT.andres limit 10000
[0m16:14:32.340067 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:14:33.305290 [debug] [ThreadPool]: SQL status: SUCCESS 28 in 0.965 seconds
[0m16:14:33.309837 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: Close
[0m16:14:33.552877 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9848a264-6ebd-45e7-aadf-58999c3d402c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbb4f38d850>]}
[0m16:14:33.554158 [info ] [MainThread]: Pulling freshness from warehouse metadata tables for 0 sources
[0m16:14:33.560719 [debug] [Thread-1 (]: Began running node source.dbt_capstone_andres.treasury.fiscal_data
[0m16:14:33.561914 [info ] [Thread-1 (]: 1 of 1 START freshness of treasury.fiscal_data ................................. [RUN]
[0m16:14:33.565251 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_DATAEXPERT_STUDENT_andres, now source.dbt_capstone_andres.treasury.fiscal_data)
[0m16:14:33.568169 [debug] [Thread-1 (]: Began compiling node source.dbt_capstone_andres.treasury.fiscal_data
[0m16:14:33.572634 [debug] [Thread-1 (]: Began executing node source.dbt_capstone_andres.treasury.fiscal_data
[0m16:14:33.585732 [debug] [Thread-1 (]: Using snowflake connection "source.dbt_capstone_andres.treasury.fiscal_data"
[0m16:14:33.586921 [debug] [Thread-1 (]: On source.dbt_capstone_andres.treasury.fiscal_data: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "source.dbt_capstone_andres.treasury.fiscal_data"} */
select
      max(record_date::timestamp) as max_loaded_at,
      convert_timezone('UTC', current_timestamp()) as snapshotted_at
    from dataexpert_student.andres.fiscal_data
[0m16:14:33.588212 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m16:14:34.317681 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 0.729 seconds
[0m16:14:34.321043 [debug] [Thread-1 (]: On source.dbt_capstone_andres.treasury.fiscal_data: Close
[0m16:14:34.591481 [info ] [Thread-1 (]: 1 of 1 PASS freshness of treasury.fiscal_data .................................. [[32mPASS[0m in 1.03s]
[0m16:14:34.593361 [debug] [Thread-1 (]: Finished running node source.dbt_capstone_andres.treasury.fiscal_data
[0m16:14:34.598905 [debug] [MainThread]: Connection 'master' was properly closed.
[0m16:14:34.600271 [debug] [MainThread]: Connection 'source.dbt_capstone_andres.treasury.fiscal_data' was properly closed.
[0m16:14:34.601916 [info ] [MainThread]: 
[0m16:14:34.603095 [info ] [MainThread]: Finished running 1 source in 0 hours 0 minutes and 2.32 seconds (2.32s).
[0m16:14:34.766604 [info ] [MainThread]: Done.
[0m16:14:34.768966 [debug] [MainThread]: Resource report: {"command_name": "freshness", "command_success": true, "command_wall_clock_time": 5.5095363, "process_user_time": 5.35416, "process_kernel_time": 0.900679, "process_mem_max_rss": "210036", "process_out_blocks": "3685", "process_in_blocks": "0"}
[0m16:14:34.770415 [debug] [MainThread]: Command `dbt source freshness` succeeded at 16:14:34.770269 after 5.51 seconds
[0m16:14:34.771395 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbb4d403e10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbb4d89b490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbb5115bf50>]}
[0m16:14:34.772639 [debug] [MainThread]: Flushing usage events
[0m16:14:35.010931 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m16:14:51.900476 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb384dd4d50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb384de0b90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb384de0f10>]}


============================== 16:14:51.906439 | 3e0fd06e-d7f8-4d98-813d-f4de7ebf1be9 ==============================
[0m16:14:51.906439 [info ] [MainThread]: Running with dbt=1.9.0-b3
[0m16:14:51.908960 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'version_check': 'True', 'log_path': '/usr/local/airflow/dags/capstone/dbt_capstone_andres/logs', 'fail_fast': 'False', 'profiles_dir': '/usr/local/airflow/dags/capstone/dbt_capstone_andres', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt source freshness', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m16:17:32.241380 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5420259410>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5420370490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f542025a210>]}


============================== 16:17:32.247532 | af5f5978-677f-400e-8b0b-26bfc2779d32 ==============================
[0m16:17:32.247532 [info ] [MainThread]: Running with dbt=1.9.0-b3
[0m16:17:32.249930 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/usr/local/airflow/dags/capstone/dbt_capstone_andres', 'fail_fast': 'False', 'warn_error': 'None', 'log_path': '/usr/local/airflow/dags/capstone/dbt_capstone_andres/logs', 'version_check': 'True', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'static_parser': 'True', 'invocation_command': 'dbt source freshness', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m16:17:34.120396 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'af5f5978-677f-400e-8b0b-26bfc2779d32', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f53f8db6610>]}
[0m16:17:34.254438 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'af5f5978-677f-400e-8b0b-26bfc2779d32', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5420359490>]}
[0m16:17:34.256375 [info ] [MainThread]: Registered adapter: snowflake=1.8.4
[0m16:17:34.359666 [debug] [MainThread]: checksum: 5f4fde221ca8627174a0341efc73099aaec05b8546408ceed6f59b1ec0755115, vars: {}, profile: , target: , version: 1.9.0b3
[0m16:17:34.986355 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m16:17:34.987145 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m16:17:34.995238 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 2 unused configuration paths:
- models.marts
- models.staging
[0m16:17:35.073292 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'af5f5978-677f-400e-8b0b-26bfc2779d32', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f53f89cca90>]}
[0m16:17:35.346631 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'af5f5978-677f-400e-8b0b-26bfc2779d32', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f53f8a9a510>]}
[0m16:17:35.347636 [info ] [MainThread]: Found 5 models, 2 data tests, 2 sources, 862 macros, 1 unit test
[0m16:17:35.348454 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'af5f5978-677f-400e-8b0b-26bfc2779d32', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f53f933a810>]}
[0m16:17:35.351046 [info ] [MainThread]: 
[0m16:17:35.352099 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m16:17:35.353014 [info ] [MainThread]: 
[0m16:17:35.354239 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m16:17:35.362003 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_DATAEXPERT_STUDENT_andres'
[0m16:17:35.389742 [debug] [ThreadPool]: Using snowflake connection "list_DATAEXPERT_STUDENT_andres"
[0m16:17:35.394681 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "connection_name": "list_DATAEXPERT_STUDENT_andres"} */
show objects in DATAEXPERT_STUDENT.andres limit 10000
[0m16:17:35.395536 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:17:36.278420 [debug] [ThreadPool]: SQL status: SUCCESS 28 in 0.883 seconds
[0m16:17:36.281849 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: Close
[0m16:17:36.548287 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'af5f5978-677f-400e-8b0b-26bfc2779d32', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f53f89ecfd0>]}
[0m16:17:36.549580 [info ] [MainThread]: Pulling freshness from warehouse metadata tables for 0 sources
[0m16:17:36.553431 [debug] [Thread-1 (]: Began running node source.dbt_capstone_andres.treasury.fiscal_data
[0m16:17:36.554379 [info ] [Thread-1 (]: 1 of 1 START freshness of treasury.fiscal_data ................................. [RUN]
[0m16:17:36.555527 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_DATAEXPERT_STUDENT_andres, now source.dbt_capstone_andres.treasury.fiscal_data)
[0m16:17:36.556482 [debug] [Thread-1 (]: Began compiling node source.dbt_capstone_andres.treasury.fiscal_data
[0m16:17:36.557559 [debug] [Thread-1 (]: Began executing node source.dbt_capstone_andres.treasury.fiscal_data
[0m16:17:36.578564 [debug] [Thread-1 (]: Using snowflake connection "source.dbt_capstone_andres.treasury.fiscal_data"
[0m16:17:36.579976 [debug] [Thread-1 (]: On source.dbt_capstone_andres.treasury.fiscal_data: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "source.dbt_capstone_andres.treasury.fiscal_data"} */
select
      max(record_date::timestamp) as max_loaded_at,
      convert_timezone('UTC', current_timestamp()) as snapshotted_at
    from dataexpert_student.andres.fiscal_data
[0m16:17:36.581340 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m16:17:37.278669 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 0.697 seconds
[0m16:17:37.280791 [debug] [Thread-1 (]: On source.dbt_capstone_andres.treasury.fiscal_data: Close
[0m16:17:37.535657 [info ] [Thread-1 (]: 1 of 1 PASS freshness of treasury.fiscal_data .................................. [[32mPASS[0m in 0.98s]
[0m16:17:37.537813 [debug] [Thread-1 (]: Finished running node source.dbt_capstone_andres.treasury.fiscal_data
[0m16:17:37.541087 [debug] [MainThread]: Connection 'master' was properly closed.
[0m16:17:37.542240 [debug] [MainThread]: Connection 'source.dbt_capstone_andres.treasury.fiscal_data' was properly closed.
[0m16:17:37.543329 [info ] [MainThread]: 
[0m16:17:37.544521 [info ] [MainThread]: Finished running 1 source in 0 hours 0 minutes and 2.19 seconds (2.19s).
[0m16:17:37.692004 [info ] [MainThread]: Done.
[0m16:17:37.694584 [debug] [MainThread]: Resource report: {"command_name": "freshness", "command_success": true, "command_wall_clock_time": 5.56922, "process_user_time": 6.327699, "process_kernel_time": 1.168773, "process_mem_max_rss": "210004", "process_out_blocks": "3685", "process_in_blocks": "0"}
[0m16:17:37.695805 [debug] [MainThread]: Command `dbt source freshness` succeeded at 16:17:37.695670 after 5.57 seconds
[0m16:17:37.696854 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5424075590>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5423febfd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5423ff4b90>]}
[0m16:17:37.698125 [debug] [MainThread]: Flushing usage events
[0m16:17:37.982018 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m16:17:53.105826 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f328a79c8d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f328a89db50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f328a79ccd0>]}


============================== 16:17:53.113641 | e97cfe01-c960-44c2-a63f-8abecc27936a ==============================
[0m16:17:53.113641 [info ] [MainThread]: Running with dbt=1.9.0-b3
[0m16:17:53.115519 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'version_check': 'True', 'log_path': '/usr/local/airflow/dags/capstone/dbt_capstone_andres/logs', 'profiles_dir': '/usr/local/airflow/dags/capstone/dbt_capstone_andres', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt source freshness', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m16:17:55.078171 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'e97cfe01-c960-44c2-a63f-8abecc27936a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f325f35ffd0>]}
[0m16:17:55.185247 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'e97cfe01-c960-44c2-a63f-8abecc27936a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f328a89d510>]}
[0m16:17:55.187202 [info ] [MainThread]: Registered adapter: snowflake=1.8.4
[0m16:17:55.308335 [debug] [MainThread]: checksum: 5f4fde221ca8627174a0341efc73099aaec05b8546408ceed6f59b1ec0755115, vars: {}, profile: , target: , version: 1.9.0b3
[0m16:17:56.413282 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m16:17:56.414407 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m16:17:56.427403 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 2 unused configuration paths:
- models.marts
- models.staging
[0m16:17:56.513990 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'e97cfe01-c960-44c2-a63f-8abecc27936a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f325f0f8090>]}
[0m16:17:56.884429 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'e97cfe01-c960-44c2-a63f-8abecc27936a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f325f2e2010>]}
[0m16:17:56.885593 [info ] [MainThread]: Found 5 models, 2 data tests, 2 sources, 862 macros, 1 unit test
[0m16:17:56.886681 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e97cfe01-c960-44c2-a63f-8abecc27936a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f325f260c90>]}
[0m16:17:56.890464 [info ] [MainThread]: 
[0m16:17:56.892243 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m16:17:56.893638 [info ] [MainThread]: 
[0m16:17:56.895195 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m16:17:56.903431 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_DATAEXPERT_STUDENT_andres'
[0m16:17:56.931019 [debug] [ThreadPool]: Using snowflake connection "list_DATAEXPERT_STUDENT_andres"
[0m16:17:56.932100 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "connection_name": "list_DATAEXPERT_STUDENT_andres"} */
show objects in DATAEXPERT_STUDENT.andres limit 10000
[0m16:17:56.933097 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:17:57.959361 [debug] [ThreadPool]: SQL status: SUCCESS 28 in 1.026 seconds
[0m16:17:57.966872 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: Close
[0m16:17:58.220109 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e97cfe01-c960-44c2-a63f-8abecc27936a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f325f10f2d0>]}
[0m16:17:58.222445 [info ] [MainThread]: Pulling freshness from warehouse metadata tables for 0 sources
[0m16:17:58.227739 [debug] [Thread-1 (]: Began running node source.dbt_capstone_andres.treasury.fiscal_data
[0m16:17:58.229404 [info ] [Thread-1 (]: 1 of 1 START freshness of treasury.fiscal_data ................................. [RUN]
[0m16:17:58.230855 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_DATAEXPERT_STUDENT_andres, now source.dbt_capstone_andres.treasury.fiscal_data)
[0m16:17:58.232458 [debug] [Thread-1 (]: Began compiling node source.dbt_capstone_andres.treasury.fiscal_data
[0m16:17:58.233900 [debug] [Thread-1 (]: Began executing node source.dbt_capstone_andres.treasury.fiscal_data
[0m16:17:58.254651 [debug] [Thread-1 (]: Using snowflake connection "source.dbt_capstone_andres.treasury.fiscal_data"
[0m16:17:58.256400 [debug] [Thread-1 (]: On source.dbt_capstone_andres.treasury.fiscal_data: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "source.dbt_capstone_andres.treasury.fiscal_data"} */
select
      max(record_date::timestamp) as max_loaded_at,
      convert_timezone('UTC', current_timestamp()) as snapshotted_at
    from dataexpert_student.andres.fiscal_data
[0m16:17:58.258014 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m16:17:59.020810 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 0.763 seconds
[0m16:17:59.025379 [debug] [Thread-1 (]: On source.dbt_capstone_andres.treasury.fiscal_data: Close
[0m16:17:59.308490 [info ] [Thread-1 (]: 1 of 1 PASS freshness of treasury.fiscal_data .................................. [[32mPASS[0m in 1.08s]
[0m16:17:59.311840 [debug] [Thread-1 (]: Finished running node source.dbt_capstone_andres.treasury.fiscal_data
[0m16:17:59.317049 [debug] [MainThread]: Connection 'master' was properly closed.
[0m16:17:59.320874 [debug] [MainThread]: Connection 'source.dbt_capstone_andres.treasury.fiscal_data' was properly closed.
[0m16:17:59.323638 [info ] [MainThread]: 
[0m16:17:59.325134 [info ] [MainThread]: Finished running 1 source in 0 hours 0 minutes and 2.43 seconds (2.43s).
[0m16:17:59.510365 [info ] [MainThread]: Done.
[0m16:17:59.513134 [debug] [MainThread]: Resource report: {"command_name": "freshness", "command_success": true, "command_wall_clock_time": 6.5513616, "process_user_time": 6.558669, "process_kernel_time": 0.884153, "process_mem_max_rss": "210132", "process_out_blocks": "3685", "process_in_blocks": "0"}
[0m16:17:59.514714 [debug] [MainThread]: Command `dbt source freshness` succeeded at 16:17:59.514561 after 6.55 seconds
[0m16:17:59.517151 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f328a80bd10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f328a80bc10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f328e53ffd0>]}
[0m16:17:59.518813 [debug] [MainThread]: Flushing usage events
[0m16:17:59.772434 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m16:18:13.487680 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f14a9f18350>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f14aba15d10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f14a9f19050>]}


============================== 16:18:13.495220 | 557c9a66-96a0-43f2-839c-c7e9be29ab6a ==============================
[0m16:18:13.495220 [info ] [MainThread]: Running with dbt=1.9.0-b3
[0m16:18:13.497782 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'profiles_dir': '/usr/local/airflow/dags/capstone/dbt_capstone_andres', 'log_path': '/usr/local/airflow/dags/capstone/dbt_capstone_andres/logs', 'version_check': 'True', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'invocation_command': 'dbt source freshness', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m16:18:15.446570 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '557c9a66-96a0-43f2-839c-c7e9be29ab6a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f147ecc3350>]}
[0m16:18:15.585131 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '557c9a66-96a0-43f2-839c-c7e9be29ab6a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f14ab30b490>]}
[0m16:18:15.591195 [info ] [MainThread]: Registered adapter: snowflake=1.8.4
[0m16:18:15.699537 [debug] [MainThread]: checksum: 5f4fde221ca8627174a0341efc73099aaec05b8546408ceed6f59b1ec0755115, vars: {}, profile: , target: , version: 1.9.0b3
[0m16:18:16.740514 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m16:18:16.742102 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m16:18:16.756926 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 2 unused configuration paths:
- models.marts
- models.staging
[0m16:18:16.855552 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '557c9a66-96a0-43f2-839c-c7e9be29ab6a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f147e910fd0>]}
[0m16:18:17.256174 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '557c9a66-96a0-43f2-839c-c7e9be29ab6a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f147eaf9f90>]}
[0m16:18:17.257584 [info ] [MainThread]: Found 5 models, 2 data tests, 2 sources, 862 macros, 1 unit test
[0m16:18:17.258924 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '557c9a66-96a0-43f2-839c-c7e9be29ab6a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f147ea78c50>]}
[0m16:18:17.262916 [info ] [MainThread]: 
[0m16:18:17.264392 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m16:18:17.265594 [info ] [MainThread]: 
[0m16:18:17.267284 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m16:18:17.284604 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_DATAEXPERT_STUDENT_andres'
[0m16:18:17.331275 [debug] [ThreadPool]: Using snowflake connection "list_DATAEXPERT_STUDENT_andres"
[0m16:18:17.333080 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "connection_name": "list_DATAEXPERT_STUDENT_andres"} */
show objects in DATAEXPERT_STUDENT.andres limit 10000
[0m16:18:17.334134 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:18:18.478996 [debug] [ThreadPool]: SQL status: SUCCESS 28 in 1.145 seconds
[0m16:18:18.488386 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: Close
[0m16:18:18.745378 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '557c9a66-96a0-43f2-839c-c7e9be29ab6a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f14abf50d90>]}
[0m16:18:18.747088 [info ] [MainThread]: Pulling freshness from warehouse metadata tables for 0 sources
[0m16:18:18.756588 [debug] [Thread-1 (]: Began running node source.dbt_capstone_andres.treasury.fiscal_data
[0m16:18:18.758189 [info ] [Thread-1 (]: 1 of 1 START freshness of treasury.fiscal_data ................................. [RUN]
[0m16:18:18.759954 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_DATAEXPERT_STUDENT_andres, now source.dbt_capstone_andres.treasury.fiscal_data)
[0m16:18:18.761563 [debug] [Thread-1 (]: Began compiling node source.dbt_capstone_andres.treasury.fiscal_data
[0m16:18:18.763280 [debug] [Thread-1 (]: Began executing node source.dbt_capstone_andres.treasury.fiscal_data
[0m16:18:18.783004 [debug] [Thread-1 (]: Using snowflake connection "source.dbt_capstone_andres.treasury.fiscal_data"
[0m16:18:18.784623 [debug] [Thread-1 (]: On source.dbt_capstone_andres.treasury.fiscal_data: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "source.dbt_capstone_andres.treasury.fiscal_data"} */
select
      max(record_date::timestamp) as max_loaded_at,
      convert_timezone('UTC', current_timestamp()) as snapshotted_at
    from dataexpert_student.andres.fiscal_data
[0m16:18:18.788577 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m16:18:19.552050 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 0.763 seconds
[0m16:18:19.558478 [debug] [Thread-1 (]: On source.dbt_capstone_andres.treasury.fiscal_data: Close
[0m16:18:19.811372 [info ] [Thread-1 (]: 1 of 1 PASS freshness of treasury.fiscal_data .................................. [[32mPASS[0m in 1.05s]
[0m16:18:19.813395 [debug] [Thread-1 (]: Finished running node source.dbt_capstone_andres.treasury.fiscal_data
[0m16:18:19.817105 [debug] [MainThread]: Connection 'master' was properly closed.
[0m16:18:19.818699 [debug] [MainThread]: Connection 'source.dbt_capstone_andres.treasury.fiscal_data' was properly closed.
[0m16:18:19.819942 [info ] [MainThread]: 
[0m16:18:19.821221 [info ] [MainThread]: Finished running 1 source in 0 hours 0 minutes and 2.55 seconds (2.55s).
[0m16:18:19.998133 [info ] [MainThread]: Done.
[0m16:18:20.000736 [debug] [MainThread]: Resource report: {"command_name": "freshness", "command_success": true, "command_wall_clock_time": 6.6472306, "process_user_time": 5.746213, "process_kernel_time": 0.882657, "process_mem_max_rss": "209644", "process_in_blocks": "360", "process_out_blocks": "3685"}
[0m16:18:20.002984 [debug] [MainThread]: Command `dbt source freshness` succeeded at 16:18:20.002641 after 6.65 seconds
[0m16:18:20.004248 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f14a9f87b50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f14a9f87990>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f14adcdffd0>]}
[0m16:18:20.005912 [debug] [MainThread]: Flushing usage events
[0m16:18:20.249365 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m16:18:33.630858 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7facc4a46a10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7facc4a8bfd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7facc4a91350>]}


============================== 16:18:33.634393 | 7cd9a7c2-859c-4fe3-b670-f492a0d410e2 ==============================
[0m16:18:33.634393 [info ] [MainThread]: Running with dbt=1.9.0-b3
[0m16:18:33.635491 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'version_check': 'True', 'log_path': '/usr/local/airflow/dags/capstone/dbt_capstone_andres/logs', 'profiles_dir': '/usr/local/airflow/dags/capstone/dbt_capstone_andres', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'target_path': 'None', 'invocation_command': 'dbt source freshness', 'send_anonymous_usage_stats': 'True'}
[0m16:18:35.131965 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '7cd9a7c2-859c-4fe3-b670-f492a0d410e2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7facc5895910>]}
[0m16:18:35.225611 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '7cd9a7c2-859c-4fe3-b670-f492a0d410e2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7facc4b45390>]}
[0m16:18:35.226705 [info ] [MainThread]: Registered adapter: snowflake=1.8.4
[0m16:18:35.277471 [debug] [MainThread]: checksum: 5f4fde221ca8627174a0341efc73099aaec05b8546408ceed6f59b1ec0755115, vars: {}, profile: , target: , version: 1.9.0b3
[0m16:18:35.865752 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m16:18:35.866755 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m16:18:35.873248 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 2 unused configuration paths:
- models.staging
- models.marts
[0m16:18:35.922563 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '7cd9a7c2-859c-4fe3-b670-f492a0d410e2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7facc59d27d0>]}
[0m16:18:36.145629 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '7cd9a7c2-859c-4fe3-b670-f492a0d410e2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fac9d5be210>]}
[0m16:18:36.146594 [info ] [MainThread]: Found 5 models, 2 data tests, 2 sources, 862 macros, 1 unit test
[0m16:18:36.147621 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '7cd9a7c2-859c-4fe3-b670-f492a0d410e2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fac9d524f90>]}
[0m16:18:36.150158 [info ] [MainThread]: 
[0m16:18:36.151149 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m16:18:36.152035 [info ] [MainThread]: 
[0m16:18:36.153512 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m16:18:36.159695 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_DATAEXPERT_STUDENT_andres'
[0m16:18:36.186532 [debug] [ThreadPool]: Using snowflake connection "list_DATAEXPERT_STUDENT_andres"
[0m16:18:36.187549 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "connection_name": "list_DATAEXPERT_STUDENT_andres"} */
show objects in DATAEXPERT_STUDENT.andres limit 10000
[0m16:18:36.188356 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:18:36.226902 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f698cdec910>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f698dac4290>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f698dbd9350>]}


============================== 16:18:36.230315 | 2b7bb24e-c414-43e9-bf60-ff206fd774f9 ==============================
[0m16:18:36.230315 [info ] [MainThread]: Running with dbt=1.9.0-b3
[0m16:18:36.231497 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'version_check': 'True', 'log_path': '/usr/local/airflow/dags/capstone/dbt_capstone_andres/logs', 'profiles_dir': '/usr/local/airflow/dags/capstone/dbt_capstone_andres', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'log_format': 'default', 'static_parser': 'True', 'invocation_command': 'dbt source freshness', 'introspect': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m16:18:37.006031 [debug] [ThreadPool]: SQL status: SUCCESS 28 in 0.818 seconds
[0m16:18:37.009991 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: Close
[0m16:18:37.246154 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '2b7bb24e-c414-43e9-bf60-ff206fd774f9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6965ad6a10>]}
[0m16:18:37.268473 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '7cd9a7c2-859c-4fe3-b670-f492a0d410e2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fac9d3e4510>]}
[0m16:18:37.269538 [info ] [MainThread]: Pulling freshness from warehouse metadata tables for 0 sources
[0m16:18:37.273664 [debug] [Thread-1 (]: Began running node source.dbt_capstone_andres.treasury.fiscal_data
[0m16:18:37.274506 [info ] [Thread-1 (]: 1 of 1 START freshness of treasury.fiscal_data ................................. [RUN]
[0m16:18:37.275373 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_DATAEXPERT_STUDENT_andres, now source.dbt_capstone_andres.treasury.fiscal_data)
[0m16:18:37.276072 [debug] [Thread-1 (]: Began compiling node source.dbt_capstone_andres.treasury.fiscal_data
[0m16:18:37.276803 [debug] [Thread-1 (]: Began executing node source.dbt_capstone_andres.treasury.fiscal_data
[0m16:18:37.286094 [debug] [Thread-1 (]: Using snowflake connection "source.dbt_capstone_andres.treasury.fiscal_data"
[0m16:18:37.286977 [debug] [Thread-1 (]: On source.dbt_capstone_andres.treasury.fiscal_data: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "source.dbt_capstone_andres.treasury.fiscal_data"} */
select
      max(record_date::timestamp) as max_loaded_at,
      convert_timezone('UTC', current_timestamp()) as snapshotted_at
    from dataexpert_student.andres.fiscal_data
[0m16:18:37.287914 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m16:18:37.307836 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '2b7bb24e-c414-43e9-bf60-ff206fd774f9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f698e1df690>]}
[0m16:18:37.308913 [info ] [MainThread]: Registered adapter: snowflake=1.8.4
[0m16:18:37.350341 [debug] [MainThread]: checksum: 5f4fde221ca8627174a0341efc73099aaec05b8546408ceed6f59b1ec0755115, vars: {}, profile: , target: , version: 1.9.0b3
[0m16:18:37.795307 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m16:18:37.796705 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m16:18:37.803871 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 2 unused configuration paths:
- models.marts
- models.staging
[0m16:18:37.857922 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '2b7bb24e-c414-43e9-bf60-ff206fd774f9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f698ecd2f90>]}
[0m16:18:38.001144 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 0.713 seconds
[0m16:18:38.003539 [debug] [Thread-1 (]: On source.dbt_capstone_andres.treasury.fiscal_data: Close
[0m16:18:38.069335 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '2b7bb24e-c414-43e9-bf60-ff206fd774f9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6965616210>]}
[0m16:18:38.070555 [info ] [MainThread]: Found 5 models, 2 data tests, 2 sources, 862 macros, 1 unit test
[0m16:18:38.071580 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2b7bb24e-c414-43e9-bf60-ff206fd774f9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6965880e50>]}
[0m16:18:38.074545 [info ] [MainThread]: 
[0m16:18:38.075776 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m16:18:38.076820 [info ] [MainThread]: 
[0m16:18:38.078498 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m16:18:38.087221 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_DATAEXPERT_STUDENT_andres'
[0m16:18:38.118246 [debug] [ThreadPool]: Using snowflake connection "list_DATAEXPERT_STUDENT_andres"
[0m16:18:38.119493 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "connection_name": "list_DATAEXPERT_STUDENT_andres"} */
show objects in DATAEXPERT_STUDENT.andres limit 10000
[0m16:18:38.120661 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:18:38.288880 [info ] [Thread-1 (]: 1 of 1 PASS freshness of treasury.fiscal_data .................................. [[32mPASS[0m in 1.01s]
[0m16:18:38.290053 [debug] [Thread-1 (]: Finished running node source.dbt_capstone_andres.treasury.fiscal_data
[0m16:18:38.292519 [debug] [MainThread]: Connection 'master' was properly closed.
[0m16:18:38.293262 [debug] [MainThread]: Connection 'source.dbt_capstone_andres.treasury.fiscal_data' was properly closed.
[0m16:18:38.293924 [info ] [MainThread]: 
[0m16:18:38.294752 [info ] [MainThread]: Finished running 1 source in 0 hours 0 minutes and 2.14 seconds (2.14s).
[0m16:18:38.431275 [info ] [MainThread]: Done.
[0m16:18:38.433028 [debug] [MainThread]: Resource report: {"command_name": "freshness", "command_success": true, "command_wall_clock_time": 4.8764277, "process_user_time": 4.623222, "process_kernel_time": 0.988263, "process_mem_max_rss": "209692", "process_out_blocks": "3685", "process_in_blocks": "0"}
[0m16:18:38.434823 [debug] [MainThread]: Command `dbt source freshness` succeeded at 16:18:38.434632 after 4.88 seconds
[0m16:18:38.436086 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7facc4ab3e90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7facc4ab3d10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7facc87cffd0>]}
[0m16:18:38.437560 [debug] [MainThread]: Flushing usage events
[0m16:18:38.657102 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m16:18:38.980911 [debug] [ThreadPool]: SQL status: SUCCESS 28 in 0.860 seconds
[0m16:18:38.986319 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: Close
[0m16:18:39.250632 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2b7bb24e-c414-43e9-bf60-ff206fd774f9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6965572110>]}
[0m16:18:39.289388 [info ] [MainThread]: Pulling freshness from warehouse metadata tables for 0 sources
[0m16:18:39.296144 [debug] [Thread-1 (]: Began running node source.dbt_capstone_andres.treasury.fiscal_data
[0m16:18:39.297525 [info ] [Thread-1 (]: 1 of 1 START freshness of treasury.fiscal_data ................................. [RUN]
[0m16:18:39.302174 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_DATAEXPERT_STUDENT_andres, now source.dbt_capstone_andres.treasury.fiscal_data)
[0m16:18:39.303626 [debug] [Thread-1 (]: Began compiling node source.dbt_capstone_andres.treasury.fiscal_data
[0m16:18:39.307022 [debug] [Thread-1 (]: Began executing node source.dbt_capstone_andres.treasury.fiscal_data
[0m16:18:39.322033 [debug] [Thread-1 (]: Using snowflake connection "source.dbt_capstone_andres.treasury.fiscal_data"
[0m16:18:39.323188 [debug] [Thread-1 (]: On source.dbt_capstone_andres.treasury.fiscal_data: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "source.dbt_capstone_andres.treasury.fiscal_data"} */
select
      max(record_date::timestamp) as max_loaded_at,
      convert_timezone('UTC', current_timestamp()) as snapshotted_at
    from dataexpert_student.andres.fiscal_data
[0m16:18:39.324485 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m16:18:39.984699 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 0.660 seconds
[0m16:18:39.990161 [debug] [Thread-1 (]: On source.dbt_capstone_andres.treasury.fiscal_data: Close
[0m16:18:40.229648 [info ] [Thread-1 (]: 1 of 1 PASS freshness of treasury.fiscal_data .................................. [[32mPASS[0m in 0.93s]
[0m16:18:40.235482 [debug] [Thread-1 (]: Finished running node source.dbt_capstone_andres.treasury.fiscal_data
[0m16:18:40.246454 [debug] [MainThread]: Connection 'master' was properly closed.
[0m16:18:40.250531 [debug] [MainThread]: Connection 'source.dbt_capstone_andres.treasury.fiscal_data' was properly closed.
[0m16:18:40.251998 [info ] [MainThread]: 
[0m16:18:40.256059 [info ] [MainThread]: Finished running 1 source in 0 hours 0 minutes and 2.17 seconds (2.17s).
[0m16:18:40.635565 [info ] [MainThread]: Done.
[0m16:18:40.639580 [debug] [MainThread]: Resource report: {"command_name": "freshness", "command_success": true, "command_wall_clock_time": 4.4798236, "process_user_time": 4.060067, "process_kernel_time": 0.870385, "process_mem_max_rss": "209960", "process_out_blocks": "3685", "process_in_blocks": "0"}
[0m16:18:40.648220 [debug] [MainThread]: Command `dbt source freshness` succeeded at 16:18:40.648022 after 4.49 seconds
[0m16:18:40.649909 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f698cc5fc50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f698cc5f8d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6990b5ffd0>]}
[0m16:18:40.651953 [debug] [MainThread]: Flushing usage events
[0m16:18:40.916853 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m16:19:07.245000 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3fcbf50e10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3fcbf490d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3fcc497f10>]}


============================== 16:19:07.249488 | 3199b050-310e-49d5-85bb-b769d54b9b30 ==============================
[0m16:19:07.249488 [info ] [MainThread]: Running with dbt=1.9.0-b3
[0m16:19:07.251335 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'profiles_dir': '/usr/local/airflow/dags/capstone/dbt_capstone_andres', 'log_path': '/usr/local/airflow/dags/capstone/dbt_capstone_andres/logs', 'version_check': 'True', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'log_format': 'default', 'invocation_command': 'dbt source freshness', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m16:19:08.369525 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '3199b050-310e-49d5-85bb-b769d54b9b30', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3fa4b68150>]}
[0m16:19:08.446197 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '3199b050-310e-49d5-85bb-b769d54b9b30', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3fcd343810>]}
[0m16:19:08.447341 [info ] [MainThread]: Registered adapter: snowflake=1.8.4
[0m16:19:08.510770 [debug] [MainThread]: checksum: 5f4fde221ca8627174a0341efc73099aaec05b8546408ceed6f59b1ec0755115, vars: {}, profile: , target: , version: 1.9.0b3
[0m16:19:09.121432 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m16:19:09.122202 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m16:19:09.129565 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 2 unused configuration paths:
- models.marts
- models.staging
[0m16:19:09.185461 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '3199b050-310e-49d5-85bb-b769d54b9b30', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3fa4cd8b10>]}
[0m16:19:09.387619 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '3199b050-310e-49d5-85bb-b769d54b9b30', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3fa4afa310>]}
[0m16:19:09.388696 [info ] [MainThread]: Found 5 models, 2 data tests, 2 sources, 862 macros, 1 unit test
[0m16:19:09.389635 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '3199b050-310e-49d5-85bb-b769d54b9b30', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3fa4a78ed0>]}
[0m16:19:09.392484 [info ] [MainThread]: 
[0m16:19:09.393518 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m16:19:09.394421 [info ] [MainThread]: 
[0m16:19:09.395536 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m16:19:09.402205 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_DATAEXPERT_STUDENT_andres'
[0m16:19:09.423443 [debug] [ThreadPool]: Using snowflake connection "list_DATAEXPERT_STUDENT_andres"
[0m16:19:09.424301 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "connection_name": "list_DATAEXPERT_STUDENT_andres"} */
show objects in DATAEXPERT_STUDENT.andres limit 10000
[0m16:19:09.425064 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:19:10.339486 [debug] [ThreadPool]: SQL status: SUCCESS 28 in 0.914 seconds
[0m16:19:10.347360 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: Close
[0m16:19:10.577140 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '3199b050-310e-49d5-85bb-b769d54b9b30', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3fa4c6b590>]}
[0m16:19:10.578587 [info ] [MainThread]: Pulling freshness from warehouse metadata tables for 0 sources
[0m16:19:10.585242 [debug] [Thread-1 (]: Began running node source.dbt_capstone_andres.treasury.fiscal_data
[0m16:19:10.587223 [info ] [Thread-1 (]: 1 of 1 START freshness of treasury.fiscal_data ................................. [RUN]
[0m16:19:10.588987 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_DATAEXPERT_STUDENT_andres, now source.dbt_capstone_andres.treasury.fiscal_data)
[0m16:19:10.590525 [debug] [Thread-1 (]: Began compiling node source.dbt_capstone_andres.treasury.fiscal_data
[0m16:19:10.592003 [debug] [Thread-1 (]: Began executing node source.dbt_capstone_andres.treasury.fiscal_data
[0m16:19:10.607017 [debug] [Thread-1 (]: Using snowflake connection "source.dbt_capstone_andres.treasury.fiscal_data"
[0m16:19:10.608406 [debug] [Thread-1 (]: On source.dbt_capstone_andres.treasury.fiscal_data: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "source.dbt_capstone_andres.treasury.fiscal_data"} */
select
      max(record_date::timestamp) as max_loaded_at,
      convert_timezone('UTC', current_timestamp()) as snapshotted_at
    from dataexpert_student.andres.fiscal_data
[0m16:19:10.609972 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m16:19:11.267490 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 0.658 seconds
[0m16:19:11.269608 [debug] [Thread-1 (]: On source.dbt_capstone_andres.treasury.fiscal_data: Close
[0m16:19:11.502021 [info ] [Thread-1 (]: 1 of 1 PASS freshness of treasury.fiscal_data .................................. [[32mPASS[0m in 0.91s]
[0m16:19:11.503347 [debug] [Thread-1 (]: Finished running node source.dbt_capstone_andres.treasury.fiscal_data
[0m16:19:11.505558 [debug] [MainThread]: Connection 'master' was properly closed.
[0m16:19:11.506298 [debug] [MainThread]: Connection 'source.dbt_capstone_andres.treasury.fiscal_data' was properly closed.
[0m16:19:11.507150 [info ] [MainThread]: 
[0m16:19:11.511158 [info ] [MainThread]: Finished running 1 source in 0 hours 0 minutes and 2.11 seconds (2.11s).
[0m16:19:11.594648 [info ] [MainThread]: Done.
[0m16:19:11.596104 [debug] [MainThread]: Resource report: {"command_name": "freshness", "command_success": true, "command_wall_clock_time": 4.4258347, "process_user_time": 4.740557, "process_kernel_time": 0.881325, "process_mem_max_rss": "203588", "process_out_blocks": "3685", "process_in_blocks": "0"}
[0m16:19:11.596982 [debug] [MainThread]: Command `dbt source freshness` succeeded at 16:19:11.596875 after 4.43 seconds
[0m16:19:11.597612 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3fcbfbf790>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3fcbfbfa90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3fcfd03fd0>]}
[0m16:19:11.598396 [debug] [MainThread]: Flushing usage events
[0m16:19:11.820651 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m16:19:23.511343 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ffbb0325290>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ffbb0531750>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ffbb1e23d10>]}


============================== 16:19:23.517348 | 6fc66a8b-7cba-4bd7-b21d-0e4f96fac467 ==============================
[0m16:19:23.517348 [info ] [MainThread]: Running with dbt=1.9.0-b3
[0m16:19:23.521518 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/usr/local/airflow/dags/capstone/dbt_capstone_andres', 'debug': 'False', 'fail_fast': 'False', 'log_path': '/usr/local/airflow/dags/capstone/dbt_capstone_andres/logs', 'version_check': 'True', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'introspect': 'True', 'invocation_command': 'dbt source freshness', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m16:19:25.426281 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '6fc66a8b-7cba-4bd7-b21d-0e4f96fac467', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ffb890dc490>]}
[0m16:19:25.566115 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '6fc66a8b-7cba-4bd7-b21d-0e4f96fac467', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ffbb1717e50>]}
[0m16:19:25.569785 [info ] [MainThread]: Registered adapter: snowflake=1.8.4
[0m16:19:25.680057 [debug] [MainThread]: checksum: 5f4fde221ca8627174a0341efc73099aaec05b8546408ceed6f59b1ec0755115, vars: {}, profile: , target: , version: 1.9.0b3
[0m16:19:26.488240 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m16:19:26.489838 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m16:19:26.503748 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 2 unused configuration paths:
- models.staging
- models.marts
[0m16:19:26.599611 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '6fc66a8b-7cba-4bd7-b21d-0e4f96fac467', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ffb89070cd0>]}
[0m16:19:27.052545 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '6fc66a8b-7cba-4bd7-b21d-0e4f96fac467', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ffb88c19f50>]}
[0m16:19:27.054241 [info ] [MainThread]: Found 5 models, 2 data tests, 2 sources, 862 macros, 1 unit test
[0m16:19:27.056077 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6fc66a8b-7cba-4bd7-b21d-0e4f96fac467', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ffb88e88f10>]}
[0m16:19:27.061451 [info ] [MainThread]: 
[0m16:19:27.063131 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m16:19:27.064527 [info ] [MainThread]: 
[0m16:19:27.066494 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m16:19:27.078604 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_DATAEXPERT_STUDENT_andres'
[0m16:19:27.120658 [debug] [ThreadPool]: Using snowflake connection "list_DATAEXPERT_STUDENT_andres"
[0m16:19:27.121658 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "connection_name": "list_DATAEXPERT_STUDENT_andres"} */
show objects in DATAEXPERT_STUDENT.andres limit 10000
[0m16:19:27.122894 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:19:28.061563 [debug] [ThreadPool]: SQL status: SUCCESS 28 in 0.939 seconds
[0m16:19:28.070076 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: Close
[0m16:19:28.360368 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6fc66a8b-7cba-4bd7-b21d-0e4f96fac467', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ffb88b66e50>]}
[0m16:19:28.363730 [info ] [MainThread]: Pulling freshness from warehouse metadata tables for 0 sources
[0m16:19:28.375566 [debug] [Thread-1 (]: Began running node source.dbt_capstone_andres.treasury.fiscal_data
[0m16:19:28.378385 [info ] [Thread-1 (]: 1 of 1 START freshness of treasury.fiscal_data ................................. [RUN]
[0m16:19:28.381015 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_DATAEXPERT_STUDENT_andres, now source.dbt_capstone_andres.treasury.fiscal_data)
[0m16:19:28.383208 [debug] [Thread-1 (]: Began compiling node source.dbt_capstone_andres.treasury.fiscal_data
[0m16:19:28.386442 [debug] [Thread-1 (]: Began executing node source.dbt_capstone_andres.treasury.fiscal_data
[0m16:19:28.416750 [debug] [Thread-1 (]: Using snowflake connection "source.dbt_capstone_andres.treasury.fiscal_data"
[0m16:19:28.419047 [debug] [Thread-1 (]: On source.dbt_capstone_andres.treasury.fiscal_data: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "source.dbt_capstone_andres.treasury.fiscal_data"} */
select
      max(record_date::timestamp) as max_loaded_at,
      convert_timezone('UTC', current_timestamp()) as snapshotted_at
    from dataexpert_student.andres.fiscal_data
[0m16:19:28.420471 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m16:19:29.117355 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 0.697 seconds
[0m16:19:29.121073 [debug] [Thread-1 (]: On source.dbt_capstone_andres.treasury.fiscal_data: Close
[0m16:19:29.431724 [info ] [Thread-1 (]: 1 of 1 PASS freshness of treasury.fiscal_data .................................. [[32mPASS[0m in 1.05s]
[0m16:19:29.433326 [debug] [Thread-1 (]: Finished running node source.dbt_capstone_andres.treasury.fiscal_data
[0m16:19:29.436133 [debug] [MainThread]: Connection 'master' was properly closed.
[0m16:19:29.437046 [debug] [MainThread]: Connection 'source.dbt_capstone_andres.treasury.fiscal_data' was properly closed.
[0m16:19:29.438039 [info ] [MainThread]: 
[0m16:19:29.439420 [info ] [MainThread]: Finished running 1 source in 0 hours 0 minutes and 2.37 seconds (2.37s).
[0m16:19:29.608955 [info ] [MainThread]: Done.
[0m16:19:29.611820 [debug] [MainThread]: Resource report: {"command_name": "freshness", "command_success": true, "command_wall_clock_time": 6.2339234, "process_user_time": 5.754287, "process_kernel_time": 0.778904, "process_mem_max_rss": "209868", "process_out_blocks": "3685", "process_in_blocks": "0"}
[0m16:19:29.614001 [debug] [MainThread]: Command `dbt source freshness` succeeded at 16:19:29.613637 after 6.24 seconds
[0m16:19:29.615775 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ffbb062b950>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ffbb40d3f50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ffbb40dcbd0>]}
[0m16:19:29.618188 [debug] [MainThread]: Flushing usage events
[0m16:19:29.843125 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m16:19:39.271560 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe923608e10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe923608ad0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe923608ed0>]}


============================== 16:19:39.280447 | b0795f6d-44c4-4c86-b489-583c0051187e ==============================
[0m16:19:39.280447 [info ] [MainThread]: Running with dbt=1.9.0-b3
[0m16:19:39.283468 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'version_check': 'True', 'log_path': '/usr/local/airflow/dags/capstone/dbt_capstone_andres/logs', 'profiles_dir': '/usr/local/airflow/dags/capstone/dbt_capstone_andres', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'invocation_command': 'dbt source freshness', 'introspect': 'True', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m16:19:41.080949 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'b0795f6d-44c4-4c86-b489-583c0051187e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe8fc3a1590>]}
[0m16:19:41.196356 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'b0795f6d-44c4-4c86-b489-583c0051187e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe924a1b550>]}
[0m16:19:41.199542 [info ] [MainThread]: Registered adapter: snowflake=1.8.4
[0m16:19:41.282404 [debug] [MainThread]: checksum: 5f4fde221ca8627174a0341efc73099aaec05b8546408ceed6f59b1ec0755115, vars: {}, profile: , target: , version: 1.9.0b3
[0m16:19:42.123210 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m16:19:42.123983 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m16:19:42.132884 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 2 unused configuration paths:
- models.marts
- models.staging
[0m16:19:42.189088 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'b0795f6d-44c4-4c86-b489-583c0051187e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe8f7fd27d0>]}
[0m16:19:42.520044 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'b0795f6d-44c4-4c86-b489-583c0051187e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe8fc1be110>]}
[0m16:19:42.521368 [info ] [MainThread]: Found 5 models, 2 data tests, 2 sources, 862 macros, 1 unit test
[0m16:19:42.522635 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b0795f6d-44c4-4c86-b489-583c0051187e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe8fc11cf10>]}
[0m16:19:42.526292 [info ] [MainThread]: 
[0m16:19:42.527425 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m16:19:42.528932 [info ] [MainThread]: 
[0m16:19:42.530390 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m16:19:42.539192 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_DATAEXPERT_STUDENT_andres'
[0m16:19:42.575780 [debug] [ThreadPool]: Using snowflake connection "list_DATAEXPERT_STUDENT_andres"
[0m16:19:42.577230 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "connection_name": "list_DATAEXPERT_STUDENT_andres"} */
show objects in DATAEXPERT_STUDENT.andres limit 10000
[0m16:19:42.578734 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:19:43.611740 [debug] [ThreadPool]: SQL status: SUCCESS 28 in 1.033 seconds
[0m16:19:43.617587 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: Close
[0m16:19:43.882809 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b0795f6d-44c4-4c86-b489-583c0051187e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe9242e8490>]}
[0m16:19:43.883949 [info ] [MainThread]: Pulling freshness from warehouse metadata tables for 0 sources
[0m16:19:43.889938 [debug] [Thread-1 (]: Began running node source.dbt_capstone_andres.treasury.fiscal_data
[0m16:19:43.891199 [info ] [Thread-1 (]: 1 of 1 START freshness of treasury.fiscal_data ................................. [RUN]
[0m16:19:43.892590 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_DATAEXPERT_STUDENT_andres, now source.dbt_capstone_andres.treasury.fiscal_data)
[0m16:19:43.895487 [debug] [Thread-1 (]: Began compiling node source.dbt_capstone_andres.treasury.fiscal_data
[0m16:19:43.897312 [debug] [Thread-1 (]: Began executing node source.dbt_capstone_andres.treasury.fiscal_data
[0m16:19:43.913531 [debug] [Thread-1 (]: Using snowflake connection "source.dbt_capstone_andres.treasury.fiscal_data"
[0m16:19:43.916179 [debug] [Thread-1 (]: On source.dbt_capstone_andres.treasury.fiscal_data: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "source.dbt_capstone_andres.treasury.fiscal_data"} */
select
      max(record_date::timestamp) as max_loaded_at,
      convert_timezone('UTC', current_timestamp()) as snapshotted_at
    from dataexpert_student.andres.fiscal_data
[0m16:19:43.917980 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m16:19:44.702496 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 0.785 seconds
[0m16:19:44.705726 [debug] [Thread-1 (]: On source.dbt_capstone_andres.treasury.fiscal_data: Close
[0m16:19:44.937278 [info ] [Thread-1 (]: 1 of 1 PASS freshness of treasury.fiscal_data .................................. [[32mPASS[0m in 1.04s]
[0m16:19:44.939383 [debug] [Thread-1 (]: Finished running node source.dbt_capstone_andres.treasury.fiscal_data
[0m16:19:44.943580 [debug] [MainThread]: Connection 'master' was properly closed.
[0m16:19:44.945174 [debug] [MainThread]: Connection 'source.dbt_capstone_andres.treasury.fiscal_data' was properly closed.
[0m16:19:44.946684 [info ] [MainThread]: 
[0m16:19:44.947800 [info ] [MainThread]: Finished running 1 source in 0 hours 0 minutes and 2.42 seconds (2.42s).
[0m16:19:45.140875 [info ] [MainThread]: Done.
[0m16:19:45.143236 [debug] [MainThread]: Resource report: {"command_name": "freshness", "command_success": true, "command_wall_clock_time": 6.0024085, "process_user_time": 5.70815, "process_kernel_time": 0.942723, "process_mem_max_rss": "209308", "process_out_blocks": "3685", "process_in_blocks": "0"}
[0m16:19:45.145462 [debug] [MainThread]: Command `dbt source freshness` succeeded at 16:19:45.145175 after 6.00 seconds
[0m16:19:45.146947 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe923656650>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe92390b990>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe927439550>]}
[0m16:19:45.148371 [debug] [MainThread]: Flushing usage events
[0m16:19:45.381911 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m16:20:06.955599 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f310e91ded0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f310e9fc810>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f310e91e1d0>]}


============================== 16:20:06.961818 | 078a55d6-97cf-431e-83f8-54cb04845dcd ==============================
[0m16:20:06.961818 [info ] [MainThread]: Running with dbt=1.9.0-b3
[0m16:20:06.963401 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'version_check': 'True', 'log_path': '/usr/local/airflow/dags/capstone/dbt_capstone_andres/logs', 'fail_fast': 'False', 'profiles_dir': '/usr/local/airflow/dags/capstone/dbt_capstone_andres', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'static_parser': 'True', 'introspect': 'True', 'invocation_command': 'dbt source freshness', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m16:20:08.727023 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '078a55d6-97cf-431e-83f8-54cb04845dcd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f30e36afc50>]}
[0m16:20:08.817597 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '078a55d6-97cf-431e-83f8-54cb04845dcd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f310ea1d690>]}
[0m16:20:08.819180 [info ] [MainThread]: Registered adapter: snowflake=1.8.4
[0m16:20:08.877451 [debug] [MainThread]: checksum: 5f4fde221ca8627174a0341efc73099aaec05b8546408ceed6f59b1ec0755115, vars: {}, profile: , target: , version: 1.9.0b3
[0m16:20:09.556454 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m16:20:09.557561 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m16:20:09.567907 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 2 unused configuration paths:
- models.marts
- models.staging
[0m16:20:09.637520 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '078a55d6-97cf-431e-83f8-54cb04845dcd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f30e32d5850>]}
[0m16:20:09.904832 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '078a55d6-97cf-431e-83f8-54cb04845dcd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f30e34c5d50>]}
[0m16:20:09.906292 [info ] [MainThread]: Found 5 models, 2 data tests, 2 sources, 862 macros, 1 unit test
[0m16:20:09.907456 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '078a55d6-97cf-431e-83f8-54cb04845dcd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f30e3428c90>]}
[0m16:20:09.910558 [info ] [MainThread]: 
[0m16:20:09.911656 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m16:20:09.912889 [info ] [MainThread]: 
[0m16:20:09.914726 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m16:20:09.922716 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_DATAEXPERT_STUDENT_andres'
[0m16:20:09.948234 [debug] [ThreadPool]: Using snowflake connection "list_DATAEXPERT_STUDENT_andres"
[0m16:20:09.949281 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "connection_name": "list_DATAEXPERT_STUDENT_andres"} */
show objects in DATAEXPERT_STUDENT.andres limit 10000
[0m16:20:09.950087 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:20:10.857840 [debug] [ThreadPool]: SQL status: SUCCESS 28 in 0.908 seconds
[0m16:20:10.867548 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: Close
[0m16:20:11.094282 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '078a55d6-97cf-431e-83f8-54cb04845dcd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f30e312b410>]}
[0m16:20:11.095579 [info ] [MainThread]: Pulling freshness from warehouse metadata tables for 0 sources
[0m16:20:11.100992 [debug] [Thread-1 (]: Began running node source.dbt_capstone_andres.treasury.fiscal_data
[0m16:20:11.102335 [info ] [Thread-1 (]: 1 of 1 START freshness of treasury.fiscal_data ................................. [RUN]
[0m16:20:11.103335 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_DATAEXPERT_STUDENT_andres, now source.dbt_capstone_andres.treasury.fiscal_data)
[0m16:20:11.104577 [debug] [Thread-1 (]: Began compiling node source.dbt_capstone_andres.treasury.fiscal_data
[0m16:20:11.105783 [debug] [Thread-1 (]: Began executing node source.dbt_capstone_andres.treasury.fiscal_data
[0m16:20:11.120388 [debug] [Thread-1 (]: Using snowflake connection "source.dbt_capstone_andres.treasury.fiscal_data"
[0m16:20:11.121282 [debug] [Thread-1 (]: On source.dbt_capstone_andres.treasury.fiscal_data: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "source.dbt_capstone_andres.treasury.fiscal_data"} */
select
      max(record_date::timestamp) as max_loaded_at,
      convert_timezone('UTC', current_timestamp()) as snapshotted_at
    from dataexpert_student.andres.fiscal_data
[0m16:20:11.122462 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m16:20:11.819390 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 0.697 seconds
[0m16:20:11.821820 [debug] [Thread-1 (]: On source.dbt_capstone_andres.treasury.fiscal_data: Close
[0m16:20:12.065633 [info ] [Thread-1 (]: 1 of 1 PASS freshness of treasury.fiscal_data .................................. [[32mPASS[0m in 0.96s]
[0m16:20:12.068720 [debug] [Thread-1 (]: Finished running node source.dbt_capstone_andres.treasury.fiscal_data
[0m16:20:12.071606 [debug] [MainThread]: Connection 'master' was properly closed.
[0m16:20:12.072504 [debug] [MainThread]: Connection 'source.dbt_capstone_andres.treasury.fiscal_data' was properly closed.
[0m16:20:12.073939 [info ] [MainThread]: 
[0m16:20:12.074864 [info ] [MainThread]: Finished running 1 source in 0 hours 0 minutes and 2.16 seconds (2.16s).
[0m16:20:12.225097 [info ] [MainThread]: Done.
[0m16:20:12.227143 [debug] [MainThread]: Resource report: {"command_name": "freshness", "command_success": true, "command_wall_clock_time": 5.3806806, "process_user_time": 5.330889, "process_kernel_time": 0.784075, "process_mem_max_rss": "209740", "process_out_blocks": "3685", "process_in_blocks": "0"}
[0m16:20:12.228886 [debug] [MainThread]: Command `dbt source freshness` succeeded at 16:20:12.228649 after 5.38 seconds
[0m16:20:12.229963 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f310e96ac10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f310e987c90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f31127754d0>]}
[0m16:20:12.230968 [debug] [MainThread]: Flushing usage events
[0m16:20:12.458242 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m16:20:26.181644 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8069a6d290>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8069a6c910>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8069a6d2d0>]}


============================== 16:20:26.188622 | 52f9ae41-0168-4dce-acc3-3cf34fd84591 ==============================
[0m16:20:26.188622 [info ] [MainThread]: Running with dbt=1.9.0-b3
[0m16:20:26.190818 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'debug': 'False', 'log_path': '/usr/local/airflow/dags/capstone/dbt_capstone_andres/logs', 'profiles_dir': '/usr/local/airflow/dags/capstone/dbt_capstone_andres', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'invocation_command': 'dbt source freshness', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m16:20:28.111569 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '52f9ae41-0168-4dce-acc3-3cf34fd84591', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8042783490>]}
[0m16:20:28.265629 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '52f9ae41-0168-4dce-acc3-3cf34fd84591', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8069b71390>]}
[0m16:20:28.271288 [info ] [MainThread]: Registered adapter: snowflake=1.8.4
[0m16:20:28.442579 [debug] [MainThread]: checksum: 5f4fde221ca8627174a0341efc73099aaec05b8546408ceed6f59b1ec0755115, vars: {}, profile: , target: , version: 1.9.0b3
[0m16:20:29.426300 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m16:20:29.427559 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m16:20:29.441828 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 2 unused configuration paths:
- models.staging
- models.marts
[0m16:20:29.529001 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '52f9ae41-0168-4dce-acc3-3cf34fd84591', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8042220a50>]}
[0m16:20:29.940894 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '52f9ae41-0168-4dce-acc3-3cf34fd84591', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f80424c1e10>]}
[0m16:20:29.942529 [info ] [MainThread]: Found 5 models, 2 data tests, 2 sources, 862 macros, 1 unit test
[0m16:20:29.943625 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '52f9ae41-0168-4dce-acc3-3cf34fd84591', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f806bcd1f10>]}
[0m16:20:29.947739 [info ] [MainThread]: 
[0m16:20:29.949131 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m16:20:29.950359 [info ] [MainThread]: 
[0m16:20:29.952932 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m16:20:29.970913 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_DATAEXPERT_STUDENT_andres'
[0m16:20:30.027840 [debug] [ThreadPool]: Using snowflake connection "list_DATAEXPERT_STUDENT_andres"
[0m16:20:30.030289 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "connection_name": "list_DATAEXPERT_STUDENT_andres"} */
show objects in DATAEXPERT_STUDENT.andres limit 10000
[0m16:20:30.031803 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:20:30.978099 [debug] [ThreadPool]: SQL status: SUCCESS 28 in 0.946 seconds
[0m16:20:30.983887 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: Close
[0m16:20:31.243603 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '52f9ae41-0168-4dce-acc3-3cf34fd84591', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f806ae87850>]}
[0m16:20:31.247247 [info ] [MainThread]: Pulling freshness from warehouse metadata tables for 0 sources
[0m16:20:31.258480 [debug] [Thread-1 (]: Began running node source.dbt_capstone_andres.treasury.fiscal_data
[0m16:20:31.260635 [info ] [Thread-1 (]: 1 of 1 START freshness of treasury.fiscal_data ................................. [RUN]
[0m16:20:31.262755 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_DATAEXPERT_STUDENT_andres, now source.dbt_capstone_andres.treasury.fiscal_data)
[0m16:20:31.264671 [debug] [Thread-1 (]: Began compiling node source.dbt_capstone_andres.treasury.fiscal_data
[0m16:20:31.266592 [debug] [Thread-1 (]: Began executing node source.dbt_capstone_andres.treasury.fiscal_data
[0m16:20:31.291167 [debug] [Thread-1 (]: Using snowflake connection "source.dbt_capstone_andres.treasury.fiscal_data"
[0m16:20:31.293050 [debug] [Thread-1 (]: On source.dbt_capstone_andres.treasury.fiscal_data: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "source.dbt_capstone_andres.treasury.fiscal_data"} */
select
      max(record_date::timestamp) as max_loaded_at,
      convert_timezone('UTC', current_timestamp()) as snapshotted_at
    from dataexpert_student.andres.fiscal_data
[0m16:20:31.295024 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m16:20:32.311179 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 1.016 seconds
[0m16:20:32.315145 [debug] [Thread-1 (]: On source.dbt_capstone_andres.treasury.fiscal_data: Close
[0m16:20:32.551765 [info ] [Thread-1 (]: 1 of 1 PASS freshness of treasury.fiscal_data .................................. [[32mPASS[0m in 1.29s]
[0m16:20:32.553274 [debug] [Thread-1 (]: Finished running node source.dbt_capstone_andres.treasury.fiscal_data
[0m16:20:32.556397 [debug] [MainThread]: Connection 'master' was properly closed.
[0m16:20:32.557401 [debug] [MainThread]: Connection 'source.dbt_capstone_andres.treasury.fiscal_data' was properly closed.
[0m16:20:32.559040 [info ] [MainThread]: 
[0m16:20:32.560412 [info ] [MainThread]: Finished running 1 source in 0 hours 0 minutes and 2.61 seconds (2.61s).
[0m16:20:32.747883 [info ] [MainThread]: Done.
[0m16:20:32.751202 [debug] [MainThread]: Resource report: {"command_name": "freshness", "command_success": true, "command_wall_clock_time": 6.683327, "process_user_time": 6.253276, "process_kernel_time": 0.90982, "process_mem_max_rss": "209628", "process_out_blocks": "3685", "process_in_blocks": "0"}
[0m16:20:32.753152 [debug] [MainThread]: Command `dbt source freshness` succeeded at 16:20:32.752894 after 6.69 seconds
[0m16:20:32.755424 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8069ab8d10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f806d875590>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f806d7f4b90>]}
[0m16:20:32.756884 [debug] [MainThread]: Flushing usage events
[0m16:20:33.003342 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m16:20:47.303653 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f574684cd90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5746b8ccd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5746846690>]}


============================== 16:20:47.310007 | 3a8bb34e-b09c-45ec-84d1-2aaf2e15fc5c ==============================
[0m16:20:47.310007 [info ] [MainThread]: Running with dbt=1.9.0-b3
[0m16:20:47.311838 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/usr/local/airflow/dags/capstone/dbt_capstone_andres', 'version_check': 'True', 'fail_fast': 'False', 'log_path': '/usr/local/airflow/dags/capstone/dbt_capstone_andres/logs', 'debug': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt source freshness', 'static_parser': 'True', 'introspect': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m16:20:48.637991 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '3a8bb34e-b09c-45ec-84d1-2aaf2e15fc5c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5749227e10>]}
[0m16:20:48.754719 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '3a8bb34e-b09c-45ec-84d1-2aaf2e15fc5c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5746951790>]}
[0m16:20:48.756694 [info ] [MainThread]: Registered adapter: snowflake=1.8.4
[0m16:20:48.840985 [debug] [MainThread]: checksum: 5f4fde221ca8627174a0341efc73099aaec05b8546408ceed6f59b1ec0755115, vars: {}, profile: , target: , version: 1.9.0b3
[0m16:20:49.598829 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m16:20:49.599698 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m16:20:49.606636 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 2 unused configuration paths:
- models.staging
- models.marts
[0m16:20:49.668096 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '3a8bb34e-b09c-45ec-84d1-2aaf2e15fc5c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f571b068950>]}
[0m16:20:49.951389 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '3a8bb34e-b09c-45ec-84d1-2aaf2e15fc5c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f571b2d1b90>]}
[0m16:20:49.954583 [info ] [MainThread]: Found 5 models, 2 data tests, 2 sources, 862 macros, 1 unit test
[0m16:20:49.959106 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '3a8bb34e-b09c-45ec-84d1-2aaf2e15fc5c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f571b254bd0>]}
[0m16:20:49.964492 [info ] [MainThread]: 
[0m16:20:49.967873 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m16:20:49.970811 [info ] [MainThread]: 
[0m16:20:49.972637 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m16:20:49.985141 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_DATAEXPERT_STUDENT_andres'
[0m16:20:50.023020 [debug] [ThreadPool]: Using snowflake connection "list_DATAEXPERT_STUDENT_andres"
[0m16:20:50.024305 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "connection_name": "list_DATAEXPERT_STUDENT_andres"} */
show objects in DATAEXPERT_STUDENT.andres limit 10000
[0m16:20:50.025283 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:20:50.814355 [debug] [ThreadPool]: SQL status: SUCCESS 28 in 0.789 seconds
[0m16:20:50.818422 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: Close
[0m16:20:51.049677 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '3a8bb34e-b09c-45ec-84d1-2aaf2e15fc5c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f571b11d550>]}
[0m16:20:51.051351 [info ] [MainThread]: Pulling freshness from warehouse metadata tables for 0 sources
[0m16:20:51.058518 [debug] [Thread-1 (]: Began running node source.dbt_capstone_andres.treasury.fiscal_data
[0m16:20:51.060578 [info ] [Thread-1 (]: 1 of 1 START freshness of treasury.fiscal_data ................................. [RUN]
[0m16:20:51.062408 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_DATAEXPERT_STUDENT_andres, now source.dbt_capstone_andres.treasury.fiscal_data)
[0m16:20:51.063554 [debug] [Thread-1 (]: Began compiling node source.dbt_capstone_andres.treasury.fiscal_data
[0m16:20:51.064956 [debug] [Thread-1 (]: Began executing node source.dbt_capstone_andres.treasury.fiscal_data
[0m16:20:51.079015 [debug] [Thread-1 (]: Using snowflake connection "source.dbt_capstone_andres.treasury.fiscal_data"
[0m16:20:51.080494 [debug] [Thread-1 (]: On source.dbt_capstone_andres.treasury.fiscal_data: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "source.dbt_capstone_andres.treasury.fiscal_data"} */
select
      max(record_date::timestamp) as max_loaded_at,
      convert_timezone('UTC', current_timestamp()) as snapshotted_at
    from dataexpert_student.andres.fiscal_data
[0m16:20:51.081738 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m16:20:51.742907 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 0.661 seconds
[0m16:20:51.745901 [debug] [Thread-1 (]: On source.dbt_capstone_andres.treasury.fiscal_data: Close
[0m16:20:51.978053 [info ] [Thread-1 (]: 1 of 1 PASS freshness of treasury.fiscal_data .................................. [[32mPASS[0m in 0.92s]
[0m16:20:51.979393 [debug] [Thread-1 (]: Finished running node source.dbt_capstone_andres.treasury.fiscal_data
[0m16:20:51.981729 [debug] [MainThread]: Connection 'master' was properly closed.
[0m16:20:51.982667 [debug] [MainThread]: Connection 'source.dbt_capstone_andres.treasury.fiscal_data' was properly closed.
[0m16:20:51.984726 [info ] [MainThread]: 
[0m16:20:51.985611 [info ] [MainThread]: Finished running 1 source in 0 hours 0 minutes and 2.01 seconds (2.01s).
[0m16:20:52.127818 [info ] [MainThread]: Done.
[0m16:20:52.129611 [debug] [MainThread]: Resource report: {"command_name": "freshness", "command_success": true, "command_wall_clock_time": 4.932022, "process_user_time": 5.00258, "process_kernel_time": 0.740386, "process_mem_max_rss": "210100", "process_out_blocks": "3685", "process_in_blocks": "0"}
[0m16:20:52.130972 [debug] [MainThread]: Command `dbt source freshness` succeeded at 16:20:52.130789 after 4.93 seconds
[0m16:20:52.132021 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f574a7b4910>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f574a5ebfd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f574a5f4bd0>]}
[0m16:20:52.132944 [debug] [MainThread]: Flushing usage events
[0m16:20:52.354212 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m16:21:12.408094 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa870a45310>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa870c51750>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa870f43a10>]}


============================== 16:21:12.412863 | 1e100e77-e77d-4dd0-89a9-3052c85a0ae1 ==============================
[0m16:21:12.412863 [info ] [MainThread]: Running with dbt=1.9.0-b3
[0m16:21:12.414587 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/usr/local/airflow/dags/capstone/dbt_capstone_andres', 'fail_fast': 'False', 'debug': 'False', 'log_path': '/usr/local/airflow/dags/capstone/dbt_capstone_andres/logs', 'warn_error': 'None', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'invocation_command': 'dbt source freshness', 'introspect': 'True', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m16:21:12.649889 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f025b1ec3d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f025a7e8590>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f025a725f10>]}


============================== 16:21:12.658341 | 9fb40a15-1e81-4276-8c6c-bdf23b0149aa ==============================
[0m16:21:12.658341 [info ] [MainThread]: Running with dbt=1.9.0-b3
[0m16:21:12.660102 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/usr/local/airflow/dags/capstone/dbt_capstone_andres', 'debug': 'False', 'version_check': 'True', 'log_path': '/usr/local/airflow/dags/capstone/dbt_capstone_andres/logs', 'warn_error': 'None', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt source freshness', 'log_format': 'default', 'introspect': 'True', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m16:21:14.014628 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '1e100e77-e77d-4dd0-89a9-3052c85a0ae1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa86cf22d90>]}
[0m16:21:14.182855 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '1e100e77-e77d-4dd0-89a9-3052c85a0ae1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa8497f2e90>]}
[0m16:21:14.186674 [info ] [MainThread]: Registered adapter: snowflake=1.8.4
[0m16:21:14.346312 [debug] [MainThread]: checksum: 5f4fde221ca8627174a0341efc73099aaec05b8546408ceed6f59b1ec0755115, vars: {}, profile: , target: , version: 1.9.0b3
[0m16:21:14.402482 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '9fb40a15-1e81-4276-8c6c-bdf23b0149aa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0256046150>]}
[0m16:21:14.558479 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '9fb40a15-1e81-4276-8c6c-bdf23b0149aa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f025b2a2f50>]}
[0m16:21:14.561328 [info ] [MainThread]: Registered adapter: snowflake=1.8.4
[0m16:21:14.714305 [debug] [MainThread]: checksum: 5f4fde221ca8627174a0341efc73099aaec05b8546408ceed6f59b1ec0755115, vars: {}, profile: , target: , version: 1.9.0b3
[0m16:21:15.332060 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m16:21:15.333552 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m16:21:15.349863 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 2 unused configuration paths:
- models.staging
- models.marts
[0m16:21:15.461275 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '1e100e77-e77d-4dd0-89a9-3052c85a0ae1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa849389590>]}
[0m16:21:15.601735 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m16:21:15.607728 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m16:21:15.621100 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 2 unused configuration paths:
- models.staging
- models.marts
[0m16:21:15.715182 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '9fb40a15-1e81-4276-8c6c-bdf23b0149aa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f022ed0b790>]}
[0m16:21:15.796579 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '1e100e77-e77d-4dd0-89a9-3052c85a0ae1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa8492961d0>]}
[0m16:21:15.798041 [info ] [MainThread]: Found 5 models, 2 data tests, 2 sources, 862 macros, 1 unit test
[0m16:21:15.799027 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '1e100e77-e77d-4dd0-89a9-3052c85a0ae1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa849500e10>]}
[0m16:21:15.801802 [info ] [MainThread]: 
[0m16:21:15.802979 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m16:21:15.804059 [info ] [MainThread]: 
[0m16:21:15.805925 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m16:21:15.814897 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_DATAEXPERT_STUDENT_andres'
[0m16:21:15.848356 [debug] [ThreadPool]: Using snowflake connection "list_DATAEXPERT_STUDENT_andres"
[0m16:21:15.849687 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "connection_name": "list_DATAEXPERT_STUDENT_andres"} */
show objects in DATAEXPERT_STUDENT.andres limit 10000
[0m16:21:15.850799 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:21:16.075957 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '9fb40a15-1e81-4276-8c6c-bdf23b0149aa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f022eda2090>]}
[0m16:21:16.079668 [info ] [MainThread]: Found 5 models, 2 data tests, 2 sources, 862 macros, 1 unit test
[0m16:21:16.081086 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9fb40a15-1e81-4276-8c6c-bdf23b0149aa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f022f010c90>]}
[0m16:21:16.084103 [info ] [MainThread]: 
[0m16:21:16.085349 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m16:21:16.086319 [info ] [MainThread]: 
[0m16:21:16.087546 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m16:21:16.094051 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_DATAEXPERT_STUDENT_andres'
[0m16:21:16.119095 [debug] [ThreadPool]: Using snowflake connection "list_DATAEXPERT_STUDENT_andres"
[0m16:21:16.120069 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "connection_name": "list_DATAEXPERT_STUDENT_andres"} */
show objects in DATAEXPERT_STUDENT.andres limit 10000
[0m16:21:16.120964 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:21:16.700548 [debug] [ThreadPool]: SQL status: SUCCESS 28 in 0.850 seconds
[0m16:21:16.706364 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: Close
[0m16:21:16.948022 [debug] [ThreadPool]: SQL status: SUCCESS 28 in 0.827 seconds
[0m16:21:16.951885 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: Close
[0m16:21:16.954208 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '1e100e77-e77d-4dd0-89a9-3052c85a0ae1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa871b521d0>]}
[0m16:21:16.955268 [info ] [MainThread]: Pulling freshness from warehouse metadata tables for 0 sources
[0m16:21:16.960286 [debug] [Thread-1 (]: Began running node source.dbt_capstone_andres.treasury.fiscal_data
[0m16:21:16.963981 [info ] [Thread-1 (]: 1 of 1 START freshness of treasury.fiscal_data ................................. [RUN]
[0m16:21:16.964998 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_DATAEXPERT_STUDENT_andres, now source.dbt_capstone_andres.treasury.fiscal_data)
[0m16:21:16.965711 [debug] [Thread-1 (]: Began compiling node source.dbt_capstone_andres.treasury.fiscal_data
[0m16:21:16.966377 [debug] [Thread-1 (]: Began executing node source.dbt_capstone_andres.treasury.fiscal_data
[0m16:21:16.976869 [debug] [Thread-1 (]: Using snowflake connection "source.dbt_capstone_andres.treasury.fiscal_data"
[0m16:21:16.977708 [debug] [Thread-1 (]: On source.dbt_capstone_andres.treasury.fiscal_data: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "source.dbt_capstone_andres.treasury.fiscal_data"} */
select
      max(record_date::timestamp) as max_loaded_at,
      convert_timezone('UTC', current_timestamp()) as snapshotted_at
    from dataexpert_student.andres.fiscal_data
[0m16:21:16.978391 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m16:21:17.192956 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9fb40a15-1e81-4276-8c6c-bdf23b0149aa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f025a618bd0>]}
[0m16:21:17.193898 [info ] [MainThread]: Pulling freshness from warehouse metadata tables for 0 sources
[0m16:21:17.197851 [debug] [Thread-1 (]: Began running node source.dbt_capstone_andres.treasury.fiscal_data
[0m16:21:17.198686 [info ] [Thread-1 (]: 1 of 1 START freshness of treasury.fiscal_data ................................. [RUN]
[0m16:21:17.199458 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_DATAEXPERT_STUDENT_andres, now source.dbt_capstone_andres.treasury.fiscal_data)
[0m16:21:17.200165 [debug] [Thread-1 (]: Began compiling node source.dbt_capstone_andres.treasury.fiscal_data
[0m16:21:17.200908 [debug] [Thread-1 (]: Began executing node source.dbt_capstone_andres.treasury.fiscal_data
[0m16:21:17.210853 [debug] [Thread-1 (]: Using snowflake connection "source.dbt_capstone_andres.treasury.fiscal_data"
[0m16:21:17.211814 [debug] [Thread-1 (]: On source.dbt_capstone_andres.treasury.fiscal_data: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "source.dbt_capstone_andres.treasury.fiscal_data"} */
select
      max(record_date::timestamp) as max_loaded_at,
      convert_timezone('UTC', current_timestamp()) as snapshotted_at
    from dataexpert_student.andres.fiscal_data
[0m16:21:17.212702 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m16:21:17.698712 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 0.720 seconds
[0m16:21:17.700721 [debug] [Thread-1 (]: On source.dbt_capstone_andres.treasury.fiscal_data: Close
[0m16:21:17.968580 [info ] [Thread-1 (]: 1 of 1 PASS freshness of treasury.fiscal_data .................................. [[32mPASS[0m in 1.00s]
[0m16:21:17.971591 [debug] [Thread-1 (]: Finished running node source.dbt_capstone_andres.treasury.fiscal_data
[0m16:21:17.974996 [debug] [MainThread]: Connection 'master' was properly closed.
[0m16:21:17.976020 [debug] [MainThread]: Connection 'source.dbt_capstone_andres.treasury.fiscal_data' was properly closed.
[0m16:21:17.976958 [info ] [MainThread]: 
[0m16:21:17.977940 [info ] [MainThread]: Finished running 1 source in 0 hours 0 minutes and 2.17 seconds (2.17s).
[0m16:21:18.026960 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 0.814 seconds
[0m16:21:18.028945 [debug] [Thread-1 (]: On source.dbt_capstone_andres.treasury.fiscal_data: Close
[0m16:21:18.066754 [info ] [MainThread]: Done.
[0m16:21:18.068251 [debug] [MainThread]: Resource report: {"command_name": "freshness", "command_success": true, "command_wall_clock_time": 5.761003, "process_user_time": 5.628878, "process_kernel_time": 0.92706, "process_mem_max_rss": "210240", "process_out_blocks": "3685", "process_in_blocks": "0"}
[0m16:21:18.069243 [debug] [MainThread]: Command `dbt source freshness` succeeded at 16:21:18.069114 after 5.76 seconds
[0m16:21:18.070218 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa870d43b90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa8708b3f90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa8747cffd0>]}
[0m16:21:18.071254 [debug] [MainThread]: Flushing usage events
[0m16:21:18.264588 [info ] [Thread-1 (]: 1 of 1 PASS freshness of treasury.fiscal_data .................................. [[32mPASS[0m in 1.07s]
[0m16:21:18.266489 [debug] [Thread-1 (]: Finished running node source.dbt_capstone_andres.treasury.fiscal_data
[0m16:21:18.271505 [debug] [MainThread]: Connection 'master' was properly closed.
[0m16:21:18.272852 [debug] [MainThread]: Connection 'source.dbt_capstone_andres.treasury.fiscal_data' was properly closed.
[0m16:21:18.274147 [info ] [MainThread]: 
[0m16:21:18.275373 [info ] [MainThread]: Finished running 1 source in 0 hours 0 minutes and 2.19 seconds (2.19s).
[0m16:21:18.291860 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m16:21:18.470723 [info ] [MainThread]: Done.
[0m16:21:18.473215 [debug] [MainThread]: Resource report: {"command_name": "freshness", "command_success": true, "command_wall_clock_time": 5.967017, "process_user_time": 5.708236, "process_kernel_time": 1.012143, "process_mem_max_rss": "209928", "process_out_blocks": "3685", "process_in_blocks": "0"}
[0m16:21:18.474503 [debug] [MainThread]: Command `dbt source freshness` succeeded at 16:21:18.474358 after 5.97 seconds
[0m16:21:18.475701 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f025a38be90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f025e2bffd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f022f0b00d0>]}
[0m16:21:18.476750 [debug] [MainThread]: Flushing usage events
[0m16:21:18.705524 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m16:21:40.776830 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0b263bd110>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0b265c9990>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0b263bc750>]}


============================== 16:21:40.783725 | e4b51308-28f6-4f47-b118-26a849d58618 ==============================
[0m16:21:40.783725 [info ] [MainThread]: Running with dbt=1.9.0-b3
[0m16:21:40.785909 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'debug': 'False', 'log_path': '/usr/local/airflow/dags/capstone/dbt_capstone_andres/logs', 'profiles_dir': '/usr/local/airflow/dags/capstone/dbt_capstone_andres', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'log_format': 'default', 'introspect': 'True', 'invocation_command': 'dbt source freshness', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m16:21:42.955857 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'e4b51308-28f6-4f47-b118-26a849d58618', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0afafe0450>]}
[0m16:21:43.110912 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'e4b51308-28f6-4f47-b118-26a849d58618', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0b1e576050>]}
[0m16:21:43.116088 [info ] [MainThread]: Registered adapter: snowflake=1.8.4
[0m16:21:43.323191 [debug] [MainThread]: checksum: 5f4fde221ca8627174a0341efc73099aaec05b8546408ceed6f59b1ec0755115, vars: {}, profile: , target: , version: 1.9.0b3
[0m16:21:44.590831 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m16:21:44.592609 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m16:21:44.617514 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 2 unused configuration paths:
- models.staging
- models.marts
[0m16:21:44.765902 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'e4b51308-28f6-4f47-b118-26a849d58618', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0afabb8ad0>]}
[0m16:21:45.203513 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'e4b51308-28f6-4f47-b118-26a849d58618', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0afaf72610>]}
[0m16:21:45.205508 [info ] [MainThread]: Found 5 models, 2 data tests, 2 sources, 862 macros, 1 unit test
[0m16:21:45.206940 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e4b51308-28f6-4f47-b118-26a849d58618', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0afaf52650>]}
[0m16:21:45.211860 [info ] [MainThread]: 
[0m16:21:45.214081 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m16:21:45.215666 [info ] [MainThread]: 
[0m16:21:45.217675 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m16:21:45.229079 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_DATAEXPERT_STUDENT_andres'
[0m16:21:45.271309 [debug] [ThreadPool]: Using snowflake connection "list_DATAEXPERT_STUDENT_andres"
[0m16:21:45.272748 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "connection_name": "list_DATAEXPERT_STUDENT_andres"} */
show objects in DATAEXPERT_STUDENT.andres limit 10000
[0m16:21:45.273952 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:21:46.274892 [debug] [ThreadPool]: SQL status: SUCCESS 28 in 1.001 seconds
[0m16:21:46.285299 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: Close
[0m16:21:46.613603 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e4b51308-28f6-4f47-b118-26a849d58618', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0b283eb490>]}
[0m16:21:46.615074 [info ] [MainThread]: Pulling freshness from warehouse metadata tables for 0 sources
[0m16:21:46.627591 [debug] [Thread-1 (]: Began running node source.dbt_capstone_andres.treasury.fiscal_data
[0m16:21:46.630225 [info ] [Thread-1 (]: 1 of 1 START freshness of treasury.fiscal_data ................................. [RUN]
[0m16:21:46.632163 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_DATAEXPERT_STUDENT_andres, now source.dbt_capstone_andres.treasury.fiscal_data)
[0m16:21:46.637710 [debug] [Thread-1 (]: Began compiling node source.dbt_capstone_andres.treasury.fiscal_data
[0m16:21:46.638876 [debug] [Thread-1 (]: Began executing node source.dbt_capstone_andres.treasury.fiscal_data
[0m16:21:46.674342 [debug] [Thread-1 (]: Using snowflake connection "source.dbt_capstone_andres.treasury.fiscal_data"
[0m16:21:46.679730 [debug] [Thread-1 (]: On source.dbt_capstone_andres.treasury.fiscal_data: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "source.dbt_capstone_andres.treasury.fiscal_data"} */
select
      max(record_date::timestamp) as max_loaded_at,
      convert_timezone('UTC', current_timestamp()) as snapshotted_at
    from dataexpert_student.andres.fiscal_data
[0m16:21:46.684379 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m16:21:47.421375 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 0.737 seconds
[0m16:21:47.428868 [debug] [Thread-1 (]: On source.dbt_capstone_andres.treasury.fiscal_data: Close
[0m16:21:47.674835 [info ] [Thread-1 (]: 1 of 1 PASS freshness of treasury.fiscal_data .................................. [[32mPASS[0m in 1.04s]
[0m16:21:47.678043 [debug] [Thread-1 (]: Finished running node source.dbt_capstone_andres.treasury.fiscal_data
[0m16:21:47.682658 [debug] [MainThread]: Connection 'master' was properly closed.
[0m16:21:47.684186 [debug] [MainThread]: Connection 'source.dbt_capstone_andres.treasury.fiscal_data' was properly closed.
[0m16:21:47.686325 [info ] [MainThread]: 
[0m16:21:47.690251 [info ] [MainThread]: Finished running 1 source in 0 hours 0 minutes and 2.47 seconds (2.47s).
[0m16:21:47.841042 [info ] [MainThread]: Done.
[0m16:21:47.844056 [debug] [MainThread]: Resource report: {"command_name": "freshness", "command_success": true, "command_wall_clock_time": 7.179435, "process_user_time": 6.532866, "process_kernel_time": 1.047409, "process_mem_max_rss": "202676", "process_in_blocks": "1552", "process_out_blocks": "3685"}
[0m16:21:47.845617 [debug] [MainThread]: Command `dbt source freshness` succeeded at 16:21:47.845317 after 7.18 seconds
[0m16:21:47.846589 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0b2642bd90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0b2642ba10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0afb011e50>]}
[0m16:21:47.847431 [debug] [MainThread]: Flushing usage events
[0m16:21:48.085927 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m16:22:04.841151 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1c3ec29050>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1c3ef5cc50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1c3ecd29d0>]}


============================== 16:22:04.848654 | ddfb0191-5b17-4def-8bbb-5577cdf9a1bd ==============================
[0m16:22:04.848654 [info ] [MainThread]: Running with dbt=1.9.0-b3
[0m16:22:04.851176 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'profiles_dir': '/usr/local/airflow/dags/capstone/dbt_capstone_andres', 'log_path': '/usr/local/airflow/dags/capstone/dbt_capstone_andres/logs', 'version_check': 'True', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'log_format': 'default', 'introspect': 'True', 'invocation_command': 'dbt source freshness', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m16:22:06.875285 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'ddfb0191-5b17-4def-8bbb-5577cdf9a1bd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1c137f0b50>]}
[0m16:22:06.984046 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'ddfb0191-5b17-4def-8bbb-5577cdf9a1bd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1c3ed29450>]}
[0m16:22:06.989598 [info ] [MainThread]: Registered adapter: snowflake=1.8.4
[0m16:22:07.067897 [debug] [MainThread]: checksum: 5f4fde221ca8627174a0341efc73099aaec05b8546408ceed6f59b1ec0755115, vars: {}, profile: , target: , version: 1.9.0b3
[0m16:22:08.145431 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m16:22:08.147125 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m16:22:08.165466 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 2 unused configuration paths:
- models.staging
- models.marts
[0m16:22:08.297088 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'ddfb0191-5b17-4def-8bbb-5577cdf9a1bd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1c132fe290>]}
[0m16:22:08.698872 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'ddfb0191-5b17-4def-8bbb-5577cdf9a1bd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1c13765bd0>]}
[0m16:22:08.700727 [info ] [MainThread]: Found 5 models, 2 data tests, 2 sources, 862 macros, 1 unit test
[0m16:22:08.703290 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ddfb0191-5b17-4def-8bbb-5577cdf9a1bd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1c136e4c10>]}
[0m16:22:08.712877 [info ] [MainThread]: 
[0m16:22:08.714664 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m16:22:08.718351 [info ] [MainThread]: 
[0m16:22:08.721144 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m16:22:08.735570 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_DATAEXPERT_STUDENT_andres'
[0m16:22:08.788288 [debug] [ThreadPool]: Using snowflake connection "list_DATAEXPERT_STUDENT_andres"
[0m16:22:08.789889 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "connection_name": "list_DATAEXPERT_STUDENT_andres"} */
show objects in DATAEXPERT_STUDENT.andres limit 10000
[0m16:22:08.792344 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:22:09.967685 [debug] [ThreadPool]: SQL status: SUCCESS 28 in 1.176 seconds
[0m16:22:09.983360 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: Close
[0m16:22:10.248167 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ddfb0191-5b17-4def-8bbb-5577cdf9a1bd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1c409e1650>]}
[0m16:22:10.250911 [info ] [MainThread]: Pulling freshness from warehouse metadata tables for 0 sources
[0m16:22:10.266250 [debug] [Thread-1 (]: Began running node source.dbt_capstone_andres.treasury.fiscal_data
[0m16:22:10.268707 [info ] [Thread-1 (]: 1 of 1 START freshness of treasury.fiscal_data ................................. [RUN]
[0m16:22:10.271105 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_DATAEXPERT_STUDENT_andres, now source.dbt_capstone_andres.treasury.fiscal_data)
[0m16:22:10.273198 [debug] [Thread-1 (]: Began compiling node source.dbt_capstone_andres.treasury.fiscal_data
[0m16:22:10.275229 [debug] [Thread-1 (]: Began executing node source.dbt_capstone_andres.treasury.fiscal_data
[0m16:22:10.322169 [debug] [Thread-1 (]: Using snowflake connection "source.dbt_capstone_andres.treasury.fiscal_data"
[0m16:22:10.323899 [debug] [Thread-1 (]: On source.dbt_capstone_andres.treasury.fiscal_data: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "source.dbt_capstone_andres.treasury.fiscal_data"} */
select
      max(record_date::timestamp) as max_loaded_at,
      convert_timezone('UTC', current_timestamp()) as snapshotted_at
    from dataexpert_student.andres.fiscal_data
[0m16:22:10.329377 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m16:22:11.171989 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 0.843 seconds
[0m16:22:11.182181 [debug] [Thread-1 (]: On source.dbt_capstone_andres.treasury.fiscal_data: Close
[0m16:22:11.420357 [info ] [Thread-1 (]: 1 of 1 PASS freshness of treasury.fiscal_data .................................. [[32mPASS[0m in 1.15s]
[0m16:22:11.423454 [debug] [Thread-1 (]: Finished running node source.dbt_capstone_andres.treasury.fiscal_data
[0m16:22:11.426985 [debug] [MainThread]: Connection 'master' was properly closed.
[0m16:22:11.428261 [debug] [MainThread]: Connection 'source.dbt_capstone_andres.treasury.fiscal_data' was properly closed.
[0m16:22:11.429703 [info ] [MainThread]: 
[0m16:22:11.432055 [info ] [MainThread]: Finished running 1 source in 0 hours 0 minutes and 2.71 seconds (2.71s).
[0m16:22:11.682245 [info ] [MainThread]: Done.
[0m16:22:11.686617 [debug] [MainThread]: Resource report: {"command_name": "freshness", "command_success": true, "command_wall_clock_time": 7.0111184, "process_user_time": 7.363338, "process_kernel_time": 0.875591, "process_mem_max_rss": "209776", "process_in_blocks": "2568", "process_out_blocks": "3685"}
[0m16:22:11.688923 [debug] [MainThread]: Command `dbt source freshness` succeeded at 16:22:11.688413 after 7.01 seconds
[0m16:22:11.690845 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1c3ea9b8d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1c3ef2b910>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1c4298ffd0>]}
[0m16:22:11.693215 [debug] [MainThread]: Flushing usage events
[0m16:22:11.944043 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m16:22:34.374413 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f83b101e8d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f83b1229750>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f83b101cd50>]}


============================== 16:22:34.380395 | 1fd38972-5949-4794-9cb8-e351d4b4e0eb ==============================
[0m16:22:34.380395 [info ] [MainThread]: Running with dbt=1.9.0-b3
[0m16:22:34.382956 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'debug': 'False', 'log_path': '/usr/local/airflow/dags/capstone/dbt_capstone_andres/logs', 'profiles_dir': '/usr/local/airflow/dags/capstone/dbt_capstone_andres', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt source freshness', 'introspect': 'True', 'log_format': 'default', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m16:22:36.718839 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '1fd38972-5949-4794-9cb8-e351d4b4e0eb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8389c5ced0>]}
[0m16:22:36.953773 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '1fd38972-5949-4794-9cb8-e351d4b4e0eb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f839c74e590>]}
[0m16:22:36.958993 [info ] [MainThread]: Registered adapter: snowflake=1.8.4
[0m16:22:37.070141 [debug] [MainThread]: checksum: 5f4fde221ca8627174a0341efc73099aaec05b8546408ceed6f59b1ec0755115, vars: {}, profile: , target: , version: 1.9.0b3
[0m16:22:38.145643 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m16:22:38.146535 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m16:22:38.158561 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 2 unused configuration paths:
- models.staging
- models.marts
[0m16:22:38.257483 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '1fd38972-5949-4794-9cb8-e351d4b4e0eb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8389766c50>]}
[0m16:22:38.716661 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '1fd38972-5949-4794-9cb8-e351d4b4e0eb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8389bddf90>]}
[0m16:22:38.718552 [info ] [MainThread]: Found 5 models, 2 data tests, 2 sources, 862 macros, 1 unit test
[0m16:22:38.720771 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '1fd38972-5949-4794-9cb8-e351d4b4e0eb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8389b60c10>]}
[0m16:22:38.725915 [info ] [MainThread]: 
[0m16:22:38.727765 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m16:22:38.729475 [info ] [MainThread]: 
[0m16:22:38.731451 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m16:22:38.748222 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_DATAEXPERT_STUDENT_andres'
[0m16:22:38.938812 [debug] [ThreadPool]: Using snowflake connection "list_DATAEXPERT_STUDENT_andres"
[0m16:22:38.942488 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "connection_name": "list_DATAEXPERT_STUDENT_andres"} */
show objects in DATAEXPERT_STUDENT.andres limit 10000
[0m16:22:38.943757 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:22:39.905467 [debug] [ThreadPool]: SQL status: SUCCESS 28 in 0.962 seconds
[0m16:22:39.910034 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: Close
[0m16:22:40.150422 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '1fd38972-5949-4794-9cb8-e351d4b4e0eb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f838994cbd0>]}
[0m16:22:40.151912 [info ] [MainThread]: Pulling freshness from warehouse metadata tables for 0 sources
[0m16:22:40.158423 [debug] [Thread-1 (]: Began running node source.dbt_capstone_andres.treasury.fiscal_data
[0m16:22:40.160532 [info ] [Thread-1 (]: 1 of 1 START freshness of treasury.fiscal_data ................................. [RUN]
[0m16:22:40.162331 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_DATAEXPERT_STUDENT_andres, now source.dbt_capstone_andres.treasury.fiscal_data)
[0m16:22:40.163531 [debug] [Thread-1 (]: Began compiling node source.dbt_capstone_andres.treasury.fiscal_data
[0m16:22:40.165014 [debug] [Thread-1 (]: Began executing node source.dbt_capstone_andres.treasury.fiscal_data
[0m16:22:40.180894 [debug] [Thread-1 (]: Using snowflake connection "source.dbt_capstone_andres.treasury.fiscal_data"
[0m16:22:40.181951 [debug] [Thread-1 (]: On source.dbt_capstone_andres.treasury.fiscal_data: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "source.dbt_capstone_andres.treasury.fiscal_data"} */
select
      max(record_date::timestamp) as max_loaded_at,
      convert_timezone('UTC', current_timestamp()) as snapshotted_at
    from dataexpert_student.andres.fiscal_data
[0m16:22:40.183125 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m16:22:40.903370 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 0.720 seconds
[0m16:22:40.906356 [debug] [Thread-1 (]: On source.dbt_capstone_andres.treasury.fiscal_data: Close
[0m16:22:41.200998 [info ] [Thread-1 (]: 1 of 1 PASS freshness of treasury.fiscal_data .................................. [[32mPASS[0m in 1.04s]
[0m16:22:41.205668 [debug] [Thread-1 (]: Finished running node source.dbt_capstone_andres.treasury.fiscal_data
[0m16:22:41.208619 [debug] [MainThread]: Connection 'master' was properly closed.
[0m16:22:41.209681 [debug] [MainThread]: Connection 'source.dbt_capstone_andres.treasury.fiscal_data' was properly closed.
[0m16:22:41.210690 [info ] [MainThread]: 
[0m16:22:41.211652 [info ] [MainThread]: Finished running 1 source in 0 hours 0 minutes and 2.48 seconds (2.48s).
[0m16:22:41.326256 [info ] [MainThread]: Done.
[0m16:22:41.327827 [debug] [MainThread]: Resource report: {"command_name": "freshness", "command_success": true, "command_wall_clock_time": 7.109592, "process_user_time": 7.583717, "process_kernel_time": 1.146307, "process_mem_max_rss": "209860", "process_out_blocks": "3685", "process_in_blocks": "0"}
[0m16:22:41.328950 [debug] [MainThread]: Command `dbt source freshness` succeeded at 16:22:41.328828 after 7.11 seconds
[0m16:22:41.330117 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f83b122a050>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f83b4e39410>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f83b4daffd0>]}
[0m16:22:41.330935 [debug] [MainThread]: Flushing usage events
[0m16:22:41.576568 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m16:22:54.596405 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f02d42e0ed0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f02d45e3710>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f02d42e0410>]}


============================== 16:22:54.600762 | f9075d02-65f2-4e2d-8fc0-8b25f2646ad9 ==============================
[0m16:22:54.600762 [info ] [MainThread]: Running with dbt=1.9.0-b3
[0m16:22:54.602666 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'version_check': 'True', 'log_path': '/usr/local/airflow/dags/capstone/dbt_capstone_andres/logs', 'fail_fast': 'False', 'profiles_dir': '/usr/local/airflow/dags/capstone/dbt_capstone_andres', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'log_format': 'default', 'invocation_command': 'dbt source freshness', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m16:22:56.480409 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'f9075d02-65f2-4e2d-8fc0-8b25f2646ad9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f02aced6010>]}
[0m16:22:56.644644 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'f9075d02-65f2-4e2d-8fc0-8b25f2646ad9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f02d56f3650>]}
[0m16:22:56.648972 [info ] [MainThread]: Registered adapter: snowflake=1.8.4
[0m16:22:56.767436 [debug] [MainThread]: checksum: 5f4fde221ca8627174a0341efc73099aaec05b8546408ceed6f59b1ec0755115, vars: {}, profile: , target: , version: 1.9.0b3
[0m16:22:57.560336 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m16:22:57.561606 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m16:22:57.572437 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 2 unused configuration paths:
- models.staging
- models.marts
[0m16:22:57.682503 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'f9075d02-65f2-4e2d-8fc0-8b25f2646ad9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f02acfd7090>]}
[0m16:22:58.065518 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'f9075d02-65f2-4e2d-8fc0-8b25f2646ad9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f02ace65f50>]}
[0m16:22:58.066870 [info ] [MainThread]: Found 5 models, 2 data tests, 2 sources, 862 macros, 1 unit test
[0m16:22:58.067929 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f9075d02-65f2-4e2d-8fc0-8b25f2646ad9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f02acde8b90>]}
[0m16:22:58.072257 [info ] [MainThread]: 
[0m16:22:58.073672 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m16:22:58.074881 [info ] [MainThread]: 
[0m16:22:58.076385 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m16:22:58.085876 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_DATAEXPERT_STUDENT_andres'
[0m16:22:58.116091 [debug] [ThreadPool]: Using snowflake connection "list_DATAEXPERT_STUDENT_andres"
[0m16:22:58.117306 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "connection_name": "list_DATAEXPERT_STUDENT_andres"} */
show objects in DATAEXPERT_STUDENT.andres limit 10000
[0m16:22:58.118456 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:22:59.090566 [debug] [ThreadPool]: SQL status: SUCCESS 28 in 0.972 seconds
[0m16:22:59.097797 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: Close
[0m16:22:59.344840 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f9075d02-65f2-4e2d-8fc0-8b25f2646ad9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f02acc7fdd0>]}
[0m16:22:59.346767 [info ] [MainThread]: Pulling freshness from warehouse metadata tables for 0 sources
[0m16:22:59.356747 [debug] [Thread-1 (]: Began running node source.dbt_capstone_andres.treasury.fiscal_data
[0m16:22:59.358323 [info ] [Thread-1 (]: 1 of 1 START freshness of treasury.fiscal_data ................................. [RUN]
[0m16:22:59.359831 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_DATAEXPERT_STUDENT_andres, now source.dbt_capstone_andres.treasury.fiscal_data)
[0m16:22:59.361578 [debug] [Thread-1 (]: Began compiling node source.dbt_capstone_andres.treasury.fiscal_data
[0m16:22:59.363091 [debug] [Thread-1 (]: Began executing node source.dbt_capstone_andres.treasury.fiscal_data
[0m16:22:59.384788 [debug] [Thread-1 (]: Using snowflake connection "source.dbt_capstone_andres.treasury.fiscal_data"
[0m16:22:59.386569 [debug] [Thread-1 (]: On source.dbt_capstone_andres.treasury.fiscal_data: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "source.dbt_capstone_andres.treasury.fiscal_data"} */
select
      max(record_date::timestamp) as max_loaded_at,
      convert_timezone('UTC', current_timestamp()) as snapshotted_at
    from dataexpert_student.andres.fiscal_data
[0m16:22:59.388270 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m16:23:00.056754 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 0.669 seconds
[0m16:23:00.060186 [debug] [Thread-1 (]: On source.dbt_capstone_andres.treasury.fiscal_data: Close
[0m16:23:00.316083 [info ] [Thread-1 (]: 1 of 1 PASS freshness of treasury.fiscal_data .................................. [[32mPASS[0m in 0.96s]
[0m16:23:00.318318 [debug] [Thread-1 (]: Finished running node source.dbt_capstone_andres.treasury.fiscal_data
[0m16:23:00.322200 [debug] [MainThread]: Connection 'master' was properly closed.
[0m16:23:00.323334 [debug] [MainThread]: Connection 'source.dbt_capstone_andres.treasury.fiscal_data' was properly closed.
[0m16:23:00.324713 [info ] [MainThread]: 
[0m16:23:00.326290 [info ] [MainThread]: Finished running 1 source in 0 hours 0 minutes and 2.25 seconds (2.25s).
[0m16:23:00.576124 [info ] [MainThread]: Done.
[0m16:23:00.579227 [debug] [MainThread]: Resource report: {"command_name": "freshness", "command_success": true, "command_wall_clock_time": 6.0529513, "process_user_time": 5.554965, "process_kernel_time": 1.052908, "process_mem_max_rss": "210168", "process_out_blocks": "3685", "process_in_blocks": "0"}
[0m16:23:00.583755 [debug] [MainThread]: Command `dbt source freshness` succeeded at 16:23:00.583497 after 6.06 seconds
[0m16:23:00.586758 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f02d432c2d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f02d8129550>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f02d80a8b50>]}
[0m16:23:00.588748 [debug] [MainThread]: Flushing usage events
[0m16:23:00.831467 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m16:23:16.199604 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd09c320f90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd09d000750>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd09d000510>]}


============================== 16:23:16.203283 | 123d07e2-443c-4cf6-a154-1e47897729e1 ==============================
[0m16:23:16.203283 [info ] [MainThread]: Running with dbt=1.9.0-b3
[0m16:23:16.204575 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'profiles_dir': '/usr/local/airflow/dags/capstone/dbt_capstone_andres', 'log_path': '/usr/local/airflow/dags/capstone/dbt_capstone_andres/logs', 'fail_fast': 'False', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'log_format': 'default', 'invocation_command': 'dbt source freshness', 'static_parser': 'True', 'introspect': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m16:23:16.484617 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7215dacf50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f72162cfc50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7215df3bd0>]}


============================== 16:23:16.488282 | 3d9547b1-9443-434a-a213-7d58e32e0520 ==============================
[0m16:23:16.488282 [info ] [MainThread]: Running with dbt=1.9.0-b3
[0m16:23:16.489428 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/usr/local/airflow/dags/capstone/dbt_capstone_andres', 'debug': 'False', 'fail_fast': 'False', 'log_path': '/usr/local/airflow/dags/capstone/dbt_capstone_andres/logs', 'version_check': 'True', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'static_parser': 'True', 'log_format': 'default', 'target_path': 'None', 'invocation_command': 'dbt source freshness', 'send_anonymous_usage_stats': 'True'}
[0m16:23:17.676379 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '123d07e2-443c-4cf6-a154-1e47897729e1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd074f35150>]}
[0m16:23:17.806298 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '123d07e2-443c-4cf6-a154-1e47897729e1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd09c429210>]}
[0m16:23:17.808229 [info ] [MainThread]: Registered adapter: snowflake=1.8.4
[0m16:23:17.891215 [debug] [MainThread]: checksum: 5f4fde221ca8627174a0341efc73099aaec05b8546408ceed6f59b1ec0755115, vars: {}, profile: , target: , version: 1.9.0b3
[0m16:23:18.031098 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '3d9547b1-9443-434a-a213-7d58e32e0520', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7216b72650>]}
[0m16:23:18.119469 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '3d9547b1-9443-434a-a213-7d58e32e0520', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f71eaa244d0>]}
[0m16:23:18.121218 [info ] [MainThread]: Registered adapter: snowflake=1.8.4
[0m16:23:18.202225 [debug] [MainThread]: checksum: 5f4fde221ca8627174a0341efc73099aaec05b8546408ceed6f59b1ec0755115, vars: {}, profile: , target: , version: 1.9.0b3
[0m16:23:18.591760 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m16:23:18.593299 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m16:23:18.605492 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 2 unused configuration paths:
- models.marts
- models.staging
[0m16:23:18.666742 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '123d07e2-443c-4cf6-a154-1e47897729e1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd074cfa850>]}
[0m16:23:18.875491 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m16:23:18.876353 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m16:23:18.883167 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 2 unused configuration paths:
- models.staging
- models.marts
[0m16:23:18.939815 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '3d9547b1-9443-434a-a213-7d58e32e0520', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f71eaa31410>]}
[0m16:23:18.947086 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '123d07e2-443c-4cf6-a154-1e47897729e1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd074ee1ed0>]}
[0m16:23:18.948184 [info ] [MainThread]: Found 5 models, 2 data tests, 2 sources, 862 macros, 1 unit test
[0m16:23:18.949820 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '123d07e2-443c-4cf6-a154-1e47897729e1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd074e60f90>]}
[0m16:23:18.952407 [info ] [MainThread]: 
[0m16:23:18.953555 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m16:23:18.954837 [info ] [MainThread]: 
[0m16:23:18.957171 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m16:23:18.965133 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_DATAEXPERT_STUDENT_andres'
[0m16:23:18.993430 [debug] [ThreadPool]: Using snowflake connection "list_DATAEXPERT_STUDENT_andres"
[0m16:23:18.994435 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "connection_name": "list_DATAEXPERT_STUDENT_andres"} */
show objects in DATAEXPERT_STUDENT.andres limit 10000
[0m16:23:18.995310 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:23:19.188331 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '3d9547b1-9443-434a-a213-7d58e32e0520', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f71ea86a350>]}
[0m16:23:19.189366 [info ] [MainThread]: Found 5 models, 2 data tests, 2 sources, 862 macros, 1 unit test
[0m16:23:19.190268 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '3d9547b1-9443-434a-a213-7d58e32e0520', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f71ea7e4d90>]}
[0m16:23:19.193561 [info ] [MainThread]: 
[0m16:23:19.194700 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m16:23:19.195743 [info ] [MainThread]: 
[0m16:23:19.197022 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m16:23:19.203646 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_DATAEXPERT_STUDENT_andres'
[0m16:23:19.229363 [debug] [ThreadPool]: Using snowflake connection "list_DATAEXPERT_STUDENT_andres"
[0m16:23:19.232888 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "connection_name": "list_DATAEXPERT_STUDENT_andres"} */
show objects in DATAEXPERT_STUDENT.andres limit 10000
[0m16:23:19.233895 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:23:19.850232 [debug] [ThreadPool]: SQL status: SUCCESS 28 in 0.855 seconds
[0m16:23:19.854380 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: Close
[0m16:23:20.096151 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '123d07e2-443c-4cf6-a154-1e47897729e1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd09e2e7e10>]}
[0m16:23:20.097328 [info ] [MainThread]: Pulling freshness from warehouse metadata tables for 0 sources
[0m16:23:20.104530 [debug] [Thread-1 (]: Began running node source.dbt_capstone_andres.treasury.fiscal_data
[0m16:23:20.105813 [info ] [Thread-1 (]: 1 of 1 START freshness of treasury.fiscal_data ................................. [RUN]
[0m16:23:20.107113 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_DATAEXPERT_STUDENT_andres, now source.dbt_capstone_andres.treasury.fiscal_data)
[0m16:23:20.108131 [debug] [Thread-1 (]: Began compiling node source.dbt_capstone_andres.treasury.fiscal_data
[0m16:23:20.109143 [debug] [Thread-1 (]: Began executing node source.dbt_capstone_andres.treasury.fiscal_data
[0m16:23:20.122369 [debug] [Thread-1 (]: Using snowflake connection "source.dbt_capstone_andres.treasury.fiscal_data"
[0m16:23:20.125450 [debug] [Thread-1 (]: On source.dbt_capstone_andres.treasury.fiscal_data: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "source.dbt_capstone_andres.treasury.fiscal_data"} */
select
      max(record_date::timestamp) as max_loaded_at,
      convert_timezone('UTC', current_timestamp()) as snapshotted_at
    from dataexpert_student.andres.fiscal_data
[0m16:23:20.126600 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m16:23:20.163380 [debug] [ThreadPool]: SQL status: SUCCESS 28 in 0.930 seconds
[0m16:23:20.169105 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: Close
[0m16:23:20.403251 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '3d9547b1-9443-434a-a213-7d58e32e0520', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f721629fc90>]}
[0m16:23:20.404472 [info ] [MainThread]: Pulling freshness from warehouse metadata tables for 0 sources
[0m16:23:20.409420 [debug] [Thread-1 (]: Began running node source.dbt_capstone_andres.treasury.fiscal_data
[0m16:23:20.410489 [info ] [Thread-1 (]: 1 of 1 START freshness of treasury.fiscal_data ................................. [RUN]
[0m16:23:20.411589 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_DATAEXPERT_STUDENT_andres, now source.dbt_capstone_andres.treasury.fiscal_data)
[0m16:23:20.412562 [debug] [Thread-1 (]: Began compiling node source.dbt_capstone_andres.treasury.fiscal_data
[0m16:23:20.413602 [debug] [Thread-1 (]: Began executing node source.dbt_capstone_andres.treasury.fiscal_data
[0m16:23:20.426267 [debug] [Thread-1 (]: Using snowflake connection "source.dbt_capstone_andres.treasury.fiscal_data"
[0m16:23:20.429310 [debug] [Thread-1 (]: On source.dbt_capstone_andres.treasury.fiscal_data: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "source.dbt_capstone_andres.treasury.fiscal_data"} */
select
      max(record_date::timestamp) as max_loaded_at,
      convert_timezone('UTC', current_timestamp()) as snapshotted_at
    from dataexpert_student.andres.fiscal_data
[0m16:23:20.430409 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m16:23:20.854284 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 0.728 seconds
[0m16:23:20.857275 [debug] [Thread-1 (]: On source.dbt_capstone_andres.treasury.fiscal_data: Close
[0m16:23:21.090622 [info ] [Thread-1 (]: 1 of 1 PASS freshness of treasury.fiscal_data .................................. [[32mPASS[0m in 0.98s]
[0m16:23:21.092227 [debug] [Thread-1 (]: Finished running node source.dbt_capstone_andres.treasury.fiscal_data
[0m16:23:21.094834 [debug] [MainThread]: Connection 'master' was properly closed.
[0m16:23:21.095636 [debug] [MainThread]: Connection 'source.dbt_capstone_andres.treasury.fiscal_data' was properly closed.
[0m16:23:21.096392 [info ] [MainThread]: 
[0m16:23:21.097137 [info ] [MainThread]: Finished running 1 source in 0 hours 0 minutes and 2.14 seconds (2.14s).
[0m16:23:21.131716 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 0.701 seconds
[0m16:23:21.134465 [debug] [Thread-1 (]: On source.dbt_capstone_andres.treasury.fiscal_data: Close
[0m16:23:21.206917 [info ] [MainThread]: Done.
[0m16:23:21.209094 [debug] [MainThread]: Resource report: {"command_name": "freshness", "command_success": true, "command_wall_clock_time": 5.077164, "process_user_time": 4.962748, "process_kernel_time": 0.953068, "process_mem_max_rss": "210032", "process_out_blocks": "3685", "process_in_blocks": "0"}
[0m16:23:21.210500 [debug] [MainThread]: Command `dbt source freshness` succeeded at 16:23:21.210326 after 5.08 seconds
[0m16:23:21.211544 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd0a016d610>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd0a00e3f50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd0a00ecbd0>]}
[0m16:23:21.212772 [debug] [MainThread]: Flushing usage events
[0m16:23:21.389044 [info ] [Thread-1 (]: 1 of 1 PASS freshness of treasury.fiscal_data .................................. [[32mPASS[0m in 0.98s]
[0m16:23:21.390223 [debug] [Thread-1 (]: Finished running node source.dbt_capstone_andres.treasury.fiscal_data
[0m16:23:21.393562 [debug] [MainThread]: Connection 'master' was properly closed.
[0m16:23:21.394624 [debug] [MainThread]: Connection 'source.dbt_capstone_andres.treasury.fiscal_data' was properly closed.
[0m16:23:21.395732 [info ] [MainThread]: 
[0m16:23:21.397247 [info ] [MainThread]: Finished running 1 source in 0 hours 0 minutes and 2.20 seconds (2.20s).
[0m16:23:21.438770 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m16:23:21.535528 [info ] [MainThread]: Done.
[0m16:23:21.537777 [debug] [MainThread]: Resource report: {"command_name": "freshness", "command_success": true, "command_wall_clock_time": 5.1274314, "process_user_time": 5.179813, "process_kernel_time": 0.947184, "process_mem_max_rss": "210180", "process_out_blocks": "3685", "process_in_blocks": "0"}
[0m16:23:21.539443 [debug] [MainThread]: Command `dbt source freshness` succeeded at 16:23:21.539272 after 5.13 seconds
[0m16:23:21.540735 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7215de0910>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7215dfb010>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7219b5ffd0>]}
[0m16:23:21.542028 [debug] [MainThread]: Flushing usage events
[0m16:23:21.762764 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m01:00:26.370175 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdacff189d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdacff18710>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdacff19e50>]}


============================== 01:00:26.374539 | 25e2b11d-3e1d-4e24-8796-451f349d19c0 ==============================
[0m01:00:26.374539 [info ] [MainThread]: Running with dbt=1.9.0-b3
[0m01:00:26.375617 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/usr/local/airflow/dags/capstone/dbt_capstone_andres', 'debug': 'False', 'warn_error': 'None', 'log_path': '/usr/local/airflow/dags/capstone/dbt_capstone_andres/logs', 'version_check': 'True', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt source freshness', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m01:00:27.415645 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '25e2b11d-3e1d-4e24-8796-451f349d19c0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdaa8b344d0>]}
[0m01:00:27.483936 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '25e2b11d-3e1d-4e24-8796-451f349d19c0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdaa8c844d0>]}
[0m01:00:27.485570 [info ] [MainThread]: Registered adapter: snowflake=1.8.4
[0m01:00:27.539227 [debug] [MainThread]: checksum: 5f4fde221ca8627174a0341efc73099aaec05b8546408ceed6f59b1ec0755115, vars: {}, profile: , target: , version: 1.9.0b3
[0m01:00:27.999045 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m01:00:27.999851 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m01:00:28.005487 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 2 unused configuration paths:
- models.marts
- models.staging
[0m01:00:28.048095 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '25e2b11d-3e1d-4e24-8796-451f349d19c0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdaa8759cd0>]}
[0m01:00:28.233653 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '25e2b11d-3e1d-4e24-8796-451f349d19c0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdaa8afa750>]}
[0m01:00:28.234490 [info ] [MainThread]: Found 5 models, 2 data tests, 2 sources, 862 macros, 1 unit test
[0m01:00:28.235381 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '25e2b11d-3e1d-4e24-8796-451f349d19c0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdaa8ada7d0>]}
[0m01:00:28.237308 [info ] [MainThread]: 
[0m01:00:28.237997 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m01:00:28.238604 [info ] [MainThread]: 
[0m01:00:28.239408 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m01:00:28.245439 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_DATAEXPERT_STUDENT_andres'
[0m01:00:28.265854 [debug] [ThreadPool]: Using snowflake connection "list_DATAEXPERT_STUDENT_andres"
[0m01:00:28.266697 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "connection_name": "list_DATAEXPERT_STUDENT_andres"} */
show objects in DATAEXPERT_STUDENT.andres limit 10000
[0m01:00:28.267354 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m01:00:29.544101 [debug] [ThreadPool]: SQL status: SUCCESS 28 in 1.277 seconds
[0m01:00:29.549236 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: Close
[0m01:00:29.800915 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '25e2b11d-3e1d-4e24-8796-451f349d19c0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdaa879cb10>]}
[0m01:00:29.802043 [info ] [MainThread]: Pulling freshness from warehouse metadata tables for 0 sources
[0m01:00:29.806713 [debug] [Thread-1 (]: Began running node source.dbt_capstone_andres.treasury.fiscal_data
[0m01:00:29.807523 [info ] [Thread-1 (]: 1 of 1 START freshness of treasury.fiscal_data ................................. [RUN]
[0m01:00:29.808481 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_DATAEXPERT_STUDENT_andres, now source.dbt_capstone_andres.treasury.fiscal_data)
[0m01:00:29.809285 [debug] [Thread-1 (]: Began compiling node source.dbt_capstone_andres.treasury.fiscal_data
[0m01:00:29.810179 [debug] [Thread-1 (]: Began executing node source.dbt_capstone_andres.treasury.fiscal_data
[0m01:00:29.820208 [debug] [Thread-1 (]: Using snowflake connection "source.dbt_capstone_andres.treasury.fiscal_data"
[0m01:00:29.821225 [debug] [Thread-1 (]: On source.dbt_capstone_andres.treasury.fiscal_data: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "source.dbt_capstone_andres.treasury.fiscal_data"} */
select
      max(record_date::timestamp) as max_loaded_at,
      convert_timezone('UTC', current_timestamp()) as snapshotted_at
    from dataexpert_student.andres.fiscal_data
[0m01:00:29.822175 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m01:00:30.896998 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 1.075 seconds
[0m01:00:30.900449 [debug] [Thread-1 (]: On source.dbt_capstone_andres.treasury.fiscal_data: Close
[0m01:00:31.141203 [info ] [Thread-1 (]: 1 of 1 PASS freshness of treasury.fiscal_data .................................. [[32mPASS[0m in 1.33s]
[0m01:00:31.142657 [debug] [Thread-1 (]: Finished running node source.dbt_capstone_andres.treasury.fiscal_data
[0m01:00:31.145085 [debug] [MainThread]: Connection 'master' was properly closed.
[0m01:00:31.146065 [debug] [MainThread]: Connection 'source.dbt_capstone_andres.treasury.fiscal_data' was properly closed.
[0m01:00:31.146980 [info ] [MainThread]: 
[0m01:00:31.147797 [info ] [MainThread]: Finished running 1 source in 0 hours 0 minutes and 2.91 seconds (2.91s).
[0m01:00:31.255210 [info ] [MainThread]: Done.
[0m01:00:31.256821 [debug] [MainThread]: Resource report: {"command_name": "freshness", "command_success": true, "command_wall_clock_time": 4.961501, "process_user_time": 3.696403, "process_kernel_time": 1.016142, "process_mem_max_rss": "210148", "process_in_blocks": "30496", "process_out_blocks": "3685"}
[0m01:00:31.257899 [debug] [MainThread]: Command `dbt source freshness` succeeded at 01:00:31.257763 after 4.96 seconds
[0m01:00:31.258796 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdacfd8fad0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdacfd8f690>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdad3cdffd0>]}
[0m01:00:31.259674 [debug] [MainThread]: Flushing usage events
[0m01:00:32.079129 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m01:09:42.703706 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f65b5924810>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f65b591e690>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f65b5925290>]}


============================== 01:09:42.708030 | da1f0c2b-b36a-444b-bd30-53abef1dfbf4 ==============================
[0m01:09:42.708030 [info ] [MainThread]: Running with dbt=1.9.0-b3
[0m01:09:42.709060 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'version_check': 'True', 'log_path': '/usr/local/airflow/dags/capstone/dbt_capstone_andres/logs', 'debug': 'False', 'profiles_dir': '/usr/local/airflow/dags/capstone/dbt_capstone_andres', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'invocation_command': 'dbt source freshness', 'send_anonymous_usage_stats': 'True'}
[0m01:09:43.984249 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'da1f0c2b-b36a-444b-bd30-53abef1dfbf4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f65b5797c10>]}
[0m01:09:44.055206 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'da1f0c2b-b36a-444b-bd30-53abef1dfbf4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f65b6d177d0>]}
[0m01:09:44.056708 [info ] [MainThread]: Registered adapter: snowflake=1.8.4
[0m01:09:44.110000 [debug] [MainThread]: checksum: 5f4fde221ca8627174a0341efc73099aaec05b8546408ceed6f59b1ec0755115, vars: {}, profile: , target: , version: 1.9.0b3
[0m01:09:44.568479 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m01:09:44.569253 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m01:09:44.575102 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 2 unused configuration paths:
- models.marts
- models.staging
[0m01:09:44.619043 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'da1f0c2b-b36a-444b-bd30-53abef1dfbf4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f658e688410>]}
[0m01:09:44.849860 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'da1f0c2b-b36a-444b-bd30-53abef1dfbf4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f658e22a3d0>]}
[0m01:09:44.850773 [info ] [MainThread]: Found 5 models, 2 data tests, 2 sources, 862 macros, 1 unit test
[0m01:09:44.851526 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'da1f0c2b-b36a-444b-bd30-53abef1dfbf4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f658e4614d0>]}
[0m01:09:44.853982 [info ] [MainThread]: 
[0m01:09:44.854734 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m01:09:44.855436 [info ] [MainThread]: 
[0m01:09:44.856392 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m01:09:44.862766 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_DATAEXPERT_STUDENT_andres'
[0m01:09:44.884479 [debug] [ThreadPool]: Using snowflake connection "list_DATAEXPERT_STUDENT_andres"
[0m01:09:44.885432 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "connection_name": "list_DATAEXPERT_STUDENT_andres"} */
show objects in DATAEXPERT_STUDENT.andres limit 10000
[0m01:09:44.886243 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m01:09:45.833893 [debug] [ThreadPool]: SQL status: SUCCESS 28 in 0.948 seconds
[0m01:09:45.838770 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: Close
[0m01:09:46.082223 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'da1f0c2b-b36a-444b-bd30-53abef1dfbf4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f65b5e29710>]}
[0m01:09:46.083766 [info ] [MainThread]: Pulling freshness from warehouse metadata tables for 0 sources
[0m01:09:46.091115 [debug] [Thread-1 (]: Began running node source.dbt_capstone_andres.treasury.fiscal_data
[0m01:09:46.092489 [info ] [Thread-1 (]: 1 of 1 START freshness of treasury.fiscal_data ................................. [RUN]
[0m01:09:46.093941 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_DATAEXPERT_STUDENT_andres, now source.dbt_capstone_andres.treasury.fiscal_data)
[0m01:09:46.095219 [debug] [Thread-1 (]: Began compiling node source.dbt_capstone_andres.treasury.fiscal_data
[0m01:09:46.096308 [debug] [Thread-1 (]: Began executing node source.dbt_capstone_andres.treasury.fiscal_data
[0m01:09:46.110869 [debug] [Thread-1 (]: Using snowflake connection "source.dbt_capstone_andres.treasury.fiscal_data"
[0m01:09:46.112021 [debug] [Thread-1 (]: On source.dbt_capstone_andres.treasury.fiscal_data: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "source.dbt_capstone_andres.treasury.fiscal_data"} */
select
      max(record_date::timestamp) as max_loaded_at,
      convert_timezone('UTC', current_timestamp()) as snapshotted_at
    from dataexpert_student.andres.fiscal_data
[0m01:09:46.113191 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m01:09:46.858101 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 0.745 seconds
[0m01:09:46.861769 [debug] [Thread-1 (]: On source.dbt_capstone_andres.treasury.fiscal_data: Close
[0m01:09:47.089437 [info ] [Thread-1 (]: 1 of 1 PASS freshness of treasury.fiscal_data .................................. [[32mPASS[0m in 1.00s]
[0m01:09:47.091142 [debug] [Thread-1 (]: Finished running node source.dbt_capstone_andres.treasury.fiscal_data
[0m01:09:47.094211 [debug] [MainThread]: Connection 'master' was properly closed.
[0m01:09:47.095244 [debug] [MainThread]: Connection 'source.dbt_capstone_andres.treasury.fiscal_data' was properly closed.
[0m01:09:47.096271 [info ] [MainThread]: 
[0m01:09:47.097298 [info ] [MainThread]: Finished running 1 source in 0 hours 0 minutes and 2.24 seconds (2.24s).
[0m01:09:47.188968 [info ] [MainThread]: Done.
[0m01:09:47.190394 [debug] [MainThread]: Resource report: {"command_name": "freshness", "command_success": true, "command_wall_clock_time": 4.5429473, "process_user_time": 4.112286, "process_kernel_time": 1.891546, "process_mem_max_rss": "210148", "process_out_blocks": "3685", "process_in_blocks": "0"}
[0m01:09:47.191318 [debug] [MainThread]: Command `dbt source freshness` succeeded at 01:09:47.191215 after 4.54 seconds
[0m01:09:47.192108 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f65b5797790>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f65b5797b90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f65b96d3f50>]}
[0m01:09:47.193237 [debug] [MainThread]: Flushing usage events
[0m01:09:47.527171 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m01:12:30.325363 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe8d791cfd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe8d7b29750>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe8d7b29410>]}


============================== 01:12:30.356721 | 462c950a-9c13-4683-8b19-49718eb5d8bd ==============================
[0m01:12:30.356721 [info ] [MainThread]: Running with dbt=1.9.0-b3
[0m01:12:30.363803 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/usr/local/airflow/dags/capstone/dbt_capstone_andres', 'debug': 'False', 'warn_error': 'None', 'log_path': '/usr/local/airflow/dags/capstone/dbt_capstone_andres/logs', 'version_check': 'True', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'invocation_command': 'dbt source freshness', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m01:12:35.298290 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '462c950a-9c13-4683-8b19-49718eb5d8bd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe8d7924a90>]}
[0m01:12:35.699569 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '462c950a-9c13-4683-8b19-49718eb5d8bd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe8d8d2f410>]}
[0m01:12:35.705611 [info ] [MainThread]: Registered adapter: snowflake=1.8.4
[0m01:12:35.946768 [debug] [MainThread]: checksum: 5f4fde221ca8627174a0341efc73099aaec05b8546408ceed6f59b1ec0755115, vars: {}, profile: , target: , version: 1.9.0b3
[0m01:12:38.155780 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m01:12:38.159675 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m01:12:38.192680 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 2 unused configuration paths:
- models.staging
- models.marts
[0m01:12:38.450608 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '462c950a-9c13-4683-8b19-49718eb5d8bd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe8cad529d0>]}
[0m01:12:39.807343 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '462c950a-9c13-4683-8b19-49718eb5d8bd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe8b04ca450>]}
[0m01:12:39.813333 [info ] [MainThread]: Found 5 models, 2 data tests, 2 sources, 862 macros, 1 unit test
[0m01:12:39.819204 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '462c950a-9c13-4683-8b19-49718eb5d8bd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe8d8a47cd0>]}
[0m01:12:39.839063 [info ] [MainThread]: 
[0m01:12:39.843341 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m01:12:39.847272 [info ] [MainThread]: 
[0m01:12:39.854357 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m01:12:39.887127 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_DATAEXPERT_STUDENT_andres'
[0m01:12:40.038366 [debug] [ThreadPool]: Using snowflake connection "list_DATAEXPERT_STUDENT_andres"
[0m01:12:40.045013 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "connection_name": "list_DATAEXPERT_STUDENT_andres"} */
show objects in DATAEXPERT_STUDENT.andres limit 10000
[0m01:12:40.049271 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m01:12:41.913419 [debug] [ThreadPool]: SQL status: SUCCESS 28 in 1.864 seconds
[0m01:12:41.931462 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: Close
[0m01:12:42.220049 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '462c950a-9c13-4683-8b19-49718eb5d8bd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe8b05d7910>]}
[0m01:12:42.225599 [info ] [MainThread]: Pulling freshness from warehouse metadata tables for 0 sources
[0m01:12:42.247229 [debug] [Thread-1 (]: Began running node source.dbt_capstone_andres.treasury.fiscal_data
[0m01:12:42.252382 [info ] [Thread-1 (]: 1 of 1 START freshness of treasury.fiscal_data ................................. [RUN]
[0m01:12:42.260424 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_DATAEXPERT_STUDENT_andres, now source.dbt_capstone_andres.treasury.fiscal_data)
[0m01:12:42.266023 [debug] [Thread-1 (]: Began compiling node source.dbt_capstone_andres.treasury.fiscal_data
[0m01:12:42.272987 [debug] [Thread-1 (]: Began executing node source.dbt_capstone_andres.treasury.fiscal_data
[0m01:12:42.327407 [debug] [Thread-1 (]: Using snowflake connection "source.dbt_capstone_andres.treasury.fiscal_data"
[0m01:12:42.332372 [debug] [Thread-1 (]: On source.dbt_capstone_andres.treasury.fiscal_data: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "source.dbt_capstone_andres.treasury.fiscal_data"} */
select
      max(record_date::timestamp) as max_loaded_at,
      convert_timezone('UTC', current_timestamp()) as snapshotted_at
    from dataexpert_student.andres.fiscal_data
[0m01:12:42.336577 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m01:12:43.249334 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 0.913 seconds
[0m01:12:43.260075 [debug] [Thread-1 (]: On source.dbt_capstone_andres.treasury.fiscal_data: Close
[0m01:12:43.508188 [info ] [Thread-1 (]: 1 of 1 PASS freshness of treasury.fiscal_data .................................. [[32mPASS[0m in 1.25s]
[0m01:12:43.513550 [debug] [Thread-1 (]: Finished running node source.dbt_capstone_andres.treasury.fiscal_data
[0m01:12:43.524241 [debug] [MainThread]: Connection 'master' was properly closed.
[0m01:12:43.527773 [debug] [MainThread]: Connection 'source.dbt_capstone_andres.treasury.fiscal_data' was properly closed.
[0m01:12:43.531164 [info ] [MainThread]: 
[0m01:12:43.535079 [info ] [MainThread]: Finished running 1 source in 0 hours 0 minutes and 3.68 seconds (3.68s).
[0m01:12:44.016019 [info ] [MainThread]: Done.
[0m01:12:44.023103 [debug] [MainThread]: Resource report: {"command_name": "freshness", "command_success": true, "command_wall_clock_time": 14.125053, "process_user_time": 18.933605, "process_kernel_time": 3.060103, "process_mem_max_rss": "209784", "process_out_blocks": "3685", "process_in_blocks": "0"}
[0m01:12:44.028755 [debug] [MainThread]: Command `dbt source freshness` succeeded at 01:12:44.027556 after 14.13 seconds
[0m01:12:44.034125 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe8db779550>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe8db6effd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe8db6f8b90>]}
[0m01:12:44.040369 [debug] [MainThread]: Flushing usage events
[0m01:12:44.500889 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m03:29:15.624893 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ffa09628d90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ffa09a00cd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ffa09629490>]}


============================== 03:29:15.628380 | 9293b086-d900-4d1c-8ba9-88a404cd6c46 ==============================
[0m03:29:15.628380 [info ] [MainThread]: Running with dbt=1.9.0-b3
[0m03:29:15.629380 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'fail_fast': 'False', 'log_path': '/usr/local/airflow/dags/capstone/dbt_capstone_andres/logs', 'profiles_dir': '/usr/local/airflow/dags/capstone/dbt_capstone_andres', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'log_format': 'default', 'introspect': 'True', 'target_path': 'None', 'invocation_command': 'dbt source freshness', 'send_anonymous_usage_stats': 'True'}
[0m03:29:17.464428 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '9293b086-d900-4d1c-8ba9-88a404cd6c46', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff9ff346150>]}
[0m03:29:17.527893 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '9293b086-d900-4d1c-8ba9-88a404cd6c46', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff9fc9635d0>]}
[0m03:29:17.529114 [info ] [MainThread]: Registered adapter: snowflake=1.8.4
[0m03:29:17.590919 [debug] [MainThread]: checksum: 5f4fde221ca8627174a0341efc73099aaec05b8546408ceed6f59b1ec0755115, vars: {}, profile: , target: , version: 1.9.0b3
[0m03:29:18.083726 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m03:29:18.084608 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m03:29:18.090890 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 2 unused configuration paths:
- models.marts
- models.staging
[0m03:29:18.136052 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '9293b086-d900-4d1c-8ba9-88a404cd6c46', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff9e1dd4a90>]}
[0m03:29:18.316138 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '9293b086-d900-4d1c-8ba9-88a404cd6c46', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff9e216e1d0>]}
[0m03:29:18.317037 [info ] [MainThread]: Found 5 models, 2 data tests, 2 sources, 862 macros, 1 unit test
[0m03:29:18.317746 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9293b086-d900-4d1c-8ba9-88a404cd6c46', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff9fc9e62d0>]}
[0m03:29:18.319562 [info ] [MainThread]: 
[0m03:29:18.320293 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m03:29:18.320875 [info ] [MainThread]: 
[0m03:29:18.321616 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m03:29:18.326705 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_DATAEXPERT_STUDENT_andres'
[0m03:29:18.345982 [debug] [ThreadPool]: Using snowflake connection "list_DATAEXPERT_STUDENT_andres"
[0m03:29:18.346762 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "connection_name": "list_DATAEXPERT_STUDENT_andres"} */
show objects in DATAEXPERT_STUDENT.andres limit 10000
[0m03:29:18.347348 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m03:29:19.354913 [debug] [ThreadPool]: SQL status: SUCCESS 28 in 1.007 seconds
[0m03:29:19.360155 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: Close
[0m03:29:19.584796 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9293b086-d900-4d1c-8ba9-88a404cd6c46', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff9e23810d0>]}
[0m03:29:19.586089 [info ] [MainThread]: Pulling freshness from warehouse metadata tables for 0 sources
[0m03:29:19.594632 [debug] [Thread-1 (]: Began running node source.dbt_capstone_andres.treasury.fiscal_data
[0m03:29:19.595787 [info ] [Thread-1 (]: 1 of 1 START freshness of treasury.fiscal_data ................................. [RUN]
[0m03:29:19.596956 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_DATAEXPERT_STUDENT_andres, now source.dbt_capstone_andres.treasury.fiscal_data)
[0m03:29:19.597967 [debug] [Thread-1 (]: Began compiling node source.dbt_capstone_andres.treasury.fiscal_data
[0m03:29:19.598956 [debug] [Thread-1 (]: Began executing node source.dbt_capstone_andres.treasury.fiscal_data
[0m03:29:19.611710 [debug] [Thread-1 (]: Using snowflake connection "source.dbt_capstone_andres.treasury.fiscal_data"
[0m03:29:19.612693 [debug] [Thread-1 (]: On source.dbt_capstone_andres.treasury.fiscal_data: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "source.dbt_capstone_andres.treasury.fiscal_data"} */
select
      max(record_date::timestamp) as max_loaded_at,
      convert_timezone('UTC', current_timestamp()) as snapshotted_at
    from dataexpert_student.andres.fiscal_data
[0m03:29:19.613612 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m03:29:20.756180 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 1.143 seconds
[0m03:29:20.759454 [debug] [Thread-1 (]: On source.dbt_capstone_andres.treasury.fiscal_data: Close
[0m03:29:20.973251 [info ] [Thread-1 (]: 1 of 1 PASS freshness of treasury.fiscal_data .................................. [[32mPASS[0m in 1.38s]
[0m03:29:20.974434 [debug] [Thread-1 (]: Finished running node source.dbt_capstone_andres.treasury.fiscal_data
[0m03:29:20.976784 [debug] [MainThread]: Connection 'master' was properly closed.
[0m03:29:20.977498 [debug] [MainThread]: Connection 'source.dbt_capstone_andres.treasury.fiscal_data' was properly closed.
[0m03:29:20.978185 [info ] [MainThread]: 
[0m03:29:20.978904 [info ] [MainThread]: Finished running 1 source in 0 hours 0 minutes and 2.66 seconds (2.66s).
[0m03:29:21.077490 [info ] [MainThread]: Done.
[0m03:29:21.080293 [debug] [MainThread]: Resource report: {"command_name": "freshness", "command_success": true, "command_wall_clock_time": 5.512975, "process_user_time": 4.127453, "process_kernel_time": 3.735329, "process_mem_max_rss": "210488", "process_out_blocks": "3685", "process_in_blocks": "0"}
[0m03:29:21.081832 [debug] [MainThread]: Command `dbt source freshness` succeeded at 03:29:21.081650 after 5.51 seconds
[0m03:29:21.082892 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ffa0d419550>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ffa0d38ffd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ffa0d398b90>]}
[0m03:29:21.083884 [debug] [MainThread]: Flushing usage events
[0m04:24:21.271125 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m01:00:22.799121 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3da0f20cd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3da1423a50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3da0f21090>]}


============================== 01:00:22.802777 | d9af9c08-e172-4b66-bb5e-bcdf94271aa3 ==============================
[0m01:00:22.802777 [info ] [MainThread]: Running with dbt=1.9.0-b3
[0m01:00:22.803868 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/usr/local/airflow/dags/capstone/dbt_capstone_andres', 'fail_fast': 'False', 'version_check': 'True', 'log_path': '/usr/local/airflow/dags/capstone/dbt_capstone_andres/logs', 'warn_error': 'None', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'invocation_command': 'dbt source freshness', 'introspect': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m01:00:23.842777 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'd9af9c08-e172-4b66-bb5e-bcdf94271aa3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3da0da14d0>]}
[0m01:00:23.902513 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'd9af9c08-e172-4b66-bb5e-bcdf94271aa3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3d79c8c3d0>]}
[0m01:00:23.903785 [info ] [MainThread]: Registered adapter: snowflake=1.8.4
[0m01:00:23.949941 [debug] [MainThread]: checksum: 5f4fde221ca8627174a0341efc73099aaec05b8546408ceed6f59b1ec0755115, vars: {}, profile: , target: , version: 1.9.0b3
[0m01:00:24.398787 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m01:00:24.399553 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m01:00:24.405431 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 2 unused configuration paths:
- models.marts
- models.staging
[0m01:00:24.449528 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'd9af9c08-e172-4b66-bb5e-bcdf94271aa3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3d79c86590>]}
[0m01:00:24.625680 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'd9af9c08-e172-4b66-bb5e-bcdf94271aa3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3d7981e490>]}
[0m01:00:24.626645 [info ] [MainThread]: Found 5 models, 2 data tests, 2 sources, 862 macros, 1 unit test
[0m01:00:24.627370 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd9af9c08-e172-4b66-bb5e-bcdf94271aa3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3d79a85110>]}
[0m01:00:24.629530 [info ] [MainThread]: 
[0m01:00:24.630272 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m01:00:24.631008 [info ] [MainThread]: 
[0m01:00:24.631891 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m01:00:24.638408 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_DATAEXPERT_STUDENT_andres'
[0m01:00:24.659773 [debug] [ThreadPool]: Using snowflake connection "list_DATAEXPERT_STUDENT_andres"
[0m01:00:24.660666 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "connection_name": "list_DATAEXPERT_STUDENT_andres"} */
show objects in DATAEXPERT_STUDENT.andres limit 10000
[0m01:00:24.661595 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m01:00:25.589278 [debug] [ThreadPool]: SQL status: SUCCESS 28 in 0.928 seconds
[0m01:00:25.593185 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: Close
[0m01:00:25.828820 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd9af9c08-e172-4b66-bb5e-bcdf94271aa3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3da0f5c0d0>]}
[0m01:00:25.829974 [info ] [MainThread]: Pulling freshness from warehouse metadata tables for 0 sources
[0m01:00:25.837333 [debug] [Thread-1 (]: Began running node source.dbt_capstone_andres.treasury.fiscal_data
[0m01:00:25.838707 [info ] [Thread-1 (]: 1 of 1 START freshness of treasury.fiscal_data ................................. [RUN]
[0m01:00:25.839890 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_DATAEXPERT_STUDENT_andres, now source.dbt_capstone_andres.treasury.fiscal_data)
[0m01:00:25.840974 [debug] [Thread-1 (]: Began compiling node source.dbt_capstone_andres.treasury.fiscal_data
[0m01:00:25.841920 [debug] [Thread-1 (]: Began executing node source.dbt_capstone_andres.treasury.fiscal_data
[0m01:00:25.854191 [debug] [Thread-1 (]: Using snowflake connection "source.dbt_capstone_andres.treasury.fiscal_data"
[0m01:00:25.855340 [debug] [Thread-1 (]: On source.dbt_capstone_andres.treasury.fiscal_data: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "source.dbt_capstone_andres.treasury.fiscal_data"} */
select
      max(record_date::timestamp) as max_loaded_at,
      convert_timezone('UTC', current_timestamp()) as snapshotted_at
    from dataexpert_student.andres.fiscal_data
[0m01:00:25.856638 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m01:00:26.729486 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 0.873 seconds
[0m01:00:26.732302 [debug] [Thread-1 (]: On source.dbt_capstone_andres.treasury.fiscal_data: Close
[0m01:00:26.995133 [info ] [Thread-1 (]: 1 of 1 PASS freshness of treasury.fiscal_data .................................. [[32mPASS[0m in 1.16s]
[0m01:00:26.996784 [debug] [Thread-1 (]: Finished running node source.dbt_capstone_andres.treasury.fiscal_data
[0m01:00:27.000006 [debug] [MainThread]: Connection 'master' was properly closed.
[0m01:00:27.000899 [debug] [MainThread]: Connection 'source.dbt_capstone_andres.treasury.fiscal_data' was properly closed.
[0m01:00:27.001872 [info ] [MainThread]: 
[0m01:00:27.002884 [info ] [MainThread]: Finished running 1 source in 0 hours 0 minutes and 2.37 seconds (2.37s).
[0m01:00:27.095667 [info ] [MainThread]: Done.
[0m01:00:27.097279 [debug] [MainThread]: Resource report: {"command_name": "freshness", "command_success": true, "command_wall_clock_time": 4.3569055, "process_user_time": 3.911027, "process_kernel_time": 1.100211, "process_mem_max_rss": "210292", "process_out_blocks": "3685", "process_in_blocks": "0"}
[0m01:00:27.098328 [debug] [MainThread]: Command `dbt source freshness` succeeded at 01:00:27.098205 after 4.36 seconds
[0m01:00:27.099192 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3da0f87f90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3da1020d90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3da1223990>]}
[0m01:00:27.100416 [debug] [MainThread]: Flushing usage events
[0m01:00:27.470211 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m02:05:56.112932 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1194a1b490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1194a1b5d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1194a18690>]}


============================== 02:05:56.118496 | f5ac39ae-e6f6-4cc4-a5a6-b62625309405 ==============================
[0m02:05:56.118496 [info ] [MainThread]: Running with dbt=1.9.0-b3
[0m02:05:56.119606 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'version_check': 'True', 'log_path': '/usr/local/airflow/dags/capstone/dbt_capstone_andres/logs', 'debug': 'False', 'profiles_dir': '/usr/local/airflow/dags/capstone/dbt_capstone_andres', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'invocation_command': 'dbt source freshness', 'introspect': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m02:05:57.362716 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'f5ac39ae-e6f6-4cc4-a5a6-b62625309405', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1194a94290>]}
[0m02:05:57.433668 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'f5ac39ae-e6f6-4cc4-a5a6-b62625309405', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1195e2b290>]}
[0m02:05:57.434984 [info ] [MainThread]: Registered adapter: snowflake=1.8.4
[0m02:05:57.483094 [debug] [MainThread]: checksum: 5f4fde221ca8627174a0341efc73099aaec05b8546408ceed6f59b1ec0755115, vars: {}, profile: , target: , version: 1.9.0b3
[0m02:05:57.968856 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m02:05:57.969538 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m02:05:57.975397 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 2 unused configuration paths:
- models.staging
- models.marts
[0m02:05:58.024982 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'f5ac39ae-e6f6-4cc4-a5a6-b62625309405', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f116d25b990>]}
[0m02:05:58.230444 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'f5ac39ae-e6f6-4cc4-a5a6-b62625309405', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f116d5bdfd0>]}
[0m02:05:58.231300 [info ] [MainThread]: Found 5 models, 2 data tests, 2 sources, 862 macros, 1 unit test
[0m02:05:58.232013 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f5ac39ae-e6f6-4cc4-a5a6-b62625309405', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f116d51d250>]}
[0m02:05:58.234105 [info ] [MainThread]: 
[0m02:05:58.234935 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m02:05:58.235606 [info ] [MainThread]: 
[0m02:05:58.236510 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m02:05:58.242390 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_DATAEXPERT_STUDENT_andres'
[0m02:05:58.266786 [debug] [ThreadPool]: Using snowflake connection "list_DATAEXPERT_STUDENT_andres"
[0m02:05:58.267665 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "connection_name": "list_DATAEXPERT_STUDENT_andres"} */
show objects in DATAEXPERT_STUDENT.andres limit 10000
[0m02:05:58.268438 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m02:05:59.314433 [debug] [ThreadPool]: SQL status: SUCCESS 28 in 1.046 seconds
[0m02:05:59.320475 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: Close
[0m02:05:59.567857 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f5ac39ae-e6f6-4cc4-a5a6-b62625309405', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f116d75fa90>]}
[0m02:05:59.568736 [info ] [MainThread]: Pulling freshness from warehouse metadata tables for 0 sources
[0m02:05:59.575491 [debug] [Thread-1 (]: Began running node source.dbt_capstone_andres.treasury.fiscal_data
[0m02:05:59.576402 [info ] [Thread-1 (]: 1 of 1 START freshness of treasury.fiscal_data ................................. [RUN]
[0m02:05:59.577272 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_DATAEXPERT_STUDENT_andres, now source.dbt_capstone_andres.treasury.fiscal_data)
[0m02:05:59.578769 [debug] [Thread-1 (]: Began compiling node source.dbt_capstone_andres.treasury.fiscal_data
[0m02:05:59.580695 [debug] [Thread-1 (]: Began executing node source.dbt_capstone_andres.treasury.fiscal_data
[0m02:05:59.595398 [debug] [Thread-1 (]: Using snowflake connection "source.dbt_capstone_andres.treasury.fiscal_data"
[0m02:05:59.596461 [debug] [Thread-1 (]: On source.dbt_capstone_andres.treasury.fiscal_data: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "source.dbt_capstone_andres.treasury.fiscal_data"} */
select
      max(record_date::timestamp) as max_loaded_at,
      convert_timezone('UTC', current_timestamp()) as snapshotted_at
    from dataexpert_student.andres.fiscal_data
[0m02:05:59.597195 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m02:06:00.949832 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 1.353 seconds
[0m02:06:00.953824 [debug] [Thread-1 (]: On source.dbt_capstone_andres.treasury.fiscal_data: Close
[0m02:06:01.196238 [info ] [Thread-1 (]: 1 of 1 PASS freshness of treasury.fiscal_data .................................. [[32mPASS[0m in 1.62s]
[0m02:06:01.197528 [debug] [Thread-1 (]: Finished running node source.dbt_capstone_andres.treasury.fiscal_data
[0m02:06:01.200319 [debug] [MainThread]: Connection 'master' was properly closed.
[0m02:06:01.201091 [debug] [MainThread]: Connection 'source.dbt_capstone_andres.treasury.fiscal_data' was properly closed.
[0m02:06:01.201733 [info ] [MainThread]: 
[0m02:06:01.203781 [info ] [MainThread]: Finished running 1 source in 0 hours 0 minutes and 2.97 seconds (2.97s).
[0m02:06:01.360782 [info ] [MainThread]: Done.
[0m02:06:01.363042 [debug] [MainThread]: Resource report: {"command_name": "freshness", "command_success": true, "command_wall_clock_time": 5.3149467, "process_user_time": 4.072234, "process_kernel_time": 1.278856, "process_mem_max_rss": "209808", "process_out_blocks": "3685", "process_in_blocks": "0"}
[0m02:06:01.364817 [debug] [MainThread]: Command `dbt source freshness` succeeded at 02:06:01.364644 after 5.32 seconds
[0m02:06:01.365921 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1194a66ed0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1198ab1e10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f11987cffd0>]}
[0m02:06:01.367168 [debug] [MainThread]: Flushing usage events
[0m02:06:01.919265 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m19:36:47.526660 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe8ac8e0e90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe8ac9c0590>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe8acc24450>]}


============================== 19:36:47.535309 | bc0455b1-d80c-438b-86e4-cf93c9f80ed3 ==============================
[0m19:36:47.535309 [info ] [MainThread]: Running with dbt=1.9.0-b3
[0m19:36:47.536617 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/usr/local/airflow/dags/capstone/dbt_capstone_andres', 'fail_fast': 'False', 'warn_error': 'None', 'log_path': '/usr/local/airflow/dags/capstone/dbt_capstone_andres/logs', 'version_check': 'True', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt source freshness', 'log_format': 'default', 'static_parser': 'True', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m19:36:48.943936 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'bc0455b1-d80c-438b-86e4-cf93c9f80ed3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe8854ea8d0>]}
[0m19:36:49.013556 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'bc0455b1-d80c-438b-86e4-cf93c9f80ed3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe8adcd3290>]}
[0m19:36:49.014831 [info ] [MainThread]: Registered adapter: snowflake=1.8.4
[0m19:36:49.093129 [debug] [MainThread]: checksum: 5f4fde221ca8627174a0341efc73099aaec05b8546408ceed6f59b1ec0755115, vars: {}, profile: , target: , version: 1.9.0b3
[0m19:36:49.584988 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m19:36:49.585681 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m19:36:49.591531 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 2 unused configuration paths:
- models.staging
- models.marts
[0m19:36:49.659850 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'bc0455b1-d80c-438b-86e4-cf93c9f80ed3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe8850026d0>]}
[0m19:36:49.865921 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'bc0455b1-d80c-438b-86e4-cf93c9f80ed3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe88546e2d0>]}
[0m19:36:49.866840 [info ] [MainThread]: Found 5 models, 2 data tests, 2 sources, 862 macros, 1 unit test
[0m19:36:49.867628 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'bc0455b1-d80c-438b-86e4-cf93c9f80ed3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe8853ecc10>]}
[0m19:36:49.870215 [info ] [MainThread]: 
[0m19:36:49.871020 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m19:36:49.871697 [info ] [MainThread]: 
[0m19:36:49.872609 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m19:36:49.878595 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_DATAEXPERT_STUDENT_andres'
[0m19:36:49.900646 [debug] [ThreadPool]: Using snowflake connection "list_DATAEXPERT_STUDENT_andres"
[0m19:36:49.901600 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "connection_name": "list_DATAEXPERT_STUDENT_andres"} */
show objects in DATAEXPERT_STUDENT.andres limit 10000
[0m19:36:49.902373 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:36:51.142409 [debug] [ThreadPool]: SQL status: SUCCESS 29 in 1.240 seconds
[0m19:36:51.146596 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: Close
[0m19:36:51.380740 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'bc0455b1-d80c-438b-86e4-cf93c9f80ed3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe8ae8b1290>]}
[0m19:36:51.385522 [info ] [MainThread]: Pulling freshness from warehouse metadata tables for 0 sources
[0m19:36:51.389572 [debug] [Thread-1 (]: Began running node source.dbt_capstone_andres.treasury.fiscal_data
[0m19:36:51.390618 [info ] [Thread-1 (]: 1 of 1 START freshness of treasury.fiscal_data ................................. [RUN]
[0m19:36:51.391596 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_DATAEXPERT_STUDENT_andres, now source.dbt_capstone_andres.treasury.fiscal_data)
[0m19:36:51.392485 [debug] [Thread-1 (]: Began compiling node source.dbt_capstone_andres.treasury.fiscal_data
[0m19:36:51.393404 [debug] [Thread-1 (]: Began executing node source.dbt_capstone_andres.treasury.fiscal_data
[0m19:36:51.405592 [debug] [Thread-1 (]: Using snowflake connection "source.dbt_capstone_andres.treasury.fiscal_data"
[0m19:36:51.406576 [debug] [Thread-1 (]: On source.dbt_capstone_andres.treasury.fiscal_data: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "source.dbt_capstone_andres.treasury.fiscal_data"} */
select
      max(record_date::timestamp) as max_loaded_at,
      convert_timezone('UTC', current_timestamp()) as snapshotted_at
    from dataexpert_student.andres.fiscal_data
[0m19:36:51.407487 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m19:36:52.211466 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 0.804 seconds
[0m19:36:52.219902 [debug] [Thread-1 (]: On source.dbt_capstone_andres.treasury.fiscal_data: Close
[0m19:36:52.564086 [info ] [Thread-1 (]: 1 of 1 PASS freshness of treasury.fiscal_data .................................. [[32mPASS[0m in 1.17s]
[0m19:36:52.565559 [debug] [Thread-1 (]: Finished running node source.dbt_capstone_andres.treasury.fiscal_data
[0m19:36:52.568499 [debug] [MainThread]: Connection 'master' was properly closed.
[0m19:36:52.569393 [debug] [MainThread]: Connection 'source.dbt_capstone_andres.treasury.fiscal_data' was properly closed.
[0m19:36:52.570210 [info ] [MainThread]: 
[0m19:36:52.571118 [info ] [MainThread]: Finished running 1 source in 0 hours 0 minutes and 2.70 seconds (2.70s).
[0m19:36:52.663412 [info ] [MainThread]: Done.
[0m19:36:52.666259 [debug] [MainThread]: Resource report: {"command_name": "freshness", "command_success": true, "command_wall_clock_time": 5.2079177, "process_user_time": 4.124384, "process_kernel_time": 2.585626, "process_mem_max_rss": "210424", "process_out_blocks": "3685", "process_in_blocks": "0"}
[0m19:36:52.667478 [debug] [MainThread]: Command `dbt source freshness` succeeded at 19:36:52.667227 after 5.21 seconds
[0m19:36:52.668322 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe8ac757c90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe8acaedb10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe8b0683f10>]}
[0m19:36:52.669083 [debug] [MainThread]: Flushing usage events
[0m19:36:53.139561 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m19:38:09.646595 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9f444cd0d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9f446d9250>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9f446d9990>]}


============================== 19:38:09.649482 | eb40feba-2b03-4da8-b2c5-91c36fff5fad ==============================
[0m19:38:09.649482 [info ] [MainThread]: Running with dbt=1.9.0-b3
[0m19:38:09.650480 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'version_check': 'True', 'log_path': '/usr/local/airflow/dags/capstone/dbt_capstone_andres/logs', 'debug': 'False', 'profiles_dir': '/usr/local/airflow/dags/capstone/dbt_capstone_andres', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'invocation_command': 'dbt source freshness', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m19:38:10.648948 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'eb40feba-2b03-4da8-b2c5-91c36fff5fad', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9f1d254290>]}
[0m19:38:10.707915 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'eb40feba-2b03-4da8-b2c5-91c36fff5fad', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9f40bee390>]}
[0m19:38:10.709041 [info ] [MainThread]: Registered adapter: snowflake=1.8.4
[0m19:38:10.750218 [debug] [MainThread]: checksum: 5f4fde221ca8627174a0341efc73099aaec05b8546408ceed6f59b1ec0755115, vars: {}, profile: , target: , version: 1.9.0b3
[0m19:38:11.160201 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m19:38:11.160909 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m19:38:11.166413 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 2 unused configuration paths:
- models.marts
- models.staging
[0m19:38:11.208131 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'eb40feba-2b03-4da8-b2c5-91c36fff5fad', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9f445f0f50>]}
[0m19:38:11.378660 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'eb40feba-2b03-4da8-b2c5-91c36fff5fad', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9f1cf5a210>]}
[0m19:38:11.380065 [info ] [MainThread]: Found 5 models, 2 data tests, 2 sources, 862 macros, 1 unit test
[0m19:38:11.381458 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'eb40feba-2b03-4da8-b2c5-91c36fff5fad', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9f1ced8f90>]}
[0m19:38:11.383677 [info ] [MainThread]: 
[0m19:38:11.384398 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m19:38:11.385052 [info ] [MainThread]: 
[0m19:38:11.386092 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m19:38:11.392835 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_DATAEXPERT_STUDENT_andres'
[0m19:38:11.413554 [debug] [ThreadPool]: Using snowflake connection "list_DATAEXPERT_STUDENT_andres"
[0m19:38:11.414397 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "connection_name": "list_DATAEXPERT_STUDENT_andres"} */
show objects in DATAEXPERT_STUDENT.andres limit 10000
[0m19:38:11.415129 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:38:12.292824 [debug] [ThreadPool]: SQL status: SUCCESS 29 in 0.878 seconds
[0m19:38:12.297119 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: Close
[0m19:38:12.529974 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'eb40feba-2b03-4da8-b2c5-91c36fff5fad', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9f1cc9d350>]}
[0m19:38:12.533171 [info ] [MainThread]: Pulling freshness from warehouse metadata tables for 0 sources
[0m19:38:12.540690 [debug] [Thread-1 (]: Began running node source.dbt_capstone_andres.treasury.fiscal_data
[0m19:38:12.542027 [info ] [Thread-1 (]: 1 of 1 START freshness of treasury.fiscal_data ................................. [RUN]
[0m19:38:12.543230 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_DATAEXPERT_STUDENT_andres, now source.dbt_capstone_andres.treasury.fiscal_data)
[0m19:38:12.544237 [debug] [Thread-1 (]: Began compiling node source.dbt_capstone_andres.treasury.fiscal_data
[0m19:38:12.545262 [debug] [Thread-1 (]: Began executing node source.dbt_capstone_andres.treasury.fiscal_data
[0m19:38:12.557678 [debug] [Thread-1 (]: Using snowflake connection "source.dbt_capstone_andres.treasury.fiscal_data"
[0m19:38:12.558702 [debug] [Thread-1 (]: On source.dbt_capstone_andres.treasury.fiscal_data: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "source.dbt_capstone_andres.treasury.fiscal_data"} */
select
      max(record_date::timestamp) as max_loaded_at,
      convert_timezone('UTC', current_timestamp()) as snapshotted_at
    from dataexpert_student.andres.fiscal_data
[0m19:38:12.559634 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m19:38:13.402043 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 0.842 seconds
[0m19:38:13.404748 [debug] [Thread-1 (]: On source.dbt_capstone_andres.treasury.fiscal_data: Close
[0m19:38:13.628652 [info ] [Thread-1 (]: 1 of 1 PASS freshness of treasury.fiscal_data .................................. [[32mPASS[0m in 1.09s]
[0m19:38:13.629851 [debug] [Thread-1 (]: Finished running node source.dbt_capstone_andres.treasury.fiscal_data
[0m19:38:13.632165 [debug] [MainThread]: Connection 'master' was properly closed.
[0m19:38:13.632998 [debug] [MainThread]: Connection 'source.dbt_capstone_andres.treasury.fiscal_data' was properly closed.
[0m19:38:13.633820 [info ] [MainThread]: 
[0m19:38:13.634662 [info ] [MainThread]: Finished running 1 source in 0 hours 0 minutes and 2.25 seconds (2.25s).
[0m19:38:13.727159 [info ] [MainThread]: Done.
[0m19:38:13.728757 [debug] [MainThread]: Resource report: {"command_name": "freshness", "command_success": true, "command_wall_clock_time": 4.1460595, "process_user_time": 3.688152, "process_kernel_time": 0.832879, "process_mem_max_rss": "210404", "process_in_blocks": "336", "process_out_blocks": "3685"}
[0m19:38:13.729861 [debug] [MainThread]: Command `dbt source freshness` succeeded at 19:38:13.729722 after 4.15 seconds
[0m19:38:13.730876 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9f485a1dd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9f451ac090>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9f48283f10>]}
[0m19:38:13.731880 [debug] [MainThread]: Flushing usage events
[0m19:38:13.943327 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m19:40:36.456574 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0ee3026090>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0ee3025f50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0ee3327850>]}


============================== 19:40:36.459652 | 76a7dec9-a53b-4a18-a65c-d5108091c5d7 ==============================
[0m19:40:36.459652 [info ] [MainThread]: Running with dbt=1.9.0-b3
[0m19:40:36.460664 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/usr/local/airflow/dags/capstone/dbt_capstone_andres', 'debug': 'False', 'version_check': 'True', 'log_path': '/usr/local/airflow/dags/capstone/dbt_capstone_andres/logs', 'warn_error': 'None', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'introspect': 'True', 'invocation_command': 'dbt source freshness', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m19:40:37.579250 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '76a7dec9-a53b-4a18-a65c-d5108091c5d7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0ee2ea4710>]}
[0m19:40:37.675543 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '76a7dec9-a53b-4a18-a65c-d5108091c5d7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0ee3121750>]}
[0m19:40:37.676972 [info ] [MainThread]: Registered adapter: snowflake=1.8.4
[0m19:40:37.727072 [debug] [MainThread]: checksum: 5f4fde221ca8627174a0341efc73099aaec05b8546408ceed6f59b1ec0755115, vars: {}, profile: , target: , version: 1.9.0b3
[0m19:40:38.219123 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m19:40:38.219815 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m19:40:38.226396 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 2 unused configuration paths:
- models.marts
- models.staging
[0m19:40:38.272743 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '76a7dec9-a53b-4a18-a65c-d5108091c5d7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0eb77cdd90>]}
[0m19:40:38.544843 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '76a7dec9-a53b-4a18-a65c-d5108091c5d7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0eb7b767d0>]}
[0m19:40:38.546004 [info ] [MainThread]: Found 5 models, 2 data tests, 2 sources, 862 macros, 1 unit test
[0m19:40:38.546988 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '76a7dec9-a53b-4a18-a65c-d5108091c5d7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0eb7af5110>]}
[0m19:40:38.550044 [info ] [MainThread]: 
[0m19:40:38.551510 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m19:40:38.552634 [info ] [MainThread]: 
[0m19:40:38.553978 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m19:40:38.561225 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_DATAEXPERT_STUDENT_andres'
[0m19:40:38.597939 [debug] [ThreadPool]: Using snowflake connection "list_DATAEXPERT_STUDENT_andres"
[0m19:40:38.598895 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "connection_name": "list_DATAEXPERT_STUDENT_andres"} */
show objects in DATAEXPERT_STUDENT.andres limit 10000
[0m19:40:38.599744 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:40:39.493164 [debug] [ThreadPool]: SQL status: SUCCESS 29 in 0.893 seconds
[0m19:40:39.496460 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: Close
[0m19:40:39.724976 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '76a7dec9-a53b-4a18-a65c-d5108091c5d7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0eb77ee050>]}
[0m19:40:39.726223 [info ] [MainThread]: Pulling freshness from warehouse metadata tables for 0 sources
[0m19:40:39.731110 [debug] [Thread-1 (]: Began running node source.dbt_capstone_andres.treasury.fiscal_data
[0m19:40:39.732147 [info ] [Thread-1 (]: 1 of 1 START freshness of treasury.fiscal_data ................................. [RUN]
[0m19:40:39.733187 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_DATAEXPERT_STUDENT_andres, now source.dbt_capstone_andres.treasury.fiscal_data)
[0m19:40:39.734113 [debug] [Thread-1 (]: Began compiling node source.dbt_capstone_andres.treasury.fiscal_data
[0m19:40:39.735017 [debug] [Thread-1 (]: Began executing node source.dbt_capstone_andres.treasury.fiscal_data
[0m19:40:39.746609 [debug] [Thread-1 (]: Using snowflake connection "source.dbt_capstone_andres.treasury.fiscal_data"
[0m19:40:39.747627 [debug] [Thread-1 (]: On source.dbt_capstone_andres.treasury.fiscal_data: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "source.dbt_capstone_andres.treasury.fiscal_data"} */
select
      max(record_date::timestamp) as max_loaded_at,
      convert_timezone('UTC', current_timestamp()) as snapshotted_at
    from dataexpert_student.andres.fiscal_data
[0m19:40:39.748499 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m19:40:40.423747 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 0.675 seconds
[0m19:40:40.426182 [debug] [Thread-1 (]: On source.dbt_capstone_andres.treasury.fiscal_data: Close
[0m19:40:40.638339 [info ] [Thread-1 (]: 1 of 1 PASS freshness of treasury.fiscal_data .................................. [[32mPASS[0m in 0.91s]
[0m19:40:40.639758 [debug] [Thread-1 (]: Finished running node source.dbt_capstone_andres.treasury.fiscal_data
[0m19:40:40.641997 [debug] [MainThread]: Connection 'master' was properly closed.
[0m19:40:40.642697 [debug] [MainThread]: Connection 'source.dbt_capstone_andres.treasury.fiscal_data' was properly closed.
[0m19:40:40.643463 [info ] [MainThread]: 
[0m19:40:40.644186 [info ] [MainThread]: Finished running 1 source in 0 hours 0 minutes and 2.09 seconds (2.09s).
[0m19:40:40.730554 [info ] [MainThread]: Done.
[0m19:40:40.732062 [debug] [MainThread]: Resource report: {"command_name": "freshness", "command_success": true, "command_wall_clock_time": 4.337214, "process_user_time": 3.939841, "process_kernel_time": 0.903038, "process_mem_max_rss": "210076", "process_out_blocks": "3685", "process_in_blocks": "0"}
[0m19:40:40.736050 [debug] [MainThread]: Command `dbt source freshness` succeeded at 19:40:40.735903 after 4.34 seconds
[0m19:40:40.736910 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0ee2e9bcd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0ee2e9bbd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0ee6daffd0>]}
[0m19:40:40.737659 [debug] [MainThread]: Flushing usage events
[0m19:40:40.942954 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m19:45:06.178524 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0bb205a210>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0bb2059ed0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0bb2059c50>]}


============================== 19:45:06.182273 | e5f135c6-da0a-430e-8f9b-e3296972a075 ==============================
[0m19:45:06.182273 [info ] [MainThread]: Running with dbt=1.9.0-b3
[0m19:45:06.183608 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/usr/local/airflow/dags/capstone/dbt_capstone_andres', 'debug': 'False', 'fail_fast': 'False', 'log_path': '/usr/local/airflow/dags/capstone/dbt_capstone_andres/logs', 'version_check': 'True', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'log_format': 'default', 'introspect': 'True', 'target_path': 'None', 'invocation_command': 'dbt source freshness', 'send_anonymous_usage_stats': 'True'}
[0m19:45:07.256100 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'e5f135c6-da0a-430e-8f9b-e3296972a075', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0b86de2390>]}
[0m19:45:07.323038 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'e5f135c6-da0a-430e-8f9b-e3296972a075', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0bb2159550>]}
[0m19:45:07.324285 [info ] [MainThread]: Registered adapter: snowflake=1.8.4
[0m19:45:07.376080 [debug] [MainThread]: checksum: 5f4fde221ca8627174a0341efc73099aaec05b8546408ceed6f59b1ec0755115, vars: {}, profile: , target: , version: 1.9.0b3
[0m19:45:07.880376 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m19:45:07.881091 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m19:45:07.887075 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 2 unused configuration paths:
- models.marts
- models.staging
[0m19:45:07.930070 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'e5f135c6-da0a-430e-8f9b-e3296972a075', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0bb2599750>]}
[0m19:45:08.109964 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'e5f135c6-da0a-430e-8f9b-e3296972a075', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0b86b621d0>]}
[0m19:45:08.110848 [info ] [MainThread]: Found 5 models, 2 data tests, 2 sources, 862 macros, 1 unit test
[0m19:45:08.111552 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e5f135c6-da0a-430e-8f9b-e3296972a075', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0b86adcad0>]}
[0m19:45:08.113624 [info ] [MainThread]: 
[0m19:45:08.114329 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m19:45:08.115006 [info ] [MainThread]: 
[0m19:45:08.115894 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m19:45:08.121906 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_DATAEXPERT_STUDENT_andres'
[0m19:45:08.143338 [debug] [ThreadPool]: Using snowflake connection "list_DATAEXPERT_STUDENT_andres"
[0m19:45:08.144278 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "connection_name": "list_DATAEXPERT_STUDENT_andres"} */
show objects in DATAEXPERT_STUDENT.andres limit 10000
[0m19:45:08.145082 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:45:09.064391 [debug] [ThreadPool]: SQL status: SUCCESS 29 in 0.919 seconds
[0m19:45:09.068909 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: Close
[0m19:45:09.288926 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e5f135c6-da0a-430e-8f9b-e3296972a075', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0bb401ec10>]}
[0m19:45:09.290076 [info ] [MainThread]: Pulling freshness from warehouse metadata tables for 0 sources
[0m19:45:09.296083 [debug] [Thread-1 (]: Began running node source.dbt_capstone_andres.treasury.fiscal_data
[0m19:45:09.298973 [info ] [Thread-1 (]: 1 of 1 START freshness of treasury.fiscal_data ................................. [RUN]
[0m19:45:09.300007 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_DATAEXPERT_STUDENT_andres, now source.dbt_capstone_andres.treasury.fiscal_data)
[0m19:45:09.300859 [debug] [Thread-1 (]: Began compiling node source.dbt_capstone_andres.treasury.fiscal_data
[0m19:45:09.301919 [debug] [Thread-1 (]: Began executing node source.dbt_capstone_andres.treasury.fiscal_data
[0m19:45:09.313438 [debug] [Thread-1 (]: Using snowflake connection "source.dbt_capstone_andres.treasury.fiscal_data"
[0m19:45:09.314397 [debug] [Thread-1 (]: On source.dbt_capstone_andres.treasury.fiscal_data: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "source.dbt_capstone_andres.treasury.fiscal_data"} */
select
      max(record_date::timestamp) as max_loaded_at,
      convert_timezone('UTC', current_timestamp()) as snapshotted_at
    from dataexpert_student.andres.fiscal_data
[0m19:45:09.315262 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m19:45:10.089611 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 0.774 seconds
[0m19:45:10.093716 [debug] [Thread-1 (]: On source.dbt_capstone_andres.treasury.fiscal_data: Close
[0m19:45:10.314765 [info ] [Thread-1 (]: 1 of 1 PASS freshness of treasury.fiscal_data .................................. [[32mPASS[0m in 1.01s]
[0m19:45:10.316068 [debug] [Thread-1 (]: Finished running node source.dbt_capstone_andres.treasury.fiscal_data
[0m19:45:10.318443 [debug] [MainThread]: Connection 'master' was properly closed.
[0m19:45:10.319898 [debug] [MainThread]: Connection 'source.dbt_capstone_andres.treasury.fiscal_data' was properly closed.
[0m19:45:10.320815 [info ] [MainThread]: 
[0m19:45:10.321974 [info ] [MainThread]: Finished running 1 source in 0 hours 0 minutes and 2.21 seconds (2.21s).
[0m19:45:10.425772 [info ] [MainThread]: Done.
[0m19:45:10.427887 [debug] [MainThread]: Resource report: {"command_name": "freshness", "command_success": true, "command_wall_clock_time": 4.365615, "process_user_time": 3.830341, "process_kernel_time": 0.978312, "process_mem_max_rss": "210468", "process_in_blocks": "144", "process_out_blocks": "3685"}
[0m19:45:10.429441 [debug] [MainThread]: Command `dbt source freshness` succeeded at 19:45:10.429247 after 4.37 seconds
[0m19:45:10.430698 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0bb1ecfb50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0bb1ecf890>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0bb5debfd0>]}
[0m19:45:10.431884 [debug] [MainThread]: Flushing usage events
[0m19:45:10.625899 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m19:45:24.373075 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb97867cd10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb978675990>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb978794550>]}


============================== 19:45:24.378376 | a41c0efe-d28a-4acb-8bef-ede983bc424f ==============================
[0m19:45:24.378376 [info ] [MainThread]: Running with dbt=1.9.0-b3
[0m19:45:24.380131 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/usr/local/airflow/dags/capstone/dbt_capstone_andres', 'version_check': 'True', 'fail_fast': 'False', 'log_path': '/usr/local/airflow/dags/capstone/dbt_capstone_andres/logs', 'warn_error': 'None', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'invocation_command': 'dbt source freshness', 'send_anonymous_usage_stats': 'True'}
[0m19:45:25.929368 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'a41c0efe-d28a-4acb-8bef-ede983bc424f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb95137f6d0>]}
[0m19:45:26.036202 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'a41c0efe-d28a-4acb-8bef-ede983bc424f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb979a6f750>]}
[0m19:45:26.038399 [info ] [MainThread]: Registered adapter: snowflake=1.8.4
[0m19:45:26.121197 [debug] [MainThread]: checksum: 5f4fde221ca8627174a0341efc73099aaec05b8546408ceed6f59b1ec0755115, vars: {}, profile: , target: , version: 1.9.0b3
[0m19:45:27.520699 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m19:45:27.521774 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m19:45:27.529453 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 2 unused configuration paths:
- models.staging
- models.marts
[0m19:45:27.594601 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'a41c0efe-d28a-4acb-8bef-ede983bc424f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb950e1d4d0>]}
[0m19:45:27.892330 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'a41c0efe-d28a-4acb-8bef-ede983bc424f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb9510c6050>]}
[0m19:45:27.893802 [info ] [MainThread]: Found 5 models, 2 data tests, 2 sources, 862 macros, 1 unit test
[0m19:45:27.895045 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a41c0efe-d28a-4acb-8bef-ede983bc424f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb9510494d0>]}
[0m19:45:27.898935 [info ] [MainThread]: 
[0m19:45:27.900947 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m19:45:27.902651 [info ] [MainThread]: 
[0m19:45:27.904123 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m19:45:27.914050 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_DATAEXPERT_STUDENT_andres'
[0m19:45:27.939361 [debug] [ThreadPool]: Using snowflake connection "list_DATAEXPERT_STUDENT_andres"
[0m19:45:27.940333 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "connection_name": "list_DATAEXPERT_STUDENT_andres"} */
show objects in DATAEXPERT_STUDENT.andres limit 10000
[0m19:45:27.941154 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:45:28.880572 [debug] [ThreadPool]: SQL status: SUCCESS 29 in 0.939 seconds
[0m19:45:28.885989 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: Close
[0m19:45:29.107755 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a41c0efe-d28a-4acb-8bef-ede983bc424f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb979439610>]}
[0m19:45:29.108913 [info ] [MainThread]: Pulling freshness from warehouse metadata tables for 0 sources
[0m19:45:29.114546 [debug] [Thread-1 (]: Began running node source.dbt_capstone_andres.treasury.fiscal_data
[0m19:45:29.115633 [info ] [Thread-1 (]: 1 of 1 START freshness of treasury.fiscal_data ................................. [RUN]
[0m19:45:29.116745 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_DATAEXPERT_STUDENT_andres, now source.dbt_capstone_andres.treasury.fiscal_data)
[0m19:45:29.117731 [debug] [Thread-1 (]: Began compiling node source.dbt_capstone_andres.treasury.fiscal_data
[0m19:45:29.118741 [debug] [Thread-1 (]: Began executing node source.dbt_capstone_andres.treasury.fiscal_data
[0m19:45:29.131438 [debug] [Thread-1 (]: Using snowflake connection "source.dbt_capstone_andres.treasury.fiscal_data"
[0m19:45:29.132659 [debug] [Thread-1 (]: On source.dbt_capstone_andres.treasury.fiscal_data: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "source.dbt_capstone_andres.treasury.fiscal_data"} */
select
      max(record_date::timestamp) as max_loaded_at,
      convert_timezone('UTC', current_timestamp()) as snapshotted_at
    from dataexpert_student.andres.fiscal_data
[0m19:45:29.133614 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m19:45:29.856153 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 0.722 seconds
[0m19:45:29.860019 [debug] [Thread-1 (]: On source.dbt_capstone_andres.treasury.fiscal_data: Close
[0m19:45:30.079202 [info ] [Thread-1 (]: 1 of 1 PASS freshness of treasury.fiscal_data .................................. [[32mPASS[0m in 0.96s]
[0m19:45:30.084190 [debug] [Thread-1 (]: Finished running node source.dbt_capstone_andres.treasury.fiscal_data
[0m19:45:30.087645 [debug] [MainThread]: Connection 'master' was properly closed.
[0m19:45:30.089076 [debug] [MainThread]: Connection 'source.dbt_capstone_andres.treasury.fiscal_data' was properly closed.
[0m19:45:30.090426 [info ] [MainThread]: 
[0m19:45:30.092393 [info ] [MainThread]: Finished running 1 source in 0 hours 0 minutes and 2.19 seconds (2.19s).
[0m19:45:30.244309 [info ] [MainThread]: Done.
[0m19:45:30.247064 [debug] [MainThread]: Resource report: {"command_name": "freshness", "command_success": true, "command_wall_clock_time": 5.968988, "process_user_time": 5.086785, "process_kernel_time": 1.314218, "process_mem_max_rss": "210064", "process_out_blocks": "3685", "process_in_blocks": "0"}
[0m19:45:30.250160 [debug] [MainThread]: Command `dbt source freshness` succeeded at 19:45:30.249926 after 5.97 seconds
[0m19:45:30.251736 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb97c475450>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb97c3ebfd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb97c3f4050>]}
[0m19:45:30.252970 [debug] [MainThread]: Flushing usage events
[0m19:45:30.424842 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m19:51:03.038074 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fee7ffc9ed0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fee800cc9d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fee80298490>]}


============================== 19:51:03.041355 | e92f1d13-e622-4711-9234-8f6b563a9374 ==============================
[0m19:51:03.041355 [info ] [MainThread]: Running with dbt=1.9.0-b3
[0m19:51:03.042655 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'profiles_dir': '/usr/local/airflow/dags/capstone/dbt_capstone_andres', 'log_path': '/usr/local/airflow/dags/capstone/dbt_capstone_andres/logs', 'fail_fast': 'False', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'invocation_command': 'dbt source freshness', 'static_parser': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m19:51:04.372122 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'e92f1d13-e622-4711-9234-8f6b563a9374', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fee58b88110>]}
[0m19:51:04.451537 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'e92f1d13-e622-4711-9234-8f6b563a9374', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fee7ffc8250>]}
[0m19:51:04.453446 [info ] [MainThread]: Registered adapter: snowflake=1.8.4
[0m19:51:04.518027 [debug] [MainThread]: checksum: 5f4fde221ca8627174a0341efc73099aaec05b8546408ceed6f59b1ec0755115, vars: {}, profile: , target: , version: 1.9.0b3
[0m19:51:05.057437 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m19:51:05.058141 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m19:51:05.063862 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 2 unused configuration paths:
- models.marts
- models.staging
[0m19:51:05.108798 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'e92f1d13-e622-4711-9234-8f6b563a9374', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fee587783d0>]}
[0m19:51:05.361057 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'e92f1d13-e622-4711-9234-8f6b563a9374', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fee5881df50>]}
[0m19:51:05.361947 [info ] [MainThread]: Found 5 models, 2 data tests, 2 sources, 862 macros, 1 unit test
[0m19:51:05.362603 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e92f1d13-e622-4711-9234-8f6b563a9374', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fee58a88bd0>]}
[0m19:51:05.364558 [info ] [MainThread]: 
[0m19:51:05.365278 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m19:51:05.366010 [info ] [MainThread]: 
[0m19:51:05.366890 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m19:51:05.372315 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_DATAEXPERT_STUDENT_andres'
[0m19:51:05.392445 [debug] [ThreadPool]: Using snowflake connection "list_DATAEXPERT_STUDENT_andres"
[0m19:51:05.393290 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "connection_name": "list_DATAEXPERT_STUDENT_andres"} */
show objects in DATAEXPERT_STUDENT.andres limit 10000
[0m19:51:05.394074 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:51:06.273769 [debug] [ThreadPool]: SQL status: SUCCESS 29 in 0.880 seconds
[0m19:51:06.277718 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: Close
[0m19:51:06.496138 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e92f1d13-e622-4711-9234-8f6b563a9374', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fee5874d050>]}
[0m19:51:06.497243 [info ] [MainThread]: Pulling freshness from warehouse metadata tables for 0 sources
[0m19:51:06.505544 [debug] [Thread-1 (]: Began running node source.dbt_capstone_andres.treasury.fiscal_data
[0m19:51:06.506547 [info ] [Thread-1 (]: 1 of 1 START freshness of treasury.fiscal_data ................................. [RUN]
[0m19:51:06.507553 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_DATAEXPERT_STUDENT_andres, now source.dbt_capstone_andres.treasury.fiscal_data)
[0m19:51:06.508480 [debug] [Thread-1 (]: Began compiling node source.dbt_capstone_andres.treasury.fiscal_data
[0m19:51:06.509304 [debug] [Thread-1 (]: Began executing node source.dbt_capstone_andres.treasury.fiscal_data
[0m19:51:06.520074 [debug] [Thread-1 (]: Using snowflake connection "source.dbt_capstone_andres.treasury.fiscal_data"
[0m19:51:06.521094 [debug] [Thread-1 (]: On source.dbt_capstone_andres.treasury.fiscal_data: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "source.dbt_capstone_andres.treasury.fiscal_data"} */
select
      max(record_date::timestamp) as max_loaded_at,
      convert_timezone('UTC', current_timestamp()) as snapshotted_at
    from dataexpert_student.andres.fiscal_data
[0m19:51:06.522173 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m19:51:07.242889 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 0.721 seconds
[0m19:51:07.244955 [debug] [Thread-1 (]: On source.dbt_capstone_andres.treasury.fiscal_data: Close
[0m19:51:07.475850 [info ] [Thread-1 (]: 1 of 1 PASS freshness of treasury.fiscal_data .................................. [[32mPASS[0m in 0.97s]
[0m19:51:07.476957 [debug] [Thread-1 (]: Finished running node source.dbt_capstone_andres.treasury.fiscal_data
[0m19:51:07.479235 [debug] [MainThread]: Connection 'master' was properly closed.
[0m19:51:07.479836 [debug] [MainThread]: Connection 'source.dbt_capstone_andres.treasury.fiscal_data' was properly closed.
[0m19:51:07.480465 [info ] [MainThread]: 
[0m19:51:07.481283 [info ] [MainThread]: Finished running 1 source in 0 hours 0 minutes and 2.11 seconds (2.11s).
[0m19:51:07.581226 [info ] [MainThread]: Done.
[0m19:51:07.583351 [debug] [MainThread]: Resource report: {"command_name": "freshness", "command_success": true, "command_wall_clock_time": 4.6082525, "process_user_time": 3.98179, "process_kernel_time": 0.932566, "process_mem_max_rss": "210500", "process_out_blocks": "3685", "process_in_blocks": "0"}
[0m19:51:07.585429 [debug] [MainThread]: Command `dbt source freshness` succeeded at 19:51:07.585272 after 4.61 seconds
[0m19:51:07.586256 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fee83dc9550>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fee83d3ffd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fee83d48b90>]}
[0m19:51:07.587256 [debug] [MainThread]: Flushing usage events
[0m19:51:07.805789 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m19:51:30.689202 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0199aed250>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0199cf9310>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0199aedcd0>]}


============================== 19:51:30.711577 | 9140cf27-d551-4779-a06e-00f683ef1e86 ==============================
[0m19:51:30.711577 [info ] [MainThread]: Running with dbt=1.9.0-b3
[0m19:51:30.728272 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/usr/local/airflow/dags/capstone/dbt_capstone_andres', 'fail_fast': 'False', 'warn_error': 'None', 'log_path': '/usr/local/airflow/dags/capstone/dbt_capstone_andres/logs', 'version_check': 'True', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'log_format': 'default', 'invocation_command': 'dbt source freshness', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m19:51:38.472410 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '9140cf27-d551-4779-a06e-00f683ef1e86', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f01728bee10>]}
[0m19:51:38.731267 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '9140cf27-d551-4779-a06e-00f683ef1e86', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f019aedf4d0>]}
[0m19:51:38.734133 [info ] [MainThread]: Registered adapter: snowflake=1.8.4
[0m19:51:38.893858 [debug] [MainThread]: checksum: 5f4fde221ca8627174a0341efc73099aaec05b8546408ceed6f59b1ec0755115, vars: {}, profile: , target: , version: 1.9.0b3
[0m19:51:40.239388 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m19:51:40.242156 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m19:51:40.253501 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 2 unused configuration paths:
- models.marts
- models.staging
[0m19:51:40.341336 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '9140cf27-d551-4779-a06e-00f683ef1e86', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0199c10c10>]}
[0m19:51:40.823227 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '9140cf27-d551-4779-a06e-00f683ef1e86', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0172672010>]}
[0m19:51:40.825477 [info ] [MainThread]: Found 5 models, 2 data tests, 2 sources, 862 macros, 1 unit test
[0m19:51:40.828031 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9140cf27-d551-4779-a06e-00f683ef1e86', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f01725f4d50>]}
[0m19:51:40.834353 [info ] [MainThread]: 
[0m19:51:40.836120 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m19:51:40.837556 [info ] [MainThread]: 
[0m19:51:40.839341 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m19:51:40.859399 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_DATAEXPERT_STUDENT_andres'
[0m19:51:40.927281 [debug] [ThreadPool]: Using snowflake connection "list_DATAEXPERT_STUDENT_andres"
[0m19:51:40.931408 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "connection_name": "list_DATAEXPERT_STUDENT_andres"} */
show objects in DATAEXPERT_STUDENT.andres limit 10000
[0m19:51:40.933457 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:51:42.038166 [debug] [ThreadPool]: SQL status: SUCCESS 29 in 1.105 seconds
[0m19:51:42.053193 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: Close
[0m19:51:42.293708 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9140cf27-d551-4779-a06e-00f683ef1e86', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0172296010>]}
[0m19:51:42.297162 [info ] [MainThread]: Pulling freshness from warehouse metadata tables for 0 sources
[0m19:51:42.309733 [debug] [Thread-1 (]: Began running node source.dbt_capstone_andres.treasury.fiscal_data
[0m19:51:42.311262 [info ] [Thread-1 (]: 1 of 1 START freshness of treasury.fiscal_data ................................. [RUN]
[0m19:51:42.313515 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_DATAEXPERT_STUDENT_andres, now source.dbt_capstone_andres.treasury.fiscal_data)
[0m19:51:42.315354 [debug] [Thread-1 (]: Began compiling node source.dbt_capstone_andres.treasury.fiscal_data
[0m19:51:42.317057 [debug] [Thread-1 (]: Began executing node source.dbt_capstone_andres.treasury.fiscal_data
[0m19:51:42.340623 [debug] [Thread-1 (]: Using snowflake connection "source.dbt_capstone_andres.treasury.fiscal_data"
[0m19:51:42.342391 [debug] [Thread-1 (]: On source.dbt_capstone_andres.treasury.fiscal_data: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "source.dbt_capstone_andres.treasury.fiscal_data"} */
select
      max(record_date::timestamp) as max_loaded_at,
      convert_timezone('UTC', current_timestamp()) as snapshotted_at
    from dataexpert_student.andres.fiscal_data
[0m19:51:42.343996 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m19:51:43.054444 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 0.710 seconds
[0m19:51:43.059501 [debug] [Thread-1 (]: On source.dbt_capstone_andres.treasury.fiscal_data: Close
[0m19:51:43.292353 [info ] [Thread-1 (]: 1 of 1 PASS freshness of treasury.fiscal_data .................................. [[32mPASS[0m in 0.98s]
[0m19:51:43.297703 [debug] [Thread-1 (]: Finished running node source.dbt_capstone_andres.treasury.fiscal_data
[0m19:51:43.305057 [debug] [MainThread]: Connection 'master' was properly closed.
[0m19:51:43.308047 [debug] [MainThread]: Connection 'source.dbt_capstone_andres.treasury.fiscal_data' was properly closed.
[0m19:51:43.311084 [info ] [MainThread]: 
[0m19:51:43.313281 [info ] [MainThread]: Finished running 1 source in 0 hours 0 minutes and 2.47 seconds (2.47s).
[0m19:51:43.650186 [info ] [MainThread]: Done.
[0m19:51:43.654712 [debug] [MainThread]: Resource report: {"command_name": "freshness", "command_success": true, "command_wall_clock_time": 13.528644, "process_user_time": 10.774292, "process_kernel_time": 5.031006, "process_mem_max_rss": "205760", "process_in_blocks": "8952", "process_out_blocks": "3685"}
[0m19:51:43.656751 [debug] [MainThread]: Command `dbt source freshness` succeeded at 19:51:43.656480 after 13.53 seconds
[0m19:51:43.659568 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0199b5b8d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0199b5b810>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f019d90d4d0>]}
[0m19:51:43.662234 [debug] [MainThread]: Flushing usage events
[0m19:51:43.864723 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m19:51:59.967566 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdf529dca10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdf52a23f10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdf529dc0d0>]}


============================== 19:51:59.972326 | 2aa90511-5700-43eb-95d2-306fb0ac4f9e ==============================
[0m19:51:59.972326 [info ] [MainThread]: Running with dbt=1.9.0-b3
[0m19:51:59.974457 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'version_check': 'True', 'log_path': '/usr/local/airflow/dags/capstone/dbt_capstone_andres/logs', 'debug': 'False', 'profiles_dir': '/usr/local/airflow/dags/capstone/dbt_capstone_andres', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt source freshness', 'introspect': 'True', 'log_format': 'default', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m19:52:01.819910 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '2aa90511-5700-43eb-95d2-306fb0ac4f9e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdf276ce8d0>]}
[0m19:52:01.942792 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '2aa90511-5700-43eb-95d2-306fb0ac4f9e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdf53dcf3d0>]}
[0m19:52:01.947276 [info ] [MainThread]: Registered adapter: snowflake=1.8.4
[0m19:52:02.046286 [debug] [MainThread]: checksum: 5f4fde221ca8627174a0341efc73099aaec05b8546408ceed6f59b1ec0755115, vars: {}, profile: , target: , version: 1.9.0b3
[0m19:52:02.989373 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m19:52:02.992051 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m19:52:03.009806 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 2 unused configuration paths:
- models.staging
- models.marts
[0m19:52:03.132763 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '2aa90511-5700-43eb-95d2-306fb0ac4f9e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdf27148550>]}
[0m19:52:03.422053 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '2aa90511-5700-43eb-95d2-306fb0ac4f9e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdf274e60d0>]}
[0m19:52:03.423031 [info ] [MainThread]: Found 5 models, 2 data tests, 2 sources, 862 macros, 1 unit test
[0m19:52:03.423811 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2aa90511-5700-43eb-95d2-306fb0ac4f9e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdf2744cb50>]}
[0m19:52:03.426953 [info ] [MainThread]: 
[0m19:52:03.428077 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m19:52:03.429017 [info ] [MainThread]: 
[0m19:52:03.430719 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m19:52:03.438022 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_DATAEXPERT_STUDENT_andres'
[0m19:52:03.479658 [debug] [ThreadPool]: Using snowflake connection "list_DATAEXPERT_STUDENT_andres"
[0m19:52:03.481093 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "connection_name": "list_DATAEXPERT_STUDENT_andres"} */
show objects in DATAEXPERT_STUDENT.andres limit 10000
[0m19:52:03.482357 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:52:04.389913 [debug] [ThreadPool]: SQL status: SUCCESS 29 in 0.908 seconds
[0m19:52:04.395655 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: Close
[0m19:52:04.622118 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2aa90511-5700-43eb-95d2-306fb0ac4f9e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdf547b91d0>]}
[0m19:52:04.623865 [info ] [MainThread]: Pulling freshness from warehouse metadata tables for 0 sources
[0m19:52:04.636278 [debug] [Thread-1 (]: Began running node source.dbt_capstone_andres.treasury.fiscal_data
[0m19:52:04.637669 [info ] [Thread-1 (]: 1 of 1 START freshness of treasury.fiscal_data ................................. [RUN]
[0m19:52:04.639249 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_DATAEXPERT_STUDENT_andres, now source.dbt_capstone_andres.treasury.fiscal_data)
[0m19:52:04.640489 [debug] [Thread-1 (]: Began compiling node source.dbt_capstone_andres.treasury.fiscal_data
[0m19:52:04.641832 [debug] [Thread-1 (]: Began executing node source.dbt_capstone_andres.treasury.fiscal_data
[0m19:52:04.654943 [debug] [Thread-1 (]: Using snowflake connection "source.dbt_capstone_andres.treasury.fiscal_data"
[0m19:52:04.656100 [debug] [Thread-1 (]: On source.dbt_capstone_andres.treasury.fiscal_data: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "source.dbt_capstone_andres.treasury.fiscal_data"} */
select
      max(record_date::timestamp) as max_loaded_at,
      convert_timezone('UTC', current_timestamp()) as snapshotted_at
    from dataexpert_student.andres.fiscal_data
[0m19:52:04.657336 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m19:52:05.341495 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 0.684 seconds
[0m19:52:05.344011 [debug] [Thread-1 (]: On source.dbt_capstone_andres.treasury.fiscal_data: Close
[0m19:52:05.569044 [info ] [Thread-1 (]: 1 of 1 PASS freshness of treasury.fiscal_data .................................. [[32mPASS[0m in 0.93s]
[0m19:52:05.573478 [debug] [Thread-1 (]: Finished running node source.dbt_capstone_andres.treasury.fiscal_data
[0m19:52:05.576544 [debug] [MainThread]: Connection 'master' was properly closed.
[0m19:52:05.577457 [debug] [MainThread]: Connection 'source.dbt_capstone_andres.treasury.fiscal_data' was properly closed.
[0m19:52:05.580361 [info ] [MainThread]: 
[0m19:52:05.584443 [info ] [MainThread]: Finished running 1 source in 0 hours 0 minutes and 2.15 seconds (2.15s).
[0m19:52:05.735615 [info ] [MainThread]: Done.
[0m19:52:05.738218 [debug] [MainThread]: Resource report: {"command_name": "freshness", "command_success": true, "command_wall_clock_time": 5.8529706, "process_user_time": 5.87776, "process_kernel_time": 1.011789, "process_mem_max_rss": "210012", "process_in_blocks": "184", "process_out_blocks": "3685"}
[0m19:52:05.739828 [debug] [MainThread]: Command `dbt source freshness` succeeded at 19:52:05.739599 after 5.85 seconds
[0m19:52:05.741114 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdf5284bc90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdf52be9810>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdf567e9550>]}
[0m19:52:05.742198 [debug] [MainThread]: Flushing usage events
[0m19:52:05.912957 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m19:52:23.786180 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3e65a6c150>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3e65aa0990>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3e67bf5850>]}


============================== 19:52:23.791284 | b3f48d5f-72df-41fd-a09a-b746974e4ac9 ==============================
[0m19:52:23.791284 [info ] [MainThread]: Running with dbt=1.9.0-b3
[0m19:52:23.793206 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/usr/local/airflow/dags/capstone/dbt_capstone_andres', 'debug': 'False', 'version_check': 'True', 'log_path': '/usr/local/airflow/dags/capstone/dbt_capstone_andres/logs', 'warn_error': 'None', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'static_parser': 'True', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'invocation_command': 'dbt source freshness', 'send_anonymous_usage_stats': 'True'}
[0m19:52:25.664691 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'b3f48d5f-72df-41fd-a09a-b746974e4ac9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3e3e629f50>]}
[0m19:52:25.793329 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'b3f48d5f-72df-41fd-a09a-b746974e4ac9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3e65b717d0>]}
[0m19:52:25.799033 [info ] [MainThread]: Registered adapter: snowflake=1.8.4
[0m19:52:25.898644 [debug] [MainThread]: checksum: 5f4fde221ca8627174a0341efc73099aaec05b8546408ceed6f59b1ec0755115, vars: {}, profile: , target: , version: 1.9.0b3
[0m19:52:27.142860 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m19:52:27.145694 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m19:52:27.159913 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 2 unused configuration paths:
- models.marts
- models.staging
[0m19:52:27.272003 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'b3f48d5f-72df-41fd-a09a-b746974e4ac9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3e3e3d0050>]}
[0m19:52:27.686207 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'b3f48d5f-72df-41fd-a09a-b746974e4ac9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3e3e4be3d0>]}
[0m19:52:27.687991 [info ] [MainThread]: Found 5 models, 2 data tests, 2 sources, 862 macros, 1 unit test
[0m19:52:27.690041 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b3f48d5f-72df-41fd-a09a-b746974e4ac9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3e3e43cb50>]}
[0m19:52:27.695301 [info ] [MainThread]: 
[0m19:52:27.701371 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m19:52:27.703169 [info ] [MainThread]: 
[0m19:52:27.705159 [debug] [MainThread]: Acquiring new snowflake connection 'master'
[0m19:52:27.715874 [debug] [ThreadPool]: Acquiring new snowflake connection 'list_DATAEXPERT_STUDENT_andres'
[0m19:52:27.757474 [debug] [ThreadPool]: Using snowflake connection "list_DATAEXPERT_STUDENT_andres"
[0m19:52:27.759606 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "connection_name": "list_DATAEXPERT_STUDENT_andres"} */
show objects in DATAEXPERT_STUDENT.andres limit 10000
[0m19:52:27.761043 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:52:28.882653 [debug] [ThreadPool]: SQL status: SUCCESS 29 in 1.121 seconds
[0m19:52:28.890249 [debug] [ThreadPool]: On list_DATAEXPERT_STUDENT_andres: Close
[0m19:52:29.122346 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b3f48d5f-72df-41fd-a09a-b746974e4ac9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3e3e43c1d0>]}
[0m19:52:29.128014 [info ] [MainThread]: Pulling freshness from warehouse metadata tables for 0 sources
[0m19:52:29.139200 [debug] [Thread-1 (]: Began running node source.dbt_capstone_andres.treasury.fiscal_data
[0m19:52:29.141092 [info ] [Thread-1 (]: 1 of 1 START freshness of treasury.fiscal_data ................................. [RUN]
[0m19:52:29.142804 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_DATAEXPERT_STUDENT_andres, now source.dbt_capstone_andres.treasury.fiscal_data)
[0m19:52:29.144198 [debug] [Thread-1 (]: Began compiling node source.dbt_capstone_andres.treasury.fiscal_data
[0m19:52:29.145902 [debug] [Thread-1 (]: Began executing node source.dbt_capstone_andres.treasury.fiscal_data
[0m19:52:29.168741 [debug] [Thread-1 (]: Using snowflake connection "source.dbt_capstone_andres.treasury.fiscal_data"
[0m19:52:29.170157 [debug] [Thread-1 (]: On source.dbt_capstone_andres.treasury.fiscal_data: /* {"app": "dbt", "dbt_version": "1.9.0b3", "profile_name": "dbt_capstone_andres", "target_name": "dev", "node_id": "source.dbt_capstone_andres.treasury.fiscal_data"} */
select
      max(record_date::timestamp) as max_loaded_at,
      convert_timezone('UTC', current_timestamp()) as snapshotted_at
    from dataexpert_student.andres.fiscal_data
[0m19:52:29.171864 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m19:52:29.891150 [debug] [Thread-1 (]: SQL status: SUCCESS 1 in 0.719 seconds
[0m19:52:29.895361 [debug] [Thread-1 (]: On source.dbt_capstone_andres.treasury.fiscal_data: Close
[0m19:52:30.121950 [info ] [Thread-1 (]: 1 of 1 PASS freshness of treasury.fiscal_data .................................. [[32mPASS[0m in 0.98s]
[0m19:52:30.124878 [debug] [Thread-1 (]: Finished running node source.dbt_capstone_andres.treasury.fiscal_data
[0m19:52:30.129213 [debug] [MainThread]: Connection 'master' was properly closed.
[0m19:52:30.130560 [debug] [MainThread]: Connection 'source.dbt_capstone_andres.treasury.fiscal_data' was properly closed.
[0m19:52:30.132174 [info ] [MainThread]: 
[0m19:52:30.133584 [info ] [MainThread]: Finished running 1 source in 0 hours 0 minutes and 2.43 seconds (2.43s).
[0m19:52:30.362067 [info ] [MainThread]: Done.
[0m19:52:30.364669 [debug] [MainThread]: Resource report: {"command_name": "freshness", "command_success": true, "command_wall_clock_time": 6.669946, "process_user_time": 6.047178, "process_kernel_time": 1.013083, "process_mem_max_rss": "203964", "process_out_blocks": "3685", "process_in_blocks": "0"}
[0m19:52:30.366570 [debug] [MainThread]: Command `dbt source freshness` succeeded at 19:52:30.366206 after 6.67 seconds
[0m19:52:30.368214 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3e65adba90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3e65adb550>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3e697ebfd0>]}
[0m19:52:30.369933 [debug] [MainThread]: Flushing usage events
[0m19:52:30.564339 [debug] [MainThread]: An error was encountered while trying to flush usage events
