 Waiting for host: postgres 5432
 Astro Runtime Version: 11.3.0
 [[34m2024-11-27T14:09:55.142+0000[0m] {[34mconfiguration.py:[0m1050} WARNING[0m - section/key [openlineage/namespace] not found in config[0m
 [[34m2024-11-27T14:10:09.169+0000[0m] {[34mplugin.py:[0m32} WARNING[0m - Astro managed secrets backend is disabled[0m
 /usr/local/lib/python3.11/site-packages/airflow/cli/commands/db_command.py:69 DeprecationWarning: `db upgrade` is deprecated. Use `db migrate` instead.
 DB: postgresql://postgres:***@postgres:5432
 Performing upgrade to the metadata database postgresql://postgres:***@postgres:5432
 [[34m2024-11-27T14:10:10.347+0000[0m] {[34mmigration.py:[0m216} INFO[0m - Context impl PostgresqlImpl.[0m
 [[34m2024-11-27T14:10:10.347+0000[0m] {[34mmigration.py:[0m219} INFO[0m - Will assume transactional DDL.[0m
 [[34m2024-11-27T14:10:10.525+0000[0m] {[34mdb.py:[0m1623} INFO[0m - Creating tables[0m
 INFO  [alembic.runtime.migration] Context impl PostgresqlImpl.
 INFO  [alembic.runtime.migration] Will assume transactional DDL.
 ERROR [airflow.utils.timeout.TimeoutPosix] Process timed out, PID: 10
 Traceback (most recent call last):
   File "/usr/local/bin/airflow", line 8, in <module>
     sys.exit(main())
              ^^^^^^
   File "/usr/local/lib/python3.11/site-packages/airflow/__main__.py", line 58, in main
     args.func(args)
   File "/usr/local/lib/python3.11/site-packages/airflow/cli/cli_config.py", line 49, in command
     return func(*args, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^
   File "/usr/local/lib/python3.11/site-packages/airflow/cli/commands/db_command.py", line 70, in upgradedb
     migratedb(args)
   File "/usr/local/lib/python3.11/site-packages/airflow/utils/cli.py", line 114, in wrapper
     return f(*args, **kwargs)
            ^^^^^^^^^^^^^^^^^^
   File "/usr/local/lib/python3.11/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
     return func(*args, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^
   File "/usr/local/lib/python3.11/site-packages/airflow/cli/commands/db_command.py", line 130, in migratedb
     db.upgradedb(
   File "/usr/local/lib/python3.11/site-packages/airflow/utils/session.py", line 79, in wrapper
     return func(*args, session=session, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "/usr/local/lib/python3.11/site-packages/airflow/utils/db.py", line 1639, in upgradedb
     _reserialize_dags(session=session)
   File "/usr/local/lib/python3.11/site-packages/airflow/utils/db.py", line 896, in _reserialize_dags
     dagbag.collect_dags(only_if_updated=False)
   File "/usr/local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
     found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "/usr/local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 318, in process_file
     mods = self._load_modules_from_file(filepath, safe_mode)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "/usr/local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
     return parse(mod_name, filepath)
            ^^^^^^^^^^^^^^^^^^^^^^^^^
   File "/usr/local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
     loader.exec_module(new_module)
   File "<frozen importlib._bootstrap_external>", line 940, in exec_module
   File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
   File "/usr/local/airflow/dags/dbt/dbt_dag_cosmos_DbtDag.py", line 28, in <module>
     dbt_dag_cosmos_DbtDag = DbtDag(
                             ^^^^^^^
   File "/usr/local/lib/python3.11/site-packages/cosmos/airflow/dag.py", line 26, in __init__
     DbtToAirflowConverter.__init__(self, *args, **specific_kwargs(**kwargs))
   File "/usr/local/lib/python3.11/site-packages/cosmos/converter.py", line 260, in __init__
     self.dbt_graph.load(method=render_config.load_method, execution_mode=execution_config.execution_mode)
   File "/usr/local/lib/python3.11/site-packages/cosmos/dbt/graph.py", line 402, in load
     self.load_via_dbt_ls()
   File "/usr/local/lib/python3.11/site-packages/cosmos/dbt/graph.py", line 461, in load_via_dbt_ls
     self.load_via_dbt_ls_without_cache()
   File "/usr/local/lib/python3.11/site-packages/cosmos/dbt/graph.py", line 580, in load_via_dbt_ls_without_cache
     nodes = self.run_dbt_ls(dbt_cmd, self.project_path, tmpdir_path, env)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "/usr/local/lib/python3.11/site-packages/cosmos/dbt/graph.py", line 453, in run_dbt_ls
     self.save_dbt_ls_cache(stdout)
   File "/usr/local/lib/python3.11/site-packages/cosmos/dbt/graph.py", line 333, in save_dbt_ls_cache
     Variable.set(self.dbt_ls_cache_key, cache_dict, serialize_json=True)
   File "/usr/local/lib/python3.11/site-packages/airflow/utils/session.py", line 79, in wrapper
     return func(*args, session=session, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "/usr/local/lib/python3.11/site-packages/airflow/api_internal/internal_api_call.py", line 115, in wrapper
     return func(*args, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^
   File "/usr/local/lib/python3.11/site-packages/airflow/models/variable.py", line 179, in set
     Variable.delete(key, session=session)
   File "/usr/local/lib/python3.11/site-packages/airflow/utils/session.py", line 76, in wrapper
     return func(*args, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^
   File "/usr/local/lib/python3.11/site-packages/airflow/api_internal/internal_api_call.py", line 115, in wrapper
     return func(*args, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^
   File "/usr/local/lib/python3.11/site-packages/airflow/models/variable.py", line 220, in delete
     rows = session.execute(delete(Variable).where(Variable.key == key)).rowcount
                                                   ^^^^^^^^^^^^^^^^^^^
   File "/usr/local/lib/python3.11/site-packages/sqlalchemy/sql/operators.py", line 387, in __eq__
     return self.operate(eq, other)
            ^^^^^^^^^^^^^^^^^^^^^^^
   File "/usr/local/lib/python3.11/site-packages/sqlalchemy/orm/attributes.py", line 322, in operate
     return op(self.comparator, *other, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "/usr/local/lib/python3.11/site-packages/sqlalchemy/sql/operators.py", line 387, in __eq__
     return self.operate(eq, other)
            ^^^^^^^^^^^^^^^^^^^^^^^
   File "/usr/local/lib/python3.11/site-packages/sqlalchemy/orm/properties.py", line 426, in operate
     return op(self.__clause_element__(), *other, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "/usr/local/lib/python3.11/site-packages/sqlalchemy/sql/annotation.py", line 225, in __eq__
     return self.__element.__class__.__eq__(self, other)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "/usr/local/lib/python3.11/site-packages/sqlalchemy/sql/operators.py", line 387, in __eq__
     return self.operate(eq, other)
            ^^^^^^^^^^^^^^^^^^^^^^^
   File "/usr/local/lib/python3.11/site-packages/sqlalchemy/sql/elements.py", line 873, in operate
     return op(self.comparator, *other, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "/usr/local/lib/python3.11/site-packages/sqlalchemy/sql/operators.py", line 387, in __eq__
     return self.operate(eq, other)
            ^^^^^^^^^^^^^^^^^^^^^^^
   File "/usr/local/lib/python3.11/site-packages/sqlalchemy/sql/type_api.py", line 77, in operate
     return o[0](self.expr, op, *(other + o[1:]), **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "/usr/local/lib/python3.11/site-packages/sqlalchemy/sql/default_comparator.py", line 30, in _boolean_compare
     def _boolean_compare(
     
   File "/usr/local/lib/python3.11/site-packages/airflow/utils/timeout.py", line 69, in handle_timeout
     raise AirflowTaskTimeout(self.error_message)
 airflow.exceptions.AirflowTaskTimeout: DagBag import timeout for /usr/local/airflow/dags/dbt/dbt_dag_cosmos_DbtDag.py after 30.0s.
 Please take a look at these docs to improve your DAG import time:
 * http://apache-airflow-docs.s3-website.eu-central-1.amazonaws.com/docs/apache-airflow/stable/best-practices.html#top-level-python-code
 * http://apache-airflow-docs.s3-website.eu-central-1.amazonaws.com/docs/apache-airflow/stable/best-practices.html#reducing-dag-complexity, PID: 10
 [[34m2024-11-27T14:11:52.058+0000[0m] {[34mconfiguration.py:[0m1050} WARNING[0m - section/key [openlineage/namespace] not found in config[0m
 [[34m2024-11-27T14:11:53.901+0000[0m] {[34mplugin.py:[0m32} WARNING[0m - Astro managed secrets backend is disabled[0m
 Usage: airflow [-h] GROUP_OR_COMMAND ...
 
 Positional Arguments:
   GROUP_OR_COMMAND
 
 
     Groups
 airflow command error: argument GROUP_OR_COMMAND: `airflow upgradedb` command, has been removed, please use `airflow db upgrade`, see help above.
       config         View configuration
       connections    Manage connections
       dags           Manage DAGs
       db             Database operations
       jobs           Manage jobs
       pools          Manage pools
       providers      Display providers
       roles          Manage roles
       tasks          Manage tasks
       users          Manage users
       variables      Manage variables
 
     Commands:
       cheat-sheet    Display cheat sheet
       dag-processor  Start a standalone Dag Processor instance
       info           Show information about current Airflow and environment
       kerberos       Start a kerberos ticket renewer
       plugins        Dump information about loaded plugins
       rotate-fernet-key
                      Rotate encrypted connection credentials and variables
       scheduler      Start a scheduler instance
       standalone     Run an all-in-one copy of Airflow
       sync-perm      Update permissions for existing roles and optionally DAGs
       triggerer      Start a triggerer instance
       version        Show the version
       webserver      Start a Airflow webserver instance
 
 Options:
   -h, --help         show this help message and exit
 Waiting for host: postgres 5432
 Astro Runtime Version: 11.3.0
 [[34m2024-11-27T14:11:59.076+0000[0m] {[34mconfiguration.py:[0m1050} WARNING[0m - section/key [openlineage/namespace] not found in config[0m
 [[34m2024-11-27T14:12:04.649+0000[0m] {[34mplugin.py:[0m32} WARNING[0m - Astro managed secrets backend is disabled[0m
 /usr/local/lib/python3.11/site-packages/airflow/cli/commands/db_command.py:69 DeprecationWarning: `db upgrade` is deprecated. Use `db migrate` instead.
 DB: postgresql://postgres:***@postgres:5432
 Performing upgrade to the metadata database postgresql://postgres:***@postgres:5432
 [[34m2024-11-27T14:12:05.692+0000[0m] {[34mmigration.py:[0m216} INFO[0m - Context impl PostgresqlImpl.[0m
 [[34m2024-11-27T14:12:05.693+0000[0m] {[34mmigration.py:[0m219} INFO[0m - Will assume transactional DDL.[0m
 [[34m2024-11-27T14:12:05.696+0000[0m] {[34mdb.py:[0m1623} INFO[0m - Creating tables[0m
 INFO  [alembic.runtime.migration] Context impl PostgresqlImpl.
 INFO  [alembic.runtime.migration] Will assume transactional DDL.
 WARNI [cosmos.airflow.graph] Unavailable conversion function for <DbtResourceType.UNIT_TEST> (node <unit_test.jaffle_shop.agg_orders.test_amount_sum>). Define a converter function using render_config.node_converters.
 /usr/local/lib/python3.11/site-packages/airflow/models/dagbag.py:351 RemovedInAirflow3Warning: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 Data loaded successfully
 Database migrating done!
 [[34m2024-11-27T14:12:56.184+0000[0m] {[34mconfiguration.py:[0m1050} WARNING[0m - section/key [openlineage/namespace] not found in config[0m
 [[34m2024-11-27T14:12:58.603+0000[0m] {[34mplugin.py:[0m32} WARNING[0m - Astro managed secrets backend is disabled[0m
   ____________       _____________
  ____    |__( )_________  __/__  /________      __
 ____  /| |_  /__  ___/_  /_ __  /_  __ \_ | /| / /
 ___  ___ |  / _  /   _  __/ _  / / /_/ /_ |/ |/ /
  _/_/  |_/_/  /_/    /_/    /_/  \____/____/|__/
 [[34m2024-11-27T14:12:58.984+0000[0m] {[34mtask_context_logger.py:[0m63} INFO[0m - Task context logging is enabled[0m
 [[34m2024-11-27T14:12:58.984+0000[0m] {[34mexecutor_loader.py:[0m235} INFO[0m - Loaded executor: LocalExecutor[0m
 [2024-11-27 14:12:59 +0000] [46] [INFO] Starting gunicorn 22.0.0
 [2024-11-27 14:12:59 +0000] [46] [INFO] Listening at: http://[::]:8793 (46)
 [2024-11-27 14:12:59 +0000] [46] [INFO] Using worker: sync
 [[34m2024-11-27T14:12:59.089+0000[0m] {[34mscheduler_job_runner.py:[0m796} INFO[0m - Starting the scheduler[0m
 [[34m2024-11-27T14:12:59.090+0000[0m] {[34mscheduler_job_runner.py:[0m803} INFO[0m - Processing each file at most -1 times[0m
 [2024-11-27 14:12:59 +0000] [48] [INFO] Booting worker with pid: 48
 [2024-11-27 14:12:59 +0000] [57] [INFO] Booting worker with pid: 57
 [[34m2024-11-27T14:12:59.706+0000[0m] {[34mmanager.py:[0m170} INFO[0m - Launched DagFileProcessorManager with pid: 179[0m
 [[34m2024-11-27T14:12:59.708+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
 [[34m2024-11-27T14:12:59.796+0000[0m] {[34msettings.py:[0m60} INFO[0m - Configured default timezone UTC[0m
 [[34m2024-11-27T14:13:05.551+0000[0m] {[34mupdate_checks.py:[0m186} INFO[0m - Checking for new version of Astronomer Runtime, previous check was performed at 2024-11-26 01:58:54.181499+00:00[0m
 [[34m2024-11-27T14:13:09.574+0000[0m] {[34mupdate_checks.py:[0m142} INFO[0m - Check finished, next check in 86400.0 seconds[0m
 [[34m2024-11-27T14:13:25.258+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
 	<TaskInstance: capstone_andres_dag_.load_polygon_data_task manual__2024-11-27T14:13:24.590066+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:13:25.259+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 0/16 running and queued tasks[0m
 [[34m2024-11-27T14:13:25.259+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
 	<TaskInstance: capstone_andres_dag_.load_polygon_data_task manual__2024-11-27T14:13:24.590066+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:13:25.265+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='load_polygon_data_task', run_id='manual__2024-11-27T14:13:24.590066+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
 [[34m2024-11-27T14:13:25.266+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'load_polygon_data_task', 'manual__2024-11-27T14:13:24.590066+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:13:25.292+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'load_polygon_data_task', 'manual__2024-11-27T14:13:24.590066+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:13:25.692+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:13:29.611+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:13:29.612+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:13:29.612+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:13:30.149+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412078178,
 "os.name" : Linux
 [0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:13:31.106+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 [[34m2024-11-27T14:13:32.087+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:13:35.104+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:13:35.463+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T14:13:35.741+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:13:36.787+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:13:37.042+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:13:37.259+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 Data loaded successfully
 [[34m2024-11-27T14:13:37.959+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:13:37.984+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 [[34m2024-11-27T14:13:39.916+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.6928589390590787[0m
 [[34m2024-11-27T14:13:39.917+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|204]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:13:39.919+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:13:39.920+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:13:39.920+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:13:39.921+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|204]: It took 1.96s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:13:39.928+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|204]: It took 0.00712s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:13:40.029+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.load_polygon_data_task manual__2024-11-27T14:13:24.590066+00:00 [queued]> on host 75994b6d2295[0m
 [2024-11-27T14:13:40.894+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=load_polygon_data_task, dagrun_id=manual__2024-11-27T14:13:24.590066+00:00, map_index=-1, run_start_date=2024-11-27 14:13:40.295015+00:00, try_number=1, job_id=804, op_classpath=airflow.operators.python.PythonOperator
 [[34m2024-11-27T14:13:48.685+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
 	<TaskInstance: capstone_andres_dag_.load_treasury_data_task manual__2024-11-27T14:13:24.590066+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:13:48.685+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 0/16 running and queued tasks[0m
 [[34m2024-11-27T14:13:48.685+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
 	<TaskInstance: capstone_andres_dag_.load_treasury_data_task manual__2024-11-27T14:13:24.590066+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:13:48.687+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='load_treasury_data_task', run_id='manual__2024-11-27T14:13:24.590066+00:00', try_number=1, map_index=-1) to executor with priority 5 and queue default[0m
 [[34m2024-11-27T14:13:48.688+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'load_treasury_data_task', 'manual__2024-11-27T14:13:24.590066+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:13:48.697+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='load_polygon_data_task', run_id='manual__2024-11-27T14:13:24.590066+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:13:48.704+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=load_polygon_data_task, run_id=manual__2024-11-27T14:13:24.590066+00:00, map_index=-1, run_start_date=2024-11-27 14:13:40.295015+00:00, run_end_date=2024-11-27 14:13:48.270658+00:00, run_duration=7.975643, state=success, executor_state=success, try_number=1, max_tries=0, job_id=804, pool=default_pool, queue=default, priority_weight=6, operator=PythonOperator, queued_dttm=2024-11-27 14:13:25.260887+00:00, queued_by_job_id=803, pid=252[0m
 [[34m2024-11-27T14:13:48.740+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'load_treasury_data_task', 'manual__2024-11-27T14:13:24.590066+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:13:48.854+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:13:49.887+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:13:49.888+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:13:49.888+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:13:50.456+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412093970,
 "os.name" : Linux
 [0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:13:51.248+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 [[34m2024-11-27T14:13:52.060+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:13:54.714+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:13:55.086+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T14:13:55.413+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:13:56.516+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:13:56.671+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:13:56.829+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 Data loaded successfully
 [[34m2024-11-27T14:13:57.449+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:13:57.461+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 [[34m2024-11-27T14:13:58.257+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.26631983486004174[0m
 [[34m2024-11-27T14:13:58.257+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|320]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:13:58.258+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:13:58.258+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:13:58.258+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:13:58.258+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|320]: It took 0.809s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:13:58.261+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|320]: It took 0.0022s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:13:58.291+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.load_treasury_data_task manual__2024-11-27T14:13:24.590066+00:00 [queued]> on host 75994b6d2295[0m
 [2024-11-27T14:13:58.734+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=load_treasury_data_task, dagrun_id=manual__2024-11-27T14:13:24.590066+00:00, map_index=-1, run_start_date=2024-11-27 14:13:58.373063+00:00, try_number=1, job_id=805, op_classpath=airflow.operators.python.PythonOperator
 [[34m2024-11-27T14:14:00.081+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.weekly_metrics'[0m
 [[34m2024-11-27T14:14:00.081+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections'[0m
 [[34m2024-11-27T14:14:00.081+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.dim_date'[0m
 [[34m2024-11-27T14:14:00.081+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_payments'[0m
 [[34m2024-11-27T14:14:00.082+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_orders'[0m
 [[34m2024-11-27T14:14:00.082+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_daily_bars'[0m
 [[34m2024-11-27T14:14:00.510+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.dim_date_run manual__2024-11-27T14:13:24.590066+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_daily_bars_run manual__2024-11-27T14:13:24.590066+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run manual__2024-11-27T14:13:24.590066+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:14:00.510+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 0/16 running and queued tasks[0m
 [[34m2024-11-27T14:14:00.510+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 1/16 running and queued tasks[0m
 [[34m2024-11-27T14:14:00.510+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 2/16 running and queued tasks[0m
 [[34m2024-11-27T14:14:00.511+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.dim_date_run manual__2024-11-27T14:13:24.590066+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_daily_bars_run manual__2024-11-27T14:13:24.590066+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run manual__2024-11-27T14:13:24.590066+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:14:00.513+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.dim_date_run', run_id='manual__2024-11-27T14:13:24.590066+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
 [[34m2024-11-27T14:14:00.513+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.dim_date_run', 'manual__2024-11-27T14:13:24.590066+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:14:00.514+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.stg_daily_bars_run', run_id='manual__2024-11-27T14:13:24.590066+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
 [[34m2024-11-27T14:14:00.514+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.stg_daily_bars_run', 'manual__2024-11-27T14:13:24.590066+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:14:00.514+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run', run_id='manual__2024-11-27T14:13:24.590066+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
 [[34m2024-11-27T14:14:00.514+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run', 'manual__2024-11-27T14:13:24.590066+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:14:00.520+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='load_treasury_data_task', run_id='manual__2024-11-27T14:13:24.590066+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:14:00.524+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=load_treasury_data_task, run_id=manual__2024-11-27T14:13:24.590066+00:00, map_index=-1, run_start_date=2024-11-27 14:13:58.373063+00:00, run_end_date=2024-11-27 14:14:00.047742+00:00, run_duration=1.674679, state=success, executor_state=success, try_number=1, max_tries=0, job_id=805, pool=default_pool, queue=default, priority_weight=5, operator=PythonOperator, queued_dttm=2024-11-27 14:13:48.686450+00:00, queued_by_job_id=803, pid=342[0m
 [[34m2024-11-27T14:14:00.526+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.stg_daily_bars_run', 'manual__2024-11-27T14:13:24.590066+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:14:00.527+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run', 'manual__2024-11-27T14:13:24.590066+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:14:00.526+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.dim_date_run', 'manual__2024-11-27T14:13:24.590066+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:14:00.569+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:14:00.570+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:14:00.570+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:14:01.442+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:14:01.443+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:14:01.443+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:14:01.443+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:14:01.444+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:14:01.444+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:14:01.446+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:14:01.447+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:14:01.447+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:14:01.894+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412087246,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T14:14:01.947+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412093974,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T14:14:01.990+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412078182,
 "os.name" : Linux
 [0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:14:02.570+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:14:02.600+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:14:02.694+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 [[34m2024-11-27T14:14:03.320+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 [[34m2024-11-27T14:14:03.426+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:14:03.757+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:14:04.816+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:14:05.142+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T14:14:05.464+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:14:06.127+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:14:06.428+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:14:06.447+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T14:14:06.575+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:14:06.723+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:14:06.730+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:14:06.766+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T14:14:06.881+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 [[34m2024-11-27T14:14:07.064+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 Data loaded successfully
 [[34m2024-11-27T14:14:07.407+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:14:07.419+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 [[34m2024-11-27T14:14:07.690+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:14:07.827+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:14:07.928+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 [[34m2024-11-27T14:14:08.060+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:14:08.193+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:14:08.306+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.31328305089846253[0m
 [[34m2024-11-27T14:14:08.306+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|350]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:14:08.307+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:14:08.307+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:14:08.307+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:14:08.307+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|350]: It took 0.9s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:14:08.310+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|350]: It took 0.0022s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:14:08.319+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 [[34m2024-11-27T14:14:08.437+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.dim_date_run manual__2024-11-27T14:13:24.590066+00:00 [queued]> on host 75994b6d2295[0m
 Data loaded successfully
 [[34m2024-11-27T14:14:08.511+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:14:08.524+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 Data loaded successfully
 [[34m2024-11-27T14:14:08.809+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:14:08.821+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 [[34m2024-11-27T14:14:09.556+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.45020957314409316[0m
 [[34m2024-11-27T14:14:09.569+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|348]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:14:09.580+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:14:09.585+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:14:09.587+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:14:09.589+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|348]: It took 1.08s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:14:09.640+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|348]: It took 0.0517s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:14:10.337+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_daily_bars_run manual__2024-11-27T14:13:24.590066+00:00 [queued]> on host 75994b6d2295[0m
 [[34m2024-11-27T14:14:10.916+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 1.5038225648459047[0m
 [[34m2024-11-27T14:14:10.920+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|349]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:14:10.925+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:14:10.926+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:14:10.928+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:14:10.931+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|349]: It took 2.12s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:14:10.981+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|349]: It took 0.0498s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:14:11.102+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run manual__2024-11-27T14:13:24.590066+00:00 [queued]> on host 75994b6d2295[0m
 [2024-11-27T14:14:11.770+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.dim_date_run, dagrun_id=manual__2024-11-27T14:13:24.590066+00:00, map_index=-1, run_start_date=2024-11-27 14:14:08.588110+00:00, try_number=1, job_id=806, op_classpath=cosmos.operators.local.DbtRunLocalOperator, cosmos.operators.base.DbtRunMixin, cosmos.operators.local.DbtLocalBaseOperator, cosmos.operators.base.AbstractDbtBaseOperator
 [2024-11-27T14:14:13.613+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.stg_daily_bars_run, dagrun_id=manual__2024-11-27T14:13:24.590066+00:00, map_index=-1, run_start_date=2024-11-27 14:14:10.759032+00:00, try_number=1, job_id=807, op_classpath=cosmos.operators.local.DbtRunLocalOperator, cosmos.operators.base.DbtRunMixin, cosmos.operators.local.DbtLocalBaseOperator, cosmos.operators.base.AbstractDbtBaseOperator
 [2024-11-27T14:14:18.928+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run, dagrun_id=manual__2024-11-27T14:13:24.590066+00:00, map_index=-1, run_start_date=2024-11-27 14:14:11.287738+00:00, try_number=1, job_id=808, op_classpath=cosmos.operators.local.DbtRunLocalOperator, cosmos.operators.base.DbtRunMixin, cosmos.operators.local.DbtLocalBaseOperator, cosmos.operators.base.AbstractDbtBaseOperator
 /usr/local/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked semaphore objects to clean up at shutdown
   warnings.warn('resource_tracker: There appear to be %d '
 /usr/local/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked semaphore objects to clean up at shutdown
   warnings.warn('resource_tracker: There appear to be %d '
 [[34m2024-11-27T14:14:50.184+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.stg_daily_bars_run', run_id='manual__2024-11-27T14:13:24.590066+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:14:50.188+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run', run_id='manual__2024-11-27T14:13:24.590066+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:14:50.195+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run, run_id=manual__2024-11-27T14:13:24.590066+00:00, map_index=-1, run_start_date=2024-11-27 14:14:11.287738+00:00, run_end_date=2024-11-27 14:14:49.679552+00:00, run_duration=38.391814, state=success, executor_state=success, try_number=1, max_tries=0, job_id=808, pool=default_pool, queue=default, priority_weight=2, operator=DbtRunLocalOperator, queued_dttm=2024-11-27 14:14:00.511556+00:00, queued_by_job_id=803, pid=401[0m
 [[34m2024-11-27T14:14:50.195+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.stg_daily_bars_run, run_id=manual__2024-11-27T14:13:24.590066+00:00, map_index=-1, run_start_date=2024-11-27 14:14:10.759032+00:00, run_end_date=2024-11-27 14:14:49.705870+00:00, run_duration=38.946838, state=success, executor_state=success, try_number=1, max_tries=0, job_id=807, pool=default_pool, queue=default, priority_weight=2, operator=DbtRunLocalOperator, queued_dttm=2024-11-27 14:14:00.511556+00:00, queued_by_job_id=803, pid=400[0m
 /usr/local/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked semaphore objects to clean up at shutdown
   warnings.warn('resource_tracker: There appear to be %d '
 [[34m2024-11-27T14:14:51.281+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.weekly_metrics_run manual__2024-11-27T14:13:24.590066+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:14:51.282+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 0/16 running and queued tasks[0m
 [[34m2024-11-27T14:14:51.282+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.weekly_metrics_run manual__2024-11-27T14:13:24.590066+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:14:51.287+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.weekly_metrics_run', run_id='manual__2024-11-27T14:13:24.590066+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
 [[34m2024-11-27T14:14:51.288+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.weekly_metrics_run', 'manual__2024-11-27T14:13:24.590066+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:14:51.294+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.dim_date_run', run_id='manual__2024-11-27T14:13:24.590066+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:14:51.302+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.dim_date_run, run_id=manual__2024-11-27T14:13:24.590066+00:00, map_index=-1, run_start_date=2024-11-27 14:14:08.588110+00:00, run_end_date=2024-11-27 14:14:50.691593+00:00, run_duration=42.103483, state=success, executor_state=success, try_number=1, max_tries=0, job_id=806, pool=default_pool, queue=default, priority_weight=2, operator=DbtRunLocalOperator, queued_dttm=2024-11-27 14:14:00.511556+00:00, queued_by_job_id=803, pid=393[0m
 [[34m2024-11-27T14:14:51.311+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.weekly_metrics_run', 'manual__2024-11-27T14:13:24.590066+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:14:51.523+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:14:53.728+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:14:53.729+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:14:53.729+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:14:54.285+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412078194,
 "os.name" : Linux
 [0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:14:55.457+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 [[34m2024-11-27T14:14:56.247+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:14:59.444+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:14:59.801+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T14:15:00.131+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.weekly_metrics'[0m
 [[34m2024-11-27T14:15:00.131+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections'[0m
 [[34m2024-11-27T14:15:00.132+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_payments'[0m
 [[34m2024-11-27T14:15:00.132+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_orders'[0m
 [[34m2024-11-27T14:15:00.132+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_daily_bars'[0m
 [[34m2024-11-27T14:15:00.249+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:15:01.375+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:15:01.627+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:15:01.772+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 Data loaded successfully
 [[34m2024-11-27T14:15:02.423+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:15:02.445+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 [[34m2024-11-27T14:15:03.654+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.5771990050561726[0m
 [[34m2024-11-27T14:15:03.655+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|562]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:15:03.658+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:15:03.658+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:15:03.659+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:15:03.660+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|562]: It took 1.24s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:15:03.672+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|562]: It took 0.0117s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:15:03.838+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.weekly_metrics_run manual__2024-11-27T14:13:24.590066+00:00 [queued]> on host 75994b6d2295[0m
 [2024-11-27T14:15:05.271+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.weekly_metrics_run, dagrun_id=manual__2024-11-27T14:13:24.590066+00:00, map_index=-1, run_start_date=2024-11-27 14:15:04.090363+00:00, try_number=1, job_id=809, op_classpath=cosmos.operators.local.DbtRunLocalOperator, cosmos.operators.base.DbtRunMixin, cosmos.operators.local.DbtLocalBaseOperator, cosmos.operators.base.AbstractDbtBaseOperator
 /usr/local/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked semaphore objects to clean up at shutdown
   warnings.warn('resource_tracker: There appear to be %d '
 [[34m2024-11-27T14:15:14.439+0000[0m] {[34mdagrun.py:[0m851} INFO[0m - Marking run <DagRun capstone_andres_dag_ @ 2024-11-27 14:13:24.590066+00:00: manual__2024-11-27T14:13:24.590066+00:00, state:running, queued_at: 2024-11-27 14:13:24.691959+00:00. externally triggered: True> successful[0m
 [[34m2024-11-27T14:15:14.445+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=capstone_andres_dag_, execution_date=2024-11-27 14:13:24.590066+00:00, run_id=manual__2024-11-27T14:13:24.590066+00:00, run_start_date=2024-11-27 14:13:25.036264+00:00, run_end_date=2024-11-27 14:15:14.444312+00:00, run_duration=109.408048, state=success, external_trigger=True, run_type=manual, data_interval_start=2024-11-27 14:13:24.590066+00:00, data_interval_end=2024-11-27 14:13:24.590066+00:00, dag_hash=c4d2181aeb8e4d6b1a7c72bd27c96d19[0m
 [[34m2024-11-27T14:15:14.464+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.weekly_metrics_run', run_id='manual__2024-11-27T14:13:24.590066+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:15:14.471+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.weekly_metrics_run, run_id=manual__2024-11-27T14:13:24.590066+00:00, map_index=-1, run_start_date=2024-11-27 14:15:04.090363+00:00, run_end_date=2024-11-27 14:15:13.659254+00:00, run_duration=9.568891, state=success, executor_state=success, try_number=1, max_tries=0, job_id=809, pool=default_pool, queue=default, priority_weight=1, operator=DbtRunLocalOperator, queued_dttm=2024-11-27 14:14:51.283408+00:00, queued_by_job_id=803, pid=592[0m
 [[34m2024-11-27T14:16:00.171+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.weekly_metrics'[0m
 [[34m2024-11-27T14:16:00.172+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections'[0m
 [[34m2024-11-27T14:16:00.172+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.dim_date'[0m
 [[34m2024-11-27T14:16:00.172+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_payments'[0m
 [[34m2024-11-27T14:16:00.173+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_orders'[0m
 [[34m2024-11-27T14:16:00.173+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_daily_bars'[0m
 [[34m2024-11-27T14:17:00.211+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.weekly_metrics'[0m
 [[34m2024-11-27T14:17:00.212+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections'[0m
 [[34m2024-11-27T14:17:00.212+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.dim_date'[0m
 [[34m2024-11-27T14:17:00.212+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_payments'[0m
 [[34m2024-11-27T14:17:00.213+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_orders'[0m
 [[34m2024-11-27T14:17:00.213+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_daily_bars'[0m
 [[34m2024-11-27T14:18:00.056+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
 [[34m2024-11-27T14:18:00.251+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.weekly_metrics'[0m
 [[34m2024-11-27T14:18:00.251+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections'[0m
 [[34m2024-11-27T14:18:00.251+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.dim_date'[0m
 [[34m2024-11-27T14:18:00.251+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_payments'[0m
 [[34m2024-11-27T14:18:00.251+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_orders'[0m
 [[34m2024-11-27T14:18:00.252+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_daily_bars'[0m
 [[34m2024-11-27T14:19:00.292+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.weekly_metrics'[0m
 [[34m2024-11-27T14:19:00.293+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections'[0m
 [[34m2024-11-27T14:19:00.293+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.dim_date'[0m
 [[34m2024-11-27T14:19:00.293+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_payments'[0m
 [[34m2024-11-27T14:19:00.293+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_orders'[0m
 [[34m2024-11-27T14:19:00.293+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_daily_bars'[0m
 [[34m2024-11-27T14:20:00.330+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.weekly_metrics'[0m
 [[34m2024-11-27T14:20:00.331+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections'[0m
 [[34m2024-11-27T14:20:00.331+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.dim_date'[0m
 [[34m2024-11-27T14:20:00.331+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_payments'[0m
 [[34m2024-11-27T14:20:00.332+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_orders'[0m
 [[34m2024-11-27T14:20:00.332+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_daily_bars'[0m
 [[34m2024-11-27T14:21:00.362+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.weekly_metrics'[0m
 [[34m2024-11-27T14:21:00.363+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections'[0m
 [[34m2024-11-27T14:21:00.363+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.dim_date'[0m
 [[34m2024-11-27T14:21:00.363+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_payments'[0m
 [[34m2024-11-27T14:21:00.363+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_orders'[0m
 [[34m2024-11-27T14:21:00.363+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_daily_bars'[0m
 [[34m2024-11-27T14:22:00.398+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.weekly_metrics'[0m
 [[34m2024-11-27T14:22:00.398+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections'[0m
 [[34m2024-11-27T14:22:00.398+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.dim_date'[0m
 [[34m2024-11-27T14:22:00.399+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_payments'[0m
 [[34m2024-11-27T14:22:00.399+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_orders'[0m
 [[34m2024-11-27T14:22:00.399+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_daily_bars'[0m
 [[34m2024-11-27T14:23:00.114+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
 [[34m2024-11-27T14:23:00.440+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.weekly_metrics'[0m
 [[34m2024-11-27T14:23:00.441+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections'[0m
 [[34m2024-11-27T14:23:00.441+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.dim_date'[0m
 [[34m2024-11-27T14:23:00.441+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_payments'[0m
 [[34m2024-11-27T14:23:00.441+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_orders'[0m
 [[34m2024-11-27T14:23:00.441+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_daily_bars'[0m
 [[34m2024-11-27T14:24:00.490+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.weekly_metrics'[0m
 [[34m2024-11-27T14:24:00.490+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections'[0m
 [[34m2024-11-27T14:24:00.491+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.dim_date'[0m
 [[34m2024-11-27T14:24:00.491+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_payments'[0m
 [[34m2024-11-27T14:24:00.491+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_orders'[0m
 [[34m2024-11-27T14:24:00.491+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_daily_bars'[0m
 [[34m2024-11-27T14:25:00.533+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.weekly_metrics'[0m
 [[34m2024-11-27T14:25:00.534+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections'[0m
 [[34m2024-11-27T14:25:00.534+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.dim_date'[0m
 [[34m2024-11-27T14:25:00.534+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_payments'[0m
 [[34m2024-11-27T14:25:00.534+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_orders'[0m
 [[34m2024-11-27T14:25:00.534+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_daily_bars'[0m
 [[34m2024-11-27T14:26:00.567+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.weekly_metrics'[0m
 [[34m2024-11-27T14:26:00.568+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections'[0m
 [[34m2024-11-27T14:26:00.568+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.dim_date'[0m
 [[34m2024-11-27T14:26:00.568+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_payments'[0m
 [[34m2024-11-27T14:26:00.568+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_orders'[0m
 [[34m2024-11-27T14:26:00.568+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_daily_bars'[0m
 [[34m2024-11-27T14:27:00.604+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.weekly_metrics'[0m
 [[34m2024-11-27T14:27:00.608+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections'[0m
 [[34m2024-11-27T14:27:00.608+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.dim_date'[0m
 [[34m2024-11-27T14:27:00.609+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_payments'[0m
 [[34m2024-11-27T14:27:00.609+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_orders'[0m
 [[34m2024-11-27T14:27:00.609+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_daily_bars'[0m
 [[34m2024-11-27T14:28:00.159+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
 [[34m2024-11-27T14:28:00.647+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.weekly_metrics'[0m
 [[34m2024-11-27T14:28:00.648+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections'[0m
 [[34m2024-11-27T14:28:00.648+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.dim_date'[0m
 [[34m2024-11-27T14:28:00.648+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_payments'[0m
 [[34m2024-11-27T14:28:00.648+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_orders'[0m
 [[34m2024-11-27T14:28:00.649+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_daily_bars'[0m
 [[34m2024-11-27T14:29:00.691+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.weekly_metrics'[0m
 [[34m2024-11-27T14:29:00.691+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections'[0m
 [[34m2024-11-27T14:29:00.691+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.dim_date'[0m
 [[34m2024-11-27T14:29:00.691+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_payments'[0m
 [[34m2024-11-27T14:29:00.692+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_orders'[0m
 [[34m2024-11-27T14:29:00.692+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_daily_bars'[0m
 [[34m2024-11-27T14:30:00.764+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.weekly_metrics'[0m
 [[34m2024-11-27T14:30:00.766+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections'[0m
 [[34m2024-11-27T14:30:00.767+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.dim_date'[0m
 [[34m2024-11-27T14:30:00.767+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_payments'[0m
 [[34m2024-11-27T14:30:00.767+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_orders'[0m
 [[34m2024-11-27T14:30:00.767+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_daily_bars'[0m
 [[34m2024-11-27T14:31:00.808+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.weekly_metrics'[0m
 [[34m2024-11-27T14:31:00.808+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections'[0m
 [[34m2024-11-27T14:31:00.809+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.dim_date'[0m
 [[34m2024-11-27T14:31:00.809+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_payments'[0m
 [[34m2024-11-27T14:31:00.809+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_orders'[0m
 [[34m2024-11-27T14:31:00.809+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_daily_bars'[0m
 [[34m2024-11-27T14:32:00.849+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.weekly_metrics'[0m
 [[34m2024-11-27T14:32:00.850+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections'[0m
 [[34m2024-11-27T14:32:00.850+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.dim_date'[0m
 [[34m2024-11-27T14:32:00.850+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_payments'[0m
 [[34m2024-11-27T14:32:00.851+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_orders'[0m
 [[34m2024-11-27T14:32:00.851+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_daily_bars'[0m
 [[34m2024-11-27T14:33:00.255+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
 [[34m2024-11-27T14:33:00.949+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.weekly_metrics'[0m
 [[34m2024-11-27T14:33:00.949+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections'[0m
 [[34m2024-11-27T14:33:00.949+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.dim_date'[0m
 [[34m2024-11-27T14:33:00.949+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_payments'[0m
 [[34m2024-11-27T14:33:00.950+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_orders'[0m
 [[34m2024-11-27T14:33:00.950+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_daily_bars'[0m
 [[34m2024-11-27T14:34:00.987+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.weekly_metrics'[0m
 [[34m2024-11-27T14:34:00.988+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections'[0m
 [[34m2024-11-27T14:34:00.988+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.dim_date'[0m
 [[34m2024-11-27T14:34:00.988+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_payments'[0m
 [[34m2024-11-27T14:34:00.988+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_orders'[0m
 [[34m2024-11-27T14:34:00.989+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_daily_bars'[0m
 [[34m2024-11-27T14:35:01.045+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.weekly_metrics'[0m
 [[34m2024-11-27T14:35:01.046+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections'[0m
 [[34m2024-11-27T14:35:01.047+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.dim_date'[0m
 [[34m2024-11-27T14:35:01.047+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_payments'[0m
 [[34m2024-11-27T14:35:01.047+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_orders'[0m
 [[34m2024-11-27T14:35:01.047+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_daily_bars'[0m
 [[34m2024-11-27T14:36:01.088+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.weekly_metrics'[0m
 [[34m2024-11-27T14:36:01.088+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections'[0m
 [[34m2024-11-27T14:36:01.089+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.dim_date'[0m
 [[34m2024-11-27T14:36:01.089+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_payments'[0m
 [[34m2024-11-27T14:36:01.089+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_orders'[0m
 [[34m2024-11-27T14:36:01.089+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_daily_bars'[0m
 [[34m2024-11-27T14:37:01.148+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.weekly_metrics'[0m
 [[34m2024-11-27T14:37:01.150+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections'[0m
 [[34m2024-11-27T14:37:01.151+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.dim_date'[0m
 [[34m2024-11-27T14:37:01.151+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_payments'[0m
 [[34m2024-11-27T14:37:01.151+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_orders'[0m
 [[34m2024-11-27T14:37:01.151+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_daily_bars'[0m
 [[34m2024-11-27T14:38:00.307+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
 [[34m2024-11-27T14:38:01.216+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.weekly_metrics'[0m
 [[34m2024-11-27T14:38:01.217+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections'[0m
 [[34m2024-11-27T14:38:01.217+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.dim_date'[0m
 [[34m2024-11-27T14:38:01.217+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_payments'[0m
 [[34m2024-11-27T14:38:01.218+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_orders'[0m
 [[34m2024-11-27T14:38:01.218+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_daily_bars'[0m
 [[34m2024-11-27T14:39:01.258+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.weekly_metrics'[0m
 [[34m2024-11-27T14:39:01.258+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections'[0m
 [[34m2024-11-27T14:39:01.259+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.dim_date'[0m
 [[34m2024-11-27T14:39:01.259+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_payments'[0m
 [[34m2024-11-27T14:39:01.259+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_orders'[0m
 [[34m2024-11-27T14:39:01.259+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_daily_bars'[0m
 [[34m2024-11-27T14:40:01.304+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.weekly_metrics'[0m
 [[34m2024-11-27T14:40:01.305+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections'[0m
 [[34m2024-11-27T14:40:01.305+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.dim_date'[0m
 [[34m2024-11-27T14:40:01.305+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_payments'[0m
 [[34m2024-11-27T14:40:01.306+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_orders'[0m
 [[34m2024-11-27T14:40:01.306+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_daily_bars'[0m
 [[34m2024-11-27T14:41:01.355+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.weekly_metrics'[0m
 [[34m2024-11-27T14:41:01.356+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections'[0m
 [[34m2024-11-27T14:41:01.356+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.dim_date'[0m
 [[34m2024-11-27T14:41:01.356+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_payments'[0m
 [[34m2024-11-27T14:41:01.357+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_orders'[0m
 [[34m2024-11-27T14:41:01.357+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_daily_bars'[0m
 [[34m2024-11-27T14:42:01.407+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.weekly_metrics'[0m
 [[34m2024-11-27T14:42:01.407+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections'[0m
 [[34m2024-11-27T14:42:01.407+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.dim_date'[0m
 [[34m2024-11-27T14:42:01.407+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_payments'[0m
 [[34m2024-11-27T14:42:01.408+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_orders'[0m
 [[34m2024-11-27T14:42:01.408+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_daily_bars'[0m
 [[34m2024-11-27T14:43:00.431+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
 [[34m2024-11-27T14:43:01.450+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.weekly_metrics'[0m
 [[34m2024-11-27T14:43:01.450+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections'[0m
 [[34m2024-11-27T14:43:01.450+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.dim_date'[0m
 [[34m2024-11-27T14:43:01.451+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_payments'[0m
 [[34m2024-11-27T14:43:01.451+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_orders'[0m
 [[34m2024-11-27T14:43:01.451+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_daily_bars'[0m
 [[34m2024-11-27T14:44:01.490+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.weekly_metrics'[0m
 [[34m2024-11-27T14:44:01.491+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections'[0m
 [[34m2024-11-27T14:44:01.491+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.dim_date'[0m
 [[34m2024-11-27T14:44:01.491+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_payments'[0m
 [[34m2024-11-27T14:44:01.491+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_orders'[0m
 [[34m2024-11-27T14:44:01.492+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_daily_bars'[0m
 [[34m2024-11-27T14:45:01.532+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.weekly_metrics'[0m
 [[34m2024-11-27T14:45:01.532+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections'[0m
 [[34m2024-11-27T14:45:01.533+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.dim_date'[0m
 [[34m2024-11-27T14:45:01.533+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_payments'[0m
 [[34m2024-11-27T14:45:01.533+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_orders'[0m
 [[34m2024-11-27T14:45:01.533+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_daily_bars'[0m
 [[34m2024-11-27T14:46:01.578+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.weekly_metrics'[0m
 [[34m2024-11-27T14:46:01.578+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections'[0m
 [[34m2024-11-27T14:46:01.578+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.dim_date'[0m
 [[34m2024-11-27T14:46:01.578+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_payments'[0m
 [[34m2024-11-27T14:46:01.579+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_orders'[0m
 [[34m2024-11-27T14:46:01.579+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_daily_bars'[0m
 [[34m2024-11-27T14:47:01.617+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.weekly_metrics'[0m
 [[34m2024-11-27T14:47:01.620+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections'[0m
 [[34m2024-11-27T14:47:01.621+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.dim_date'[0m
 [[34m2024-11-27T14:47:01.621+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_payments'[0m
 [[34m2024-11-27T14:47:01.621+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_orders'[0m
 [[34m2024-11-27T14:47:01.621+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_daily_bars'[0m
 [[34m2024-11-27T14:47:18.061+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
 	<TaskInstance: capstone_andres_dag_.load_polygon_data_task manual__2024-11-27T14:47:16.886975+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:47:18.062+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 0/16 running and queued tasks[0m
 [[34m2024-11-27T14:47:18.062+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
 	<TaskInstance: capstone_andres_dag_.load_polygon_data_task manual__2024-11-27T14:47:16.886975+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:47:18.073+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='load_polygon_data_task', run_id='manual__2024-11-27T14:47:16.886975+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
 [[34m2024-11-27T14:47:18.075+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'load_polygon_data_task', 'manual__2024-11-27T14:47:16.886975+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:47:18.151+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'load_polygon_data_task', 'manual__2024-11-27T14:47:16.886975+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:47:18.610+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:47:19.848+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:47:19.849+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:47:19.849+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:47:20.428+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412087654,
 "os.name" : Linux
 [0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:47:21.236+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 [[34m2024-11-27T14:47:22.074+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:47:25.194+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:47:25.518+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T14:47:25.930+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:47:27.071+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:47:27.216+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:47:27.317+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 Data loaded successfully
 [[34m2024-11-27T14:47:27.930+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:47:27.946+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 [[34m2024-11-27T14:47:28.891+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.36181414988823235[0m
 [[34m2024-11-27T14:47:28.891+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|6753]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:47:28.892+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:47:28.892+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:47:28.893+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:47:28.893+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|6753]: It took 0.963s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:47:28.897+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|6753]: It took 0.00366s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:47:28.968+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.load_polygon_data_task manual__2024-11-27T14:47:16.886975+00:00 [queued]> on host 75994b6d2295[0m
 [2024-11-27T14:47:29.504+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=load_polygon_data_task, dagrun_id=manual__2024-11-27T14:47:16.886975+00:00, map_index=-1, run_start_date=2024-11-27 14:47:29.094330+00:00, try_number=1, job_id=810, op_classpath=airflow.operators.python.PythonOperator
 [[34m2024-11-27T14:47:37.536+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
 	<TaskInstance: capstone_andres_dag_.load_treasury_data_task manual__2024-11-27T14:47:16.886975+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:47:37.536+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 0/16 running and queued tasks[0m
 [[34m2024-11-27T14:47:37.537+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
 	<TaskInstance: capstone_andres_dag_.load_treasury_data_task manual__2024-11-27T14:47:16.886975+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:47:37.540+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='load_treasury_data_task', run_id='manual__2024-11-27T14:47:16.886975+00:00', try_number=1, map_index=-1) to executor with priority 5 and queue default[0m
 [[34m2024-11-27T14:47:37.540+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'load_treasury_data_task', 'manual__2024-11-27T14:47:16.886975+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:47:37.595+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='load_polygon_data_task', run_id='manual__2024-11-27T14:47:16.886975+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:47:37.601+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=load_polygon_data_task, run_id=manual__2024-11-27T14:47:16.886975+00:00, map_index=-1, run_start_date=2024-11-27 14:47:29.094330+00:00, run_end_date=2024-11-27 14:47:37.409569+00:00, run_duration=8.315239, state=success, executor_state=success, try_number=1, max_tries=0, job_id=810, pool=default_pool, queue=default, priority_weight=6, operator=PythonOperator, queued_dttm=2024-11-27 14:47:18.062816+00:00, queued_by_job_id=803, pid=6806[0m
 [[34m2024-11-27T14:47:37.638+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'load_treasury_data_task', 'manual__2024-11-27T14:47:16.886975+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:47:37.782+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:47:38.624+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:47:38.625+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:47:38.625+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:47:39.155+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412087666,
 "os.name" : Linux
 [0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:47:39.850+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 [[34m2024-11-27T14:47:40.567+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:47:43.320+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:47:43.639+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T14:47:43.879+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:47:44.902+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:47:45.036+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:47:45.180+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 Data loaded successfully
 [[34m2024-11-27T14:47:45.651+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:47:45.665+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 [[34m2024-11-27T14:47:46.501+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.3021319769322872[0m
 [[34m2024-11-27T14:47:46.501+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|6847]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:47:46.502+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:47:46.502+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:47:46.502+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:47:46.503+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|6847]: It took 0.851s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:47:46.506+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|6847]: It took 0.00329s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:47:46.547+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.load_treasury_data_task manual__2024-11-27T14:47:16.886975+00:00 [queued]> on host 75994b6d2295[0m
 [2024-11-27T14:47:47.170+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=load_treasury_data_task, dagrun_id=manual__2024-11-27T14:47:16.886975+00:00, map_index=-1, run_start_date=2024-11-27 14:47:46.622509+00:00, try_number=1, job_id=811, op_classpath=airflow.operators.python.PythonOperator
 [[34m2024-11-27T14:47:48.249+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.dim_date_run manual__2024-11-27T14:47:16.886975+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_daily_bars_run manual__2024-11-27T14:47:16.886975+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run manual__2024-11-27T14:47:16.886975+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:47:48.249+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 0/16 running and queued tasks[0m
 [[34m2024-11-27T14:47:48.249+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 1/16 running and queued tasks[0m
 [[34m2024-11-27T14:47:48.250+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 2/16 running and queued tasks[0m
 [[34m2024-11-27T14:47:48.250+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.dim_date_run manual__2024-11-27T14:47:16.886975+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_daily_bars_run manual__2024-11-27T14:47:16.886975+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run manual__2024-11-27T14:47:16.886975+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:47:48.253+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.dim_date_run', run_id='manual__2024-11-27T14:47:16.886975+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
 [[34m2024-11-27T14:47:48.253+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.dim_date_run', 'manual__2024-11-27T14:47:16.886975+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:47:48.254+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.stg_daily_bars_run', run_id='manual__2024-11-27T14:47:16.886975+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
 [[34m2024-11-27T14:47:48.254+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.stg_daily_bars_run', 'manual__2024-11-27T14:47:16.886975+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:47:48.254+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run', run_id='manual__2024-11-27T14:47:16.886975+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
 [[34m2024-11-27T14:47:48.254+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run', 'manual__2024-11-27T14:47:16.886975+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:47:48.326+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.dim_date_run', 'manual__2024-11-27T14:47:16.886975+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:47:48.336+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run', 'manual__2024-11-27T14:47:16.886975+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:47:48.352+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.stg_daily_bars_run', 'manual__2024-11-27T14:47:16.886975+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:47:48.436+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:47:48.467+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:47:48.521+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:47:49.211+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:47:49.212+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:47:49.212+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:47:49.266+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:47:49.267+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:47:49.267+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:47:49.299+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:47:49.300+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:47:49.300+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:47:49.362+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='load_treasury_data_task', run_id='manual__2024-11-27T14:47:16.886975+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:47:49.366+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=load_treasury_data_task, run_id=manual__2024-11-27T14:47:16.886975+00:00, map_index=-1, run_start_date=2024-11-27 14:47:46.622509+00:00, run_end_date=2024-11-27 14:47:48.111003+00:00, run_duration=1.488494, state=success, executor_state=success, try_number=1, max_tries=0, job_id=811, pool=default_pool, queue=default, priority_weight=5, operator=PythonOperator, queued_dttm=2024-11-27 14:47:37.538175+00:00, queued_by_job_id=803, pid=6891[0m
 [[34m2024-11-27T14:47:49.700+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412087674,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T14:47:49.711+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412087678,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T14:47:49.730+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412087682,
 "os.name" : Linux
 [0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:47:50.387+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:47:50.421+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:47:50.441+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 [[34m2024-11-27T14:47:51.363+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:47:51.388+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 [[34m2024-11-27T14:47:51.600+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:47:52.914+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:47:53.260+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T14:47:53.669+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:47:54.108+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:47:54.413+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T14:47:54.483+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:47:54.655+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:47:54.776+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T14:47:54.805+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:47:54.944+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:47:55.032+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:47:55.064+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 Data loaded successfully
 [[34m2024-11-27T14:47:55.722+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:47:55.742+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:47:55.747+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 [[34m2024-11-27T14:47:55.875+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:47:55.982+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 [[34m2024-11-27T14:47:55.995+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:47:56.151+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:47:56.261+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 Data loaded successfully
 [[34m2024-11-27T14:47:56.611+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:47:56.636+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 Data loaded successfully
 [[34m2024-11-27T14:47:56.769+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:47:56.783+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 [[34m2024-11-27T14:47:56.822+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.4292200761847198[0m
 [[34m2024-11-27T14:47:56.823+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|6894]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:47:56.823+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:47:56.823+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:47:56.823+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:47:56.824+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|6894]: It took 1.1s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:47:56.826+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|6894]: It took 0.00263s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:47:56.858+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.dim_date_run manual__2024-11-27T14:47:16.886975+00:00 [queued]> on host 75994b6d2295[0m
 [[34m2024-11-27T14:47:57.544+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.3698925378266722[0m
 [[34m2024-11-27T14:47:57.545+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|6895]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:47:57.545+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:47:57.546+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:47:57.546+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:47:57.546+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|6895]: It took 0.935s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:47:57.548+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|6895]: It took 0.00233s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:47:57.586+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run manual__2024-11-27T14:47:16.886975+00:00 [queued]> on host 75994b6d2295[0m
 [2024-11-27T14:47:57.592+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.dim_date_run, dagrun_id=manual__2024-11-27T14:47:16.886975+00:00, map_index=-1, run_start_date=2024-11-27 14:47:56.928404+00:00, try_number=1, job_id=812, op_classpath=cosmos.operators.local.DbtRunLocalOperator, cosmos.operators.base.DbtRunMixin, cosmos.operators.local.DbtLocalBaseOperator, cosmos.operators.base.AbstractDbtBaseOperator
 [[34m2024-11-27T14:47:57.674+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.3541243199724704[0m
 [[34m2024-11-27T14:47:57.674+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|6896]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:47:57.675+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:47:57.675+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:47:57.675+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:47:57.675+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|6896]: It took 0.906s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:47:57.678+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|6896]: It took 0.00264s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:47:57.715+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_daily_bars_run manual__2024-11-27T14:47:16.886975+00:00 [queued]> on host 75994b6d2295[0m
 [2024-11-27T14:47:58.636+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run, dagrun_id=manual__2024-11-27T14:47:16.886975+00:00, map_index=-1, run_start_date=2024-11-27 14:47:57.653344+00:00, try_number=1, job_id=813, op_classpath=cosmos.operators.local.DbtRunLocalOperator, cosmos.operators.base.DbtRunMixin, cosmos.operators.local.DbtLocalBaseOperator, cosmos.operators.base.AbstractDbtBaseOperator
 [2024-11-27T14:48:00.138+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.stg_daily_bars_run, dagrun_id=manual__2024-11-27T14:47:16.886975+00:00, map_index=-1, run_start_date=2024-11-27 14:47:57.786483+00:00, try_number=1, job_id=814, op_classpath=cosmos.operators.local.DbtRunLocalOperator, cosmos.operators.base.DbtRunMixin, cosmos.operators.local.DbtLocalBaseOperator, cosmos.operators.base.AbstractDbtBaseOperator
 [[34m2024-11-27T14:48:00.511+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
 [[34m2024-11-27T14:48:01.683+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.weekly_metrics'[0m
 [[34m2024-11-27T14:48:01.683+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections'[0m
 [[34m2024-11-27T14:48:01.683+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.dim_date'[0m
 [[34m2024-11-27T14:48:01.684+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_payments'[0m
 [[34m2024-11-27T14:48:01.684+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_orders'[0m
 [[34m2024-11-27T14:48:01.684+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_daily_bars'[0m
 /usr/local/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked semaphore objects to clean up at shutdown
   warnings.warn('resource_tracker: There appear to be %d '
 [[34m2024-11-27T14:48:07.655+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run', run_id='manual__2024-11-27T14:47:16.886975+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:48:07.664+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run, run_id=manual__2024-11-27T14:47:16.886975+00:00, map_index=-1, run_start_date=2024-11-27 14:47:57.653344+00:00, run_end_date=2024-11-27 14:48:07.258305+00:00, run_duration=9.604961, state=success, executor_state=success, try_number=1, max_tries=0, job_id=813, pool=default_pool, queue=default, priority_weight=2, operator=DbtRunLocalOperator, queued_dttm=2024-11-27 14:47:48.251671+00:00, queued_by_job_id=803, pid=6944[0m
 /usr/local/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked semaphore objects to clean up at shutdown
   warnings.warn('resource_tracker: There appear to be %d '
 /usr/local/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked semaphore objects to clean up at shutdown
   warnings.warn('resource_tracker: There appear to be %d '
 [[34m2024-11-27T14:48:08.707+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.weekly_metrics_run manual__2024-11-27T14:47:16.886975+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:48:08.707+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 0/16 running and queued tasks[0m
 [[34m2024-11-27T14:48:08.707+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.weekly_metrics_run manual__2024-11-27T14:47:16.886975+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:48:08.711+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.weekly_metrics_run', run_id='manual__2024-11-27T14:47:16.886975+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
 [[34m2024-11-27T14:48:08.711+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.weekly_metrics_run', 'manual__2024-11-27T14:47:16.886975+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:48:08.715+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.stg_daily_bars_run', run_id='manual__2024-11-27T14:47:16.886975+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:48:08.715+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.dim_date_run', run_id='manual__2024-11-27T14:47:16.886975+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:48:08.719+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.stg_daily_bars_run, run_id=manual__2024-11-27T14:47:16.886975+00:00, map_index=-1, run_start_date=2024-11-27 14:47:57.786483+00:00, run_end_date=2024-11-27 14:48:07.613964+00:00, run_duration=9.827481, state=success, executor_state=success, try_number=1, max_tries=0, job_id=814, pool=default_pool, queue=default, priority_weight=2, operator=DbtRunLocalOperator, queued_dttm=2024-11-27 14:47:48.251671+00:00, queued_by_job_id=803, pid=6945[0m
 [[34m2024-11-27T14:48:08.719+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.dim_date_run, run_id=manual__2024-11-27T14:47:16.886975+00:00, map_index=-1, run_start_date=2024-11-27 14:47:56.928404+00:00, run_end_date=2024-11-27 14:48:08.092964+00:00, run_duration=11.16456, state=success, executor_state=success, try_number=1, max_tries=0, job_id=812, pool=default_pool, queue=default, priority_weight=2, operator=DbtRunLocalOperator, queued_dttm=2024-11-27 14:47:48.251671+00:00, queued_by_job_id=803, pid=6941[0m
 [[34m2024-11-27T14:48:08.784+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.weekly_metrics_run', 'manual__2024-11-27T14:47:16.886975+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:48:08.892+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:48:09.843+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:48:09.844+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:48:09.844+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:48:10.370+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412094450,
 "os.name" : Linux
 [0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:48:11.260+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 [[34m2024-11-27T14:48:11.758+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:48:14.525+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:48:14.889+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T14:48:15.133+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:48:16.166+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:48:16.380+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:48:16.482+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 Data loaded successfully
 [[34m2024-11-27T14:48:17.157+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:48:17.171+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 [[34m2024-11-27T14:48:18.045+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.34323321701958776[0m
 [[34m2024-11-27T14:48:18.046+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|7012]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:48:18.046+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:48:18.046+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:48:18.047+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:48:18.047+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|7012]: It took 0.89s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:48:18.049+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|7012]: It took 0.00218s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:48:18.089+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.weekly_metrics_run manual__2024-11-27T14:47:16.886975+00:00 [queued]> on host 75994b6d2295[0m
 [2024-11-27T14:48:18.559+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.weekly_metrics_run, dagrun_id=manual__2024-11-27T14:47:16.886975+00:00, map_index=-1, run_start_date=2024-11-27 14:48:18.160840+00:00, try_number=1, job_id=815, op_classpath=cosmos.operators.local.DbtRunLocalOperator, cosmos.operators.base.DbtRunMixin, cosmos.operators.local.DbtLocalBaseOperator, cosmos.operators.base.AbstractDbtBaseOperator
 /usr/local/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked semaphore objects to clean up at shutdown
   warnings.warn('resource_tracker: There appear to be %d '
 [[34m2024-11-27T14:48:24.742+0000[0m] {[34mdagrun.py:[0m851} INFO[0m - Marking run <DagRun capstone_andres_dag_ @ 2024-11-27 14:47:16.886975+00:00: manual__2024-11-27T14:47:16.886975+00:00, state:running, queued_at: 2024-11-27 14:47:17.003419+00:00. externally triggered: True> successful[0m
 [[34m2024-11-27T14:48:24.743+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=capstone_andres_dag_, execution_date=2024-11-27 14:47:16.886975+00:00, run_id=manual__2024-11-27T14:47:16.886975+00:00, run_start_date=2024-11-27 14:47:17.967066+00:00, run_end_date=2024-11-27 14:48:24.742480+00:00, run_duration=66.775414, state=success, external_trigger=True, run_type=manual, data_interval_start=2024-11-27 14:47:16.886975+00:00, data_interval_end=2024-11-27 14:47:16.886975+00:00, dag_hash=379740adad8f86a29acc5f908c07d357[0m
 [[34m2024-11-27T14:48:24.759+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.weekly_metrics_run', run_id='manual__2024-11-27T14:47:16.886975+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:48:24.763+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.weekly_metrics_run, run_id=manual__2024-11-27T14:47:16.886975+00:00, map_index=-1, run_start_date=2024-11-27 14:48:18.160840+00:00, run_end_date=2024-11-27 14:48:23.992868+00:00, run_duration=5.832028, state=success, executor_state=success, try_number=1, max_tries=0, job_id=815, pool=default_pool, queue=default, priority_weight=1, operator=DbtRunLocalOperator, queued_dttm=2024-11-27 14:48:08.708197+00:00, queued_by_job_id=803, pid=7063[0m
 [[34m2024-11-27T14:49:01.723+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.weekly_metrics'[0m
 [[34m2024-11-27T14:49:01.723+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections'[0m
 [[34m2024-11-27T14:49:01.724+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.dim_date'[0m
 [[34m2024-11-27T14:49:01.724+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_payments'[0m
 [[34m2024-11-27T14:49:01.724+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_orders'[0m
 [[34m2024-11-27T14:49:01.724+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_daily_bars'[0m
 [[34m2024-11-27T14:50:01.781+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.weekly_metrics'[0m
 [[34m2024-11-27T14:50:01.783+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections'[0m
 [[34m2024-11-27T14:50:01.784+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.dim_date'[0m
 [[34m2024-11-27T14:50:01.784+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_payments'[0m
 [[34m2024-11-27T14:50:01.784+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_orders'[0m
 [[34m2024-11-27T14:50:01.784+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_daily_bars'[0m
 [[34m2024-11-27T14:51:01.831+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.weekly_metrics'[0m
 [[34m2024-11-27T14:51:01.831+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections'[0m
 [[34m2024-11-27T14:51:01.831+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.dim_date'[0m
 [[34m2024-11-27T14:51:01.832+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_payments'[0m
 [[34m2024-11-27T14:51:01.832+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_orders'[0m
 [[34m2024-11-27T14:51:01.832+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_daily_bars'[0m
 [[34m2024-11-27T14:51:04.238+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-09-10 01:00:00+00:00, run_after=2024-09-11 01:00:00+00:00[0m
 [[34m2024-11-27T14:51:04.288+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
 	<TaskInstance: capstone_andres_dag_.load_polygon_data_task scheduled__2024-09-09T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:51:04.289+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 0/16 running and queued tasks[0m
 [[34m2024-11-27T14:51:04.289+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
 	<TaskInstance: capstone_andres_dag_.load_polygon_data_task scheduled__2024-09-09T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:51:04.290+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='load_polygon_data_task', run_id='scheduled__2024-09-09T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
 [[34m2024-11-27T14:51:04.291+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'load_polygon_data_task', 'scheduled__2024-09-09T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:51:04.317+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-09-11 01:00:00+00:00, run_after=2024-09-12 01:00:00+00:00[0m
 [[34m2024-11-27T14:51:04.368+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'load_polygon_data_task', 'scheduled__2024-09-09T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:51:04.479+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:51:05.325+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:51:05.326+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:51:05.326+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:51:05.394+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-09-12 01:00:00+00:00, run_after=2024-09-13 01:00:00+00:00[0m
 [[34m2024-11-27T14:51:05.792+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412087790,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T14:51:06.474+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-09-13 01:00:00+00:00, run_after=2024-09-14 01:00:00+00:00[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:51:06.741+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:51:06.999+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-09-14 01:00:00+00:00, run_after=2024-09-15 01:00:00+00:00[0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 [[34m2024-11-27T14:51:07.421+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:51:08.135+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-09-15 01:00:00+00:00, run_after=2024-09-16 01:00:00+00:00[0m
 [[34m2024-11-27T14:51:09.284+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-09-16 01:00:00+00:00, run_after=2024-09-17 01:00:00+00:00[0m
 [[34m2024-11-27T14:51:10.196+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:51:10.583+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T14:51:10.868+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:51:11.969+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:51:12.152+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:51:12.261+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 Data loaded successfully
 [[34m2024-11-27T14:51:12.943+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:51:12.956+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 [[34m2024-11-27T14:51:13.984+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.5028105601668358[0m
 [[34m2024-11-27T14:51:13.984+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|7609]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:51:13.985+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:51:13.985+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:51:13.986+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:51:13.986+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|7609]: It took 1.04s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:51:13.989+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|7609]: It took 0.00281s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:51:14.003+0000[0m] {[34mlocal_executor.py:[0m139} ERROR[0m - Failed to execute task DagRun for capstone_andres_dag_ with run_id or execution_date of 'scheduled__2024-09-09T01:00:00+00:00' not found.[0m
 Traceback (most recent call last):
   File "/usr/local/lib/python3.11/site-packages/airflow/executors/local_executor.py", line 135, in _execute_work_in_fork
     args.func(args)
   File "/usr/local/lib/python3.11/site-packages/airflow/cli/cli_config.py", line 49, in command
     return func(*args, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^
   File "/usr/local/lib/python3.11/site-packages/airflow/utils/cli.py", line 114, in wrapper
     return f(*args, **kwargs)
            ^^^^^^^^^^^^^^^^^^
   File "/usr/local/lib/python3.11/site-packages/airflow/cli/commands/task_command.py", line 421, in task_run
     ti, _ = _get_ti(task, args.map_index, exec_date_or_run_id=args.execution_date_or_run_id, pool=args.pool)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "/usr/local/lib/python3.11/site-packages/airflow/utils/session.py", line 79, in wrapper
     return func(*args, session=session, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "/usr/local/lib/python3.11/site-packages/airflow/cli/commands/task_command.py", line 179, in _get_ti
     dag_run, dr_created = _get_dag_run(
                           ^^^^^^^^^^^^^
   File "/usr/local/lib/python3.11/site-packages/airflow/cli/commands/task_command.py", line 128, in _get_dag_run
     raise DagRunNotFound(
 airflow.exceptions.DagRunNotFound: DagRun for capstone_andres_dag_ with run_id or execution_date of 'scheduled__2024-09-09T01:00:00+00:00' not found
 [[34m2024-11-27T14:51:14.266+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state failed for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='load_polygon_data_task', run_id='scheduled__2024-09-09T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:51:57.779+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-09-10 01:00:00+00:00, run_after=2024-09-11 01:00:00+00:00[0m
 [[34m2024-11-27T14:51:57.831+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
 	<TaskInstance: capstone_andres_dag_.load_polygon_data_task scheduled__2024-09-09T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:51:57.831+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 0/16 running and queued tasks[0m
 [[34m2024-11-27T14:51:57.832+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
 	<TaskInstance: capstone_andres_dag_.load_polygon_data_task scheduled__2024-09-09T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:51:57.837+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='load_polygon_data_task', run_id='scheduled__2024-09-09T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
 [[34m2024-11-27T14:51:57.838+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'load_polygon_data_task', 'scheduled__2024-09-09T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:51:57.893+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-09-11 01:00:00+00:00, run_after=2024-09-12 01:00:00+00:00[0m
 [[34m2024-11-27T14:51:57.970+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'load_polygon_data_task', 'scheduled__2024-09-09T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:51:58.211+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:51:58.971+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-09-12 01:00:00+00:00, run_after=2024-09-13 01:00:00+00:00[0m
 [[34m2024-11-27T14:51:59.105+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:51:59.106+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:51:59.106+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:51:59.649+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412087802,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T14:52:00.057+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-09-13 01:00:00+00:00, run_after=2024-09-14 01:00:00+00:00[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:52:00.428+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 [[34m2024-11-27T14:52:01.148+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-09-14 01:00:00+00:00, run_after=2024-09-15 01:00:00+00:00[0m
 [[34m2024-11-27T14:52:01.226+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:52:01.711+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-09-15 01:00:00+00:00, run_after=2024-09-16 01:00:00+00:00[0m
 [[34m2024-11-27T14:52:01.855+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-09-16 01:00:00+00:00, run_after=2024-09-17 01:00:00+00:00[0m
 [[34m2024-11-27T14:52:01.966+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.weekly_metrics'[0m
 [[34m2024-11-27T14:52:01.966+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections'[0m
 [[34m2024-11-27T14:52:01.967+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.dim_date'[0m
 [[34m2024-11-27T14:52:01.967+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_payments'[0m
 [[34m2024-11-27T14:52:01.967+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_orders'[0m
 [[34m2024-11-27T14:52:01.967+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_daily_bars'[0m
 [[34m2024-11-27T14:52:02.088+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-09-17 01:00:00+00:00, run_after=2024-09-18 01:00:00+00:00[0m
 [[34m2024-11-27T14:52:02.797+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:52:02.880+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-09-18 01:00:00+00:00, run_after=2024-09-19 01:00:00+00:00[0m
 [[34m2024-11-27T14:52:03.139+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T14:52:03.444+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:52:04.047+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-09-19 01:00:00+00:00, run_after=2024-09-20 01:00:00+00:00[0m
 [[34m2024-11-27T14:52:04.519+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:52:04.725+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:52:04.880+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 [[34m2024-11-27T14:52:05.219+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-09-20 01:00:00+00:00, run_after=2024-09-21 01:00:00+00:00[0m
 Data loaded successfully
 [[34m2024-11-27T14:52:05.517+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:52:05.529+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 [[34m2024-11-27T14:52:06.421+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-09-21 01:00:00+00:00, run_after=2024-09-22 01:00:00+00:00[0m
 [[34m2024-11-27T14:52:06.474+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.3334977049380541[0m
 [[34m2024-11-27T14:52:06.475+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|7812]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:52:06.475+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:52:06.475+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:52:06.475+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:52:06.476+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|7812]: It took 0.959s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:52:06.478+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|7812]: It took 0.00217s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:52:06.511+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.load_polygon_data_task scheduled__2024-09-09T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [2024-11-27T14:52:07.046+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=load_polygon_data_task, dagrun_id=scheduled__2024-09-09T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:52:06.588380+00:00, try_number=1, job_id=816, op_classpath=airflow.operators.python.PythonOperator
 [[34m2024-11-27T14:52:07.248+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-09-22 01:00:00+00:00, run_after=2024-09-23 01:00:00+00:00[0m
 [[34m2024-11-27T14:52:08.043+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-09-23 01:00:00+00:00, run_after=2024-09-24 01:00:00+00:00[0m
 [[34m2024-11-27T14:52:09.341+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-09-24 01:00:00+00:00, run_after=2024-09-25 01:00:00+00:00[0m
 [[34m2024-11-27T14:52:09.623+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 2 tasks up for execution:
 	<TaskInstance: capstone_andres_dag_.load_polygon_data_task scheduled__2024-09-10T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.load_treasury_data_task scheduled__2024-09-09T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:52:09.623+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 0/16 running and queued tasks[0m
 [[34m2024-11-27T14:52:09.624+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 1/16 running and queued tasks[0m
 [[34m2024-11-27T14:52:09.624+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
 	<TaskInstance: capstone_andres_dag_.load_polygon_data_task scheduled__2024-09-10T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.load_treasury_data_task scheduled__2024-09-09T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:52:09.629+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='load_polygon_data_task', run_id='scheduled__2024-09-10T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
 [[34m2024-11-27T14:52:09.629+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'load_polygon_data_task', 'scheduled__2024-09-10T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:52:09.629+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='load_treasury_data_task', run_id='scheduled__2024-09-09T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 5 and queue default[0m
 [[34m2024-11-27T14:52:09.629+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'load_treasury_data_task', 'scheduled__2024-09-09T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:52:09.634+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='load_polygon_data_task', run_id='scheduled__2024-09-09T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:52:09.637+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=load_polygon_data_task, run_id=scheduled__2024-09-09T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:52:06.588380+00:00, run_end_date=2024-11-27 14:52:08.638414+00:00, run_duration=2.050034, state=success, executor_state=success, try_number=1, max_tries=0, job_id=816, pool=default_pool, queue=default, priority_weight=6, operator=PythonOperator, queued_dttm=2024-11-27 14:51:57.833979+00:00, queued_by_job_id=803, pid=7829[0m
 [[34m2024-11-27T14:52:09.658+0000[0m] {[34mscheduler_job_runner.py:[0m1328} INFO[0m - DAG capstone_andres_dag_ is at (or above) max_active_runs (16 of 16), not creating any more runs[0m
 [[34m2024-11-27T14:52:09.699+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'load_polygon_data_task', 'scheduled__2024-09-10T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:52:09.706+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'load_treasury_data_task', 'scheduled__2024-09-09T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:52:09.825+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:52:09.855+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:52:10.632+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:52:10.633+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:52:10.633+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:52:10.648+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:52:10.648+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:52:10.648+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:52:11.129+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412087814,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T14:52:11.132+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412094586,
 "os.name" : Linux
 [0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:52:11.823+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:52:11.831+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 [[34m2024-11-27T14:52:12.316+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:52:12.421+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:52:15.208+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:52:15.325+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:52:15.505+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T14:52:15.627+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T14:52:15.740+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:52:15.852+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:52:16.735+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:52:16.890+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:52:16.991+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:52:16.995+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 [[34m2024-11-27T14:52:17.153+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:52:17.259+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 Data loaded successfully
 [[34m2024-11-27T14:52:17.635+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:52:17.647+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 Data loaded successfully
 [[34m2024-11-27T14:52:17.774+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:52:17.786+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 [[34m2024-11-27T14:52:18.485+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.308871430112049[0m
 [[34m2024-11-27T14:52:18.485+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|7845]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:52:18.485+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:52:18.486+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:52:18.486+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:52:18.486+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|7845]: It took 0.851s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:52:18.488+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|7845]: It took 0.0022s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:52:18.524+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.load_polygon_data_task scheduled__2024-09-10T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [[34m2024-11-27T14:52:18.674+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.3656010739505291[0m
 [[34m2024-11-27T14:52:18.674+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|7846]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:52:18.675+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:52:18.675+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:52:18.675+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:52:18.675+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|7846]: It took 0.902s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:52:18.679+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|7846]: It took 0.00311s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:52:18.719+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.load_treasury_data_task scheduled__2024-09-09T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [2024-11-27T14:52:18.936+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=load_polygon_data_task, dagrun_id=scheduled__2024-09-10T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:52:18.598455+00:00, try_number=1, job_id=817, op_classpath=airflow.operators.python.PythonOperator
 [2024-11-27T14:52:19.262+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=load_treasury_data_task, dagrun_id=scheduled__2024-09-09T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:52:18.811120+00:00, try_number=1, job_id=818, op_classpath=airflow.operators.python.PythonOperator
 [[34m2024-11-27T14:52:21.313+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.dim_date_run scheduled__2024-09-09T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_daily_bars_run scheduled__2024-09-09T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run scheduled__2024-09-09T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:52:21.314+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 1/16 running and queued tasks[0m
 [[34m2024-11-27T14:52:21.314+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 2/16 running and queued tasks[0m
 [[34m2024-11-27T14:52:21.315+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 3/16 running and queued tasks[0m
 [[34m2024-11-27T14:52:21.315+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.dim_date_run scheduled__2024-09-09T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_daily_bars_run scheduled__2024-09-09T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run scheduled__2024-09-09T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:52:21.319+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.dim_date_run', run_id='scheduled__2024-09-09T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
 [[34m2024-11-27T14:52:21.319+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.dim_date_run', 'scheduled__2024-09-09T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:52:21.320+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.stg_daily_bars_run', run_id='scheduled__2024-09-09T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
 [[34m2024-11-27T14:52:21.320+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.stg_daily_bars_run', 'scheduled__2024-09-09T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:52:21.321+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run', run_id='scheduled__2024-09-09T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
 [[34m2024-11-27T14:52:21.321+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run', 'scheduled__2024-09-09T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:52:21.338+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='load_treasury_data_task', run_id='scheduled__2024-09-09T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:52:21.343+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=load_treasury_data_task, run_id=scheduled__2024-09-09T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:52:18.811120+00:00, run_end_date=2024-11-27 14:52:20.193639+00:00, run_duration=1.382519, state=success, executor_state=success, try_number=1, max_tries=0, job_id=818, pool=default_pool, queue=default, priority_weight=5, operator=PythonOperator, queued_dttm=2024-11-27 14:52:09.625459+00:00, queued_by_job_id=803, pid=7894[0m
 [[34m2024-11-27T14:52:21.413+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run', 'scheduled__2024-09-09T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:52:21.417+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.stg_daily_bars_run', 'scheduled__2024-09-09T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:52:21.427+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.dim_date_run', 'scheduled__2024-09-09T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:52:21.531+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:52:21.551+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:52:21.665+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:52:22.737+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:52:22.738+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:52:22.739+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:52:22.745+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:52:22.746+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:52:22.747+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:52:23.033+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:52:23.035+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:52:23.036+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:52:23.272+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412087830,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T14:52:23.313+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412094590,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T14:52:23.617+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412094594,
 "os.name" : Linux
 [0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:52:24.054+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:52:24.112+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:52:24.349+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 [[34m2024-11-27T14:52:24.556+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 [[34m2024-11-27T14:52:24.602+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:52:24.762+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:52:26.795+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 2 tasks up for execution:
 	<TaskInstance: capstone_andres_dag_.load_polygon_data_task scheduled__2024-09-11T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.load_treasury_data_task scheduled__2024-09-10T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:52:26.795+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 3/16 running and queued tasks[0m
 [[34m2024-11-27T14:52:26.795+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 4/16 running and queued tasks[0m
 [[34m2024-11-27T14:52:26.796+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
 	<TaskInstance: capstone_andres_dag_.load_polygon_data_task scheduled__2024-09-11T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.load_treasury_data_task scheduled__2024-09-10T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:52:26.798+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='load_polygon_data_task', run_id='scheduled__2024-09-11T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
 [[34m2024-11-27T14:52:26.798+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'load_polygon_data_task', 'scheduled__2024-09-11T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:52:26.799+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='load_treasury_data_task', run_id='scheduled__2024-09-10T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 5 and queue default[0m
 [[34m2024-11-27T14:52:26.799+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'load_treasury_data_task', 'scheduled__2024-09-10T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:52:26.806+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='load_polygon_data_task', run_id='scheduled__2024-09-10T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:52:26.811+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=load_polygon_data_task, run_id=scheduled__2024-09-10T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:52:18.598455+00:00, run_end_date=2024-11-27 14:52:25.532023+00:00, run_duration=6.933568, state=success, executor_state=success, try_number=1, max_tries=0, job_id=817, pool=default_pool, queue=default, priority_weight=6, operator=PythonOperator, queued_dttm=2024-11-27 14:52:09.625459+00:00, queued_by_job_id=803, pid=7893[0m
 [[34m2024-11-27T14:52:26.849+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'load_treasury_data_task', 'scheduled__2024-09-10T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:52:26.856+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'load_polygon_data_task', 'scheduled__2024-09-11T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:52:26.993+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:52:27.003+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:52:27.118+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:52:27.189+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:52:27.428+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T14:52:27.442+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:52:27.554+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T14:52:27.636+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:52:27.729+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T14:52:27.795+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:52:27.930+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:52:28.138+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:52:28.139+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:52:28.139+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:52:28.151+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:52:28.151+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:52:28.152+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:52:28.597+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412087842,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T14:52:28.600+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412094598,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T14:52:28.635+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:52:28.788+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:52:28.801+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:52:28.897+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 [[34m2024-11-27T14:52:29.001+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:52:29.100+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:52:29.120+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 [[34m2024-11-27T14:52:29.268+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:52:29.332+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:52:29.343+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:52:29.376+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 Data loaded successfully
 [[34m2024-11-27T14:52:29.461+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:52:29.477+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 Data loaded successfully
 [[34m2024-11-27T14:52:29.611+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:52:29.625+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 Data loaded successfully
 [[34m2024-11-27T14:52:29.830+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:52:29.843+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 [[34m2024-11-27T14:52:30.101+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 [[34m2024-11-27T14:52:30.334+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:52:30.382+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.3564342411700636[0m
 [[34m2024-11-27T14:52:30.382+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|7925]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:52:30.383+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:52:30.383+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:52:30.383+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:52:30.384+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|7925]: It took 0.923s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:52:30.387+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|7925]: It took 0.00312s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:52:30.428+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_daily_bars_run scheduled__2024-09-09T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [[34m2024-11-27T14:52:30.566+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.38800045289099216[0m
 [[34m2024-11-27T14:52:30.566+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|7924]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:52:30.567+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:52:30.567+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:52:30.567+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:52:30.568+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|7924]: It took 0.957s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:52:30.571+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|7924]: It took 0.00311s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:52:30.609+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run scheduled__2024-09-09T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [[34m2024-11-27T14:52:30.803+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.433510776842013[0m
 [[34m2024-11-27T14:52:30.804+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|7926]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:52:30.804+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:52:30.804+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:52:30.805+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:52:30.805+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|7926]: It took 0.975s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:52:30.808+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|7926]: It took 0.00269s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:52:30.858+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.dim_date_run scheduled__2024-09-09T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [2024-11-27T14:52:31.287+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.stg_daily_bars_run, dagrun_id=scheduled__2024-09-09T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:52:30.524441+00:00, try_number=1, job_id=819, op_classpath=cosmos.operators.local.DbtRunLocalOperator, cosmos.operators.base.DbtRunMixin, cosmos.operators.local.DbtLocalBaseOperator, cosmos.operators.base.AbstractDbtBaseOperator
 [2024-11-27T14:52:31.422+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run, dagrun_id=scheduled__2024-09-09T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:52:30.698136+00:00, try_number=1, job_id=820, op_classpath=cosmos.operators.local.DbtRunLocalOperator, cosmos.operators.base.DbtRunMixin, cosmos.operators.local.DbtLocalBaseOperator, cosmos.operators.base.AbstractDbtBaseOperator
 [2024-11-27T14:52:32.301+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.dim_date_run, dagrun_id=scheduled__2024-09-09T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:52:31.059087+00:00, try_number=1, job_id=821, op_classpath=cosmos.operators.local.DbtRunLocalOperator, cosmos.operators.base.DbtRunMixin, cosmos.operators.local.DbtLocalBaseOperator, cosmos.operators.base.AbstractDbtBaseOperator
 [[34m2024-11-27T14:52:32.916+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:52:33.130+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:52:33.203+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T14:52:33.455+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:52:33.455+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T14:52:33.834+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:52:34.583+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:52:34.787+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:52:34.917+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 [[34m2024-11-27T14:52:35.003+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:52:35.194+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:52:35.311+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 Data loaded successfully
 [[34m2024-11-27T14:52:35.541+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:52:35.591+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 Data loaded successfully
 [[34m2024-11-27T14:52:36.025+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:52:36.040+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 [[34m2024-11-27T14:52:36.799+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.5316415191628039[0m
 [[34m2024-11-27T14:52:36.800+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|7985]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:52:36.800+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:52:36.801+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:52:36.801+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:52:36.801+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|7985]: It took 1.26s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:52:36.804+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|7985]: It took 0.00312s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:52:36.893+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.load_polygon_data_task scheduled__2024-09-11T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [[34m2024-11-27T14:52:37.355+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.6693948057945818[0m
 [[34m2024-11-27T14:52:37.356+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|7984]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:52:37.356+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:52:37.356+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:52:37.357+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:52:37.357+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|7984]: It took 1.33s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:52:37.360+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|7984]: It took 0.00268s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:52:37.461+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.load_treasury_data_task scheduled__2024-09-10T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [2024-11-27T14:52:38.751+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=load_polygon_data_task, dagrun_id=scheduled__2024-09-11T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:52:37.180031+00:00, try_number=1, job_id=822, op_classpath=airflow.operators.python.PythonOperator
 /usr/local/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked semaphore objects to clean up at shutdown
   warnings.warn('resource_tracker: There appear to be %d '
 /usr/local/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked semaphore objects to clean up at shutdown
   warnings.warn('resource_tracker: There appear to be %d '
 [2024-11-27T14:52:41.756+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=load_treasury_data_task, dagrun_id=scheduled__2024-09-10T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:52:37.665887+00:00, try_number=1, job_id=823, op_classpath=airflow.operators.python.PythonOperator
 [[34m2024-11-27T14:52:42.851+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.stg_daily_bars_run', run_id='scheduled__2024-09-09T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:52:42.855+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run', run_id='scheduled__2024-09-09T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:52:42.947+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.stg_daily_bars_run, run_id=scheduled__2024-09-09T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:52:30.524441+00:00, run_end_date=2024-11-27 14:52:41.175664+00:00, run_duration=10.651223, state=success, executor_state=success, try_number=1, max_tries=0, job_id=819, pool=default_pool, queue=default, priority_weight=2, operator=DbtRunLocalOperator, queued_dttm=2024-11-27 14:52:21.316622+00:00, queued_by_job_id=803, pid=8020[0m
 [[34m2024-11-27T14:52:42.954+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run, run_id=scheduled__2024-09-09T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:52:30.698136+00:00, run_end_date=2024-11-27 14:52:41.382269+00:00, run_duration=10.684133, state=success, executor_state=success, try_number=1, max_tries=0, job_id=820, pool=default_pool, queue=default, priority_weight=2, operator=DbtRunLocalOperator, queued_dttm=2024-11-27 14:52:21.316622+00:00, queued_by_job_id=803, pid=8021[0m
 /usr/local/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked semaphore objects to clean up at shutdown
   warnings.warn('resource_tracker: There appear to be %d '
 [[34m2024-11-27T14:52:45.368+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.weekly_metrics_run scheduled__2024-09-09T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:52:45.370+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 1/16 running and queued tasks[0m
 [[34m2024-11-27T14:52:45.371+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.weekly_metrics_run scheduled__2024-09-09T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:52:45.378+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.weekly_metrics_run', run_id='scheduled__2024-09-09T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
 [[34m2024-11-27T14:52:45.380+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.weekly_metrics_run', 'scheduled__2024-09-09T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:52:45.391+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.dim_date_run', run_id='scheduled__2024-09-09T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:52:45.416+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.dim_date_run, run_id=scheduled__2024-09-09T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:52:31.059087+00:00, run_end_date=2024-11-27 14:52:43.727362+00:00, run_duration=12.668275, state=success, executor_state=success, try_number=1, max_tries=0, job_id=821, pool=default_pool, queue=default, priority_weight=2, operator=DbtRunLocalOperator, queued_dttm=2024-11-27 14:52:21.316622+00:00, queued_by_job_id=803, pid=8028[0m
 [[34m2024-11-27T14:52:45.489+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.weekly_metrics_run', 'scheduled__2024-09-09T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:52:45.956+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:52:46.481+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.dim_date_run scheduled__2024-09-10T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_daily_bars_run scheduled__2024-09-10T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run scheduled__2024-09-10T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:52:46.482+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 2/16 running and queued tasks[0m
 [[34m2024-11-27T14:52:46.482+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 3/16 running and queued tasks[0m
 [[34m2024-11-27T14:52:46.483+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 4/16 running and queued tasks[0m
 [[34m2024-11-27T14:52:46.483+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.dim_date_run scheduled__2024-09-10T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_daily_bars_run scheduled__2024-09-10T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run scheduled__2024-09-10T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:52:46.487+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.dim_date_run', run_id='scheduled__2024-09-10T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
 [[34m2024-11-27T14:52:46.488+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.dim_date_run', 'scheduled__2024-09-10T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:52:46.489+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.stg_daily_bars_run', run_id='scheduled__2024-09-10T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
 [[34m2024-11-27T14:52:46.490+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.stg_daily_bars_run', 'scheduled__2024-09-10T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:52:46.491+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run', run_id='scheduled__2024-09-10T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
 [[34m2024-11-27T14:52:46.492+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run', 'scheduled__2024-09-10T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:52:46.499+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='load_treasury_data_task', run_id='scheduled__2024-09-10T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:52:46.509+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=load_treasury_data_task, run_id=scheduled__2024-09-10T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:52:37.665887+00:00, run_end_date=2024-11-27 14:52:45.241929+00:00, run_duration=7.576042, state=success, executor_state=success, try_number=1, max_tries=0, job_id=823, pool=default_pool, queue=default, priority_weight=5, operator=PythonOperator, queued_dttm=2024-11-27 14:52:26.796732+00:00, queued_by_job_id=803, pid=8036[0m
 [[34m2024-11-27T14:52:46.527+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.dim_date_run', 'scheduled__2024-09-10T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:52:46.527+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.stg_daily_bars_run', 'scheduled__2024-09-10T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:52:46.542+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run', 'scheduled__2024-09-10T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:52:46.638+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:52:46.652+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:52:46.700+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:52:48.796+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:52:48.797+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:52:48.797+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:52:49.078+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:52:49.079+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:52:49.079+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:52:49.085+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:52:49.086+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:52:49.087+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:52:49.104+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:52:49.105+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:52:49.106+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:52:49.211+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 2 tasks up for execution:
 	<TaskInstance: capstone_andres_dag_.load_polygon_data_task scheduled__2024-09-12T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.load_treasury_data_task scheduled__2024-09-11T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:52:49.211+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 4/16 running and queued tasks[0m
 [[34m2024-11-27T14:52:49.212+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 5/16 running and queued tasks[0m
 [[34m2024-11-27T14:52:49.212+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
 	<TaskInstance: capstone_andres_dag_.load_polygon_data_task scheduled__2024-09-12T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.load_treasury_data_task scheduled__2024-09-11T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:52:49.217+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='load_polygon_data_task', run_id='scheduled__2024-09-12T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
 [[34m2024-11-27T14:52:49.217+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'load_polygon_data_task', 'scheduled__2024-09-12T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:52:49.218+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='load_treasury_data_task', run_id='scheduled__2024-09-11T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 5 and queue default[0m
 [[34m2024-11-27T14:52:49.218+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'load_treasury_data_task', 'scheduled__2024-09-11T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:52:49.225+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='load_polygon_data_task', run_id='scheduled__2024-09-11T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:52:49.230+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=load_polygon_data_task, run_id=scheduled__2024-09-11T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:52:37.180031+00:00, run_end_date=2024-11-27 14:52:46.830831+00:00, run_duration=9.6508, state=success, executor_state=success, try_number=1, max_tries=0, job_id=822, pool=default_pool, queue=default, priority_weight=6, operator=PythonOperator, queued_dttm=2024-11-27 14:52:26.796732+00:00, queued_by_job_id=803, pid=8034[0m
 [[34m2024-11-27T14:52:49.262+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'load_treasury_data_task', 'scheduled__2024-09-11T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:52:49.260+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'load_polygon_data_task', 'scheduled__2024-09-12T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:52:49.336+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412094622,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T14:52:49.374+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:52:49.374+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:52:49.580+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412094626,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T14:52:49.605+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412087890,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T14:52:49.644+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412087894,
 "os.name" : Linux
 [0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:52:50.124+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:52:50.436+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:52:50.467+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:52:50.473+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:52:50.540+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:52:50.541+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:52:50.541+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:52:50.547+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:52:50.547+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:52:50.548+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 [[34m2024-11-27T14:52:51.018+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412094630,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T14:52:51.045+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412087902,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T14:52:51.062+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 [[34m2024-11-27T14:52:51.188+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 [[34m2024-11-27T14:52:51.286+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:52:51.483+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:52:51.972+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:52:52.098+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 [[34m2024-11-27T14:52:52.432+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:52:52.529+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:52:52.534+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:52:52.857+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T14:52:53.134+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:52:54.018+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:52:54.192+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:52:54.290+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:52:54.318+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T14:52:54.434+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:52:54.515+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T14:52:54.580+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T14:52:54.626+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:52:54.665+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:52:54.732+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 [[34m2024-11-27T14:52:54.778+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:52:54.846+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:52:55.177+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 Data loaded successfully
 [[34m2024-11-27T14:52:55.212+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:52:55.222+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 [[34m2024-11-27T14:52:55.335+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:52:55.465+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T14:52:55.630+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T14:52:55.762+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:52:55.841+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:52:55.885+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:52:55.925+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:52:56.001+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:52:56.059+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:52:56.092+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:52:56.094+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.35434334399178624[0m
 [[34m2024-11-27T14:52:56.094+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|8080]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:52:56.095+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:52:56.095+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:52:56.095+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:52:56.096+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|8080]: It took 0.884s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:52:56.098+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|8080]: It took 0.00226s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:52:56.132+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.weekly_metrics_run scheduled__2024-09-09T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [[34m2024-11-27T14:52:56.139+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:52:56.164+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 [[34m2024-11-27T14:52:56.221+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 [[34m2024-11-27T14:52:56.241+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 [2024-11-27T14:52:56.737+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.weekly_metrics_run, dagrun_id=scheduled__2024-09-09T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:52:56.218840+00:00, try_number=1, job_id=824, op_classpath=cosmos.operators.local.DbtRunLocalOperator, cosmos.operators.base.DbtRunMixin, cosmos.operators.local.DbtLocalBaseOperator, cosmos.operators.base.AbstractDbtBaseOperator
 Data loaded successfully
 [[34m2024-11-27T14:52:56.742+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:52:56.756+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 Data loaded successfully
 [[34m2024-11-27T14:52:56.779+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:52:56.791+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 [[34m2024-11-27T14:52:56.866+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 Data loaded successfully
 [[34m2024-11-27T14:52:56.888+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:52:56.900+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 [[34m2024-11-27T14:52:57.051+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:52:57.079+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:52:57.152+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 [[34m2024-11-27T14:52:57.250+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:52:57.359+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 Data loaded successfully
 [[34m2024-11-27T14:52:57.847+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:52:57.859+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 Data loaded successfully
 [[34m2024-11-27T14:52:57.873+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:52:57.878+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.5710846169386059[0m
 [[34m2024-11-27T14:52:57.879+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|8086]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:52:57.880+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:52:57.881+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:52:57.882+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:52:57.883+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|8086]: It took 1.14s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:52:57.889+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|8086]: It took 0.0064s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:52:57.896+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 [[34m2024-11-27T14:52:57.915+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.565637718886137[0m
 [[34m2024-11-27T14:52:57.915+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|8085]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:52:57.916+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:52:57.916+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:52:57.917+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:52:57.917+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|8085]: It took 1.14s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:52:57.923+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|8085]: It took 0.00648s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:52:57.947+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run scheduled__2024-09-10T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [[34m2024-11-27T14:52:57.982+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_daily_bars_run scheduled__2024-09-10T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [[34m2024-11-27T14:52:58.005+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.5666124448180199[0m
 [[34m2024-11-27T14:52:58.005+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|8084]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:52:58.006+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:52:58.006+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:52:58.007+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:52:58.007+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|8084]: It took 1.12s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:52:58.010+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|8084]: It took 0.00286s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:52:58.062+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.dim_date_run scheduled__2024-09-10T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [2024-11-27T14:52:58.977+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run, dagrun_id=scheduled__2024-09-10T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:52:58.037089+00:00, try_number=1, job_id=825, op_classpath=cosmos.operators.local.DbtRunLocalOperator, cosmos.operators.base.DbtRunMixin, cosmos.operators.local.DbtLocalBaseOperator, cosmos.operators.base.AbstractDbtBaseOperator
 [2024-11-27T14:52:59.282+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.stg_daily_bars_run, dagrun_id=scheduled__2024-09-10T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:52:58.086699+00:00, try_number=1, job_id=826, op_classpath=cosmos.operators.local.DbtRunLocalOperator, cosmos.operators.base.DbtRunMixin, cosmos.operators.local.DbtLocalBaseOperator, cosmos.operators.base.AbstractDbtBaseOperator
 [[34m2024-11-27T14:52:59.535+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 1.0299095760565251[0m
 [[34m2024-11-27T14:52:59.535+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|8094]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:52:59.538+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:52:59.539+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:52:59.539+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:52:59.540+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|8094]: It took 1.69s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:52:59.547+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|8094]: It took 0.00713s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:52:59.575+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.9875688508618623[0m
 [[34m2024-11-27T14:52:59.575+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|8093]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:52:59.577+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:52:59.577+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:52:59.577+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:52:59.578+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|8093]: It took 1.7s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:52:59.581+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|8093]: It took 0.00329s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:52:59.732+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.load_polygon_data_task scheduled__2024-09-12T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [[34m2024-11-27T14:52:59.760+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.load_treasury_data_task scheduled__2024-09-11T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [[34m2024-11-27T14:53:02.308+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
 [[34m2024-11-27T14:53:02.355+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.weekly_metrics'[0m
 [[34m2024-11-27T14:53:02.356+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections'[0m
 [[34m2024-11-27T14:53:02.357+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_payments'[0m
 [[34m2024-11-27T14:53:02.357+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_orders'[0m
 [[34m2024-11-27T14:53:02.357+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_daily_bars'[0m
 [2024-11-27T14:53:02.492+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=load_polygon_data_task, dagrun_id=scheduled__2024-09-12T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:53:00.052825+00:00, try_number=1, job_id=829, op_classpath=airflow.operators.python.PythonOperator
 [2024-11-27T14:53:03.352+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.dim_date_run, dagrun_id=scheduled__2024-09-10T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:52:58.224100+00:00, try_number=1, job_id=827, op_classpath=cosmos.operators.local.DbtRunLocalOperator, cosmos.operators.base.DbtRunMixin, cosmos.operators.local.DbtLocalBaseOperator, cosmos.operators.base.AbstractDbtBaseOperator
 [2024-11-27T14:53:11.725+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=load_treasury_data_task, dagrun_id=scheduled__2024-09-11T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:52:59.975592+00:00, try_number=1, job_id=828, op_classpath=airflow.operators.python.PythonOperator
 /usr/local/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked semaphore objects to clean up at shutdown
   warnings.warn('resource_tracker: There appear to be %d '
 [[34m2024-11-27T14:53:17.358+0000[0m] {[34mdagrun.py:[0m851} INFO[0m - Marking run <DagRun capstone_andres_dag_ @ 2024-09-09 01:00:00+00:00: scheduled__2024-09-09T01:00:00+00:00, state:running, queued_at: 2024-11-27 14:51:57.761799+00:00. externally triggered: False> successful[0m
 [[34m2024-11-27T14:53:17.388+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=capstone_andres_dag_, execution_date=2024-09-09 01:00:00+00:00, run_id=scheduled__2024-09-09T01:00:00+00:00, run_start_date=2024-11-27 14:51:57.788445+00:00, run_end_date=2024-11-27 14:53:17.373593+00:00, run_duration=79.585148, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-09-09 01:00:00+00:00, data_interval_end=2024-09-10 01:00:00+00:00, dag_hash=77fa8f64adac9391cccaaae88e2be5d8[0m
 [[34m2024-11-27T14:53:17.900+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-09-10 01:00:00+00:00, run_after=2024-09-11 01:00:00+00:00[0m
 [[34m2024-11-27T14:53:18.890+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.dim_date_run scheduled__2024-09-11T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_daily_bars_run scheduled__2024-09-11T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run scheduled__2024-09-11T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:53:18.892+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 4/16 running and queued tasks[0m
 [[34m2024-11-27T14:53:18.893+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 5/16 running and queued tasks[0m
 [[34m2024-11-27T14:53:18.894+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 6/16 running and queued tasks[0m
 [[34m2024-11-27T14:53:18.895+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.dim_date_run scheduled__2024-09-11T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_daily_bars_run scheduled__2024-09-11T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run scheduled__2024-09-11T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:53:18.906+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.dim_date_run', run_id='scheduled__2024-09-11T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
 [[34m2024-11-27T14:53:18.909+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.dim_date_run', 'scheduled__2024-09-11T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:53:18.912+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.stg_daily_bars_run', run_id='scheduled__2024-09-11T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
 [[34m2024-11-27T14:53:18.914+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.stg_daily_bars_run', 'scheduled__2024-09-11T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:53:18.916+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run', run_id='scheduled__2024-09-11T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
 [[34m2024-11-27T14:53:18.917+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run', 'scheduled__2024-09-11T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:53:18.931+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.weekly_metrics_run', run_id='scheduled__2024-09-09T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:53:18.932+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='load_treasury_data_task', run_id='scheduled__2024-09-11T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:53:18.998+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.weekly_metrics_run, run_id=scheduled__2024-09-09T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:52:56.218840+00:00, run_end_date=2024-11-27 14:53:14.871322+00:00, run_duration=18.652482, state=success, executor_state=success, try_number=1, max_tries=0, job_id=824, pool=default_pool, queue=default, priority_weight=1, operator=DbtRunLocalOperator, queued_dttm=2024-11-27 14:52:45.373370+00:00, queued_by_job_id=803, pid=8188[0m
 [[34m2024-11-27T14:53:18.999+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=load_treasury_data_task, run_id=scheduled__2024-09-11T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:52:59.975592+00:00, run_end_date=2024-11-27 14:53:15.126582+00:00, run_duration=15.15099, state=success, executor_state=success, try_number=1, max_tries=0, job_id=828, pool=default_pool, queue=default, priority_weight=5, operator=PythonOperator, queued_dttm=2024-11-27 14:52:49.214129+00:00, queued_by_job_id=803, pid=8209[0m
 [[34m2024-11-27T14:53:19.042+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.dim_date_run', 'scheduled__2024-09-11T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:53:19.064+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.stg_daily_bars_run', 'scheduled__2024-09-11T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:53:19.090+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-09-11 01:00:00+00:00, run_after=2024-09-12 01:00:00+00:00[0m
 [[34m2024-11-27T14:53:19.069+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run', 'scheduled__2024-09-11T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:53:19.553+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:53:19.555+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:53:19.560+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:53:20.907+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-09-12 01:00:00+00:00, run_after=2024-09-13 01:00:00+00:00[0m
 [[34m2024-11-27T14:53:21.794+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 2 tasks up for execution:
 	<TaskInstance: capstone_andres_dag_.load_polygon_data_task scheduled__2024-09-13T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.load_treasury_data_task scheduled__2024-09-12T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:53:21.796+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 6/16 running and queued tasks[0m
 [[34m2024-11-27T14:53:21.796+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 7/16 running and queued tasks[0m
 [[34m2024-11-27T14:53:21.797+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
 	<TaskInstance: capstone_andres_dag_.load_polygon_data_task scheduled__2024-09-13T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.load_treasury_data_task scheduled__2024-09-12T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:53:21.803+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='load_polygon_data_task', run_id='scheduled__2024-09-13T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
 [[34m2024-11-27T14:53:21.804+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'load_polygon_data_task', 'scheduled__2024-09-13T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:53:21.804+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='load_treasury_data_task', run_id='scheduled__2024-09-12T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 5 and queue default[0m
 [[34m2024-11-27T14:53:21.805+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'load_treasury_data_task', 'scheduled__2024-09-12T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:53:21.816+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='load_polygon_data_task', run_id='scheduled__2024-09-12T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:53:21.832+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=load_polygon_data_task, run_id=scheduled__2024-09-12T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:53:00.052825+00:00, run_end_date=2024-11-27 14:53:19.124527+00:00, run_duration=19.071702, state=success, executor_state=success, try_number=1, max_tries=0, job_id=829, pool=default_pool, queue=default, priority_weight=6, operator=PythonOperator, queued_dttm=2024-11-27 14:52:49.214129+00:00, queued_by_job_id=803, pid=8210[0m
 [[34m2024-11-27T14:53:21.874+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-09-13 01:00:00+00:00, run_after=2024-09-14 01:00:00+00:00[0m
 [[34m2024-11-27T14:53:21.895+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'load_polygon_data_task', 'scheduled__2024-09-13T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:53:21.900+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'load_treasury_data_task', 'scheduled__2024-09-12T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:53:22.370+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:53:22.420+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:53:23.842+0000[0m] {[34mscheduler_job_runner.py:[0m1328} INFO[0m - DAG capstone_andres_dag_ is at (or above) max_active_runs (16 of 16), not creating any more runs[0m
 [[34m2024-11-27T14:53:24.059+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:53:24.060+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:53:24.062+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:53:24.063+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:53:24.063+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:53:24.064+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:53:24.069+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:53:24.071+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:53:24.073+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:53:24.668+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412087950,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T14:53:24.670+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412087954,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T14:53:24.671+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412094686,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T14:53:25.091+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:53:25.093+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:53:25.093+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:53:25.120+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:53:25.121+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:53:25.122+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:53:25.638+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412094694,
 "os.name" : Linux
 [0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:53:25.661+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:53:25.661+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:53:25.667+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412087958,
 "os.name" : Linux
 [0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:53:25.726+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 /usr/local/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked semaphore objects to clean up at shutdown
   warnings.warn('resource_tracker: There appear to be %d '
 /usr/local/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked semaphore objects to clean up at shutdown
   warnings.warn('resource_tracker: There appear to be %d '
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 [[34m2024-11-27T14:53:26.365+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:53:26.526+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:53:26.555+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:53:26.582+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:53:26.665+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 [[34m2024-11-27T14:53:26.954+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:53:27.123+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:53:27.548+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run', run_id='scheduled__2024-09-10T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:53:27.549+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.stg_daily_bars_run', run_id='scheduled__2024-09-10T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:53:27.566+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.stg_daily_bars_run, run_id=scheduled__2024-09-10T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:52:58.086699+00:00, run_end_date=2024-11-27 14:53:25.542916+00:00, run_duration=27.456217, state=success, executor_state=success, try_number=1, max_tries=0, job_id=826, pool=default_pool, queue=default, priority_weight=2, operator=DbtRunLocalOperator, queued_dttm=2024-11-27 14:52:46.484375+00:00, queued_by_job_id=803, pid=8201[0m
 [[34m2024-11-27T14:53:27.567+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run, run_id=scheduled__2024-09-10T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:52:58.037089+00:00, run_end_date=2024-11-27 14:53:25.880559+00:00, run_duration=27.84347, state=success, executor_state=success, try_number=1, max_tries=0, job_id=825, pool=default_pool, queue=default, priority_weight=2, operator=DbtRunLocalOperator, queued_dttm=2024-11-27 14:52:46.484375+00:00, queued_by_job_id=803, pid=8200[0m
 [[34m2024-11-27T14:53:28.212+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:53:28.587+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T14:53:28.804+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 /usr/local/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked semaphore objects to clean up at shutdown
   warnings.warn('resource_tracker: There appear to be %d '
 [[34m2024-11-27T14:53:29.554+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:53:29.642+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:53:29.858+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T14:53:29.860+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.weekly_metrics_run scheduled__2024-09-10T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:53:29.861+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 5/16 running and queued tasks[0m
 [[34m2024-11-27T14:53:29.861+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.weekly_metrics_run scheduled__2024-09-10T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:53:29.865+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.weekly_metrics_run', run_id='scheduled__2024-09-10T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
 [[34m2024-11-27T14:53:29.865+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.weekly_metrics_run', 'scheduled__2024-09-10T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:53:29.869+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.dim_date_run', run_id='scheduled__2024-09-10T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:53:29.871+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:53:29.874+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.dim_date_run, run_id=scheduled__2024-09-10T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:52:58.224100+00:00, run_end_date=2024-11-27 14:53:29.430478+00:00, run_duration=31.206378, state=success, executor_state=success, try_number=1, max_tries=0, job_id=827, pool=default_pool, queue=default, priority_weight=2, operator=DbtRunLocalOperator, queued_dttm=2024-11-27 14:52:46.484375+00:00, queued_by_job_id=803, pid=8202[0m
 [[34m2024-11-27T14:53:29.934+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:53:29.941+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:53:29.916+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.weekly_metrics_run', 'scheduled__2024-09-10T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:53:30.030+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T14:53:30.111+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:53:30.133+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:53:30.171+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:53:30.172+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T14:53:30.247+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T14:53:30.248+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 [[34m2024-11-27T14:53:30.305+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:53:30.431+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:53:30.499+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 Data loaded successfully
 [[34m2024-11-27T14:53:30.793+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:53:30.822+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 [[34m2024-11-27T14:53:31.137+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:53:31.282+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:53:31.396+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 [[34m2024-11-27T14:53:31.406+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:53:31.415+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:53:31.430+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:53:31.579+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:53:31.581+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:53:31.591+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:53:31.694+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 [[34m2024-11-27T14:53:31.702+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 [[34m2024-11-27T14:53:31.726+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 Data loaded successfully
 [[34m2024-11-27T14:53:31.983+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:53:32.011+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 [[34m2024-11-27T14:53:32.013+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:53:32.014+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:53:32.014+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 Data loaded successfully
 [[34m2024-11-27T14:53:32.275+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 Data loaded successfully
 [[34m2024-11-27T14:53:32.281+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:53:32.314+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 [[34m2024-11-27T14:53:32.319+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 Data loaded successfully
 [[34m2024-11-27T14:53:32.402+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:53:32.435+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 [[34m2024-11-27T14:53:32.719+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 1.132853533141315[0m
 [[34m2024-11-27T14:53:32.720+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|8243]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:53:32.721+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:53:32.722+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:53:32.722+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:53:32.722+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|8243]: It took 1.93s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:53:32.728+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|8243]: It took 0.00557s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:53:32.747+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412094702,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T14:53:32.814+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run scheduled__2024-09-11T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:53:34.001+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:53:34.169+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 1.319770404836163[0m
 [[34m2024-11-27T14:53:34.170+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|8241]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:53:34.171+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:53:34.171+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:53:34.172+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:53:34.173+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|8241]: It took 2.19s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:53:34.181+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|8241]: It took 0.00808s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:53:34.271+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.dim_date_run scheduled__2024-09-11T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [[34m2024-11-27T14:53:34.370+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 1.29504968598485[0m
 [[34m2024-11-27T14:53:34.371+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|8242]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:53:34.371+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:53:34.372+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:53:34.372+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:53:34.372+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|8242]: It took 2.09s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:53:34.376+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|8242]: It took 0.00399s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:53:34.386+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 1.3083271190989763[0m
 [[34m2024-11-27T14:53:34.386+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|8255]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:53:34.387+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:53:34.387+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:53:34.388+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:53:34.388+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|8255]: It took 2.11s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:53:34.395+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|8255]: It took 0.007s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:53:34.454+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 1.2662081208545715[0m
 [[34m2024-11-27T14:53:34.455+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|8254]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:53:34.456+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:53:34.457+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:53:34.457+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [2024-11-27T14:53:34.458+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run, dagrun_id=scheduled__2024-09-11T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:53:33.002462+00:00, try_number=1, job_id=830, op_classpath=cosmos.operators.local.DbtRunLocalOperator, cosmos.operators.base.DbtRunMixin, cosmos.operators.local.DbtLocalBaseOperator, cosmos.operators.base.AbstractDbtBaseOperator
 [[34m2024-11-27T14:53:34.458+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|8254]: It took 2.06s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:53:34.464+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|8254]: It took 0.006s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:53:34.475+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_daily_bars_run scheduled__2024-09-11T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [[34m2024-11-27T14:53:34.498+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.load_polygon_data_task scheduled__2024-09-13T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [[34m2024-11-27T14:53:34.585+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.load_treasury_data_task scheduled__2024-09-12T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 [[34m2024-11-27T14:53:34.777+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [2024-11-27T14:53:35.421+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=load_polygon_data_task, dagrun_id=scheduled__2024-09-13T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:53:34.747964+00:00, try_number=1, job_id=833, op_classpath=airflow.operators.python.PythonOperator
 [2024-11-27T14:53:35.463+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=load_treasury_data_task, dagrun_id=scheduled__2024-09-12T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:53:34.803323+00:00, try_number=1, job_id=834, op_classpath=airflow.operators.python.PythonOperator
 [2024-11-27T14:53:35.464+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.dim_date_run, dagrun_id=scheduled__2024-09-11T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:53:34.414968+00:00, try_number=1, job_id=831, op_classpath=cosmos.operators.local.DbtRunLocalOperator, cosmos.operators.base.DbtRunMixin, cosmos.operators.local.DbtLocalBaseOperator, cosmos.operators.base.AbstractDbtBaseOperator
 [2024-11-27T14:53:35.605+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.stg_daily_bars_run, dagrun_id=scheduled__2024-09-11T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:53:34.672430+00:00, try_number=1, job_id=832, op_classpath=cosmos.operators.local.DbtRunLocalOperator, cosmos.operators.base.DbtRunMixin, cosmos.operators.local.DbtLocalBaseOperator, cosmos.operators.base.AbstractDbtBaseOperator
 [[34m2024-11-27T14:53:37.638+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:53:37.941+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T14:53:38.193+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:53:38.846+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.dim_date_run scheduled__2024-09-12T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_daily_bars_run scheduled__2024-09-12T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run scheduled__2024-09-12T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:53:38.846+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 5/16 running and queued tasks[0m
 [[34m2024-11-27T14:53:38.847+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 6/16 running and queued tasks[0m
 [[34m2024-11-27T14:53:38.847+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 7/16 running and queued tasks[0m
 [[34m2024-11-27T14:53:38.847+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.dim_date_run scheduled__2024-09-12T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_daily_bars_run scheduled__2024-09-12T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run scheduled__2024-09-12T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:53:38.849+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.dim_date_run', run_id='scheduled__2024-09-12T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
 [[34m2024-11-27T14:53:38.849+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.dim_date_run', 'scheduled__2024-09-12T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:53:38.849+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.stg_daily_bars_run', run_id='scheduled__2024-09-12T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
 [[34m2024-11-27T14:53:38.850+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.stg_daily_bars_run', 'scheduled__2024-09-12T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:53:38.850+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run', run_id='scheduled__2024-09-12T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
 [[34m2024-11-27T14:53:38.850+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run', 'scheduled__2024-09-12T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:53:38.863+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='load_treasury_data_task', run_id='scheduled__2024-09-12T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:53:38.867+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=load_treasury_data_task, run_id=scheduled__2024-09-12T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:53:34.803323+00:00, run_end_date=2024-11-27 14:53:37.657286+00:00, run_duration=2.853963, state=success, executor_state=success, try_number=1, max_tries=0, job_id=834, pool=default_pool, queue=default, priority_weight=5, operator=PythonOperator, queued_dttm=2024-11-27 14:53:21.799678+00:00, queued_by_job_id=803, pid=8399[0m
 [[34m2024-11-27T14:53:38.938+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.dim_date_run', 'scheduled__2024-09-12T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:53:38.937+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.stg_daily_bars_run', 'scheduled__2024-09-12T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:53:38.943+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run', 'scheduled__2024-09-12T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:53:39.174+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:53:39.248+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:53:39.252+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:53:39.268+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:53:39.350+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:53:39.459+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 Data loaded successfully
 [[34m2024-11-27T14:53:40.033+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:53:40.055+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 [[34m2024-11-27T14:53:40.416+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:53:40.417+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:53:40.417+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:53:40.473+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:53:40.473+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:53:40.474+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:53:40.489+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:53:40.490+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:53:40.490+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:53:40.875+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.3068658688571304[0m
 [[34m2024-11-27T14:53:40.875+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|8381]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:53:40.876+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:53:40.876+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:53:40.876+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:53:40.876+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|8381]: It took 0.844s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:53:40.879+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|8381]: It took 0.00244s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:53:40.880+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412094726,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T14:53:40.913+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.weekly_metrics_run scheduled__2024-09-10T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [[34m2024-11-27T14:53:40.924+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412094730,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T14:53:40.950+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412094734,
 "os.name" : Linux
 [0m
 [2024-11-27T14:53:41.424+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.weekly_metrics_run, dagrun_id=scheduled__2024-09-10T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:53:40.997142+00:00, try_number=1, job_id=835, op_classpath=cosmos.operators.local.DbtRunLocalOperator, cosmos.operators.base.DbtRunMixin, cosmos.operators.local.DbtLocalBaseOperator, cosmos.operators.base.AbstractDbtBaseOperator
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:53:41.715+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:53:41.724+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:53:41.773+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 /usr/local/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked semaphore objects to clean up at shutdown
   warnings.warn('resource_tracker: There appear to be %d '
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 [[34m2024-11-27T14:53:42.461+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:53:42.588+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 [[34m2024-11-27T14:53:42.869+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 /usr/local/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked semaphore objects to clean up at shutdown
   warnings.warn('resource_tracker: There appear to be %d '
 [[34m2024-11-27T14:53:43.692+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 2 tasks up for execution:
 	<TaskInstance: capstone_andres_dag_.load_polygon_data_task scheduled__2024-09-14T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.load_treasury_data_task scheduled__2024-09-13T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:53:43.693+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 5/16 running and queued tasks[0m
 [[34m2024-11-27T14:53:43.693+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 6/16 running and queued tasks[0m
 [[34m2024-11-27T14:53:43.694+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
 	<TaskInstance: capstone_andres_dag_.load_polygon_data_task scheduled__2024-09-14T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.load_treasury_data_task scheduled__2024-09-13T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:53:43.703+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='load_polygon_data_task', run_id='scheduled__2024-09-14T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
 [[34m2024-11-27T14:53:43.704+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'load_polygon_data_task', 'scheduled__2024-09-14T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:53:43.705+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='load_treasury_data_task', run_id='scheduled__2024-09-13T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 5 and queue default[0m
 [[34m2024-11-27T14:53:43.706+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'load_treasury_data_task', 'scheduled__2024-09-13T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:53:43.731+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run', run_id='scheduled__2024-09-11T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:53:43.731+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='load_polygon_data_task', run_id='scheduled__2024-09-13T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:53:43.731+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.stg_daily_bars_run', run_id='scheduled__2024-09-11T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:53:43.849+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=load_polygon_data_task, run_id=scheduled__2024-09-13T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:53:34.747964+00:00, run_end_date=2024-11-27 14:53:41.232860+00:00, run_duration=6.484896, state=success, executor_state=success, try_number=1, max_tries=0, job_id=833, pool=default_pool, queue=default, priority_weight=6, operator=PythonOperator, queued_dttm=2024-11-27 14:53:21.799678+00:00, queued_by_job_id=803, pid=8398[0m
 [[34m2024-11-27T14:53:43.852+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.stg_daily_bars_run, run_id=scheduled__2024-09-11T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:53:34.672430+00:00, run_end_date=2024-11-27 14:53:42.993529+00:00, run_duration=8.321099, state=success, executor_state=success, try_number=1, max_tries=0, job_id=832, pool=default_pool, queue=default, priority_weight=2, operator=DbtRunLocalOperator, queued_dttm=2024-11-27 14:53:18.898433+00:00, queued_by_job_id=803, pid=8390[0m
 [[34m2024-11-27T14:53:43.855+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run, run_id=scheduled__2024-09-11T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:53:33.002462+00:00, run_end_date=2024-11-27 14:53:41.889031+00:00, run_duration=8.886569, state=success, executor_state=success, try_number=1, max_tries=0, job_id=830, pool=default_pool, queue=default, priority_weight=2, operator=DbtRunLocalOperator, queued_dttm=2024-11-27 14:53:18.898433+00:00, queued_by_job_id=803, pid=8383[0m
 [[34m2024-11-27T14:53:44.102+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:53:44.159+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:53:43.904+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'load_polygon_data_task', 'scheduled__2024-09-14T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:53:43.996+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'load_treasury_data_task', 'scheduled__2024-09-13T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:53:44.495+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T14:53:44.595+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T14:53:44.955+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:53:45.023+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:53:45.211+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:53:45.229+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 /usr/local/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked semaphore objects to clean up at shutdown
   warnings.warn('resource_tracker: There appear to be %d '
 [[34m2024-11-27T14:53:45.700+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:53:46.021+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T14:53:46.122+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:53:46.204+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:53:46.387+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:53:46.392+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:53:46.421+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:53:46.556+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 [[34m2024-11-27T14:53:46.612+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 [[34m2024-11-27T14:53:47.568+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:53:47.890+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 Data loaded successfully
 [[34m2024-11-27T14:53:47.998+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:53:48.005+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 Data loaded successfully
 [[34m2024-11-27T14:53:48.113+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:53:48.156+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 [[34m2024-11-27T14:53:48.174+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.weekly_metrics_run scheduled__2024-09-11T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:53:48.178+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 6/16 running and queued tasks[0m
 [[34m2024-11-27T14:53:48.178+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.weekly_metrics_run scheduled__2024-09-11T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:53:48.185+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.weekly_metrics_run', run_id='scheduled__2024-09-11T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
 [[34m2024-11-27T14:53:48.187+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.weekly_metrics_run', 'scheduled__2024-09-11T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:53:48.201+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.dim_date_run', run_id='scheduled__2024-09-11T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:53:48.214+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 [[34m2024-11-27T14:53:48.223+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.dim_date_run, run_id=scheduled__2024-09-11T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:53:34.414968+00:00, run_end_date=2024-11-27 14:53:44.794736+00:00, run_duration=10.379768, state=success, executor_state=success, try_number=1, max_tries=0, job_id=831, pool=default_pool, queue=default, priority_weight=2, operator=DbtRunLocalOperator, queued_dttm=2024-11-27 14:53:18.898433+00:00, queued_by_job_id=803, pid=8389[0m
 [[34m2024-11-27T14:53:48.248+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.weekly_metrics_run', 'scheduled__2024-09-11T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:53:48.593+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 Data loaded successfully
 [[34m2024-11-27T14:53:48.846+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:53:48.865+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 [[34m2024-11-27T14:53:49.480+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:53:49.480+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:53:49.481+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:53:49.493+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:53:49.494+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:53:49.494+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:53:50.058+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.9446700520347804[0m
 [[34m2024-11-27T14:53:50.060+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|8434]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:53:50.063+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:53:50.064+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:53:50.064+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:53:50.065+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|8434]: It took 2.07s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:53:50.077+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.9645081760827452[0m
 [[34m2024-11-27T14:53:50.080+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|8433]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:53:50.080+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|8434]: It took 0.0148s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:53:50.082+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:53:50.082+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:53:50.083+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:53:50.084+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|8433]: It took 1.97s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:53:50.102+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|8433]: It took 0.0176s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:53:50.184+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412088002,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T14:53:50.238+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.dim_date_run scheduled__2024-09-12T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [[34m2024-11-27T14:53:50.253+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412094746,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T14:53:50.261+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_daily_bars_run scheduled__2024-09-12T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [[34m2024-11-27T14:53:50.614+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:53:50.616+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:53:50.616+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:53:50.662+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 1.1750318000558764[0m
 [[34m2024-11-27T14:53:50.663+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|8435]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:53:50.665+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:53:50.665+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:53:50.666+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:53:50.666+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|8435]: It took 1.82s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:53:50.673+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|8435]: It took 0.00711s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:53:50.787+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run scheduled__2024-09-12T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [[34m2024-11-27T14:53:51.188+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412094754,
 "os.name" : Linux
 [0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:53:51.388+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:53:51.461+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [2024-11-27T14:53:51.531+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.dim_date_run, dagrun_id=scheduled__2024-09-12T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:53:50.463611+00:00, try_number=1, job_id=836, op_classpath=cosmos.operators.local.DbtRunLocalOperator, cosmos.operators.base.DbtRunMixin, cosmos.operators.local.DbtLocalBaseOperator, cosmos.operators.base.AbstractDbtBaseOperator
 [2024-11-27T14:53:51.611+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.stg_daily_bars_run, dagrun_id=scheduled__2024-09-12T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:53:50.489989+00:00, try_number=1, job_id=837, op_classpath=cosmos.operators.local.DbtRunLocalOperator, cosmos.operators.base.DbtRunMixin, cosmos.operators.local.DbtLocalBaseOperator, cosmos.operators.base.AbstractDbtBaseOperator
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 [2024-11-27T14:53:52.124+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run, dagrun_id=scheduled__2024-09-12T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:53:51.012644+00:00, try_number=1, job_id=838, op_classpath=cosmos.operators.local.DbtRunLocalOperator, cosmos.operators.base.DbtRunMixin, cosmos.operators.local.DbtLocalBaseOperator, cosmos.operators.base.AbstractDbtBaseOperator
 [[34m2024-11-27T14:53:52.155+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:53:52.179+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:53:52.217+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 [[34m2024-11-27T14:53:52.606+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 /usr/local/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked semaphore objects to clean up at shutdown
   warnings.warn('resource_tracker: There appear to be %d '
 [[34m2024-11-27T14:53:53.187+0000[0m] {[34mdagrun.py:[0m851} INFO[0m - Marking run <DagRun capstone_andres_dag_ @ 2024-09-10 01:00:00+00:00: scheduled__2024-09-10T01:00:00+00:00, state:running, queued_at: 2024-11-27 14:51:57.874559+00:00. externally triggered: False> successful[0m
 [[34m2024-11-27T14:53:53.188+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=capstone_andres_dag_, execution_date=2024-09-10 01:00:00+00:00, run_id=scheduled__2024-09-10T01:00:00+00:00, run_start_date=2024-11-27 14:51:57.902432+00:00, run_end_date=2024-11-27 14:53:53.188069+00:00, run_duration=115.285637, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-09-10 01:00:00+00:00, data_interval_end=2024-09-11 01:00:00+00:00, dag_hash=77fa8f64adac9391cccaaae88e2be5d8[0m
 [[34m2024-11-27T14:53:53.233+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-09-11 01:00:00+00:00, run_after=2024-09-12 01:00:00+00:00[0m
 [[34m2024-11-27T14:53:53.834+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:53:53.970+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.weekly_metrics_run', run_id='scheduled__2024-09-10T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:53:53.980+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.weekly_metrics_run, run_id=scheduled__2024-09-10T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:53:40.997142+00:00, run_end_date=2024-11-27 14:53:53.029600+00:00, run_duration=12.032458, state=success, executor_state=success, try_number=1, max_tries=0, job_id=835, pool=default_pool, queue=default, priority_weight=1, operator=DbtRunLocalOperator, queued_dttm=2024-11-27 14:53:29.862493+00:00, queued_by_job_id=803, pid=8450[0m
 [[34m2024-11-27T14:53:54.207+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T14:53:54.445+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:53:55.020+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-09-12 01:00:00+00:00, run_after=2024-09-13 01:00:00+00:00[0m
 [[34m2024-11-27T14:53:55.126+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:53:55.467+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T14:53:55.483+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:53:55.664+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:53:55.737+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:53:55.768+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 [[34m2024-11-27T14:53:55.934+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:53:56.286+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 Data loaded successfully
 [[34m2024-11-27T14:53:56.421+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:53:56.445+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 [[34m2024-11-27T14:53:56.450+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-09-13 01:00:00+00:00, run_after=2024-09-14 01:00:00+00:00[0m
 [[34m2024-11-27T14:53:56.536+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:53:56.945+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:53:57.094+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:53:57.215+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 [[34m2024-11-27T14:53:57.261+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-09-14 01:00:00+00:00, run_after=2024-09-15 01:00:00+00:00[0m
 [[34m2024-11-27T14:53:57.632+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:53:57.778+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 Data loaded successfully
 [[34m2024-11-27T14:53:57.806+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:53:57.833+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 [[34m2024-11-27T14:53:57.849+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.6912966000381857[0m
 [[34m2024-11-27T14:53:57.850+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|8492]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:53:57.851+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:53:57.851+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:53:57.852+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:53:57.853+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|8492]: It took 1.43s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:53:57.857+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|8492]: It took 0.00409s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:53:57.887+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 [[34m2024-11-27T14:53:57.925+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.load_polygon_data_task scheduled__2024-09-14T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [[34m2024-11-27T14:53:58.276+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-09-15 01:00:00+00:00, run_after=2024-09-16 01:00:00+00:00[0m
 Data loaded successfully
 [[34m2024-11-27T14:53:58.469+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:53:58.517+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 [2024-11-27T14:53:58.674+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=load_polygon_data_task, dagrun_id=scheduled__2024-09-14T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:53:58.066374+00:00, try_number=1, job_id=839, op_classpath=airflow.operators.python.PythonOperator
 [[34m2024-11-27T14:53:59.019+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-09-16 01:00:00+00:00, run_after=2024-09-17 01:00:00+00:00[0m
 [[34m2024-11-27T14:53:59.020+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.5007826229557395[0m
 [[34m2024-11-27T14:53:59.021+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|8491]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:53:59.022+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:53:59.022+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:53:59.022+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:53:59.023+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|8491]: It took 1.22s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:53:59.028+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|8491]: It took 0.00548s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:53:59.097+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.load_treasury_data_task scheduled__2024-09-13T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [[34m2024-11-27T14:53:59.618+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.4715727500151843[0m
 [[34m2024-11-27T14:53:59.618+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|8495]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:53:59.619+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:53:59.619+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:53:59.619+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:53:59.619+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|8495]: It took 1.15s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:53:59.622+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|8495]: It took 0.00262s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:53:59.663+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.weekly_metrics_run scheduled__2024-09-11T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [2024-11-27T14:53:59.689+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=load_treasury_data_task, dagrun_id=scheduled__2024-09-13T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:53:59.260592+00:00, try_number=1, job_id=840, op_classpath=airflow.operators.python.PythonOperator
 [2024-11-27T14:54:00.376+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.weekly_metrics_run, dagrun_id=scheduled__2024-09-11T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:53:59.760474+00:00, try_number=1, job_id=841, op_classpath=cosmos.operators.local.DbtRunLocalOperator, cosmos.operators.base.DbtRunMixin, cosmos.operators.local.DbtLocalBaseOperator, cosmos.operators.base.AbstractDbtBaseOperator
 [[34m2024-11-27T14:54:00.377+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-09-17 01:00:00+00:00, run_after=2024-09-18 01:00:00+00:00[0m
 /usr/local/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked semaphore objects to clean up at shutdown
   warnings.warn('resource_tracker: There appear to be %d '
 /usr/local/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked semaphore objects to clean up at shutdown
   warnings.warn('resource_tracker: There appear to be %d '
 [[34m2024-11-27T14:54:01.630+0000[0m] {[34mscheduler_job_runner.py:[0m1328} INFO[0m - DAG capstone_andres_dag_ is at (or above) max_active_runs (16 of 16), not creating any more runs[0m
 [[34m2024-11-27T14:54:01.890+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.stg_daily_bars_run', run_id='scheduled__2024-09-12T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:54:01.890+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run', run_id='scheduled__2024-09-12T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:54:01.894+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.stg_daily_bars_run, run_id=scheduled__2024-09-12T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:53:50.489989+00:00, run_end_date=2024-11-27 14:54:01.141253+00:00, run_duration=10.651264, state=success, executor_state=success, try_number=1, max_tries=0, job_id=837, pool=default_pool, queue=default, priority_weight=2, operator=DbtRunLocalOperator, queued_dttm=2024-11-27 14:53:38.848007+00:00, queued_by_job_id=803, pid=8503[0m
 [[34m2024-11-27T14:54:01.894+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run, run_id=scheduled__2024-09-12T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:53:51.012644+00:00, run_end_date=2024-11-27 14:54:01.378413+00:00, run_duration=10.365769, state=success, executor_state=success, try_number=1, max_tries=0, job_id=838, pool=default_pool, queue=default, priority_weight=2, operator=DbtRunLocalOperator, queued_dttm=2024-11-27 14:53:38.848007+00:00, queued_by_job_id=803, pid=8508[0m
 /usr/local/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked semaphore objects to clean up at shutdown
   warnings.warn('resource_tracker: There appear to be %d '
 [[34m2024-11-27T14:54:02.537+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 4 tasks up for execution:
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.dim_date_run scheduled__2024-09-13T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_daily_bars_run scheduled__2024-09-13T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run scheduled__2024-09-13T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.weekly_metrics_run scheduled__2024-09-12T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:54:02.537+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 2/16 running and queued tasks[0m
 [[34m2024-11-27T14:54:02.538+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 3/16 running and queued tasks[0m
 [[34m2024-11-27T14:54:02.538+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 4/16 running and queued tasks[0m
 [[34m2024-11-27T14:54:02.538+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 5/16 running and queued tasks[0m
 [[34m2024-11-27T14:54:02.538+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.dim_date_run scheduled__2024-09-13T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_daily_bars_run scheduled__2024-09-13T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run scheduled__2024-09-13T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.weekly_metrics_run scheduled__2024-09-12T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:54:02.542+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.dim_date_run', run_id='scheduled__2024-09-13T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
 [[34m2024-11-27T14:54:02.542+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.dim_date_run', 'scheduled__2024-09-13T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:54:02.542+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.stg_daily_bars_run', run_id='scheduled__2024-09-13T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
 [[34m2024-11-27T14:54:02.542+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.stg_daily_bars_run', 'scheduled__2024-09-13T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:54:02.542+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run', run_id='scheduled__2024-09-13T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
 [[34m2024-11-27T14:54:02.543+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run', 'scheduled__2024-09-13T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:54:02.543+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.weekly_metrics_run', run_id='scheduled__2024-09-12T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
 [[34m2024-11-27T14:54:02.543+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.weekly_metrics_run', 'scheduled__2024-09-12T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:54:02.550+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.dim_date_run', run_id='scheduled__2024-09-12T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:54:02.550+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='load_treasury_data_task', run_id='scheduled__2024-09-13T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:54:02.555+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=load_treasury_data_task, run_id=scheduled__2024-09-13T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:53:59.260592+00:00, run_end_date=2024-11-27 14:54:02.001221+00:00, run_duration=2.740629, state=success, executor_state=success, try_number=1, max_tries=0, job_id=840, pool=default_pool, queue=default, priority_weight=5, operator=PythonOperator, queued_dttm=2024-11-27 14:53:43.697181+00:00, queued_by_job_id=803, pid=8577[0m
 [[34m2024-11-27T14:54:02.555+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.dim_date_run, run_id=scheduled__2024-09-12T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:53:50.463611+00:00, run_end_date=2024-11-27 14:54:01.954965+00:00, run_duration=11.491354, state=success, executor_state=success, try_number=1, max_tries=0, job_id=836, pool=default_pool, queue=default, priority_weight=2, operator=DbtRunLocalOperator, queued_dttm=2024-11-27 14:53:38.848007+00:00, queued_by_job_id=803, pid=8502[0m
 [[34m2024-11-27T14:54:02.577+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.weekly_metrics'[0m
 [[34m2024-11-27T14:54:02.577+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections'[0m
 [[34m2024-11-27T14:54:02.577+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_payments'[0m
 [[34m2024-11-27T14:54:02.578+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_orders'[0m
 [[34m2024-11-27T14:54:02.578+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_daily_bars'[0m
 [[34m2024-11-27T14:54:02.564+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.stg_daily_bars_run', 'scheduled__2024-09-13T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:54:02.560+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.dim_date_run', 'scheduled__2024-09-13T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:54:02.569+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run', 'scheduled__2024-09-13T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:54:02.565+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.weekly_metrics_run', 'scheduled__2024-09-12T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:54:02.628+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:54:02.638+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:54:02.641+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:54:02.646+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:54:03.755+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:54:03.756+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:54:03.756+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:54:03.773+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:54:03.774+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:54:03.775+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:54:03.777+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:54:03.778+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:54:03.779+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:54:03.785+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:54:03.786+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:54:03.787+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:54:04.205+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412094798,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T14:54:04.209+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412088022,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T14:54:04.235+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412088026,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T14:54:04.240+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412094802,
 "os.name" : Linux
 [0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:54:05.008+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:54:05.030+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:54:05.053+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:54:05.177+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 [[34m2024-11-27T14:54:05.496+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 [[34m2024-11-27T14:54:05.619+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 [[34m2024-11-27T14:54:05.752+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:54:05.853+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 /usr/local/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked semaphore objects to clean up at shutdown
   warnings.warn('resource_tracker: There appear to be %d '
 [[34m2024-11-27T14:54:06.922+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:54:07.075+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 2 tasks up for execution:
 	<TaskInstance: capstone_andres_dag_.load_polygon_data_task scheduled__2024-09-15T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.load_treasury_data_task scheduled__2024-09-14T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:54:07.075+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 4/16 running and queued tasks[0m
 [[34m2024-11-27T14:54:07.075+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 5/16 running and queued tasks[0m
 [[34m2024-11-27T14:54:07.076+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
 	<TaskInstance: capstone_andres_dag_.load_polygon_data_task scheduled__2024-09-15T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.load_treasury_data_task scheduled__2024-09-14T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:54:07.077+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='load_polygon_data_task', run_id='scheduled__2024-09-15T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
 [[34m2024-11-27T14:54:07.077+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'load_polygon_data_task', 'scheduled__2024-09-15T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:54:07.078+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='load_treasury_data_task', run_id='scheduled__2024-09-14T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 5 and queue default[0m
 [[34m2024-11-27T14:54:07.078+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'load_treasury_data_task', 'scheduled__2024-09-14T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:54:07.081+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='load_polygon_data_task', run_id='scheduled__2024-09-14T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:54:07.081+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.weekly_metrics_run', run_id='scheduled__2024-09-11T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:54:07.085+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.weekly_metrics_run, run_id=scheduled__2024-09-11T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:53:59.760474+00:00, run_end_date=2024-11-27 14:54:06.812271+00:00, run_duration=7.051797, state=success, executor_state=success, try_number=1, max_tries=0, job_id=841, pool=default_pool, queue=default, priority_weight=1, operator=DbtRunLocalOperator, queued_dttm=2024-11-27 14:53:48.180079+00:00, queued_by_job_id=803, pid=8578[0m
 [[34m2024-11-27T14:54:07.086+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=load_polygon_data_task, run_id=scheduled__2024-09-14T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:53:58.066374+00:00, run_end_date=2024-11-27 14:54:05.908457+00:00, run_duration=7.842083, state=success, executor_state=success, try_number=1, max_tries=0, job_id=839, pool=default_pool, queue=default, priority_weight=6, operator=PythonOperator, queued_dttm=2024-11-27 14:53:43.697181+00:00, queued_by_job_id=803, pid=8565[0m
 [[34m2024-11-27T14:54:07.089+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'load_treasury_data_task', 'scheduled__2024-09-14T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:54:07.092+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'load_polygon_data_task', 'scheduled__2024-09-15T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:54:07.122+0000[0m] {[34mdagrun.py:[0m851} INFO[0m - Marking run <DagRun capstone_andres_dag_ @ 2024-09-11 01:00:00+00:00: scheduled__2024-09-11T01:00:00+00:00, state:running, queued_at: 2024-11-27 14:51:58.965517+00:00. externally triggered: False> successful[0m
 [[34m2024-11-27T14:54:07.123+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=capstone_andres_dag_, execution_date=2024-09-11 01:00:00+00:00, run_id=scheduled__2024-09-11T01:00:00+00:00, run_start_date=2024-11-27 14:51:58.978754+00:00, run_end_date=2024-11-27 14:54:07.123441+00:00, run_duration=128.144687, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-09-11 01:00:00+00:00, data_interval_end=2024-09-12 01:00:00+00:00, dag_hash=77fa8f64adac9391cccaaae88e2be5d8[0m
 [[34m2024-11-27T14:54:07.128+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-09-12 01:00:00+00:00, run_after=2024-09-13 01:00:00+00:00[0m
 [[34m2024-11-27T14:54:07.146+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:54:07.161+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:54:07.235+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T14:54:07.525+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:54:08.048+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:54:08.049+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:54:08.049+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:54:08.089+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:54:08.089+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:54:08.089+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:54:08.250+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:54:08.373+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-09-13 01:00:00+00:00, run_after=2024-09-14 01:00:00+00:00[0m
 [[34m2024-11-27T14:54:08.426+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:54:08.496+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412088030,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T14:54:08.503+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:54:08.511+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412088034,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T14:54:08.571+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T14:54:08.582+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:54:08.724+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T14:54:08.734+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-09-14 01:00:00+00:00, run_after=2024-09-15 01:00:00+00:00[0m
 [[34m2024-11-27T14:54:08.738+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:54:08.802+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T14:54:08.843+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 [[34m2024-11-27T14:54:08.881+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:54:09.033+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:54:09.122+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:54:09.202+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:54:09.323+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:54:09.690+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-09-15 01:00:00+00:00, run_after=2024-09-16 01:00:00+00:00[0m
 Data loaded successfully
 [[34m2024-11-27T14:54:09.764+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:54:09.777+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 [[34m2024-11-27T14:54:10.039+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:54:10.138+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:54:10.155+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 [[34m2024-11-27T14:54:10.196+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:54:10.302+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 [[34m2024-11-27T14:54:10.303+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:54:10.321+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 [[34m2024-11-27T14:54:10.334+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:54:10.409+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 [[34m2024-11-27T14:54:10.423+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 [[34m2024-11-27T14:54:10.540+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:54:10.915+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.5108944380190223[0m
 [[34m2024-11-27T14:54:10.915+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|8616]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:54:10.916+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:54:10.916+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:54:10.917+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:54:10.917+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|8616]: It took 1.15s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:54:10.918+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-09-16 01:00:00+00:00, run_after=2024-09-17 01:00:00+00:00[0m
 [[34m2024-11-27T14:54:10.919+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|8616]: It took 0.00252s to build the Airflow DAG.[0m
 Data loaded successfully
 [[34m2024-11-27T14:54:10.922+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:54:10.934+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 Data loaded successfully
 [[34m2024-11-27T14:54:10.956+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:54:10.963+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.dim_date_run scheduled__2024-09-13T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [[34m2024-11-27T14:54:10.969+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 Data loaded successfully
 [[34m2024-11-27T14:54:11.108+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:54:11.121+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 [2024-11-27T14:54:11.488+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.dim_date_run, dagrun_id=scheduled__2024-09-13T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:54:11.065271+00:00, try_number=1, job_id=842, op_classpath=cosmos.operators.local.DbtRunLocalOperator, cosmos.operators.base.DbtRunMixin, cosmos.operators.local.DbtLocalBaseOperator, cosmos.operators.base.AbstractDbtBaseOperator
 [[34m2024-11-27T14:54:11.983+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.517254845937714[0m
 [[34m2024-11-27T14:54:11.984+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|8617]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:54:11.984+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:54:11.984+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:54:11.985+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:54:11.985+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|8617]: It took 1.06s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:54:11.988+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|8617]: It took 0.00285s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:54:12.018+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.49912352091632783[0m
 [[34m2024-11-27T14:54:12.018+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|8621]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:54:12.018+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:54:12.019+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:54:12.019+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:54:12.019+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|8621]: It took 1.06s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:54:12.022+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|8621]: It took 0.00289s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:54:12.025+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_daily_bars_run scheduled__2024-09-13T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [[34m2024-11-27T14:54:12.065+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run scheduled__2024-09-13T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [[34m2024-11-27T14:54:12.167+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-09-17 01:00:00+00:00, run_after=2024-09-18 01:00:00+00:00[0m
 [[34m2024-11-27T14:54:12.271+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.5914539129007608[0m
 [[34m2024-11-27T14:54:12.272+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|8619]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:54:12.272+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:54:12.272+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:54:12.273+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:54:12.273+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|8619]: It took 1.17s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:54:12.276+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|8619]: It took 0.00341s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:54:12.324+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.weekly_metrics_run scheduled__2024-09-12T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [2024-11-27T14:54:12.595+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.stg_daily_bars_run, dagrun_id=scheduled__2024-09-13T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:54:12.104626+00:00, try_number=1, job_id=843, op_classpath=cosmos.operators.local.DbtRunLocalOperator, cosmos.operators.base.DbtRunMixin, cosmos.operators.local.DbtLocalBaseOperator, cosmos.operators.base.AbstractDbtBaseOperator
 [2024-11-27T14:54:12.632+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run, dagrun_id=scheduled__2024-09-13T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:54:12.146265+00:00, try_number=1, job_id=844, op_classpath=cosmos.operators.local.DbtRunLocalOperator, cosmos.operators.base.DbtRunMixin, cosmos.operators.local.DbtLocalBaseOperator, cosmos.operators.base.AbstractDbtBaseOperator
 [[34m2024-11-27T14:54:12.724+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-09-18 01:00:00+00:00, run_after=2024-09-19 01:00:00+00:00[0m
 [2024-11-27T14:54:12.933+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.weekly_metrics_run, dagrun_id=scheduled__2024-09-12T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:54:12.441125+00:00, try_number=1, job_id=845, op_classpath=cosmos.operators.local.DbtRunLocalOperator, cosmos.operators.base.DbtRunMixin, cosmos.operators.local.DbtLocalBaseOperator, cosmos.operators.base.AbstractDbtBaseOperator
 [[34m2024-11-27T14:54:13.037+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:54:13.258+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:54:13.376+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T14:54:13.598+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T14:54:13.713+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:54:13.907+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:54:14.027+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-09-19 01:00:00+00:00, run_after=2024-09-20 01:00:00+00:00[0m
 [[34m2024-11-27T14:54:14.699+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:54:14.906+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:54:14.915+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-09-20 01:00:00+00:00, run_after=2024-09-21 01:00:00+00:00[0m
 [[34m2024-11-27T14:54:14.939+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:54:15.007+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 [[34m2024-11-27T14:54:15.072+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:54:15.184+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 Data loaded successfully
 [[34m2024-11-27T14:54:15.661+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:54:15.676+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 Data loaded successfully
 [[34m2024-11-27T14:54:15.794+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:54:15.807+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 [[34m2024-11-27T14:54:16.184+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-09-21 01:00:00+00:00, run_after=2024-09-22 01:00:00+00:00[0m
 [[34m2024-11-27T14:54:16.554+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.3508162221405655[0m
 [[34m2024-11-27T14:54:16.554+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|8695]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:54:16.554+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:54:16.555+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:54:16.555+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:54:16.555+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|8695]: It took 0.894s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:54:16.557+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|8695]: It took 0.00219s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:54:16.596+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.load_treasury_data_task scheduled__2024-09-14T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [[34m2024-11-27T14:54:16.687+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.37980490317568183[0m
 [[34m2024-11-27T14:54:16.687+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|8696]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:54:16.687+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:54:16.688+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:54:16.688+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:54:16.688+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|8696]: It took 0.894s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:54:16.694+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|8696]: It took 0.00563s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:54:16.738+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.load_polygon_data_task scheduled__2024-09-15T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [2024-11-27T14:54:17.062+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=load_treasury_data_task, dagrun_id=scheduled__2024-09-14T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:54:16.679549+00:00, try_number=1, job_id=846, op_classpath=airflow.operators.python.PythonOperator
 [2024-11-27T14:54:17.216+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=load_polygon_data_task, dagrun_id=scheduled__2024-09-15T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:54:16.844325+00:00, try_number=1, job_id=847, op_classpath=airflow.operators.python.PythonOperator
 [[34m2024-11-27T14:54:17.434+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-09-22 01:00:00+00:00, run_after=2024-09-23 01:00:00+00:00[0m
 /usr/local/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked semaphore objects to clean up at shutdown
   warnings.warn('resource_tracker: There appear to be %d '
 /usr/local/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked semaphore objects to clean up at shutdown
   warnings.warn('resource_tracker: There appear to be %d '
 [[34m2024-11-27T14:54:18.692+0000[0m] {[34mscheduler_job_runner.py:[0m1328} INFO[0m - DAG capstone_andres_dag_ is at (or above) max_active_runs (16 of 16), not creating any more runs[0m
 /usr/local/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked semaphore objects to clean up at shutdown
   warnings.warn('resource_tracker: There appear to be %d '
 [[34m2024-11-27T14:54:18.948+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 2 tasks up for execution:
 	<TaskInstance: capstone_andres_dag_.load_polygon_data_task scheduled__2024-09-16T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.weekly_metrics_run scheduled__2024-09-13T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:54:18.948+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 2/16 running and queued tasks[0m
 [[34m2024-11-27T14:54:18.948+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 3/16 running and queued tasks[0m
 [[34m2024-11-27T14:54:18.948+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
 	<TaskInstance: capstone_andres_dag_.load_polygon_data_task scheduled__2024-09-16T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.weekly_metrics_run scheduled__2024-09-13T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:54:18.951+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='load_polygon_data_task', run_id='scheduled__2024-09-16T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
 [[34m2024-11-27T14:54:18.951+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'load_polygon_data_task', 'scheduled__2024-09-16T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:54:18.951+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.weekly_metrics_run', run_id='scheduled__2024-09-13T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
 [[34m2024-11-27T14:54:18.951+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.weekly_metrics_run', 'scheduled__2024-09-13T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:54:18.956+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='load_polygon_data_task', run_id='scheduled__2024-09-15T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:54:18.957+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.dim_date_run', run_id='scheduled__2024-09-13T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:54:18.957+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run', run_id='scheduled__2024-09-13T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:54:18.957+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.stg_daily_bars_run', run_id='scheduled__2024-09-13T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:54:18.954+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'load_polygon_data_task', 'scheduled__2024-09-16T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 /usr/local/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked semaphore objects to clean up at shutdown
   warnings.warn('resource_tracker: There appear to be %d '
 [[34m2024-11-27T14:54:18.967+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.stg_daily_bars_run, run_id=scheduled__2024-09-13T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:54:12.104626+00:00, run_end_date=2024-11-27 14:54:18.663729+00:00, run_duration=6.559103, state=success, executor_state=success, try_number=1, max_tries=0, job_id=843, pool=default_pool, queue=default, priority_weight=2, operator=DbtRunLocalOperator, queued_dttm=2024-11-27 14:54:02.539407+00:00, queued_by_job_id=803, pid=8735[0m
 [[34m2024-11-27T14:54:18.968+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.dim_date_run, run_id=scheduled__2024-09-13T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:54:11.065271+00:00, run_end_date=2024-11-27 14:54:18.484805+00:00, run_duration=7.419534, state=success, executor_state=success, try_number=1, max_tries=0, job_id=842, pool=default_pool, queue=default, priority_weight=2, operator=DbtRunLocalOperator, queued_dttm=2024-11-27 14:54:02.539407+00:00, queued_by_job_id=803, pid=8728[0m
 [[34m2024-11-27T14:54:18.968+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=load_polygon_data_task, run_id=scheduled__2024-09-15T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:54:16.844325+00:00, run_end_date=2024-11-27 14:54:17.865450+00:00, run_duration=1.021125, state=success, executor_state=success, try_number=1, max_tries=0, job_id=847, pool=default_pool, queue=default, priority_weight=6, operator=PythonOperator, queued_dttm=2024-11-27 14:54:07.076472+00:00, queued_by_job_id=803, pid=8769[0m
 [[34m2024-11-27T14:54:18.968+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run, run_id=scheduled__2024-09-13T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:54:12.146265+00:00, run_end_date=2024-11-27 14:54:18.582188+00:00, run_duration=6.435923, state=success, executor_state=success, try_number=1, max_tries=0, job_id=844, pool=default_pool, queue=default, priority_weight=2, operator=DbtRunLocalOperator, queued_dttm=2024-11-27 14:54:02.539407+00:00, queued_by_job_id=803, pid=8736[0m
 [[34m2024-11-27T14:54:18.963+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.weekly_metrics_run', 'scheduled__2024-09-13T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:54:18.991+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:54:19.006+0000[0m] {[34mdagrun.py:[0m851} INFO[0m - Marking run <DagRun capstone_andres_dag_ @ 2024-09-12 01:00:00+00:00: scheduled__2024-09-12T01:00:00+00:00, state:running, queued_at: 2024-11-27 14:52:00.050772+00:00. externally triggered: False> successful[0m
 [[34m2024-11-27T14:54:19.006+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=capstone_andres_dag_, execution_date=2024-09-12 01:00:00+00:00, run_id=scheduled__2024-09-12T01:00:00+00:00, run_start_date=2024-11-27 14:52:00.066550+00:00, run_end_date=2024-11-27 14:54:19.006573+00:00, run_duration=138.940023, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-09-12 01:00:00+00:00, data_interval_end=2024-09-13 01:00:00+00:00, dag_hash=77fa8f64adac9391cccaaae88e2be5d8[0m
 [[34m2024-11-27T14:54:19.011+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-09-13 01:00:00+00:00, run_after=2024-09-14 01:00:00+00:00[0m
 [[34m2024-11-27T14:54:19.019+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:54:19.240+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.weekly_metrics_run', run_id='scheduled__2024-09-12T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:54:19.249+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.weekly_metrics_run, run_id=scheduled__2024-09-12T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:54:12.441125+00:00, run_end_date=2024-11-27 14:54:18.858339+00:00, run_duration=6.417214, state=success, executor_state=success, try_number=1, max_tries=0, job_id=845, pool=default_pool, queue=default, priority_weight=1, operator=DbtRunLocalOperator, queued_dttm=2024-11-27 14:54:02.539407+00:00, queued_by_job_id=803, pid=8737[0m
 [[34m2024-11-27T14:54:20.182+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-09-14 01:00:00+00:00, run_after=2024-09-15 01:00:00+00:00[0m
 [[34m2024-11-27T14:54:20.214+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:54:20.215+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:54:20.216+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:54:20.247+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:54:20.248+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:54:20.248+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:54:20.448+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 4 tasks up for execution:
 	<TaskInstance: capstone_andres_dag_.load_treasury_data_task scheduled__2024-09-15T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.dim_date_run scheduled__2024-09-14T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_daily_bars_run scheduled__2024-09-14T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run scheduled__2024-09-14T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:54:20.448+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 2/16 running and queued tasks[0m
 [[34m2024-11-27T14:54:20.449+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 3/16 running and queued tasks[0m
 [[34m2024-11-27T14:54:20.449+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 4/16 running and queued tasks[0m
 [[34m2024-11-27T14:54:20.449+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 5/16 running and queued tasks[0m
 [[34m2024-11-27T14:54:20.449+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
 	<TaskInstance: capstone_andres_dag_.load_treasury_data_task scheduled__2024-09-15T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.dim_date_run scheduled__2024-09-14T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_daily_bars_run scheduled__2024-09-14T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run scheduled__2024-09-14T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:54:20.453+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='load_treasury_data_task', run_id='scheduled__2024-09-15T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 5 and queue default[0m
 [[34m2024-11-27T14:54:20.454+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'load_treasury_data_task', 'scheduled__2024-09-15T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:54:20.454+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.dim_date_run', run_id='scheduled__2024-09-14T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
 [[34m2024-11-27T14:54:20.454+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.dim_date_run', 'scheduled__2024-09-14T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:54:20.455+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.stg_daily_bars_run', run_id='scheduled__2024-09-14T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
 [[34m2024-11-27T14:54:20.455+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.stg_daily_bars_run', 'scheduled__2024-09-14T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:54:20.455+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run', run_id='scheduled__2024-09-14T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
 [[34m2024-11-27T14:54:20.455+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run', 'scheduled__2024-09-14T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:54:20.463+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='load_treasury_data_task', run_id='scheduled__2024-09-14T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:54:20.474+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=load_treasury_data_task, run_id=scheduled__2024-09-14T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:54:16.679549+00:00, run_end_date=2024-11-27 14:54:19.058697+00:00, run_duration=2.379148, state=success, executor_state=success, try_number=1, max_tries=0, job_id=846, pool=default_pool, queue=default, priority_weight=5, operator=PythonOperator, queued_dttm=2024-11-27 14:54:07.076472+00:00, queued_by_job_id=803, pid=8768[0m
 [[34m2024-11-27T14:54:20.471+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'load_treasury_data_task', 'scheduled__2024-09-15T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:54:20.473+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.dim_date_run', 'scheduled__2024-09-14T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:54:20.474+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.stg_daily_bars_run', 'scheduled__2024-09-14T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:54:20.473+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run', 'scheduled__2024-09-14T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:54:20.532+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-09-15 01:00:00+00:00, run_after=2024-09-16 01:00:00+00:00[0m
 [[34m2024-11-27T14:54:20.559+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:54:20.560+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:54:20.561+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:54:20.570+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:54:20.740+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412088074,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T14:54:20.754+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412094846,
 "os.name" : Linux
 [0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:54:21.696+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:54:21.775+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:54:22.312+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-09-16 01:00:00+00:00, run_after=2024-09-17 01:00:00+00:00[0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 [[34m2024-11-27T14:54:23.269+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:54:23.303+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:54:23.576+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:54:23.578+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:54:23.582+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:54:23.601+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:54:23.609+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:54:23.610+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:54:23.626+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:54:23.632+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:54:23.634+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:54:23.685+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:54:23.689+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:54:23.690+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:54:24.261+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412094854,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T14:54:24.370+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412088078,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T14:54:24.410+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412088082,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T14:54:24.418+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412094858,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T14:54:25.046+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:54:25.079+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:54:25.088+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:54:25.198+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-09-17 01:00:00+00:00, run_after=2024-09-18 01:00:00+00:00[0m
 [[34m2024-11-27T14:54:25.362+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 [[34m2024-11-27T14:54:25.393+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:54:25.405+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:54:25.418+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:54:25.418+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:54:25.639+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:54:25.645+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:54:25.705+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 [[34m2024-11-27T14:54:25.946+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:54:25.948+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:54:26.077+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:54:26.811+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:54:26.853+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-09-18 01:00:00+00:00, run_after=2024-09-19 01:00:00+00:00[0m
 [[34m2024-11-27T14:54:27.047+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:54:27.052+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:54:27.168+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 [[34m2024-11-27T14:54:27.209+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:54:27.328+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 Data loaded successfully
 [[34m2024-11-27T14:54:27.799+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:54:27.819+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 Data loaded successfully
 [[34m2024-11-27T14:54:27.847+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:54:27.866+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 [[34m2024-11-27T14:54:28.028+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-09-19 01:00:00+00:00, run_after=2024-09-20 01:00:00+00:00[0m
 [[34m2024-11-27T14:54:28.690+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:54:28.879+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:54:28.938+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:54:28.977+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.5504722020123154[0m
 [[34m2024-11-27T14:54:28.977+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|8783]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:54:28.978+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:54:28.979+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:54:28.979+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:54:28.979+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|8783]: It took 1.18s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:54:28.985+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|8783]: It took 0.00574s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:54:28.998+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-09-20 01:00:00+00:00, run_after=2024-09-21 01:00:00+00:00[0m
 [[34m2024-11-27T14:54:29.013+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.53077432513237[0m
 [[34m2024-11-27T14:54:29.014+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|8782]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:54:29.015+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:54:29.015+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:54:29.015+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:54:29.016+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|8782]: It took 1.17s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:54:29.023+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|8782]: It took 0.00772s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:54:29.036+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:54:29.062+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.weekly_metrics_run scheduled__2024-09-13T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [[34m2024-11-27T14:54:29.082+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T14:54:29.111+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.load_polygon_data_task scheduled__2024-09-16T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [[34m2024-11-27T14:54:29.222+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T14:54:29.222+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T14:54:29.403+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T14:54:29.494+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:54:29.522+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:54:29.527+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:54:29.675+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [2024-11-27T14:54:29.802+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=load_polygon_data_task, dagrun_id=scheduled__2024-09-16T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:54:29.285954+00:00, try_number=1, job_id=849, op_classpath=airflow.operators.python.PythonOperator
 [2024-11-27T14:54:29.825+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.weekly_metrics_run, dagrun_id=scheduled__2024-09-13T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:54:29.237008+00:00, try_number=1, job_id=848, op_classpath=cosmos.operators.local.DbtRunLocalOperator, cosmos.operators.base.DbtRunMixin, cosmos.operators.local.DbtLocalBaseOperator, cosmos.operators.base.AbstractDbtBaseOperator
 [[34m2024-11-27T14:54:30.348+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-09-21 01:00:00+00:00, run_after=2024-09-22 01:00:00+00:00[0m
 [[34m2024-11-27T14:54:30.557+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:54:30.559+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
 	<TaskInstance: capstone_andres_dag_.load_polygon_data_task scheduled__2024-09-17T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:54:30.559+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 5/16 running and queued tasks[0m
 [[34m2024-11-27T14:54:30.560+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
 	<TaskInstance: capstone_andres_dag_.load_polygon_data_task scheduled__2024-09-17T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:54:30.561+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='load_polygon_data_task', run_id='scheduled__2024-09-17T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
 [[34m2024-11-27T14:54:30.562+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'load_polygon_data_task', 'scheduled__2024-09-17T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:54:30.566+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='load_polygon_data_task', run_id='scheduled__2024-09-16T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:54:30.570+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=load_polygon_data_task, run_id=scheduled__2024-09-16T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:54:29.285954+00:00, run_end_date=2024-11-27 14:54:30.447169+00:00, run_duration=1.161215, state=success, executor_state=success, try_number=1, max_tries=0, job_id=849, pool=default_pool, queue=default, priority_weight=6, operator=PythonOperator, queued_dttm=2024-11-27 14:54:18.949653+00:00, queued_by_job_id=803, pid=8870[0m
 [[34m2024-11-27T14:54:30.589+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-09-22 01:00:00+00:00, run_after=2024-09-23 01:00:00+00:00[0m
 [[34m2024-11-27T14:54:30.596+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'load_polygon_data_task', 'scheduled__2024-09-17T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:54:30.692+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:54:30.695+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:54:30.702+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:54:30.749+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:54:30.834+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:54:30.835+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-09-23 01:00:00+00:00, run_after=2024-09-24 01:00:00+00:00[0m
 [[34m2024-11-27T14:54:30.858+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:54:30.865+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 [[34m2024-11-27T14:54:30.887+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:54:30.941+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 [[34m2024-11-27T14:54:30.958+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 [[34m2024-11-27T14:54:31.021+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 Data loaded successfully
 [[34m2024-11-27T14:54:31.426+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:54:31.447+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 [[34m2024-11-27T14:54:31.484+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 Data loaded successfully
 [[34m2024-11-27T14:54:31.547+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:54:31.565+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 Data loaded successfully
 [[34m2024-11-27T14:54:31.720+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:54:31.734+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 [[34m2024-11-27T14:54:31.826+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:54:31.827+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:54:31.827+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 Data loaded successfully
 [[34m2024-11-27T14:54:32.029+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:54:32.047+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 [[34m2024-11-27T14:54:32.140+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-09-24 01:00:00+00:00, run_after=2024-09-25 01:00:00+00:00[0m
 [[34m2024-11-27T14:54:32.343+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412088090,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T14:54:32.596+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.5763847711496055[0m
 [[34m2024-11-27T14:54:32.597+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|8788]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:54:32.597+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:54:32.598+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:54:32.598+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:54:32.598+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|8788]: It took 1.17s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:54:32.601+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|8788]: It took 0.00255s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:54:32.642+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_daily_bars_run scheduled__2024-09-14T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [[34m2024-11-27T14:54:32.642+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.5066995050292462[0m
 [[34m2024-11-27T14:54:32.642+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|8789]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:54:32.643+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:54:32.643+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:54:32.644+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:54:32.644+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|8789]: It took 1.1s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:54:32.647+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|8789]: It took 0.00271s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:54:32.699+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run scheduled__2024-09-14T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [[34m2024-11-27T14:54:32.767+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.4638406978920102[0m
 [[34m2024-11-27T14:54:32.767+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|8787]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:54:32.768+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:54:32.768+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:54:32.768+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:54:32.769+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|8787]: It took 1.05s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:54:32.773+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|8787]: It took 0.00427s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:54:32.827+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.load_treasury_data_task scheduled__2024-09-15T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [[34m2024-11-27T14:54:33.207+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.5819814670830965[0m
 [[34m2024-11-27T14:54:33.207+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|8786]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:54:33.208+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:54:33.209+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:54:33.209+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:54:33.210+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|8786]: It took 1.18s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:54:33.217+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|8786]: It took 0.00735s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:54:33.293+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.dim_date_run scheduled__2024-09-14T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:54:33.308+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [2024-11-27T14:54:33.372+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.stg_daily_bars_run, dagrun_id=scheduled__2024-09-14T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:54:32.734209+00:00, try_number=1, job_id=850, op_classpath=cosmos.operators.local.DbtRunLocalOperator, cosmos.operators.base.DbtRunMixin, cosmos.operators.local.DbtLocalBaseOperator, cosmos.operators.base.AbstractDbtBaseOperator
 [[34m2024-11-27T14:54:33.454+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-09-25 01:00:00+00:00, run_after=2024-09-26 01:00:00+00:00[0m
 [2024-11-27T14:54:33.489+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run, dagrun_id=scheduled__2024-09-14T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:54:32.811883+00:00, try_number=1, job_id=851, op_classpath=cosmos.operators.local.DbtRunLocalOperator, cosmos.operators.base.DbtRunMixin, cosmos.operators.local.DbtLocalBaseOperator, cosmos.operators.base.AbstractDbtBaseOperator
 [2024-11-27T14:54:33.500+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=load_treasury_data_task, dagrun_id=scheduled__2024-09-15T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:54:32.961416+00:00, try_number=1, job_id=852, op_classpath=airflow.operators.python.PythonOperator
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 [2024-11-27T14:54:34.076+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.dim_date_run, dagrun_id=scheduled__2024-09-14T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:54:33.442370+00:00, try_number=1, job_id=853, op_classpath=cosmos.operators.local.DbtRunLocalOperator, cosmos.operators.base.DbtRunMixin, cosmos.operators.local.DbtLocalBaseOperator, cosmos.operators.base.AbstractDbtBaseOperator
 [[34m2024-11-27T14:54:34.104+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:54:34.812+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-09-26 01:00:00+00:00, run_after=2024-09-27 01:00:00+00:00[0m
 [[34m2024-11-27T14:54:35.086+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 4 tasks up for execution:
 	<TaskInstance: capstone_andres_dag_.load_treasury_data_task scheduled__2024-09-16T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.dim_date_run scheduled__2024-09-15T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_daily_bars_run scheduled__2024-09-15T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run scheduled__2024-09-15T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:54:35.086+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 5/16 running and queued tasks[0m
 [[34m2024-11-27T14:54:35.086+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 6/16 running and queued tasks[0m
 [[34m2024-11-27T14:54:35.087+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 7/16 running and queued tasks[0m
 [[34m2024-11-27T14:54:35.087+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 8/16 running and queued tasks[0m
 [[34m2024-11-27T14:54:35.087+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
 	<TaskInstance: capstone_andres_dag_.load_treasury_data_task scheduled__2024-09-16T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.dim_date_run scheduled__2024-09-15T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_daily_bars_run scheduled__2024-09-15T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run scheduled__2024-09-15T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:54:35.091+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='load_treasury_data_task', run_id='scheduled__2024-09-16T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 5 and queue default[0m
 [[34m2024-11-27T14:54:35.091+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'load_treasury_data_task', 'scheduled__2024-09-16T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:54:35.092+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.dim_date_run', run_id='scheduled__2024-09-15T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
 [[34m2024-11-27T14:54:35.092+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.dim_date_run', 'scheduled__2024-09-15T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:54:35.092+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.stg_daily_bars_run', run_id='scheduled__2024-09-15T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
 [[34m2024-11-27T14:54:35.092+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.stg_daily_bars_run', 'scheduled__2024-09-15T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:54:35.092+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run', run_id='scheduled__2024-09-15T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
 [[34m2024-11-27T14:54:35.093+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run', 'scheduled__2024-09-15T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:54:35.100+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='load_treasury_data_task', run_id='scheduled__2024-09-15T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:54:35.107+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=load_treasury_data_task, run_id=scheduled__2024-09-15T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:54:32.961416+00:00, run_end_date=2024-11-27 14:54:34.551891+00:00, run_duration=1.590475, state=success, executor_state=success, try_number=1, max_tries=0, job_id=852, pool=default_pool, queue=default, priority_weight=5, operator=PythonOperator, queued_dttm=2024-11-27 14:54:20.450679+00:00, queued_by_job_id=803, pid=8891[0m
 [[34m2024-11-27T14:54:35.104+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run', 'scheduled__2024-09-15T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:54:35.103+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.dim_date_run', 'scheduled__2024-09-15T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:54:35.108+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'load_treasury_data_task', 'scheduled__2024-09-16T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:54:35.109+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.stg_daily_bars_run', 'scheduled__2024-09-15T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:54:35.137+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-09-27 01:00:00+00:00, run_after=2024-09-28 01:00:00+00:00[0m
 [[34m2024-11-27T14:54:35.190+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:54:35.190+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:54:35.192+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:54:35.210+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:54:36.137+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-09-28 01:00:00+00:00, run_after=2024-09-29 01:00:00+00:00[0m
 [[34m2024-11-27T14:54:36.611+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:54:36.612+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:54:36.613+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:54:36.680+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:54:36.680+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:54:36.681+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:54:36.691+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:54:36.693+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:54:36.693+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:54:36.733+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:54:36.736+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:54:36.736+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 /usr/local/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked semaphore objects to clean up at shutdown
   warnings.warn('resource_tracker: There appear to be %d '
 [[34m2024-11-27T14:54:36.895+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:54:37.187+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412088106,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T14:54:37.189+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412088102,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T14:54:37.206+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412094870,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T14:54:37.207+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T14:54:37.217+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412094874,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T14:54:37.495+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:54:37.502+0000[0m] {[34mscheduler_job_runner.py:[0m1328} INFO[0m - DAG capstone_andres_dag_ is at (or above) max_active_runs (16 of 16), not creating any more runs[0m
 [[34m2024-11-27T14:54:37.582+0000[0m] {[34mdagrun.py:[0m851} INFO[0m - Marking run <DagRun capstone_andres_dag_ @ 2024-09-13 01:00:00+00:00: scheduled__2024-09-13T01:00:00+00:00, state:running, queued_at: 2024-11-27 14:52:01.141709+00:00. externally triggered: False> successful[0m
 [[34m2024-11-27T14:54:37.582+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=capstone_andres_dag_, execution_date=2024-09-13 01:00:00+00:00, run_id=scheduled__2024-09-13T01:00:00+00:00, run_start_date=2024-11-27 14:52:01.155433+00:00, run_end_date=2024-11-27 14:54:37.582448+00:00, run_duration=156.427015, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-09-13 01:00:00+00:00, data_interval_end=2024-09-14 01:00:00+00:00, dag_hash=77fa8f64adac9391cccaaae88e2be5d8[0m
 [[34m2024-11-27T14:54:37.591+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-09-14 01:00:00+00:00, run_after=2024-09-15 01:00:00+00:00[0m
 [[34m2024-11-27T14:54:37.937+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.weekly_metrics_run', run_id='scheduled__2024-09-13T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:54:37.943+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.weekly_metrics_run, run_id=scheduled__2024-09-13T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:54:29.237008+00:00, run_end_date=2024-11-27 14:54:36.827426+00:00, run_duration=7.590418, state=success, executor_state=success, try_number=1, max_tries=0, job_id=848, pool=default_pool, queue=default, priority_weight=1, operator=DbtRunLocalOperator, queued_dttm=2024-11-27 14:54:18.949653+00:00, queued_by_job_id=803, pid=8869[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:54:38.066+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:54:38.082+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:54:38.106+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:54:38.121+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 [[34m2024-11-27T14:54:38.528+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:54:38.528+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:54:38.579+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:54:38.580+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:54:38.663+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:54:38.743+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:54:38.848+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 [[34m2024-11-27T14:54:38.964+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-09-15 01:00:00+00:00, run_after=2024-09-16 01:00:00+00:00[0m
 [[34m2024-11-27T14:54:39.348+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-09-16 01:00:00+00:00, run_after=2024-09-17 01:00:00+00:00[0m
 Data loaded successfully
 [[34m2024-11-27T14:54:39.437+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:54:39.457+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 [[34m2024-11-27T14:54:40.577+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.5767497378401458[0m
 [[34m2024-11-27T14:54:40.577+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|8873]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:54:40.579+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:54:40.579+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:54:40.580+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:54:40.580+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|8873]: It took 1.14s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:54:40.585+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|8873]: It took 0.00436s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:54:40.744+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-09-17 01:00:00+00:00, run_after=2024-09-18 01:00:00+00:00[0m
 [[34m2024-11-27T14:54:40.765+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.load_polygon_data_task scheduled__2024-09-17T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [[34m2024-11-27T14:54:41.282+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:54:41.293+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:54:41.331+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:54:41.339+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:54:41.506+0000[0m] {[34mscheduler_job_runner.py:[0m1328} INFO[0m - DAG capstone_andres_dag_ is at (or above) max_active_runs (16 of 16), not creating any more runs[0m
 [[34m2024-11-27T14:54:41.618+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T14:54:41.652+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T14:54:41.655+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T14:54:41.657+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [2024-11-27T14:54:41.811+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=load_polygon_data_task, dagrun_id=scheduled__2024-09-17T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:54:40.931906+00:00, try_number=1, job_id=854, op_classpath=airflow.operators.python.PythonOperator
 /usr/local/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked semaphore objects to clean up at shutdown
   warnings.warn('resource_tracker: There appear to be %d '
 /usr/local/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked semaphore objects to clean up at shutdown
   warnings.warn('resource_tracker: There appear to be %d '
 [[34m2024-11-27T14:54:41.999+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:54:42.011+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:54:42.029+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:54:42.052+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:54:43.015+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:54:43.034+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:54:43.042+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:54:43.102+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:54:43.166+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.stg_daily_bars_run', run_id='scheduled__2024-09-14T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:54:43.167+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run', run_id='scheduled__2024-09-14T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:54:43.172+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.stg_daily_bars_run, run_id=scheduled__2024-09-14T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:54:32.734209+00:00, run_end_date=2024-11-27 14:54:41.566212+00:00, run_duration=8.832003, state=success, executor_state=success, try_number=1, max_tries=0, job_id=850, pool=default_pool, queue=default, priority_weight=2, operator=DbtRunLocalOperator, queued_dttm=2024-11-27 14:54:20.450679+00:00, queued_by_job_id=803, pid=8889[0m
 [[34m2024-11-27T14:54:43.172+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run, run_id=scheduled__2024-09-14T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:54:32.811883+00:00, run_end_date=2024-11-27 14:54:41.514762+00:00, run_duration=8.702879, state=success, executor_state=success, try_number=1, max_tries=0, job_id=851, pool=default_pool, queue=default, priority_weight=2, operator=DbtRunLocalOperator, queued_dttm=2024-11-27 14:54:20.450679+00:00, queued_by_job_id=803, pid=8890[0m
 [[34m2024-11-27T14:54:43.173+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:54:43.173+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:54:43.232+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:54:43.236+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:54:43.272+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 [[34m2024-11-27T14:54:43.285+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 [[34m2024-11-27T14:54:43.343+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 [[34m2024-11-27T14:54:43.349+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 /usr/local/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked semaphore objects to clean up at shutdown
   warnings.warn('resource_tracker: There appear to be %d '
 [[34m2024-11-27T14:54:43.669+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.weekly_metrics_run scheduled__2024-09-14T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:54:43.669+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 5/16 running and queued tasks[0m
 [[34m2024-11-27T14:54:43.670+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.weekly_metrics_run scheduled__2024-09-14T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:54:43.672+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.weekly_metrics_run', run_id='scheduled__2024-09-14T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
 [[34m2024-11-27T14:54:43.672+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.weekly_metrics_run', 'scheduled__2024-09-14T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:54:43.674+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.dim_date_run', run_id='scheduled__2024-09-14T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:54:43.678+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.dim_date_run, run_id=scheduled__2024-09-14T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:54:33.442370+00:00, run_end_date=2024-11-27 14:54:43.380824+00:00, run_duration=9.938454, state=success, executor_state=success, try_number=1, max_tries=0, job_id=853, pool=default_pool, queue=default, priority_weight=2, operator=DbtRunLocalOperator, queued_dttm=2024-11-27 14:54:20.450679+00:00, queued_by_job_id=803, pid=8898[0m
 [[34m2024-11-27T14:54:43.677+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.weekly_metrics_run', 'scheduled__2024-09-14T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:54:43.732+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 Data loaded successfully
 [[34m2024-11-27T14:54:43.855+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:54:43.878+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 Data loaded successfully
 [[34m2024-11-27T14:54:43.926+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 Data loaded successfully
 [[34m2024-11-27T14:54:43.939+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:54:43.947+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 [[34m2024-11-27T14:54:43.956+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 Data loaded successfully
 [[34m2024-11-27T14:54:43.996+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:54:44.031+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 [[34m2024-11-27T14:54:45.149+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.5500564789399505[0m
 [[34m2024-11-27T14:54:45.149+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|8915]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:54:45.150+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:54:45.150+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:54:45.150+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:54:45.151+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|8915]: It took 1.3s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:54:45.151+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:54:45.152+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:54:45.152+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:54:45.155+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|8915]: It took 0.00466s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:54:45.185+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.5182921169325709[0m
 [[34m2024-11-27T14:54:45.185+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|8917]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:54:45.186+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:54:45.186+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:54:45.186+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:54:45.187+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|8917]: It took 1.26s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:54:45.195+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|8917]: It took 0.00835s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:54:45.207+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run scheduled__2024-09-15T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [[34m2024-11-27T14:54:45.243+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.load_treasury_data_task scheduled__2024-09-16T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [[34m2024-11-27T14:54:45.251+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.4926339390221983[0m
 [[34m2024-11-27T14:54:45.251+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|8918]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:54:45.252+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:54:45.252+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:54:45.253+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:54:45.253+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|8918]: It took 1.31s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:54:45.260+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|8918]: It took 0.00671s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:54:45.318+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_daily_bars_run scheduled__2024-09-15T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [[34m2024-11-27T14:54:45.398+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.5766369919292629[0m
 [[34m2024-11-27T14:54:45.398+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|8916]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:54:45.399+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:54:45.400+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:54:45.400+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:54:45.400+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|8916]: It took 1.4s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:54:45.404+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|8916]: It took 0.00417s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:54:45.483+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.dim_date_run scheduled__2024-09-15T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [[34m2024-11-27T14:54:45.714+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412094902,
 "os.name" : Linux
 [0m
 [2024-11-27T14:54:45.877+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=load_treasury_data_task, dagrun_id=scheduled__2024-09-16T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:54:45.364271+00:00, try_number=1, job_id=856, op_classpath=airflow.operators.python.PythonOperator
 [2024-11-27T14:54:45.951+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run, dagrun_id=scheduled__2024-09-15T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:54:45.318373+00:00, try_number=1, job_id=855, op_classpath=cosmos.operators.local.DbtRunLocalOperator, cosmos.operators.base.DbtRunMixin, cosmos.operators.local.DbtLocalBaseOperator, cosmos.operators.base.AbstractDbtBaseOperator
 [2024-11-27T14:54:46.068+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.stg_daily_bars_run, dagrun_id=scheduled__2024-09-15T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:54:45.468013+00:00, try_number=1, job_id=857, op_classpath=cosmos.operators.local.DbtRunLocalOperator, cosmos.operators.base.DbtRunMixin, cosmos.operators.local.DbtLocalBaseOperator, cosmos.operators.base.AbstractDbtBaseOperator
 [2024-11-27T14:54:46.157+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.dim_date_run, dagrun_id=scheduled__2024-09-15T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:54:45.603410+00:00, try_number=1, job_id=858, op_classpath=cosmos.operators.local.DbtRunLocalOperator, cosmos.operators.base.DbtRunMixin, cosmos.operators.local.DbtLocalBaseOperator, cosmos.operators.base.AbstractDbtBaseOperator
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:54:46.422+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 [[34m2024-11-27T14:54:47.407+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:54:47.775+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.dim_date_run scheduled__2024-09-16T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_daily_bars_run scheduled__2024-09-16T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run scheduled__2024-09-16T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:54:47.775+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 5/16 running and queued tasks[0m
 [[34m2024-11-27T14:54:47.776+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 6/16 running and queued tasks[0m
 [[34m2024-11-27T14:54:47.776+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 7/16 running and queued tasks[0m
 [[34m2024-11-27T14:54:47.776+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.dim_date_run scheduled__2024-09-16T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_daily_bars_run scheduled__2024-09-16T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run scheduled__2024-09-16T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:54:47.784+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.dim_date_run', run_id='scheduled__2024-09-16T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
 [[34m2024-11-27T14:54:47.784+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.dim_date_run', 'scheduled__2024-09-16T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:54:47.785+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.stg_daily_bars_run', run_id='scheduled__2024-09-16T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
 [[34m2024-11-27T14:54:47.785+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.stg_daily_bars_run', 'scheduled__2024-09-16T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:54:47.786+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run', run_id='scheduled__2024-09-16T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
 [[34m2024-11-27T14:54:47.786+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run', 'scheduled__2024-09-16T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:54:47.792+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='load_treasury_data_task', run_id='scheduled__2024-09-16T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:54:47.793+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run', 'scheduled__2024-09-16T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:54:47.798+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=load_treasury_data_task, run_id=scheduled__2024-09-16T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:54:45.364271+00:00, run_end_date=2024-11-27 14:54:46.810195+00:00, run_duration=1.445924, state=success, executor_state=success, try_number=1, max_tries=0, job_id=856, pool=default_pool, queue=default, priority_weight=5, operator=PythonOperator, queued_dttm=2024-11-27 14:54:35.089089+00:00, queued_by_job_id=803, pid=9040[0m
 [[34m2024-11-27T14:54:47.797+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.stg_daily_bars_run', 'scheduled__2024-09-16T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:54:47.801+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.dim_date_run', 'scheduled__2024-09-16T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:54:47.860+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:54:47.887+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:54:47.892+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:54:48.971+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:54:49.266+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T14:54:49.477+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:54:49.478+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:54:49.479+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:54:49.484+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:54:49.485+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:54:49.486+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:54:49.525+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:54:49.527+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:54:49.528+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:54:49.540+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:54:49.783+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 2 tasks up for execution:
 	<TaskInstance: capstone_andres_dag_.load_polygon_data_task scheduled__2024-09-18T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.load_treasury_data_task scheduled__2024-09-17T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:54:49.783+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 7/16 running and queued tasks[0m
 [[34m2024-11-27T14:54:49.783+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 8/16 running and queued tasks[0m
 [[34m2024-11-27T14:54:49.784+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
 	<TaskInstance: capstone_andres_dag_.load_polygon_data_task scheduled__2024-09-18T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.load_treasury_data_task scheduled__2024-09-17T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:54:49.788+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='load_polygon_data_task', run_id='scheduled__2024-09-18T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
 [[34m2024-11-27T14:54:49.788+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'load_polygon_data_task', 'scheduled__2024-09-18T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:54:49.789+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='load_treasury_data_task', run_id='scheduled__2024-09-17T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 5 and queue default[0m
 [[34m2024-11-27T14:54:49.789+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'load_treasury_data_task', 'scheduled__2024-09-17T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:54:49.796+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='load_polygon_data_task', run_id='scheduled__2024-09-17T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:54:49.802+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=load_polygon_data_task, run_id=scheduled__2024-09-17T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:54:40.931906+00:00, run_end_date=2024-11-27 14:54:48.568761+00:00, run_duration=7.636855, state=success, executor_state=success, try_number=1, max_tries=0, job_id=854, pool=default_pool, queue=default, priority_weight=6, operator=PythonOperator, queued_dttm=2024-11-27 14:54:30.560575+00:00, queued_by_job_id=803, pid=9006[0m
 [[34m2024-11-27T14:54:49.803+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'load_treasury_data_task', 'scheduled__2024-09-17T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:54:49.800+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'load_polygon_data_task', 'scheduled__2024-09-18T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:54:49.871+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:54:49.908+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:54:49.992+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412094914,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T14:54:50.007+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412094910,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T14:54:50.120+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412088134,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T14:54:50.578+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:54:50.730+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:54:50.858+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:54:50.985+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:54:51.100+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:54:51.101+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 Data loaded successfully
 [[34m2024-11-27T14:54:51.433+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:54:51.459+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 [[34m2024-11-27T14:54:51.481+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:54:51.482+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:54:51.483+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:54:51.515+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:54:51.516+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:54:51.516+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 [[34m2024-11-27T14:54:51.951+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412088142,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T14:54:51.994+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412094926,
 "os.name" : Linux
 [0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 [[34m2024-11-27T14:54:52.089+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 [[34m2024-11-27T14:54:52.223+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:54:52.404+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:54:52.440+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.4061567971948534[0m
 [[34m2024-11-27T14:54:52.440+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|9029]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:54:52.441+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:54:52.441+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:54:52.441+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:54:52.441+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|9029]: It took 1.01s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:54:52.444+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|9029]: It took 0.00244s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:54:52.487+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.weekly_metrics_run scheduled__2024-09-14T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:54:52.658+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:54:52.758+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 [[34m2024-11-27T14:54:53.161+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:54:53.173+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [2024-11-27T14:54:53.374+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.weekly_metrics_run, dagrun_id=scheduled__2024-09-14T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:54:52.590296+00:00, try_number=1, job_id=859, op_classpath=cosmos.operators.local.DbtRunLocalOperator, cosmos.operators.base.DbtRunMixin, cosmos.operators.local.DbtLocalBaseOperator, cosmos.operators.base.AbstractDbtBaseOperator
 [[34m2024-11-27T14:54:53.805+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:54:54.120+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T14:54:54.383+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 /usr/local/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked semaphore objects to clean up at shutdown
   warnings.warn('resource_tracker: There appear to be %d '
 /usr/local/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked semaphore objects to clean up at shutdown
   warnings.warn('resource_tracker: There appear to be %d '
 [[34m2024-11-27T14:54:54.969+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:54:55.213+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:54:55.253+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T14:54:55.421+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:54:55.516+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T14:54:55.522+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:54:55.553+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:54:55.669+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run', run_id='scheduled__2024-09-15T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:54:55.669+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.stg_daily_bars_run', run_id='scheduled__2024-09-15T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:54:55.672+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run, run_id=scheduled__2024-09-15T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:54:45.318373+00:00, run_end_date=2024-11-27 14:54:54.406065+00:00, run_duration=9.087692, state=success, executor_state=success, try_number=1, max_tries=0, job_id=855, pool=default_pool, queue=default, priority_weight=2, operator=DbtRunLocalOperator, queued_dttm=2024-11-27 14:54:35.089089+00:00, queued_by_job_id=803, pid=9038[0m
 [[34m2024-11-27T14:54:55.673+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.stg_daily_bars_run, run_id=scheduled__2024-09-15T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:54:45.468013+00:00, run_end_date=2024-11-27 14:54:54.671759+00:00, run_duration=9.203746, state=success, executor_state=success, try_number=1, max_tries=0, job_id=857, pool=default_pool, queue=default, priority_weight=2, operator=DbtRunLocalOperator, queued_dttm=2024-11-27 14:54:35.089089+00:00, queued_by_job_id=803, pid=9039[0m
 [[34m2024-11-27T14:54:55.677+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 [[34m2024-11-27T14:54:55.736+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 /usr/local/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked semaphore objects to clean up at shutdown
   warnings.warn('resource_tracker: There appear to be %d '
 Data loaded successfully
 [[34m2024-11-27T14:54:56.088+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:54:56.099+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:54:56.109+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:54:56.109+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 [[34m2024-11-27T14:54:56.384+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T14:54:56.393+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T14:54:56.634+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:54:56.695+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:54:56.708+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:54:56.770+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:54:56.860+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:54:56.880+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 [[34m2024-11-27T14:54:56.941+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.weekly_metrics_run scheduled__2024-09-15T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:54:56.942+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 6/16 running and queued tasks[0m
 [[34m2024-11-27T14:54:56.942+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.weekly_metrics_run scheduled__2024-09-15T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:54:56.943+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.weekly_metrics_run', run_id='scheduled__2024-09-15T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
 [[34m2024-11-27T14:54:56.944+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.weekly_metrics_run', 'scheduled__2024-09-15T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:54:56.946+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.dim_date_run', run_id='scheduled__2024-09-15T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:54:56.946+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.weekly_metrics_run', 'scheduled__2024-09-15T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:54:56.949+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.dim_date_run, run_id=scheduled__2024-09-15T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:54:45.603410+00:00, run_end_date=2024-11-27 14:54:55.792182+00:00, run_duration=10.188772, state=success, executor_state=success, try_number=1, max_tries=0, job_id=858, pool=default_pool, queue=default, priority_weight=2, operator=DbtRunLocalOperator, queued_dttm=2024-11-27 14:54:35.089089+00:00, queued_by_job_id=803, pid=9041[0m
 [[34m2024-11-27T14:54:56.986+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:54:57.017+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:54:57.154+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 [[34m2024-11-27T14:54:57.174+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.4989433940500021[0m
 [[34m2024-11-27T14:54:57.174+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|9052]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:54:57.175+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:54:57.175+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:54:57.175+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:54:57.175+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|9052]: It took 1.09s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:54:57.178+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|9052]: It took 0.00251s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:54:57.216+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.dim_date_run scheduled__2024-09-16T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 Data loaded successfully
 [[34m2024-11-27T14:54:57.455+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:54:57.469+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 Data loaded successfully
 [[34m2024-11-27T14:54:57.699+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:54:57.714+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 [2024-11-27T14:54:57.753+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.dim_date_run, dagrun_id=scheduled__2024-09-16T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:54:57.297877+00:00, try_number=1, job_id=860, op_classpath=cosmos.operators.local.DbtRunLocalOperator, cosmos.operators.base.DbtRunMixin, cosmos.operators.local.DbtLocalBaseOperator, cosmos.operators.base.AbstractDbtBaseOperator
 [[34m2024-11-27T14:54:57.861+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:54:57.862+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:54:57.923+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:54:57.924+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:54:57.924+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:54:58.001+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:54:58.018+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:54:58.108+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 [[34m2024-11-27T14:54:58.134+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 [[34m2024-11-27T14:54:58.373+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412088178,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T14:54:58.459+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.40873672789894044[0m
 [[34m2024-11-27T14:54:58.459+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|9050]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:54:58.460+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:54:58.460+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:54:58.460+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:54:58.460+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|9050]: It took 1.01s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:54:58.464+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|9050]: It took 0.00334s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:54:58.503+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run scheduled__2024-09-16T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 Data loaded successfully
 [[34m2024-11-27T14:54:58.639+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:54:58.656+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 Data loaded successfully
 [[34m2024-11-27T14:54:58.799+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:54:58.827+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 [[34m2024-11-27T14:54:59.052+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.7436001058667898[0m
 [[34m2024-11-27T14:54:59.053+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|9051]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:54:59.054+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:54:59.055+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:54:59.055+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:54:59.055+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|9051]: It took 1.36s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:54:59.063+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|9051]: It took 0.00778s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:54:59.142+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_daily_bars_run scheduled__2024-09-16T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [2024-11-27T14:54:59.306+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run, dagrun_id=scheduled__2024-09-16T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:54:58.596339+00:00, try_number=1, job_id=861, op_classpath=cosmos.operators.local.DbtRunLocalOperator, cosmos.operators.base.DbtRunMixin, cosmos.operators.local.DbtLocalBaseOperator, cosmos.operators.base.AbstractDbtBaseOperator
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:54:59.466+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 [2024-11-27T14:54:59.947+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.stg_daily_bars_run, dagrun_id=scheduled__2024-09-16T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:54:59.311937+00:00, try_number=1, job_id=862, op_classpath=cosmos.operators.local.DbtRunLocalOperator, cosmos.operators.base.DbtRunMixin, cosmos.operators.local.DbtLocalBaseOperator, cosmos.operators.base.AbstractDbtBaseOperator
 [[34m2024-11-27T14:54:59.979+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:55:00.058+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.6825256738811731[0m
 [[34m2024-11-27T14:55:00.058+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|9061]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:55:00.059+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:55:00.059+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:55:00.060+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:55:00.060+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|9061]: It took 1.42s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:55:00.066+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|9061]: It took 0.00621s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:55:00.130+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.load_treasury_data_task scheduled__2024-09-17T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 /usr/local/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked semaphore objects to clean up at shutdown
   warnings.warn('resource_tracker: There appear to be %d '
 [[34m2024-11-27T14:55:00.310+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.7132294790353626[0m
 [[34m2024-11-27T14:55:00.310+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|9062]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:55:00.311+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:55:00.312+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:55:00.312+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:55:00.312+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|9062]: It took 1.51s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:55:00.319+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|9062]: It took 0.00665s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:55:00.416+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.load_polygon_data_task scheduled__2024-09-18T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [2024-11-27T14:55:00.864+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=load_treasury_data_task, dagrun_id=scheduled__2024-09-17T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:55:00.256751+00:00, try_number=1, job_id=863, op_classpath=airflow.operators.python.PythonOperator
 [2024-11-27T14:55:01.090+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=load_polygon_data_task, dagrun_id=scheduled__2024-09-18T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:55:00.641727+00:00, try_number=1, job_id=864, op_classpath=airflow.operators.python.PythonOperator
 [[34m2024-11-27T14:55:01.173+0000[0m] {[34mdagrun.py:[0m851} INFO[0m - Marking run <DagRun capstone_andres_dag_ @ 2024-09-14 01:00:00+00:00: scheduled__2024-09-14T01:00:00+00:00, state:running, queued_at: 2024-11-27 14:52:01.701423+00:00. externally triggered: False> successful[0m
 [[34m2024-11-27T14:55:01.174+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=capstone_andres_dag_, execution_date=2024-09-14 01:00:00+00:00, run_id=scheduled__2024-09-14T01:00:00+00:00, run_start_date=2024-11-27 14:52:01.720089+00:00, run_end_date=2024-11-27 14:55:01.173814+00:00, run_duration=179.453725, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-09-14 01:00:00+00:00, data_interval_end=2024-09-15 01:00:00+00:00, dag_hash=77fa8f64adac9391cccaaae88e2be5d8[0m
 [[34m2024-11-27T14:55:01.178+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-09-15 01:00:00+00:00, run_after=2024-09-16 01:00:00+00:00[0m
 [[34m2024-11-27T14:55:01.342+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.weekly_metrics_run', run_id='scheduled__2024-09-14T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:55:01.346+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.weekly_metrics_run, run_id=scheduled__2024-09-14T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:54:52.590296+00:00, run_end_date=2024-11-27 14:55:00.182735+00:00, run_duration=7.592439, state=success, executor_state=success, try_number=1, max_tries=0, job_id=859, pool=default_pool, queue=default, priority_weight=1, operator=DbtRunLocalOperator, queued_dttm=2024-11-27 14:54:43.670822+00:00, queued_by_job_id=803, pid=9099[0m
 [[34m2024-11-27T14:55:02.343+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-09-16 01:00:00+00:00, run_after=2024-09-17 01:00:00+00:00[0m
 [[34m2024-11-27T14:55:02.620+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections'[0m
 [[34m2024-11-27T14:55:02.620+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.dim_date'[0m
 [[34m2024-11-27T14:55:02.620+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_payments'[0m
 [[34m2024-11-27T14:55:02.621+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_orders'[0m
 [[34m2024-11-27T14:55:02.621+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_daily_bars'[0m
 [[34m2024-11-27T14:55:02.918+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:55:03.260+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T14:55:03.296+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-09-17 01:00:00+00:00, run_after=2024-09-18 01:00:00+00:00[0m
 [[34m2024-11-27T14:55:03.492+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run scheduled__2024-09-17T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.dim_date_run scheduled__2024-09-17T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_daily_bars_run scheduled__2024-09-17T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:55:03.492+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 5/16 running and queued tasks[0m
 [[34m2024-11-27T14:55:03.492+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 6/16 running and queued tasks[0m
 [[34m2024-11-27T14:55:03.492+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 7/16 running and queued tasks[0m
 [[34m2024-11-27T14:55:03.493+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run scheduled__2024-09-17T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.dim_date_run scheduled__2024-09-17T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_daily_bars_run scheduled__2024-09-17T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:55:03.495+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run', run_id='scheduled__2024-09-17T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
 [[34m2024-11-27T14:55:03.495+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run', 'scheduled__2024-09-17T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:55:03.495+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.dim_date_run', run_id='scheduled__2024-09-17T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
 [[34m2024-11-27T14:55:03.495+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.dim_date_run', 'scheduled__2024-09-17T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:55:03.495+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.stg_daily_bars_run', run_id='scheduled__2024-09-17T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
 [[34m2024-11-27T14:55:03.496+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.stg_daily_bars_run', 'scheduled__2024-09-17T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:55:03.499+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.dim_date_run', 'scheduled__2024-09-17T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:55:03.499+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='load_treasury_data_task', run_id='scheduled__2024-09-17T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:55:03.499+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.stg_daily_bars_run', 'scheduled__2024-09-17T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:55:03.499+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run', 'scheduled__2024-09-17T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:55:03.505+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=load_treasury_data_task, run_id=scheduled__2024-09-17T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:55:00.256751+00:00, run_end_date=2024-11-27 14:55:03.091128+00:00, run_duration=2.834377, state=success, executor_state=success, try_number=1, max_tries=0, job_id=863, pool=default_pool, queue=default, priority_weight=5, operator=PythonOperator, queued_dttm=2024-11-27 14:54:49.785500+00:00, queued_by_job_id=803, pid=9179[0m
 [[34m2024-11-27T14:55:03.533+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:55:03.533+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:55:03.545+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-09-18 01:00:00+00:00, run_after=2024-09-19 01:00:00+00:00[0m
 [[34m2024-11-27T14:55:03.553+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:55:03.623+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:55:04.687+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:55:04.811+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:55:04.812+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:55:04.812+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:55:04.813+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:55:04.814+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:55:04.814+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:55:04.830+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:55:04.831+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:55:04.831+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:55:04.843+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:55:04.880+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-09-19 01:00:00+00:00, run_after=2024-09-20 01:00:00+00:00[0m
 [[34m2024-11-27T14:55:04.962+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 [[34m2024-11-27T14:55:05.255+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412094954,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T14:55:05.269+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412094958,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T14:55:05.303+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412094962,
 "os.name" : Linux
 [0m
 Data loaded successfully
 [[34m2024-11-27T14:55:05.567+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:55:05.579+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:55:06.101+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:55:06.103+0000[0m] {[34mscheduler_job_runner.py:[0m1328} INFO[0m - DAG capstone_andres_dag_ is at (or above) max_active_runs (16 of 16), not creating any more runs[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:55:06.126+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:55:06.205+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 /usr/local/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked semaphore objects to clean up at shutdown
   warnings.warn('resource_tracker: There appear to be %d '
 /usr/local/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked semaphore objects to clean up at shutdown
   warnings.warn('resource_tracker: There appear to be %d '
 /usr/local/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked semaphore objects to clean up at shutdown
   warnings.warn('resource_tracker: There appear to be %d '
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 [[34m2024-11-27T14:55:06.599+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.4862980009056628[0m
 [[34m2024-11-27T14:55:06.600+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|9155]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:55:06.600+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:55:06.600+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:55:06.601+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:55:06.601+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|9155]: It took 1.03s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 [[34m2024-11-27T14:55:06.603+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|9155]: It took 0.00219s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:55:06.608+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:55:06.638+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.weekly_metrics_run scheduled__2024-09-15T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 [[34m2024-11-27T14:55:06.754+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:55:06.967+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [2024-11-27T14:55:07.392+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.weekly_metrics_run, dagrun_id=scheduled__2024-09-15T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:55:06.715927+00:00, try_number=1, job_id=865, op_classpath=cosmos.operators.local.DbtRunLocalOperator, cosmos.operators.base.DbtRunMixin, cosmos.operators.local.DbtLocalBaseOperator, cosmos.operators.base.AbstractDbtBaseOperator
 [[34m2024-11-27T14:55:07.651+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.weekly_metrics_run scheduled__2024-09-16T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:55:07.651+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 5/16 running and queued tasks[0m
 [[34m2024-11-27T14:55:07.652+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.weekly_metrics_run scheduled__2024-09-16T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:55:07.653+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.weekly_metrics_run', run_id='scheduled__2024-09-16T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
 [[34m2024-11-27T14:55:07.654+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.weekly_metrics_run', 'scheduled__2024-09-16T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:55:07.656+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.weekly_metrics_run', 'scheduled__2024-09-16T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:55:07.657+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.dim_date_run', run_id='scheduled__2024-09-16T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:55:07.658+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run', run_id='scheduled__2024-09-16T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:55:07.658+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.stg_daily_bars_run', run_id='scheduled__2024-09-16T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:55:07.662+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run, run_id=scheduled__2024-09-16T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:54:58.596339+00:00, run_end_date=2024-11-27 14:55:06.160375+00:00, run_duration=7.564036, state=success, executor_state=success, try_number=1, max_tries=0, job_id=861, pool=default_pool, queue=default, priority_weight=2, operator=DbtRunLocalOperator, queued_dttm=2024-11-27 14:54:47.777628+00:00, queued_by_job_id=803, pid=9165[0m
 [[34m2024-11-27T14:55:07.662+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.stg_daily_bars_run, run_id=scheduled__2024-09-16T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:54:59.311937+00:00, run_end_date=2024-11-27 14:55:06.358013+00:00, run_duration=7.046076, state=success, executor_state=success, try_number=1, max_tries=0, job_id=862, pool=default_pool, queue=default, priority_weight=2, operator=DbtRunLocalOperator, queued_dttm=2024-11-27 14:54:47.777628+00:00, queued_by_job_id=803, pid=9169[0m
 [[34m2024-11-27T14:55:07.663+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.dim_date_run, run_id=scheduled__2024-09-16T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:54:57.297877+00:00, run_end_date=2024-11-27 14:55:06.309100+00:00, run_duration=9.011223, state=success, executor_state=success, try_number=1, max_tries=0, job_id=860, pool=default_pool, queue=default, priority_weight=2, operator=DbtRunLocalOperator, queued_dttm=2024-11-27 14:54:47.777628+00:00, queued_by_job_id=803, pid=9159[0m
 [[34m2024-11-27T14:55:07.686+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:55:08.176+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:55:08.514+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T14:55:08.528+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:55:08.529+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:55:08.530+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:55:08.731+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 2 tasks up for execution:
 	<TaskInstance: capstone_andres_dag_.load_polygon_data_task scheduled__2024-09-19T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.load_treasury_data_task scheduled__2024-09-18T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:55:08.731+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 5/16 running and queued tasks[0m
 [[34m2024-11-27T14:55:08.731+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 6/16 running and queued tasks[0m
 [[34m2024-11-27T14:55:08.731+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
 	<TaskInstance: capstone_andres_dag_.load_polygon_data_task scheduled__2024-09-19T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.load_treasury_data_task scheduled__2024-09-18T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:55:08.733+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='load_polygon_data_task', run_id='scheduled__2024-09-19T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
 [[34m2024-11-27T14:55:08.734+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'load_polygon_data_task', 'scheduled__2024-09-19T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:55:08.734+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='load_treasury_data_task', run_id='scheduled__2024-09-18T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 5 and queue default[0m
 [[34m2024-11-27T14:55:08.734+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'load_treasury_data_task', 'scheduled__2024-09-18T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:55:08.736+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'load_polygon_data_task', 'scheduled__2024-09-19T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:55:08.736+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'load_treasury_data_task', 'scheduled__2024-09-18T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:55:08.737+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='load_polygon_data_task', run_id='scheduled__2024-09-18T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:55:08.741+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=load_polygon_data_task, run_id=scheduled__2024-09-18T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:55:00.641727+00:00, run_end_date=2024-11-27 14:55:08.109285+00:00, run_duration=7.467558, state=success, executor_state=success, try_number=1, max_tries=0, job_id=864, pool=default_pool, queue=default, priority_weight=6, operator=PythonOperator, queued_dttm=2024-11-27 14:54:49.785500+00:00, queued_by_job_id=803, pid=9187[0m
 [[34m2024-11-27T14:55:08.776+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:55:08.777+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:55:08.817+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:55:09.007+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412088218,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T14:55:09.299+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:55:09.595+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T14:55:09.645+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:55:09.646+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:55:09.646+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:55:09.647+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:55:09.648+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:55:09.648+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:55:09.680+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:55:09.730+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:55:09.820+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:55:09.890+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:55:09.975+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T14:55:10.035+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 [[34m2024-11-27T14:55:10.147+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 [[34m2024-11-27T14:55:10.159+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412088222,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T14:55:10.162+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412094966,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T14:55:10.218+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:55:10.251+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 Data loaded successfully
 [[34m2024-11-27T14:55:10.633+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:55:10.646+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 [[34m2024-11-27T14:55:10.869+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:55:10.933+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:55:10.948+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:55:11.017+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:55:11.139+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 [[34m2024-11-27T14:55:11.421+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 Data loaded successfully
 [[34m2024-11-27T14:55:11.596+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:55:11.598+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:55:11.603+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.3880928489379585[0m
 [[34m2024-11-27T14:55:11.603+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|9219]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:55:11.603+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:55:11.604+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:55:11.604+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:55:11.604+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|9219]: It took 0.971s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:55:11.606+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|9219]: It took 0.0022s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:55:11.616+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 [[34m2024-11-27T14:55:11.628+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:55:11.656+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run scheduled__2024-09-17T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [[34m2024-11-27T14:55:11.707+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 [[34m2024-11-27T14:55:11.797+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [2024-11-27T14:55:12.198+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run, dagrun_id=scheduled__2024-09-17T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:55:11.740701+00:00, try_number=1, job_id=866, op_classpath=cosmos.operators.local.DbtRunLocalOperator, cosmos.operators.base.DbtRunMixin, cosmos.operators.local.DbtLocalBaseOperator, cosmos.operators.base.AbstractDbtBaseOperator
 Data loaded successfully
 [[34m2024-11-27T14:55:12.394+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:55:12.424+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 [[34m2024-11-27T14:55:12.659+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.5117170079611242[0m
 [[34m2024-11-27T14:55:12.659+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|9217]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:55:12.660+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:55:12.660+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:55:12.660+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:55:12.660+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|9217]: It took 1.07s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:55:12.663+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|9217]: It took 0.00223s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:55:12.698+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.dim_date_run scheduled__2024-09-17T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [[34m2024-11-27T14:55:12.982+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [2024-11-27T14:55:13.236+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.dim_date_run, dagrun_id=scheduled__2024-09-17T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:55:12.773122+00:00, try_number=1, job_id=867, op_classpath=cosmos.operators.local.DbtRunLocalOperator, cosmos.operators.base.DbtRunMixin, cosmos.operators.local.DbtLocalBaseOperator, cosmos.operators.base.AbstractDbtBaseOperator
 [[34m2024-11-27T14:55:13.306+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T14:55:13.501+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.49238258111290634[0m
 [[34m2024-11-27T14:55:13.502+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|9218]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:55:13.502+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:55:13.502+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:55:13.503+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:55:13.503+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|9218]: It took 1.11s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:55:13.505+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|9218]: It took 0.00242s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:55:13.559+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_daily_bars_run scheduled__2024-09-17T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 /usr/local/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked semaphore objects to clean up at shutdown
   warnings.warn('resource_tracker: There appear to be %d '
 [[34m2024-11-27T14:55:13.698+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:55:13.781+0000[0m] {[34mdagrun.py:[0m851} INFO[0m - Marking run <DagRun capstone_andres_dag_ @ 2024-09-15 01:00:00+00:00: scheduled__2024-09-15T01:00:00+00:00, state:running, queued_at: 2024-11-27 14:52:01.848711+00:00. externally triggered: False> successful[0m
 [[34m2024-11-27T14:55:13.781+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=capstone_andres_dag_, execution_date=2024-09-15 01:00:00+00:00, run_id=scheduled__2024-09-15T01:00:00+00:00, run_start_date=2024-11-27 14:52:01.861646+00:00, run_end_date=2024-11-27 14:55:13.781326+00:00, run_duration=191.91968, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-09-15 01:00:00+00:00, data_interval_end=2024-09-16 01:00:00+00:00, dag_hash=77fa8f64adac9391cccaaae88e2be5d8[0m
 [[34m2024-11-27T14:55:13.784+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-09-16 01:00:00+00:00, run_after=2024-09-17 01:00:00+00:00[0m
 [[34m2024-11-27T14:55:13.986+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.weekly_metrics_run', run_id='scheduled__2024-09-15T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:55:13.991+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.weekly_metrics_run, run_id=scheduled__2024-09-15T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:55:06.715927+00:00, run_end_date=2024-11-27 14:55:13.620169+00:00, run_duration=6.904242, state=success, executor_state=success, try_number=1, max_tries=0, job_id=865, pool=default_pool, queue=default, priority_weight=1, operator=DbtRunLocalOperator, queued_dttm=2024-11-27 14:54:56.942668+00:00, queued_by_job_id=803, pid=9254[0m
 [2024-11-27T14:55:14.096+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.stg_daily_bars_run, dagrun_id=scheduled__2024-09-17T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:55:13.672382+00:00, try_number=1, job_id=868, op_classpath=cosmos.operators.local.DbtRunLocalOperator, cosmos.operators.base.DbtRunMixin, cosmos.operators.local.DbtLocalBaseOperator, cosmos.operators.base.AbstractDbtBaseOperator
 [[34m2024-11-27T14:55:14.216+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-09-17 01:00:00+00:00, run_after=2024-09-18 01:00:00+00:00[0m
 [[34m2024-11-27T14:55:14.308+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:55:14.587+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:55:14.591+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T14:55:14.824+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:55:14.872+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:55:14.902+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T14:55:15.079+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:55:15.133+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:55:15.181+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 [[34m2024-11-27T14:55:15.489+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-09-18 01:00:00+00:00, run_after=2024-09-19 01:00:00+00:00[0m
 Data loaded successfully
 [[34m2024-11-27T14:55:15.778+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:55:15.792+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 [[34m2024-11-27T14:55:15.947+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:55:16.111+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:55:16.147+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:55:16.223+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 [[34m2024-11-27T14:55:16.300+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:55:16.403+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 [[34m2024-11-27T14:55:16.709+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-09-19 01:00:00+00:00, run_after=2024-09-20 01:00:00+00:00[0m
 [[34m2024-11-27T14:55:16.743+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.42950264597311616[0m
 [[34m2024-11-27T14:55:16.744+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|9284]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:55:16.744+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:55:16.745+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:55:16.745+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:55:16.745+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|9284]: It took 0.967s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:55:16.748+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|9284]: It took 0.00274s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:55:16.790+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.weekly_metrics_run scheduled__2024-09-16T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 Data loaded successfully
 [[34m2024-11-27T14:55:16.825+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:55:16.839+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 Data loaded successfully
 [[34m2024-11-27T14:55:16.897+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:55:16.921+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 [2024-11-27T14:55:17.389+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.weekly_metrics_run, dagrun_id=scheduled__2024-09-16T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:55:16.905515+00:00, try_number=1, job_id=869, op_classpath=cosmos.operators.local.DbtRunLocalOperator, cosmos.operators.base.DbtRunMixin, cosmos.operators.local.DbtLocalBaseOperator, cosmos.operators.base.AbstractDbtBaseOperator
 [[34m2024-11-27T14:55:17.784+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.38037351006641984[0m
 [[34m2024-11-27T14:55:17.784+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|9290]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:55:17.785+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:55:17.785+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:55:17.785+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:55:17.785+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|9290]: It took 0.961s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:55:17.787+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|9290]: It took 0.00217s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:55:17.828+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.load_treasury_data_task scheduled__2024-09-18T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [[34m2024-11-27T14:55:17.979+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.4766253719571978[0m
 [[34m2024-11-27T14:55:17.979+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|9289]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:55:17.980+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:55:17.980+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:55:17.981+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:55:17.981+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|9289]: It took 1.08s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:55:17.984+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|9289]: It took 0.00318s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:55:18.029+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-09-20 01:00:00+00:00, run_after=2024-09-21 01:00:00+00:00[0m
 [[34m2024-11-27T14:55:18.038+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.load_polygon_data_task scheduled__2024-09-19T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [2024-11-27T14:55:18.347+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=load_treasury_data_task, dagrun_id=scheduled__2024-09-18T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:55:17.934904+00:00, try_number=1, job_id=870, op_classpath=airflow.operators.python.PythonOperator
 /usr/local/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked semaphore objects to clean up at shutdown
   warnings.warn('resource_tracker: There appear to be %d '
 [2024-11-27T14:55:18.533+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=load_polygon_data_task, dagrun_id=scheduled__2024-09-19T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:55:18.148648+00:00, try_number=1, job_id=871, op_classpath=airflow.operators.python.PythonOperator
 [[34m2024-11-27T14:55:19.023+0000[0m] {[34mscheduler_job_runner.py:[0m1328} INFO[0m - DAG capstone_andres_dag_ is at (or above) max_active_runs (16 of 16), not creating any more runs[0m
 [[34m2024-11-27T14:55:19.207+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run', run_id='scheduled__2024-09-17T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:55:19.210+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run, run_id=scheduled__2024-09-17T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:55:11.740701+00:00, run_end_date=2024-11-27 14:55:18.390087+00:00, run_duration=6.649386, state=success, executor_state=success, try_number=1, max_tries=0, job_id=866, pool=default_pool, queue=default, priority_weight=2, operator=DbtRunLocalOperator, queued_dttm=2024-11-27 14:55:03.493495+00:00, queued_by_job_id=803, pid=9315[0m
 /usr/local/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked semaphore objects to clean up at shutdown
   warnings.warn('resource_tracker: There appear to be %d '
 /usr/local/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked semaphore objects to clean up at shutdown
   warnings.warn('resource_tracker: There appear to be %d '
 [[34m2024-11-27T14:55:20.677+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.weekly_metrics_run scheduled__2024-09-17T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:55:20.678+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 3/16 running and queued tasks[0m
 [[34m2024-11-27T14:55:20.678+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.weekly_metrics_run scheduled__2024-09-17T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:55:20.680+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.weekly_metrics_run', run_id='scheduled__2024-09-17T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
 [[34m2024-11-27T14:55:20.680+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.weekly_metrics_run', 'scheduled__2024-09-17T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:55:20.683+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.weekly_metrics_run', 'scheduled__2024-09-17T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:55:20.684+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.stg_daily_bars_run', run_id='scheduled__2024-09-17T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:55:20.684+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.dim_date_run', run_id='scheduled__2024-09-17T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:55:20.689+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.dim_date_run, run_id=scheduled__2024-09-17T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:55:12.773122+00:00, run_end_date=2024-11-27 14:55:20.226490+00:00, run_duration=7.453368, state=success, executor_state=success, try_number=1, max_tries=0, job_id=867, pool=default_pool, queue=default, priority_weight=2, operator=DbtRunLocalOperator, queued_dttm=2024-11-27 14:55:03.493495+00:00, queued_by_job_id=803, pid=9330[0m
 [[34m2024-11-27T14:55:20.689+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.stg_daily_bars_run, run_id=scheduled__2024-09-17T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:55:13.672382+00:00, run_end_date=2024-11-27 14:55:19.874723+00:00, run_duration=6.202341, state=success, executor_state=success, try_number=1, max_tries=0, job_id=868, pool=default_pool, queue=default, priority_weight=2, operator=DbtRunLocalOperator, queued_dttm=2024-11-27 14:55:03.493495+00:00, queued_by_job_id=803, pid=9341[0m
 [[34m2024-11-27T14:55:20.714+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:55:21.005+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run scheduled__2024-09-18T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.dim_date_run scheduled__2024-09-18T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_daily_bars_run scheduled__2024-09-18T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:55:21.006+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 3/16 running and queued tasks[0m
 [[34m2024-11-27T14:55:21.006+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 4/16 running and queued tasks[0m
 [[34m2024-11-27T14:55:21.007+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 5/16 running and queued tasks[0m
 [[34m2024-11-27T14:55:21.007+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run scheduled__2024-09-18T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.dim_date_run scheduled__2024-09-18T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_daily_bars_run scheduled__2024-09-18T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:55:21.010+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run', run_id='scheduled__2024-09-18T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
 [[34m2024-11-27T14:55:21.010+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run', 'scheduled__2024-09-18T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:55:21.010+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.dim_date_run', run_id='scheduled__2024-09-18T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
 [[34m2024-11-27T14:55:21.010+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.dim_date_run', 'scheduled__2024-09-18T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:55:21.011+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.stg_daily_bars_run', run_id='scheduled__2024-09-18T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
 [[34m2024-11-27T14:55:21.011+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.stg_daily_bars_run', 'scheduled__2024-09-18T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:55:21.014+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run', 'scheduled__2024-09-18T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:55:21.014+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.dim_date_run', 'scheduled__2024-09-18T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:55:21.016+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='load_treasury_data_task', run_id='scheduled__2024-09-18T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:55:21.023+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=load_treasury_data_task, run_id=scheduled__2024-09-18T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:55:17.934904+00:00, run_end_date=2024-11-27 14:55:20.483635+00:00, run_duration=2.548731, state=success, executor_state=success, try_number=1, max_tries=0, job_id=870, pool=default_pool, queue=default, priority_weight=5, operator=PythonOperator, queued_dttm=2024-11-27 14:55:08.732461+00:00, queued_by_job_id=803, pid=9372[0m
 [[34m2024-11-27T14:55:21.019+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.stg_daily_bars_run', 'scheduled__2024-09-18T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:55:21.070+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:55:21.072+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:55:21.107+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:55:22.171+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:55:22.171+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:55:22.171+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:55:22.415+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:55:22.416+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:55:22.416+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:55:22.427+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:55:22.428+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:55:22.428+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:55:22.454+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:55:22.455+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:55:22.455+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:55:22.665+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412095002,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T14:55:22.865+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412088266,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T14:55:22.867+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412095006,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T14:55:22.892+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412088270,
 "os.name" : Linux
 [0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:55:23.526+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:55:23.556+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:55:23.556+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:55:23.670+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 /usr/local/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked semaphore objects to clean up at shutdown
   warnings.warn('resource_tracker: There appear to be %d '
 [[34m2024-11-27T14:55:23.874+0000[0m] {[34mdagrun.py:[0m851} INFO[0m - Marking run <DagRun capstone_andres_dag_ @ 2024-09-16 01:00:00+00:00: scheduled__2024-09-16T01:00:00+00:00, state:running, queued_at: 2024-11-27 14:52:02.081162+00:00. externally triggered: False> successful[0m
 [[34m2024-11-27T14:55:23.875+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=capstone_andres_dag_, execution_date=2024-09-16 01:00:00+00:00, run_id=scheduled__2024-09-16T01:00:00+00:00, run_start_date=2024-11-27 14:52:02.095000+00:00, run_end_date=2024-11-27 14:55:23.874997+00:00, run_duration=201.779997, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-09-16 01:00:00+00:00, data_interval_end=2024-09-17 01:00:00+00:00, dag_hash=77fa8f64adac9391cccaaae88e2be5d8[0m
 [[34m2024-11-27T14:55:23.878+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-09-17 01:00:00+00:00, run_after=2024-09-18 01:00:00+00:00[0m
 [[34m2024-11-27T14:55:24.023+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.weekly_metrics_run', run_id='scheduled__2024-09-16T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:55:24.026+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.weekly_metrics_run, run_id=scheduled__2024-09-16T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:55:16.905515+00:00, run_end_date=2024-11-27 14:55:23.722518+00:00, run_duration=6.817003, state=success, executor_state=success, try_number=1, max_tries=0, job_id=869, pool=default_pool, queue=default, priority_weight=1, operator=DbtRunLocalOperator, queued_dttm=2024-11-27 14:55:07.652420+00:00, queued_by_job_id=803, pid=9364[0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 [[34m2024-11-27T14:55:24.225+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:55:24.239+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-09-18 01:00:00+00:00, run_after=2024-09-19 01:00:00+00:00[0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 [[34m2024-11-27T14:55:24.378+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 [[34m2024-11-27T14:55:24.561+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:55:24.763+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:55:25.449+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-09-19 01:00:00+00:00, run_after=2024-09-20 01:00:00+00:00[0m
 [[34m2024-11-27T14:55:26.693+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-09-20 01:00:00+00:00, run_after=2024-09-21 01:00:00+00:00[0m
 [[34m2024-11-27T14:55:26.868+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:55:26.873+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 2 tasks up for execution:
 	<TaskInstance: capstone_andres_dag_.load_polygon_data_task scheduled__2024-09-20T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.load_treasury_data_task scheduled__2024-09-19T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:55:26.873+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 4/16 running and queued tasks[0m
 [[34m2024-11-27T14:55:26.873+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 5/16 running and queued tasks[0m
 [[34m2024-11-27T14:55:26.873+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
 	<TaskInstance: capstone_andres_dag_.load_polygon_data_task scheduled__2024-09-20T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.load_treasury_data_task scheduled__2024-09-19T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:55:26.875+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='load_polygon_data_task', run_id='scheduled__2024-09-20T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
 [[34m2024-11-27T14:55:26.875+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'load_polygon_data_task', 'scheduled__2024-09-20T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:55:26.876+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='load_treasury_data_task', run_id='scheduled__2024-09-19T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 5 and queue default[0m
 [[34m2024-11-27T14:55:26.876+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'load_treasury_data_task', 'scheduled__2024-09-19T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:55:26.878+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='load_polygon_data_task', run_id='scheduled__2024-09-19T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:55:26.878+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'load_polygon_data_task', 'scheduled__2024-09-20T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:55:26.878+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'load_treasury_data_task', 'scheduled__2024-09-19T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:55:26.883+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=load_polygon_data_task, run_id=scheduled__2024-09-19T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:55:18.148648+00:00, run_end_date=2024-11-27 14:55:26.289438+00:00, run_duration=8.14079, state=success, executor_state=success, try_number=1, max_tries=0, job_id=871, pool=default_pool, queue=default, priority_weight=6, operator=PythonOperator, queued_dttm=2024-11-27 14:55:08.732461+00:00, queued_by_job_id=803, pid=9373[0m
 [[34m2024-11-27T14:55:26.904+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-09-21 01:00:00+00:00, run_after=2024-09-22 01:00:00+00:00[0m
 [[34m2024-11-27T14:55:26.909+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:55:26.910+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:55:27.090+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:55:27.179+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T14:55:27.186+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:55:27.387+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:55:27.387+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T14:55:27.404+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:55:27.497+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T14:55:27.679+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:55:27.704+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T14:55:27.732+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:55:27.799+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:55:27.799+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:55:27.800+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:55:27.801+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:55:27.801+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:55:27.802+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:55:27.977+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:55:28.116+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-09-22 01:00:00+00:00, run_after=2024-09-23 01:00:00+00:00[0m
 [[34m2024-11-27T14:55:28.241+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412088274,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T14:55:28.290+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412095014,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T14:55:28.355+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:55:28.525+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:55:28.637+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 [[34m2024-11-27T14:55:28.735+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:55:28.754+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:55:28.913+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:55:28.923+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:55:28.948+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:55:29.018+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 [[34m2024-11-27T14:55:29.025+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:55:29.036+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:55:29.139+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 [[34m2024-11-27T14:55:29.307+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 [[34m2024-11-27T14:55:29.330+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:55:29.331+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-09-23 01:00:00+00:00, run_after=2024-09-24 01:00:00+00:00[0m
 [[34m2024-11-27T14:55:29.408+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 Data loaded successfully
 [[34m2024-11-27T14:55:29.496+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 Data loaded successfully
 [[34m2024-11-27T14:55:29.507+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:55:29.511+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:55:29.519+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 Data loaded successfully
 [[34m2024-11-27T14:55:29.529+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:55:29.539+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 [[34m2024-11-27T14:55:29.553+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 Data loaded successfully
 [[34m2024-11-27T14:55:30.101+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:55:30.116+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 [[34m2024-11-27T14:55:30.458+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.3589901861269027[0m
 [[34m2024-11-27T14:55:30.458+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|9393]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:55:30.459+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:55:30.459+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:55:30.459+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:55:30.460+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|9393]: It took 0.93s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:55:30.463+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|9393]: It took 0.00331s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:55:30.482+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.34979269886389375[0m
 [[34m2024-11-27T14:55:30.483+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|9392]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:55:30.483+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:55:30.483+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:55:30.484+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:55:30.484+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|9392]: It took 0.977s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:55:30.487+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|9392]: It took 0.00336s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:55:30.500+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run scheduled__2024-09-18T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [[34m2024-11-27T14:55:30.532+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.dim_date_run scheduled__2024-09-18T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [[34m2024-11-27T14:55:30.568+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-09-24 01:00:00+00:00, run_after=2024-09-25 01:00:00+00:00[0m
 [[34m2024-11-27T14:55:30.692+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.39960021199658513[0m
 [[34m2024-11-27T14:55:30.692+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|9384]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:55:30.693+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:55:30.693+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:55:30.694+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:55:30.694+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|9384]: It took 1.2s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:55:30.698+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|9384]: It took 0.00382s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:55:30.750+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.weekly_metrics_run scheduled__2024-09-17T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [2024-11-27T14:55:31.045+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run, dagrun_id=scheduled__2024-09-18T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:55:30.574710+00:00, try_number=1, job_id=872, op_classpath=cosmos.operators.local.DbtRunLocalOperator, cosmos.operators.base.DbtRunMixin, cosmos.operators.local.DbtLocalBaseOperator, cosmos.operators.base.AbstractDbtBaseOperator
 [2024-11-27T14:55:31.102+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.dim_date_run, dagrun_id=scheduled__2024-09-18T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:55:30.612343+00:00, try_number=1, job_id=873, op_classpath=cosmos.operators.local.DbtRunLocalOperator, cosmos.operators.base.DbtRunMixin, cosmos.operators.local.DbtLocalBaseOperator, cosmos.operators.base.AbstractDbtBaseOperator
 [[34m2024-11-27T14:55:31.288+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.6485109198838472[0m
 [[34m2024-11-27T14:55:31.288+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|9394]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:55:31.289+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:55:31.289+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:55:31.289+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:55:31.289+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|9394]: It took 1.19s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:55:31.292+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|9394]: It took 0.00291s to build the Airflow DAG.[0m
 [2024-11-27T14:55:31.326+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.weekly_metrics_run, dagrun_id=scheduled__2024-09-17T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:55:30.849159+00:00, try_number=1, job_id=874, op_classpath=cosmos.operators.local.DbtRunLocalOperator, cosmos.operators.base.DbtRunMixin, cosmos.operators.local.DbtLocalBaseOperator, cosmos.operators.base.AbstractDbtBaseOperator
 [[34m2024-11-27T14:55:31.333+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_daily_bars_run scheduled__2024-09-18T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [[34m2024-11-27T14:55:31.854+0000[0m] {[34mscheduler_job_runner.py:[0m1328} INFO[0m - DAG capstone_andres_dag_ is at (or above) max_active_runs (16 of 16), not creating any more runs[0m
 [2024-11-27T14:55:31.923+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.stg_daily_bars_run, dagrun_id=scheduled__2024-09-18T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:55:31.414782+00:00, try_number=1, job_id=875, op_classpath=cosmos.operators.local.DbtRunLocalOperator, cosmos.operators.base.DbtRunMixin, cosmos.operators.local.DbtLocalBaseOperator, cosmos.operators.base.AbstractDbtBaseOperator
 [[34m2024-11-27T14:55:32.232+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:55:32.336+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:55:32.559+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T14:55:32.673+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T14:55:32.808+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:55:32.993+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:55:33.962+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:55:34.081+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:55:34.129+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:55:34.235+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:55:34.245+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 [[34m2024-11-27T14:55:34.346+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 Data loaded successfully
 [[34m2024-11-27T14:55:34.904+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:55:34.939+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 Data loaded successfully
 [[34m2024-11-27T14:55:34.959+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:55:34.988+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 [[34m2024-11-27T14:55:36.293+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.6825285512022674[0m
 [[34m2024-11-27T14:55:36.293+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|9448]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:55:36.294+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:55:36.294+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:55:36.294+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:55:36.294+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|9448]: It took 1.39s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:55:36.299+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|9448]: It took 0.00412s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:55:36.354+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.load_treasury_data_task scheduled__2024-09-19T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [[34m2024-11-27T14:55:36.389+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.7077869400382042[0m
 [[34m2024-11-27T14:55:36.390+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|9449]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:55:36.391+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:55:36.391+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:55:36.391+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:55:36.392+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|9449]: It took 1.43s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:55:36.395+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|9449]: It took 0.00382s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:55:36.463+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.load_polygon_data_task scheduled__2024-09-20T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [2024-11-27T14:55:37.041+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=load_treasury_data_task, dagrun_id=scheduled__2024-09-19T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:55:36.483771+00:00, try_number=1, job_id=876, op_classpath=airflow.operators.python.PythonOperator
 [2024-11-27T14:55:37.062+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=load_polygon_data_task, dagrun_id=scheduled__2024-09-20T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:55:36.635565+00:00, try_number=1, job_id=877, op_classpath=airflow.operators.python.PythonOperator
 /usr/local/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked semaphore objects to clean up at shutdown
   warnings.warn('resource_tracker: There appear to be %d '
 /usr/local/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked semaphore objects to clean up at shutdown
   warnings.warn('resource_tracker: There appear to be %d '
 /usr/local/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked semaphore objects to clean up at shutdown
   warnings.warn('resource_tracker: There appear to be %d '
 /usr/local/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked semaphore objects to clean up at shutdown
   warnings.warn('resource_tracker: There appear to be %d '
 [[34m2024-11-27T14:55:39.853+0000[0m] {[34mdagrun.py:[0m851} INFO[0m - Marking run <DagRun capstone_andres_dag_ @ 2024-09-17 01:00:00+00:00: scheduled__2024-09-17T01:00:00+00:00, state:running, queued_at: 2024-11-27 14:52:02.873709+00:00. externally triggered: False> successful[0m
 [[34m2024-11-27T14:55:39.854+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=capstone_andres_dag_, execution_date=2024-09-17 01:00:00+00:00, run_id=scheduled__2024-09-17T01:00:00+00:00, run_start_date=2024-11-27 14:52:02.886445+00:00, run_end_date=2024-11-27 14:55:39.854259+00:00, run_duration=216.967814, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-09-17 01:00:00+00:00, data_interval_end=2024-09-18 01:00:00+00:00, dag_hash=77fa8f64adac9391cccaaae88e2be5d8[0m
 [[34m2024-11-27T14:55:39.857+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-09-18 01:00:00+00:00, run_after=2024-09-19 01:00:00+00:00[0m
 [[34m2024-11-27T14:55:40.006+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 4 tasks up for execution:
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run scheduled__2024-09-19T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.dim_date_run scheduled__2024-09-19T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_daily_bars_run scheduled__2024-09-19T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.weekly_metrics_run scheduled__2024-09-18T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:55:40.006+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 1/16 running and queued tasks[0m
 [[34m2024-11-27T14:55:40.006+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 2/16 running and queued tasks[0m
 [[34m2024-11-27T14:55:40.007+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 3/16 running and queued tasks[0m
 [[34m2024-11-27T14:55:40.007+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 4/16 running and queued tasks[0m
 [[34m2024-11-27T14:55:40.007+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run scheduled__2024-09-19T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.dim_date_run scheduled__2024-09-19T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_daily_bars_run scheduled__2024-09-19T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.weekly_metrics_run scheduled__2024-09-18T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:55:40.010+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run', run_id='scheduled__2024-09-19T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
 [[34m2024-11-27T14:55:40.010+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run', 'scheduled__2024-09-19T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:55:40.010+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.dim_date_run', run_id='scheduled__2024-09-19T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
 [[34m2024-11-27T14:55:40.010+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.dim_date_run', 'scheduled__2024-09-19T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:55:40.011+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.stg_daily_bars_run', run_id='scheduled__2024-09-19T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
 [[34m2024-11-27T14:55:40.011+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.stg_daily_bars_run', 'scheduled__2024-09-19T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:55:40.011+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.weekly_metrics_run', run_id='scheduled__2024-09-18T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
 [[34m2024-11-27T14:55:40.011+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.weekly_metrics_run', 'scheduled__2024-09-18T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:55:40.016+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.dim_date_run', 'scheduled__2024-09-19T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:55:40.016+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run', 'scheduled__2024-09-19T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:55:40.017+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.stg_daily_bars_run', 'scheduled__2024-09-19T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:55:40.018+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.weekly_metrics_run', 'scheduled__2024-09-18T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:55:40.019+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run', run_id='scheduled__2024-09-18T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:55:40.020+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.weekly_metrics_run', run_id='scheduled__2024-09-17T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:55:40.020+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='load_treasury_data_task', run_id='scheduled__2024-09-19T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:55:40.020+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.stg_daily_bars_run', run_id='scheduled__2024-09-18T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:55:40.020+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.dim_date_run', run_id='scheduled__2024-09-18T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:55:40.032+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run, run_id=scheduled__2024-09-18T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:55:30.574710+00:00, run_end_date=2024-11-27 14:55:38.620569+00:00, run_duration=8.045859, state=success, executor_state=success, try_number=1, max_tries=0, job_id=872, pool=default_pool, queue=default, priority_weight=2, operator=DbtRunLocalOperator, queued_dttm=2024-11-27 14:55:21.008148+00:00, queued_by_job_id=803, pid=9481[0m
 [[34m2024-11-27T14:55:40.033+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=load_treasury_data_task, run_id=scheduled__2024-09-19T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:55:36.483771+00:00, run_end_date=2024-11-27 14:55:39.185544+00:00, run_duration=2.701773, state=success, executor_state=success, try_number=1, max_tries=0, job_id=876, pool=default_pool, queue=default, priority_weight=5, operator=PythonOperator, queued_dttm=2024-11-27 14:55:26.874364+00:00, queued_by_job_id=803, pid=9526[0m
 [[34m2024-11-27T14:55:40.033+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.dim_date_run, run_id=scheduled__2024-09-18T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:55:30.612343+00:00, run_end_date=2024-11-27 14:55:39.627511+00:00, run_duration=9.015168, state=success, executor_state=success, try_number=1, max_tries=0, job_id=873, pool=default_pool, queue=default, priority_weight=2, operator=DbtRunLocalOperator, queued_dttm=2024-11-27 14:55:21.008148+00:00, queued_by_job_id=803, pid=9482[0m
 [[34m2024-11-27T14:55:40.034+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.weekly_metrics_run, run_id=scheduled__2024-09-17T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:55:30.849159+00:00, run_end_date=2024-11-27 14:55:38.859227+00:00, run_duration=8.010068, state=success, executor_state=success, try_number=1, max_tries=0, job_id=874, pool=default_pool, queue=default, priority_weight=1, operator=DbtRunLocalOperator, queued_dttm=2024-11-27 14:55:20.678859+00:00, queued_by_job_id=803, pid=9483[0m
 [[34m2024-11-27T14:55:40.034+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.stg_daily_bars_run, run_id=scheduled__2024-09-18T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:55:31.414782+00:00, run_end_date=2024-11-27 14:55:39.592209+00:00, run_duration=8.177427, state=success, executor_state=success, try_number=1, max_tries=0, job_id=875, pool=default_pool, queue=default, priority_weight=2, operator=DbtRunLocalOperator, queued_dttm=2024-11-27 14:55:21.008148+00:00, queued_by_job_id=803, pid=9490[0m
 [[34m2024-11-27T14:55:40.047+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:55:40.048+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:55:40.048+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:55:40.048+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:55:40.062+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-09-19 01:00:00+00:00, run_after=2024-09-20 01:00:00+00:00[0m
 [[34m2024-11-27T14:55:40.845+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-09-20 01:00:00+00:00, run_after=2024-09-21 01:00:00+00:00[0m
 [[34m2024-11-27T14:55:41.032+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:55:41.033+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:55:41.033+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:55:41.035+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:55:41.036+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:55:41.037+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:55:41.037+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:55:41.038+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:55:41.038+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:55:41.039+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:55:41.040+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:55:41.041+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:55:41.545+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412095046,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T14:55:41.546+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412088318,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T14:55:41.548+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412088322,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T14:55:41.574+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412096518,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T14:55:42.108+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-09-21 01:00:00+00:00, run_after=2024-09-22 01:00:00+00:00[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:55:42.274+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:55:42.278+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:55:42.282+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:55:42.282+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 [[34m2024-11-27T14:55:43.026+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 [[34m2024-11-27T14:55:43.186+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 [[34m2024-11-27T14:55:43.301+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:55:43.311+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-09-22 01:00:00+00:00, run_after=2024-09-23 01:00:00+00:00[0m
 [[34m2024-11-27T14:55:43.527+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:55:44.535+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-09-23 01:00:00+00:00, run_after=2024-09-24 01:00:00+00:00[0m
 [[34m2024-11-27T14:55:44.763+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 2 tasks up for execution:
 	<TaskInstance: capstone_andres_dag_.load_polygon_data_task scheduled__2024-09-21T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.load_treasury_data_task scheduled__2024-09-20T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:55:44.764+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 4/16 running and queued tasks[0m
 [[34m2024-11-27T14:55:44.764+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 5/16 running and queued tasks[0m
 [[34m2024-11-27T14:55:44.764+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
 	<TaskInstance: capstone_andres_dag_.load_polygon_data_task scheduled__2024-09-21T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.load_treasury_data_task scheduled__2024-09-20T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:55:44.766+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='load_polygon_data_task', run_id='scheduled__2024-09-21T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
 [[34m2024-11-27T14:55:44.766+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'load_polygon_data_task', 'scheduled__2024-09-21T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:55:44.767+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='load_treasury_data_task', run_id='scheduled__2024-09-20T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 5 and queue default[0m
 [[34m2024-11-27T14:55:44.767+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'load_treasury_data_task', 'scheduled__2024-09-20T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:55:44.769+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'load_polygon_data_task', 'scheduled__2024-09-21T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:55:44.770+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='load_polygon_data_task', run_id='scheduled__2024-09-20T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:55:44.769+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'load_treasury_data_task', 'scheduled__2024-09-20T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:55:44.774+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=load_polygon_data_task, run_id=scheduled__2024-09-20T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:55:36.635565+00:00, run_end_date=2024-11-27 14:55:43.774735+00:00, run_duration=7.13917, state=success, executor_state=success, try_number=1, max_tries=0, job_id=877, pool=default_pool, queue=default, priority_weight=6, operator=PythonOperator, queued_dttm=2024-11-27 14:55:26.874364+00:00, queued_by_job_id=803, pid=9530[0m
 [[34m2024-11-27T14:55:44.797+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-09-24 01:00:00+00:00, run_after=2024-09-25 01:00:00+00:00[0m
 [[34m2024-11-27T14:55:44.811+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:55:44.812+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:55:45.062+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-09-25 01:00:00+00:00, run_after=2024-09-26 01:00:00+00:00[0m
 [[34m2024-11-27T14:55:45.641+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:55:45.641+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:55:45.641+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:55:45.644+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:55:45.644+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:55:45.644+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:55:45.729+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:55:45.909+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:55:45.959+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:55:46.054+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T14:55:46.172+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412096526,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T14:55:46.172+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412096522,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T14:55:46.198+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T14:55:46.262+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T14:55:46.315+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:55:46.324+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:55:46.343+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-09-26 01:00:00+00:00, run_after=2024-09-27 01:00:00+00:00[0m
 [[34m2024-11-27T14:55:46.443+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:55:46.542+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:55:46.619+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T14:55:46.839+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:55:47.014+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:55:47.019+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 [[34m2024-11-27T14:55:47.368+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:55:47.449+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:55:47.516+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:55:47.546+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-09-27 01:00:00+00:00, run_after=2024-09-28 01:00:00+00:00[0m
 [[34m2024-11-27T14:55:47.547+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:55:47.547+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:55:47.656+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 [[34m2024-11-27T14:55:47.658+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:55:47.734+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:55:47.795+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:55:47.842+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 [[34m2024-11-27T14:55:47.880+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:55:47.896+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 [[34m2024-11-27T14:55:48.018+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:55:48.124+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 Data loaded successfully
 [[34m2024-11-27T14:55:48.206+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:55:48.219+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 Data loaded successfully
 [[34m2024-11-27T14:55:48.436+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:55:48.450+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 Data loaded successfully
 [[34m2024-11-27T14:55:48.479+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:55:48.490+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 Data loaded successfully
 [[34m2024-11-27T14:55:48.632+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:55:48.646+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 [[34m2024-11-27T14:55:48.798+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-09-28 01:00:00+00:00, run_after=2024-09-29 01:00:00+00:00[0m
 [[34m2024-11-27T14:55:48.867+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:55:49.087+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.3446608688682318[0m
 [[34m2024-11-27T14:55:49.087+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|9571]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:55:49.088+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:55:49.088+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:55:49.088+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:55:49.088+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|9571]: It took 0.882s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:55:49.091+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|9571]: It took 0.00258s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:55:49.129+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.weekly_metrics_run scheduled__2024-09-18T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [[34m2024-11-27T14:55:49.182+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T14:55:49.296+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.3269350219052285[0m
 [[34m2024-11-27T14:55:49.297+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|9568]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:55:49.297+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:55:49.298+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:55:49.298+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:55:49.298+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|9568]: It took 0.862s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:55:49.301+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|9568]: It took 0.00327s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:55:49.347+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run scheduled__2024-09-19T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [[34m2024-11-27T14:55:49.357+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.3442614779341966[0m
 [[34m2024-11-27T14:55:49.357+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|9569]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:55:49.358+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:55:49.358+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:55:49.358+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:55:49.359+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|9569]: It took 0.88s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:55:49.365+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|9569]: It took 0.00595s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:55:49.415+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.dim_date_run scheduled__2024-09-19T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [[34m2024-11-27T14:55:49.531+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [2024-11-27T14:55:49.623+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.weekly_metrics_run, dagrun_id=scheduled__2024-09-18T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:55:49.199407+00:00, try_number=1, job_id=878, op_classpath=cosmos.operators.local.DbtRunLocalOperator, cosmos.operators.base.DbtRunMixin, cosmos.operators.local.DbtLocalBaseOperator, cosmos.operators.base.AbstractDbtBaseOperator
 [[34m2024-11-27T14:55:49.657+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.4726110310293734[0m
 [[34m2024-11-27T14:55:49.657+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|9570]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:55:49.658+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:55:49.658+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:55:49.658+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:55:49.658+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|9570]: It took 1.03s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:55:49.661+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|9570]: It took 0.00295s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:55:49.709+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_daily_bars_run scheduled__2024-09-19T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [2024-11-27T14:55:49.899+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run, dagrun_id=scheduled__2024-09-19T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:55:49.446278+00:00, try_number=1, job_id=879, op_classpath=cosmos.operators.local.DbtRunLocalOperator, cosmos.operators.base.DbtRunMixin, cosmos.operators.local.DbtLocalBaseOperator, cosmos.operators.base.AbstractDbtBaseOperator
 [2024-11-27T14:55:49.960+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.dim_date_run, dagrun_id=scheduled__2024-09-19T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:55:49.517657+00:00, try_number=1, job_id=880, op_classpath=cosmos.operators.local.DbtRunLocalOperator, cosmos.operators.base.DbtRunMixin, cosmos.operators.local.DbtLocalBaseOperator, cosmos.operators.base.AbstractDbtBaseOperator
 [[34m2024-11-27T14:55:49.998+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-09-29 01:00:00+00:00, run_after=2024-09-30 01:00:00+00:00[0m
 [[34m2024-11-27T14:55:50.186+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [2024-11-27T14:55:50.251+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.stg_daily_bars_run, dagrun_id=scheduled__2024-09-19T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:55:49.816692+00:00, try_number=1, job_id=881, op_classpath=cosmos.operators.local.DbtRunLocalOperator, cosmos.operators.base.DbtRunMixin, cosmos.operators.local.DbtLocalBaseOperator, cosmos.operators.base.AbstractDbtBaseOperator
 [[34m2024-11-27T14:55:50.342+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-09-30 01:00:00+00:00, run_after=2024-10-01 01:00:00+00:00[0m
 [[34m2024-11-27T14:55:50.551+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T14:55:50.826+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:55:50.879+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:55:51.065+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:55:51.108+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-10-01 01:00:00+00:00, run_after=2024-10-02 01:00:00+00:00[0m
 [[34m2024-11-27T14:55:51.173+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 Data loaded successfully
 [[34m2024-11-27T14:55:51.839+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:55:51.854+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 [[34m2024-11-27T14:55:51.912+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:55:52.100+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:55:52.217+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 [[34m2024-11-27T14:55:52.363+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-10-02 01:00:00+00:00, run_after=2024-10-03 01:00:00+00:00[0m
 [[34m2024-11-27T14:55:52.870+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.47176759014837444[0m
 [[34m2024-11-27T14:55:52.870+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|9636]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:55:52.871+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:55:52.871+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:55:52.872+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:55:52.872+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|9636]: It took 1.03s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:55:52.875+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|9636]: It took 0.00298s to build the Airflow DAG.[0m
 Data loaded successfully
 [[34m2024-11-27T14:55:52.903+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:55:52.917+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 [[34m2024-11-27T14:55:52.928+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.load_polygon_data_task scheduled__2024-09-21T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [2024-11-27T14:55:53.360+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=load_polygon_data_task, dagrun_id=scheduled__2024-09-21T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:55:53.026558+00:00, try_number=1, job_id=882, op_classpath=airflow.operators.python.PythonOperator
 [[34m2024-11-27T14:55:53.771+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.3244772220496088[0m
 [[34m2024-11-27T14:55:53.772+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|9635]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:55:53.772+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:55:53.772+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:55:53.772+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:55:53.773+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|9635]: It took 0.87s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:55:53.775+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|9635]: It took 0.00217s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:55:53.789+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-10-03 01:00:00+00:00, run_after=2024-10-04 01:00:00+00:00[0m
 [[34m2024-11-27T14:55:53.808+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.load_treasury_data_task scheduled__2024-09-20T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [2024-11-27T14:55:54.232+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=load_treasury_data_task, dagrun_id=scheduled__2024-09-20T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:55:53.891662+00:00, try_number=1, job_id=883, op_classpath=airflow.operators.python.PythonOperator
 [[34m2024-11-27T14:55:55.023+0000[0m] {[34mscheduler_job_runner.py:[0m1328} INFO[0m - DAG capstone_andres_dag_ is at (or above) max_active_runs (16 of 16), not creating any more runs[0m
 /usr/local/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked semaphore objects to clean up at shutdown
   warnings.warn('resource_tracker: There appear to be %d '
 /usr/local/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked semaphore objects to clean up at shutdown
   warnings.warn('resource_tracker: There appear to be %d '
 /usr/local/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked semaphore objects to clean up at shutdown
   warnings.warn('resource_tracker: There appear to be %d '
 [[34m2024-11-27T14:55:56.794+0000[0m] {[34mdagrun.py:[0m851} INFO[0m - Marking run <DagRun capstone_andres_dag_ @ 2024-09-18 01:00:00+00:00: scheduled__2024-09-18T01:00:00+00:00, state:running, queued_at: 2024-11-27 14:52:04.040251+00:00. externally triggered: False> successful[0m
 [[34m2024-11-27T14:55:56.795+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=capstone_andres_dag_, execution_date=2024-09-18 01:00:00+00:00, run_id=scheduled__2024-09-18T01:00:00+00:00, run_start_date=2024-11-27 14:52:04.054859+00:00, run_end_date=2024-11-27 14:55:56.795287+00:00, run_duration=232.740428, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-09-18 01:00:00+00:00, data_interval_end=2024-09-19 01:00:00+00:00, dag_hash=77fa8f64adac9391cccaaae88e2be5d8[0m
 [[34m2024-11-27T14:55:56.798+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-09-19 01:00:00+00:00, run_after=2024-09-20 01:00:00+00:00[0m
 /usr/local/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked semaphore objects to clean up at shutdown
   warnings.warn('resource_tracker: There appear to be %d '
 [[34m2024-11-27T14:55:56.987+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.dim_date_run scheduled__2024-09-20T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_daily_bars_run scheduled__2024-09-20T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run scheduled__2024-09-20T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:55:56.987+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 1/16 running and queued tasks[0m
 [[34m2024-11-27T14:55:56.988+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 2/16 running and queued tasks[0m
 [[34m2024-11-27T14:55:56.988+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 3/16 running and queued tasks[0m
 [[34m2024-11-27T14:55:56.988+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.dim_date_run scheduled__2024-09-20T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_daily_bars_run scheduled__2024-09-20T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run scheduled__2024-09-20T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:55:56.990+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.dim_date_run', run_id='scheduled__2024-09-20T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
 [[34m2024-11-27T14:55:56.990+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.dim_date_run', 'scheduled__2024-09-20T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:55:56.990+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.stg_daily_bars_run', run_id='scheduled__2024-09-20T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
 [[34m2024-11-27T14:55:56.990+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.stg_daily_bars_run', 'scheduled__2024-09-20T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:55:56.990+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run', run_id='scheduled__2024-09-20T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
 [[34m2024-11-27T14:55:56.991+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run', 'scheduled__2024-09-20T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:55:56.992+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.dim_date_run', 'scheduled__2024-09-20T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:55:56.993+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run', 'scheduled__2024-09-20T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:55:56.993+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.stg_daily_bars_run', 'scheduled__2024-09-20T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:55:56.995+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run', run_id='scheduled__2024-09-19T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:55:56.995+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.weekly_metrics_run', run_id='scheduled__2024-09-18T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:55:56.996+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='load_treasury_data_task', run_id='scheduled__2024-09-20T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:55:56.996+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.stg_daily_bars_run', run_id='scheduled__2024-09-19T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:55:56.996+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.dim_date_run', run_id='scheduled__2024-09-19T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:55:57.004+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.stg_daily_bars_run, run_id=scheduled__2024-09-19T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:55:49.816692+00:00, run_end_date=2024-11-27 14:55:56.279465+00:00, run_duration=6.462773, state=success, executor_state=success, try_number=1, max_tries=0, job_id=881, pool=default_pool, queue=default, priority_weight=2, operator=DbtRunLocalOperator, queued_dttm=2024-11-27 14:55:40.008146+00:00, queued_by_job_id=803, pid=9675[0m
 [[34m2024-11-27T14:55:57.005+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run, run_id=scheduled__2024-09-19T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:55:49.446278+00:00, run_end_date=2024-11-27 14:55:55.725384+00:00, run_duration=6.279106, state=success, executor_state=success, try_number=1, max_tries=0, job_id=879, pool=default_pool, queue=default, priority_weight=2, operator=DbtRunLocalOperator, queued_dttm=2024-11-27 14:55:40.008146+00:00, queued_by_job_id=803, pid=9673[0m
 [[34m2024-11-27T14:55:57.005+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=load_treasury_data_task, run_id=scheduled__2024-09-20T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:55:53.891662+00:00, run_end_date=2024-11-27 14:55:56.272627+00:00, run_duration=2.380965, state=success, executor_state=success, try_number=1, max_tries=0, job_id=883, pool=default_pool, queue=default, priority_weight=5, operator=PythonOperator, queued_dttm=2024-11-27 14:55:44.765160+00:00, queued_by_job_id=803, pid=9689[0m
 [[34m2024-11-27T14:55:57.005+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.weekly_metrics_run, run_id=scheduled__2024-09-18T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:55:49.199407+00:00, run_end_date=2024-11-27 14:55:55.581637+00:00, run_duration=6.38223, state=success, executor_state=success, try_number=1, max_tries=0, job_id=878, pool=default_pool, queue=default, priority_weight=1, operator=DbtRunLocalOperator, queued_dttm=2024-11-27 14:55:40.008146+00:00, queued_by_job_id=803, pid=9672[0m
 [[34m2024-11-27T14:55:57.005+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.dim_date_run, run_id=scheduled__2024-09-19T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:55:49.517657+00:00, run_end_date=2024-11-27 14:55:56.880741+00:00, run_duration=7.363084, state=success, executor_state=success, try_number=1, max_tries=0, job_id=880, pool=default_pool, queue=default, priority_weight=2, operator=DbtRunLocalOperator, queued_dttm=2024-11-27 14:55:40.008146+00:00, queued_by_job_id=803, pid=9674[0m
 [[34m2024-11-27T14:55:57.020+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:55:57.021+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:55:57.021+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:55:57.025+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-09-20 01:00:00+00:00, run_after=2024-09-21 01:00:00+00:00[0m
 [[34m2024-11-27T14:55:57.248+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.weekly_metrics_run scheduled__2024-09-19T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:55:57.249+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 4/16 running and queued tasks[0m
 [[34m2024-11-27T14:55:57.249+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.weekly_metrics_run scheduled__2024-09-19T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:55:57.251+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.weekly_metrics_run', run_id='scheduled__2024-09-19T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
 [[34m2024-11-27T14:55:57.251+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.weekly_metrics_run', 'scheduled__2024-09-19T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:55:57.253+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.weekly_metrics_run', 'scheduled__2024-09-19T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:55:57.272+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-09-21 01:00:00+00:00, run_after=2024-09-22 01:00:00+00:00[0m
 [[34m2024-11-27T14:55:57.282+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:55:57.914+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:55:57.914+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:55:57.915+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:55:57.915+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:55:57.915+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:55:57.915+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:55:57.925+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:55:57.926+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:55:57.927+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:55:58.148+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:55:58.148+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:55:58.148+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:55:58.368+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412095082,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T14:55:58.381+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412095086,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T14:55:58.383+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412096562,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T14:55:58.578+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-09-22 01:00:00+00:00, run_after=2024-09-23 01:00:00+00:00[0m
 [[34m2024-11-27T14:55:58.608+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412095090,
 "os.name" : Linux
 [0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:55:59.126+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:55:59.139+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:55:59.157+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:55:59.359+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 [[34m2024-11-27T14:55:59.780+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-09-23 01:00:00+00:00, run_after=2024-09-24 01:00:00+00:00[0m
 [[34m2024-11-27T14:55:59.911+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 [[34m2024-11-27T14:55:59.965+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 2 tasks up for execution:
 	<TaskInstance: capstone_andres_dag_.load_polygon_data_task scheduled__2024-09-22T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.load_treasury_data_task scheduled__2024-09-21T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:55:59.965+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 4/16 running and queued tasks[0m
 [[34m2024-11-27T14:55:59.966+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 5/16 running and queued tasks[0m
 [[34m2024-11-27T14:55:59.966+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
 	<TaskInstance: capstone_andres_dag_.load_polygon_data_task scheduled__2024-09-22T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.load_treasury_data_task scheduled__2024-09-21T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:55:59.968+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='load_polygon_data_task', run_id='scheduled__2024-09-22T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
 [[34m2024-11-27T14:55:59.969+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'load_polygon_data_task', 'scheduled__2024-09-22T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:55:59.969+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='load_treasury_data_task', run_id='scheduled__2024-09-21T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 5 and queue default[0m
 [[34m2024-11-27T14:55:59.969+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'load_treasury_data_task', 'scheduled__2024-09-21T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:55:59.971+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'load_polygon_data_task', 'scheduled__2024-09-22T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:55:59.972+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'load_treasury_data_task', 'scheduled__2024-09-21T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:55:59.973+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='load_polygon_data_task', run_id='scheduled__2024-09-21T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:55:59.977+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=load_polygon_data_task, run_id=scheduled__2024-09-21T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:55:53.026558+00:00, run_end_date=2024-11-27 14:55:58.898607+00:00, run_duration=5.872049, state=success, executor_state=success, try_number=1, max_tries=0, job_id=882, pool=default_pool, queue=default, priority_weight=6, operator=PythonOperator, queued_dttm=2024-11-27 14:55:44.765160+00:00, queued_by_job_id=803, pid=9684[0m
 [[34m2024-11-27T14:56:00.001+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:56:00.001+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:56:00.003+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-09-24 01:00:00+00:00, run_after=2024-09-25 01:00:00+00:00[0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 [[34m2024-11-27T14:56:00.184+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 [[34m2024-11-27T14:56:00.296+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:56:00.474+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:56:00.777+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-09-25 01:00:00+00:00, run_after=2024-09-26 01:00:00+00:00[0m
 [[34m2024-11-27T14:56:00.853+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:56:00.854+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:56:00.854+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:56:00.855+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:56:00.855+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:56:00.856+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:56:01.308+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412095098,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T14:56:01.363+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-09-26 01:00:00+00:00, run_after=2024-09-27 01:00:00+00:00[0m
 [[34m2024-11-27T14:56:01.372+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412095102,
 "os.name" : Linux
 [0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:56:02.113+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:56:02.123+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 [[34m2024-11-27T14:56:02.498+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:56:02.588+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-09-27 01:00:00+00:00, run_after=2024-09-28 01:00:00+00:00[0m
 [[34m2024-11-27T14:56:02.601+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:56:02.603+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:56:02.786+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.weekly_metrics'[0m
 [[34m2024-11-27T14:56:02.786+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections'[0m
 [[34m2024-11-27T14:56:02.786+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_payments'[0m
 [[34m2024-11-27T14:56:02.787+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_orders'[0m
 [[34m2024-11-27T14:56:02.787+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_daily_bars'[0m
 [[34m2024-11-27T14:56:02.833+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T14:56:02.861+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:56:02.890+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:56:03.155+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:56:03.172+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:56:03.188+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T14:56:03.200+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T14:56:03.444+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:56:03.444+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:56:03.527+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T14:56:03.800+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-09-28 01:00:00+00:00, run_after=2024-09-29 01:00:00+00:00[0m
 [[34m2024-11-27T14:56:03.890+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:56:04.380+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:56:04.540+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:56:04.645+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 [[34m2024-11-27T14:56:04.659+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:56:04.660+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:56:04.845+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:56:04.853+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:56:04.950+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 [[34m2024-11-27T14:56:04.957+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 [[34m2024-11-27T14:56:04.969+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:56:04.995+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-09-29 01:00:00+00:00, run_after=2024-09-30 01:00:00+00:00[0m
 [[34m2024-11-27T14:56:05.136+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:56:05.229+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 Data loaded successfully
 [[34m2024-11-27T14:56:05.239+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:56:05.245+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 [[34m2024-11-27T14:56:05.251+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 [[34m2024-11-27T14:56:05.255+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 Data loaded successfully
 [[34m2024-11-27T14:56:05.459+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:56:05.474+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 Data loaded successfully
 [[34m2024-11-27T14:56:05.538+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:56:05.554+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 [[34m2024-11-27T14:56:05.559+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T14:56:05.587+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 Data loaded successfully
 [[34m2024-11-27T14:56:05.782+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:56:05.795+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 [[34m2024-11-27T14:56:05.803+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:56:05.838+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:56:05.994+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-09-30 01:00:00+00:00, run_after=2024-10-01 01:00:00+00:00[0m
 [[34m2024-11-27T14:56:06.222+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.4238345199264586[0m
 [[34m2024-11-27T14:56:06.222+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|9712]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:56:06.223+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:56:06.223+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:56:06.223+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:56:06.223+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|9712]: It took 0.984s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:56:06.227+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|9712]: It took 0.00371s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:56:06.269+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run scheduled__2024-09-20T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [[34m2024-11-27T14:56:06.441+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.41687386808916926[0m
 [[34m2024-11-27T14:56:06.441+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|9713]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:56:06.442+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:56:06.442+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:56:06.442+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:56:06.442+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|9713]: It took 0.984s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:56:06.447+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|9713]: It took 0.0044s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:56:06.484+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_daily_bars_run scheduled__2024-09-20T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [[34m2024-11-27T14:56:06.488+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.3977186761330813[0m
 [[34m2024-11-27T14:56:06.489+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|9714]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:56:06.489+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:56:06.489+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:56:06.490+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:56:06.490+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|9714]: It took 0.952s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:56:06.495+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|9714]: It took 0.00508s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:56:06.549+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.dim_date_run scheduled__2024-09-20T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [2024-11-27T14:56:06.765+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run, dagrun_id=scheduled__2024-09-20T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:56:06.348491+00:00, try_number=1, job_id=884, op_classpath=cosmos.operators.local.DbtRunLocalOperator, cosmos.operators.base.DbtRunMixin, cosmos.operators.local.DbtLocalBaseOperator, cosmos.operators.base.AbstractDbtBaseOperator
 [[34m2024-11-27T14:56:06.807+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.4644699420314282[0m
 [[34m2024-11-27T14:56:06.807+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|9718]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:56:06.808+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:56:06.808+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:56:06.808+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:56:06.809+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|9718]: It took 1.03s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:56:06.812+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|9718]: It took 0.00304s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:56:06.861+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.weekly_metrics_run scheduled__2024-09-19T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [[34m2024-11-27T14:56:07.026+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [2024-11-27T14:56:07.047+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.stg_daily_bars_run, dagrun_id=scheduled__2024-09-20T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:56:06.570255+00:00, try_number=1, job_id=885, op_classpath=cosmos.operators.local.DbtRunLocalOperator, cosmos.operators.base.DbtRunMixin, cosmos.operators.local.DbtLocalBaseOperator, cosmos.operators.base.AbstractDbtBaseOperator
 [[34m2024-11-27T14:56:07.084+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [2024-11-27T14:56:07.118+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.dim_date_run, dagrun_id=scheduled__2024-09-20T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:56:06.650161+00:00, try_number=1, job_id=886, op_classpath=cosmos.operators.local.DbtRunLocalOperator, cosmos.operators.base.DbtRunMixin, cosmos.operators.local.DbtLocalBaseOperator, cosmos.operators.base.AbstractDbtBaseOperator
 [[34m2024-11-27T14:56:07.243+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:56:07.261+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-10-01 01:00:00+00:00, run_after=2024-10-02 01:00:00+00:00[0m
 [[34m2024-11-27T14:56:07.300+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:56:07.355+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 [[34m2024-11-27T14:56:07.414+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 [2024-11-27T14:56:07.486+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.weekly_metrics_run, dagrun_id=scheduled__2024-09-19T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:56:06.973844+00:00, try_number=1, job_id=887, op_classpath=cosmos.operators.local.DbtRunLocalOperator, cosmos.operators.base.DbtRunMixin, cosmos.operators.local.DbtLocalBaseOperator, cosmos.operators.base.AbstractDbtBaseOperator
 Data loaded successfully
 [[34m2024-11-27T14:56:07.975+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 Data loaded successfully
 [[34m2024-11-27T14:56:07.978+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:56:07.998+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 [[34m2024-11-27T14:56:08.002+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 [[34m2024-11-27T14:56:08.611+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-10-02 01:00:00+00:00, run_after=2024-10-03 01:00:00+00:00[0m
 [[34m2024-11-27T14:56:09.493+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.8913324940949678[0m
 [[34m2024-11-27T14:56:09.495+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|9735]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:56:09.496+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:56:09.496+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:56:09.497+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:56:09.497+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|9735]: It took 1.52s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:56:09.507+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|9735]: It took 0.00919s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:56:09.573+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.9517922950908542[0m
 [[34m2024-11-27T14:56:09.574+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|9736]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:56:09.591+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:56:09.592+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:56:09.593+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:56:09.594+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|9736]: It took 1.62s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:56:09.602+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|9736]: It took 0.00804s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:56:09.660+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.load_treasury_data_task scheduled__2024-09-21T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [[34m2024-11-27T14:56:09.791+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.load_polygon_data_task scheduled__2024-09-22T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [[34m2024-11-27T14:56:10.045+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-10-03 01:00:00+00:00, run_after=2024-10-04 01:00:00+00:00[0m
 [2024-11-27T14:56:10.464+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=load_treasury_data_task, dagrun_id=scheduled__2024-09-21T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:56:09.933764+00:00, try_number=1, job_id=888, op_classpath=airflow.operators.python.PythonOperator
 [2024-11-27T14:56:10.496+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=load_polygon_data_task, dagrun_id=scheduled__2024-09-22T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:56:09.983826+00:00, try_number=1, job_id=889, op_classpath=airflow.operators.python.PythonOperator
 [[34m2024-11-27T14:56:11.270+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-10-04 01:00:00+00:00, run_after=2024-10-05 01:00:00+00:00[0m
 [[34m2024-11-27T14:56:13.349+0000[0m] {[34mscheduler_job_runner.py:[0m1328} INFO[0m - DAG capstone_andres_dag_ is at (or above) max_active_runs (16 of 16), not creating any more runs[0m
 [[34m2024-11-27T14:56:13.702+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 5 tasks up for execution:
 	<TaskInstance: capstone_andres_dag_.load_polygon_data_task scheduled__2024-09-23T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.load_treasury_data_task scheduled__2024-09-22T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.dim_date_run scheduled__2024-09-21T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_daily_bars_run scheduled__2024-09-21T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run scheduled__2024-09-21T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:56:13.703+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 4/16 running and queued tasks[0m
 [[34m2024-11-27T14:56:13.703+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 5/16 running and queued tasks[0m
 [[34m2024-11-27T14:56:13.703+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 6/16 running and queued tasks[0m
 [[34m2024-11-27T14:56:13.703+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 7/16 running and queued tasks[0m
 [[34m2024-11-27T14:56:13.704+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 8/16 running and queued tasks[0m
 [[34m2024-11-27T14:56:13.704+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
 	<TaskInstance: capstone_andres_dag_.load_polygon_data_task scheduled__2024-09-23T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.load_treasury_data_task scheduled__2024-09-22T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.dim_date_run scheduled__2024-09-21T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_daily_bars_run scheduled__2024-09-21T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run scheduled__2024-09-21T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:56:13.709+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='load_polygon_data_task', run_id='scheduled__2024-09-23T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
 [[34m2024-11-27T14:56:13.709+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'load_polygon_data_task', 'scheduled__2024-09-23T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:56:13.710+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='load_treasury_data_task', run_id='scheduled__2024-09-22T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 5 and queue default[0m
 [[34m2024-11-27T14:56:13.710+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'load_treasury_data_task', 'scheduled__2024-09-22T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:56:13.710+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.dim_date_run', run_id='scheduled__2024-09-21T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
 [[34m2024-11-27T14:56:13.710+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.dim_date_run', 'scheduled__2024-09-21T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:56:13.711+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.stg_daily_bars_run', run_id='scheduled__2024-09-21T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
 [[34m2024-11-27T14:56:13.711+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.stg_daily_bars_run', 'scheduled__2024-09-21T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:56:13.711+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run', run_id='scheduled__2024-09-21T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
 [[34m2024-11-27T14:56:13.712+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run', 'scheduled__2024-09-21T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:56:13.715+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'load_treasury_data_task', 'scheduled__2024-09-22T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:56:13.715+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'load_polygon_data_task', 'scheduled__2024-09-23T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:56:13.716+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.dim_date_run', 'scheduled__2024-09-21T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:56:13.717+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.stg_daily_bars_run', 'scheduled__2024-09-21T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:56:13.718+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run', 'scheduled__2024-09-21T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:56:13.719+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='load_polygon_data_task', run_id='scheduled__2024-09-22T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:56:13.720+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='load_treasury_data_task', run_id='scheduled__2024-09-21T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:56:13.731+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=load_polygon_data_task, run_id=scheduled__2024-09-22T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:56:09.983826+00:00, run_end_date=2024-11-27 14:56:11.626977+00:00, run_duration=1.643151, state=success, executor_state=success, try_number=1, max_tries=0, job_id=889, pool=default_pool, queue=default, priority_weight=6, operator=PythonOperator, queued_dttm=2024-11-27 14:55:59.967062+00:00, queued_by_job_id=803, pid=9827[0m
 [[34m2024-11-27T14:56:13.731+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=load_treasury_data_task, run_id=scheduled__2024-09-21T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:56:09.933764+00:00, run_end_date=2024-11-27 14:56:13.005062+00:00, run_duration=3.071298, state=success, executor_state=success, try_number=1, max_tries=0, job_id=888, pool=default_pool, queue=default, priority_weight=5, operator=PythonOperator, queued_dttm=2024-11-27 14:55:59.967062+00:00, queued_by_job_id=803, pid=9826[0m
 [[34m2024-11-27T14:56:13.768+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:56:13.770+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:56:13.771+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:56:13.771+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:56:13.775+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:56:16.256+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:56:16.257+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:56:16.258+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:56:16.260+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:56:16.262+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:56:16.263+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:56:16.295+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:56:16.296+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:56:16.296+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:56:16.323+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:56:16.324+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:56:16.324+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:56:16.382+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:56:16.383+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:56:16.383+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:56:16.756+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412096590,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T14:56:16.757+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412095150,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T14:56:16.791+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412095154,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T14:56:16.827+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412095146,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T14:56:16.851+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412095158,
 "os.name" : Linux
 [0m
 /usr/local/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked semaphore objects to clean up at shutdown
   warnings.warn('resource_tracker: There appear to be %d '
 /usr/local/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked semaphore objects to clean up at shutdown
   warnings.warn('resource_tracker: There appear to be %d '
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:56:17.676+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:56:17.743+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:56:17.757+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:56:17.774+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run', run_id='scheduled__2024-09-20T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:56:17.775+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.stg_daily_bars_run', run_id='scheduled__2024-09-20T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:56:17.779+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run, run_id=scheduled__2024-09-20T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:56:06.348491+00:00, run_end_date=2024-11-27 14:56:17.163440+00:00, run_duration=10.814949, state=success, executor_state=success, try_number=1, max_tries=0, job_id=884, pool=default_pool, queue=default, priority_weight=2, operator=DbtRunLocalOperator, queued_dttm=2024-11-27 14:55:56.988916+00:00, queued_by_job_id=803, pid=9812[0m
 [[34m2024-11-27T14:56:17.780+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.stg_daily_bars_run, run_id=scheduled__2024-09-20T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:56:06.570255+00:00, run_end_date=2024-11-27 14:56:17.219721+00:00, run_duration=10.649466, state=success, executor_state=success, try_number=1, max_tries=0, job_id=885, pool=default_pool, queue=default, priority_weight=2, operator=DbtRunLocalOperator, queued_dttm=2024-11-27 14:55:56.988916+00:00, queued_by_job_id=803, pid=9813[0m
 /usr/local/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked semaphore objects to clean up at shutdown
   warnings.warn('resource_tracker: There appear to be %d '
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:56:17.803+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:56:17.839+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 /usr/local/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked semaphore objects to clean up at shutdown
   warnings.warn('resource_tracker: There appear to be %d '
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 [[34m2024-11-27T14:56:18.237+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 [[34m2024-11-27T14:56:18.448+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 [[34m2024-11-27T14:56:18.557+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 [[34m2024-11-27T14:56:18.782+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:56:18.820+0000[0m] {[34mdagrun.py:[0m851} INFO[0m - Marking run <DagRun capstone_andres_dag_ @ 2024-09-19 01:00:00+00:00: scheduled__2024-09-19T01:00:00+00:00, state:running, queued_at: 2024-11-27 14:52:05.205875+00:00. externally triggered: False> successful[0m
 [[34m2024-11-27T14:56:18.821+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=capstone_andres_dag_, execution_date=2024-09-19 01:00:00+00:00, run_id=scheduled__2024-09-19T01:00:00+00:00, run_start_date=2024-11-27 14:52:05.229262+00:00, run_end_date=2024-11-27 14:56:18.821267+00:00, run_duration=253.592005, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-09-19 01:00:00+00:00, data_interval_end=2024-09-20 01:00:00+00:00, dag_hash=77fa8f64adac9391cccaaae88e2be5d8[0m
 [[34m2024-11-27T14:56:18.824+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-09-20 01:00:00+00:00, run_after=2024-09-21 01:00:00+00:00[0m
 [[34m2024-11-27T14:56:18.967+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:56:18.995+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.weekly_metrics_run scheduled__2024-09-20T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:56:18.995+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 5/16 running and queued tasks[0m
 [[34m2024-11-27T14:56:18.995+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.weekly_metrics_run scheduled__2024-09-20T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:56:18.996+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.weekly_metrics_run', run_id='scheduled__2024-09-20T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
 [[34m2024-11-27T14:56:18.997+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.weekly_metrics_run', 'scheduled__2024-09-20T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:56:18.999+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.weekly_metrics_run', run_id='scheduled__2024-09-19T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:56:18.998+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.weekly_metrics_run', 'scheduled__2024-09-20T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:56:18.999+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.dim_date_run', run_id='scheduled__2024-09-20T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:56:19.003+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.weekly_metrics_run, run_id=scheduled__2024-09-19T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:56:06.973844+00:00, run_end_date=2024-11-27 14:56:17.594769+00:00, run_duration=10.620925, state=success, executor_state=success, try_number=1, max_tries=0, job_id=887, pool=default_pool, queue=default, priority_weight=1, operator=DbtRunLocalOperator, queued_dttm=2024-11-27 14:55:57.249944+00:00, queued_by_job_id=803, pid=9815[0m
 [[34m2024-11-27T14:56:19.003+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.dim_date_run, run_id=scheduled__2024-09-20T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:56:06.650161+00:00, run_end_date=2024-11-27 14:56:18.169097+00:00, run_duration=11.518936, state=success, executor_state=success, try_number=1, max_tries=0, job_id=886, pool=default_pool, queue=default, priority_weight=2, operator=DbtRunLocalOperator, queued_dttm=2024-11-27 14:55:56.988916+00:00, queued_by_job_id=803, pid=9814[0m
 [[34m2024-11-27T14:56:19.021+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-09-21 01:00:00+00:00, run_after=2024-09-22 01:00:00+00:00[0m
 [[34m2024-11-27T14:56:19.023+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:56:19.986+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:56:19.987+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:56:19.987+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:56:20.237+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-09-22 01:00:00+00:00, run_after=2024-09-23 01:00:00+00:00[0m
 [[34m2024-11-27T14:56:20.501+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412096594,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T14:56:21.067+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:56:21.182+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:56:21.241+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:56:21.280+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:56:21.392+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T14:56:21.463+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:56:21.472+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-09-23 01:00:00+00:00, run_after=2024-09-24 01:00:00+00:00[0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 [[34m2024-11-27T14:56:21.565+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T14:56:21.565+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T14:56:21.666+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:56:21.687+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:56:21.769+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T14:56:21.814+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:56:21.851+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:56:21.855+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:56:21.959+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T14:56:22.036+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:56:22.250+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:56:22.322+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-09-24 01:00:00+00:00, run_after=2024-09-25 01:00:00+00:00[0m
 [[34m2024-11-27T14:56:22.765+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:56:22.812+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-09-25 01:00:00+00:00, run_after=2024-09-26 01:00:00+00:00[0m
 [[34m2024-11-27T14:56:22.836+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:56:22.925+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:56:22.954+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:56:22.971+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:56:23.056+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 [[34m2024-11-27T14:56:23.073+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 [[34m2024-11-27T14:56:23.083+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:56:23.083+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:56:23.223+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 [[34m2024-11-27T14:56:23.233+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:56:23.343+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 [[34m2024-11-27T14:56:23.468+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 Data loaded successfully
 [[34m2024-11-27T14:56:23.683+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 Data loaded successfully
 [[34m2024-11-27T14:56:23.694+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:56:23.698+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 [[34m2024-11-27T14:56:23.706+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 [[34m2024-11-27T14:56:23.710+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 Data loaded successfully
 [[34m2024-11-27T14:56:23.798+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:56:23.810+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 [[34m2024-11-27T14:56:23.817+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 Data loaded successfully
 [[34m2024-11-27T14:56:23.929+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:56:23.942+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 [[34m2024-11-27T14:56:24.066+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-09-26 01:00:00+00:00, run_after=2024-09-27 01:00:00+00:00[0m
 [[34m2024-11-27T14:56:24.430+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 Data loaded successfully
 [[34m2024-11-27T14:56:24.518+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:56:24.530+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 [[34m2024-11-27T14:56:24.573+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.3501597710419446[0m
 [[34m2024-11-27T14:56:24.573+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|9853]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:56:24.574+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:56:24.574+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.3487797970883548[0m
 [[34m2024-11-27T14:56:24.574+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:56:24.574+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:56:24.574+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|9852]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:56:24.575+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|9853]: It took 0.892s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:56:24.575+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:56:24.575+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:56:24.575+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:56:24.576+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|9852]: It took 0.882s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:56:24.577+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|9853]: It took 0.00272s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:56:24.578+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|9852]: It took 0.00281s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:56:24.611+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.load_treasury_data_task scheduled__2024-09-22T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [[34m2024-11-27T14:56:24.611+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.dim_date_run scheduled__2024-09-21T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [[34m2024-11-27T14:56:24.648+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.33560818899422884[0m
 [[34m2024-11-27T14:56:24.649+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|9851]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:56:24.649+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:56:24.649+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:56:24.649+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:56:24.650+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|9851]: It took 0.851s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:56:24.652+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|9851]: It took 0.00256s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:56:24.694+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.load_polygon_data_task scheduled__2024-09-23T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [[34m2024-11-27T14:56:24.750+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T14:56:24.925+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.47255318192765117[0m
 [[34m2024-11-27T14:56:24.925+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|9855]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:56:24.926+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:56:24.927+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:56:24.927+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:56:24.927+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|9855]: It took 0.998s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:56:24.930+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|9855]: It took 0.00322s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:56:24.983+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run scheduled__2024-09-21T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [[34m2024-11-27T14:56:24.998+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [2024-11-27T14:56:25.088+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=load_treasury_data_task, dagrun_id=scheduled__2024-09-22T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:56:24.690376+00:00, try_number=1, job_id=890, op_classpath=airflow.operators.python.PythonOperator
 [2024-11-27T14:56:25.196+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.dim_date_run, dagrun_id=scheduled__2024-09-21T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:56:24.690119+00:00, try_number=1, job_id=891, op_classpath=cosmos.operators.local.DbtRunLocalOperator, cosmos.operators.base.DbtRunMixin, cosmos.operators.local.DbtLocalBaseOperator, cosmos.operators.base.AbstractDbtBaseOperator
 [2024-11-27T14:56:25.224+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=load_polygon_data_task, dagrun_id=scheduled__2024-09-23T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:56:24.811948+00:00, try_number=1, job_id=892, op_classpath=airflow.operators.python.PythonOperator
 [[34m2024-11-27T14:56:25.260+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-09-27 01:00:00+00:00, run_after=2024-09-28 01:00:00+00:00[0m
 [2024-11-27T14:56:25.547+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run, dagrun_id=scheduled__2024-09-21T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:56:25.094357+00:00, try_number=1, job_id=893, op_classpath=cosmos.operators.local.DbtRunLocalOperator, cosmos.operators.base.DbtRunMixin, cosmos.operators.local.DbtLocalBaseOperator, cosmos.operators.base.AbstractDbtBaseOperator
 [[34m2024-11-27T14:56:25.547+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.48148288601078093[0m
 [[34m2024-11-27T14:56:25.547+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|9854]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:56:25.548+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:56:25.548+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:56:25.548+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:56:25.549+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|9854]: It took 1.03s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:56:25.551+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|9854]: It took 0.00228s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:56:25.588+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_daily_bars_run scheduled__2024-09-21T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [2024-11-27T14:56:26.084+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.stg_daily_bars_run, dagrun_id=scheduled__2024-09-21T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:56:25.662154+00:00, try_number=1, job_id=894, op_classpath=cosmos.operators.local.DbtRunLocalOperator, cosmos.operators.base.DbtRunMixin, cosmos.operators.local.DbtLocalBaseOperator, cosmos.operators.base.AbstractDbtBaseOperator
 [[34m2024-11-27T14:56:26.118+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:56:26.280+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:56:26.382+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 [[34m2024-11-27T14:56:26.501+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-09-28 01:00:00+00:00, run_after=2024-09-29 01:00:00+00:00[0m
 [[34m2024-11-27T14:56:26.712+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 5 tasks up for execution:
 	<TaskInstance: capstone_andres_dag_.load_polygon_data_task scheduled__2024-09-24T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.load_treasury_data_task scheduled__2024-09-23T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.dim_date_run scheduled__2024-09-22T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_daily_bars_run scheduled__2024-09-22T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run scheduled__2024-09-22T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:56:26.712+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 4/16 running and queued tasks[0m
 [[34m2024-11-27T14:56:26.712+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 5/16 running and queued tasks[0m
 [[34m2024-11-27T14:56:26.713+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 6/16 running and queued tasks[0m
 [[34m2024-11-27T14:56:26.713+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 7/16 running and queued tasks[0m
 [[34m2024-11-27T14:56:26.713+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 8/16 running and queued tasks[0m
 [[34m2024-11-27T14:56:26.714+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
 	<TaskInstance: capstone_andres_dag_.load_polygon_data_task scheduled__2024-09-24T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.load_treasury_data_task scheduled__2024-09-23T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.dim_date_run scheduled__2024-09-22T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_daily_bars_run scheduled__2024-09-22T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run scheduled__2024-09-22T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:56:26.717+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='load_polygon_data_task', run_id='scheduled__2024-09-24T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
 [[34m2024-11-27T14:56:26.717+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'load_polygon_data_task', 'scheduled__2024-09-24T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:56:26.717+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='load_treasury_data_task', run_id='scheduled__2024-09-23T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 5 and queue default[0m
 [[34m2024-11-27T14:56:26.718+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'load_treasury_data_task', 'scheduled__2024-09-23T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:56:26.718+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.dim_date_run', run_id='scheduled__2024-09-22T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
 [[34m2024-11-27T14:56:26.718+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.dim_date_run', 'scheduled__2024-09-22T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:56:26.718+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.stg_daily_bars_run', run_id='scheduled__2024-09-22T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
 [[34m2024-11-27T14:56:26.718+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.stg_daily_bars_run', 'scheduled__2024-09-22T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:56:26.719+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run', run_id='scheduled__2024-09-22T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
 [[34m2024-11-27T14:56:26.719+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run', 'scheduled__2024-09-22T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:56:26.721+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'load_polygon_data_task', 'scheduled__2024-09-24T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:56:26.722+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'load_treasury_data_task', 'scheduled__2024-09-23T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:56:26.722+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.dim_date_run', 'scheduled__2024-09-22T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:56:26.723+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.stg_daily_bars_run', 'scheduled__2024-09-22T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:56:26.723+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run', 'scheduled__2024-09-22T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:56:26.725+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='load_polygon_data_task', run_id='scheduled__2024-09-23T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:56:26.725+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='load_treasury_data_task', run_id='scheduled__2024-09-22T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:56:26.732+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=load_treasury_data_task, run_id=scheduled__2024-09-22T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:56:24.690376+00:00, run_end_date=2024-11-27 14:56:26.045237+00:00, run_duration=1.354861, state=success, executor_state=success, try_number=1, max_tries=0, job_id=890, pool=default_pool, queue=default, priority_weight=5, operator=PythonOperator, queued_dttm=2024-11-27 14:56:13.706066+00:00, queued_by_job_id=803, pid=9986[0m
 [[34m2024-11-27T14:56:26.733+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=load_polygon_data_task, run_id=scheduled__2024-09-23T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:56:24.811948+00:00, run_end_date=2024-11-27 14:56:25.900647+00:00, run_duration=1.088699, state=success, executor_state=success, try_number=1, max_tries=0, job_id=892, pool=default_pool, queue=default, priority_weight=6, operator=PythonOperator, queued_dttm=2024-11-27 14:56:13.706066+00:00, queued_by_job_id=803, pid=9987[0m
 [[34m2024-11-27T14:56:26.757+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-09-29 01:00:00+00:00, run_after=2024-09-30 01:00:00+00:00[0m
 [[34m2024-11-27T14:56:26.756+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:56:26.758+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:56:26.761+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:56:26.771+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:56:26.773+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 Data loaded successfully
 [[34m2024-11-27T14:56:27.124+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:56:27.142+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 [[34m2024-11-27T14:56:27.998+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:56:27.999+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:56:27.999+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:56:28.000+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:56:28.001+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:56:28.001+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:56:28.004+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:56:28.005+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:56:28.005+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:56:28.005+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:56:28.006+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:56:28.006+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:56:28.045+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:56:28.045+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:56:28.046+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:56:28.067+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-09-30 01:00:00+00:00, run_after=2024-10-01 01:00:00+00:00[0m
 [[34m2024-11-27T14:56:28.396+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.5405098469927907[0m
 [[34m2024-11-27T14:56:28.396+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|9933]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:56:28.397+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:56:28.397+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:56:28.398+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:56:28.398+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|9933]: It took 1.27s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:56:28.401+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|9933]: It took 0.00301s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:56:28.451+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.weekly_metrics_run scheduled__2024-09-20T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [[34m2024-11-27T14:56:28.455+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412096602,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T14:56:28.518+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412096610,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T14:56:28.522+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412096606,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T14:56:28.528+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412095182,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T14:56:28.535+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412095178,
 "os.name" : Linux
 [0m
 [2024-11-27T14:56:29.038+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.weekly_metrics_run, dagrun_id=scheduled__2024-09-20T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:56:28.541943+00:00, try_number=1, job_id=895, op_classpath=cosmos.operators.local.DbtRunLocalOperator, cosmos.operators.base.DbtRunMixin, cosmos.operators.local.DbtLocalBaseOperator, cosmos.operators.base.AbstractDbtBaseOperator
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:56:29.278+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:56:29.278+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:56:29.278+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:56:29.281+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-10-01 01:00:00+00:00, run_after=2024-10-02 01:00:00+00:00[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:56:29.293+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:56:29.481+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 [[34m2024-11-27T14:56:29.838+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 [[34m2024-11-27T14:56:30.030+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:56:30.127+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 [[34m2024-11-27T14:56:30.281+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:56:30.466+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:56:30.499+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-10-02 01:00:00+00:00, run_after=2024-10-03 01:00:00+00:00[0m
 [[34m2024-11-27T14:56:31.742+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-10-03 01:00:00+00:00, run_after=2024-10-04 01:00:00+00:00[0m
 /usr/local/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked semaphore objects to clean up at shutdown
   warnings.warn('resource_tracker: There appear to be %d '
 [[34m2024-11-27T14:56:32.467+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:56:32.550+0000[0m] {[34mscheduler_job_runner.py:[0m1328} INFO[0m - DAG capstone_andres_dag_ is at (or above) max_active_runs (16 of 16), not creating any more runs[0m
 [[34m2024-11-27T14:56:32.612+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 /usr/local/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked semaphore objects to clean up at shutdown
   warnings.warn('resource_tracker: There appear to be %d '
 /usr/local/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked semaphore objects to clean up at shutdown
   warnings.warn('resource_tracker: There appear to be %d '
 [[34m2024-11-27T14:56:32.787+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T14:56:32.818+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run', run_id='scheduled__2024-09-21T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:56:32.818+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.stg_daily_bars_run', run_id='scheduled__2024-09-21T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:56:32.818+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.dim_date_run', run_id='scheduled__2024-09-21T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:56:32.822+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.stg_daily_bars_run, run_id=scheduled__2024-09-21T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:56:25.662154+00:00, run_end_date=2024-11-27 14:56:32.669902+00:00, run_duration=7.007748, state=success, executor_state=success, try_number=1, max_tries=0, job_id=894, pool=default_pool, queue=default, priority_weight=2, operator=DbtRunLocalOperator, queued_dttm=2024-11-27 14:56:13.706066+00:00, queued_by_job_id=803, pid=9989[0m
 [[34m2024-11-27T14:56:32.822+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run, run_id=scheduled__2024-09-21T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:56:25.094357+00:00, run_end_date=2024-11-27 14:56:32.021108+00:00, run_duration=6.926751, state=success, executor_state=success, try_number=1, max_tries=0, job_id=893, pool=default_pool, queue=default, priority_weight=2, operator=DbtRunLocalOperator, queued_dttm=2024-11-27 14:56:13.706066+00:00, queued_by_job_id=803, pid=9988[0m
 [[34m2024-11-27T14:56:32.822+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.dim_date_run, run_id=scheduled__2024-09-21T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:56:24.690119+00:00, run_end_date=2024-11-27 14:56:32.690875+00:00, run_duration=8.000756, state=success, executor_state=success, try_number=1, max_tries=0, job_id=891, pool=default_pool, queue=default, priority_weight=2, operator=DbtRunLocalOperator, queued_dttm=2024-11-27 14:56:13.706066+00:00, queued_by_job_id=803, pid=9985[0m
 [[34m2024-11-27T14:56:32.828+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:56:32.901+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T14:56:33.065+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:56:33.088+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:56:33.127+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T14:56:33.137+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:56:33.163+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:56:33.433+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T14:56:33.441+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T14:56:33.452+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.weekly_metrics_run scheduled__2024-09-21T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:56:33.452+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 6/16 running and queued tasks[0m
 [[34m2024-11-27T14:56:33.452+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.weekly_metrics_run scheduled__2024-09-21T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:56:33.454+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.weekly_metrics_run', run_id='scheduled__2024-09-21T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
 [[34m2024-11-27T14:56:33.454+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.weekly_metrics_run', 'scheduled__2024-09-21T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:56:33.456+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.weekly_metrics_run', 'scheduled__2024-09-21T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:56:33.481+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:56:33.514+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:56:33.692+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:56:33.695+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:56:34.361+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:56:34.475+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:56:34.476+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:56:34.476+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:56:34.492+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:56:34.548+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:56:34.623+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:56:34.658+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 [[34m2024-11-27T14:56:34.671+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:56:34.770+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 [[34m2024-11-27T14:56:34.778+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:56:34.916+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 [[34m2024-11-27T14:56:34.920+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412096630,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T14:56:34.932+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:56:34.974+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:56:35.150+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:56:35.159+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 Data loaded successfully
 [[34m2024-11-27T14:56:35.163+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:56:35.188+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 [[34m2024-11-27T14:56:35.256+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 [[34m2024-11-27T14:56:35.262+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 Data loaded successfully
 [[34m2024-11-27T14:56:35.271+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:56:35.294+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 /usr/local/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked semaphore objects to clean up at shutdown
   warnings.warn('resource_tracker: There appear to be %d '
 Data loaded successfully
 [[34m2024-11-27T14:56:35.437+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:56:35.451+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:56:35.669+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 Data loaded successfully
 [[34m2024-11-27T14:56:35.786+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:56:35.798+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 Data loaded successfully
 [[34m2024-11-27T14:56:35.832+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:56:35.843+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 [[34m2024-11-27T14:56:35.973+0000[0m] {[34mdagrun.py:[0m851} INFO[0m - Marking run <DagRun capstone_andres_dag_ @ 2024-09-20 01:00:00+00:00: scheduled__2024-09-20T01:00:00+00:00, state:running, queued_at: 2024-11-27 14:52:06.412766+00:00. externally triggered: False> successful[0m
 [[34m2024-11-27T14:56:35.973+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=capstone_andres_dag_, execution_date=2024-09-20 01:00:00+00:00, run_id=scheduled__2024-09-20T01:00:00+00:00, run_start_date=2024-11-27 14:52:06.427638+00:00, run_end_date=2024-11-27 14:56:35.973900+00:00, run_duration=269.546262, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-09-20 01:00:00+00:00, data_interval_end=2024-09-21 01:00:00+00:00, dag_hash=77fa8f64adac9391cccaaae88e2be5d8[0m
 [[34m2024-11-27T14:56:35.979+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-09-21 01:00:00+00:00, run_after=2024-09-22 01:00:00+00:00[0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 [[34m2024-11-27T14:56:36.315+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:56:36.557+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.8251979991327971[0m
 [[34m2024-11-27T14:56:36.558+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|9992]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:56:36.559+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:56:36.560+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:56:36.560+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:56:36.561+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|9992]: It took 1.4s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:56:36.573+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|9992]: It took 0.0118s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:56:36.674+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.weekly_metrics_run', run_id='scheduled__2024-09-20T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:56:36.682+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.dim_date_run scheduled__2024-09-22T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [[34m2024-11-27T14:56:36.685+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.weekly_metrics_run, run_id=scheduled__2024-09-20T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:56:28.541943+00:00, run_end_date=2024-11-27 14:56:35.288296+00:00, run_duration=6.746353, state=success, executor_state=success, try_number=1, max_tries=0, job_id=895, pool=default_pool, queue=default, priority_weight=1, operator=DbtRunLocalOperator, queued_dttm=2024-11-27 14:56:18.995838+00:00, queued_by_job_id=803, pid=10004[0m
 [[34m2024-11-27T14:56:36.822+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.975639611016959[0m
 [[34m2024-11-27T14:56:36.823+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|9991]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:56:36.825+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:56:36.827+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:56:36.827+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:56:36.830+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|9991]: It took 1.56s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:56:36.841+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|9991]: It took 0.0113s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:56:36.933+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_daily_bars_run scheduled__2024-09-22T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [[34m2024-11-27T14:56:37.155+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 1.1798611260019243[0m
 [[34m2024-11-27T14:56:37.156+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|9994]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:56:37.158+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:56:37.158+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:56:37.158+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:56:37.159+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|9994]: It took 1.72s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:56:37.165+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|9994]: It took 0.0065s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:56:37.246+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run scheduled__2024-09-22T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [[34m2024-11-27T14:56:37.441+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.756843555951491[0m
 [[34m2024-11-27T14:56:37.442+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|9993]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:56:37.443+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:56:37.443+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:56:37.444+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:56:37.444+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|9993]: It took 1.61s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:56:37.447+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.7623036799486727[0m
 [[34m2024-11-27T14:56:37.449+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|9990]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:56:37.450+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:56:37.450+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:56:37.451+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:56:37.452+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|9990]: It took 1.67s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:56:37.453+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|9993]: It took 0.00883s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:56:37.460+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|9990]: It took 0.0085s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:56:37.535+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.load_treasury_data_task scheduled__2024-09-23T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [[34m2024-11-27T14:56:37.556+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.load_polygon_data_task scheduled__2024-09-24T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [2024-11-27T14:56:37.576+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.dim_date_run, dagrun_id=scheduled__2024-09-22T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:56:36.871100+00:00, try_number=1, job_id=896, op_classpath=cosmos.operators.local.DbtRunLocalOperator, cosmos.operators.base.DbtRunMixin, cosmos.operators.local.DbtLocalBaseOperator, cosmos.operators.base.AbstractDbtBaseOperator
 [[34m2024-11-27T14:56:37.712+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-09-22 01:00:00+00:00, run_after=2024-09-23 01:00:00+00:00[0m
 [2024-11-27T14:56:37.838+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.stg_daily_bars_run, dagrun_id=scheduled__2024-09-22T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:56:37.078077+00:00, try_number=1, job_id=897, op_classpath=cosmos.operators.local.DbtRunLocalOperator, cosmos.operators.base.DbtRunMixin, cosmos.operators.local.DbtLocalBaseOperator, cosmos.operators.base.AbstractDbtBaseOperator
 [2024-11-27T14:56:38.169+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run, dagrun_id=scheduled__2024-09-22T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:56:37.378413+00:00, try_number=1, job_id=898, op_classpath=cosmos.operators.local.DbtRunLocalOperator, cosmos.operators.base.DbtRunMixin, cosmos.operators.local.DbtLocalBaseOperator, cosmos.operators.base.AbstractDbtBaseOperator
 [2024-11-27T14:56:38.300+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=load_treasury_data_task, dagrun_id=scheduled__2024-09-23T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:56:37.703715+00:00, try_number=1, job_id=899, op_classpath=airflow.operators.python.PythonOperator
 [2024-11-27T14:56:38.326+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=load_polygon_data_task, dagrun_id=scheduled__2024-09-24T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:56:37.748893+00:00, try_number=1, job_id=900, op_classpath=airflow.operators.python.PythonOperator
 [[34m2024-11-27T14:56:38.483+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-09-23 01:00:00+00:00, run_after=2024-09-24 01:00:00+00:00[0m
 [[34m2024-11-27T14:56:39.207+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:56:39.516+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T14:56:39.807+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-09-24 01:00:00+00:00, run_after=2024-09-25 01:00:00+00:00[0m
 [[34m2024-11-27T14:56:39.819+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:56:40.106+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.dim_date_run scheduled__2024-09-23T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_daily_bars_run scheduled__2024-09-23T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run scheduled__2024-09-23T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:56:40.106+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 5/16 running and queued tasks[0m
 [[34m2024-11-27T14:56:40.107+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 6/16 running and queued tasks[0m
 [[34m2024-11-27T14:56:40.107+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 7/16 running and queued tasks[0m
 [[34m2024-11-27T14:56:40.107+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.dim_date_run scheduled__2024-09-23T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_daily_bars_run scheduled__2024-09-23T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run scheduled__2024-09-23T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:56:40.111+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.dim_date_run', run_id='scheduled__2024-09-23T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
 [[34m2024-11-27T14:56:40.111+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.dim_date_run', 'scheduled__2024-09-23T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:56:40.112+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.stg_daily_bars_run', run_id='scheduled__2024-09-23T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
 [[34m2024-11-27T14:56:40.112+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.stg_daily_bars_run', 'scheduled__2024-09-23T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:56:40.113+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run', run_id='scheduled__2024-09-23T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
 [[34m2024-11-27T14:56:40.113+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run', 'scheduled__2024-09-23T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:56:40.116+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.dim_date_run', 'scheduled__2024-09-23T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:56:40.118+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.stg_daily_bars_run', 'scheduled__2024-09-23T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:56:40.119+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='load_treasury_data_task', run_id='scheduled__2024-09-23T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:56:40.118+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run', 'scheduled__2024-09-23T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:56:40.125+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=load_treasury_data_task, run_id=scheduled__2024-09-23T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:56:37.703715+00:00, run_end_date=2024-11-27 14:56:39.238870+00:00, run_duration=1.535155, state=success, executor_state=success, try_number=1, max_tries=0, job_id=899, pool=default_pool, queue=default, priority_weight=5, operator=PythonOperator, queued_dttm=2024-11-27 14:56:26.714804+00:00, queued_by_job_id=803, pid=10115[0m
 [[34m2024-11-27T14:56:40.153+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-09-25 01:00:00+00:00, run_after=2024-09-26 01:00:00+00:00[0m
 [[34m2024-11-27T14:56:40.160+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:56:40.163+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:56:40.163+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:56:40.904+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:56:41.208+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:56:41.308+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 [[34m2024-11-27T14:56:41.337+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:56:41.338+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:56:41.338+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:56:41.342+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:56:41.342+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:56:41.342+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:56:41.357+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:56:41.358+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:56:41.358+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:56:41.570+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-09-26 01:00:00+00:00, run_after=2024-09-27 01:00:00+00:00[0m
 Data loaded successfully
 [[34m2024-11-27T14:56:41.864+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:56:41.871+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412096638,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T14:56:41.871+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412096642,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T14:56:41.881+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 [[34m2024-11-27T14:56:41.893+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412095230,
 "os.name" : Linux
 [0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:56:42.622+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:56:42.631+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:56:42.683+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:56:42.848+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-09-27 01:00:00+00:00, run_after=2024-09-28 01:00:00+00:00[0m
 [[34m2024-11-27T14:56:42.872+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.40613261912949383[0m
 [[34m2024-11-27T14:56:42.872+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|10083]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:56:42.873+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:56:42.873+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:56:42.874+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:56:42.874+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|10083]: It took 1.01s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:56:42.880+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|10083]: It took 0.00634s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:56:42.944+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.weekly_metrics_run scheduled__2024-09-21T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 [[34m2024-11-27T14:56:43.214+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 [[34m2024-11-27T14:56:43.325+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:56:43.430+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [2024-11-27T14:56:43.536+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.weekly_metrics_run, dagrun_id=scheduled__2024-09-21T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:56:43.069816+00:00, try_number=1, job_id=901, op_classpath=cosmos.operators.local.DbtRunLocalOperator, cosmos.operators.base.DbtRunMixin, cosmos.operators.local.DbtLocalBaseOperator, cosmos.operators.base.AbstractDbtBaseOperator
 [[34m2024-11-27T14:56:43.812+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-09-28 01:00:00+00:00, run_after=2024-09-29 01:00:00+00:00[0m
 [[34m2024-11-27T14:56:44.586+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:56:44.918+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T14:56:45.021+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:56:45.269+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-09-29 01:00:00+00:00, run_after=2024-09-30 01:00:00+00:00[0m
 [[34m2024-11-27T14:56:45.312+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:56:45.356+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 /usr/local/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked semaphore objects to clean up at shutdown
   warnings.warn('resource_tracker: There appear to be %d '
 /usr/local/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked semaphore objects to clean up at shutdown
   warnings.warn('resource_tracker: There appear to be %d '
 [[34m2024-11-27T14:56:45.598+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:56:45.742+0000[0m] {[34mscheduler_job_runner.py:[0m1328} INFO[0m - DAG capstone_andres_dag_ is at (or above) max_active_runs (16 of 16), not creating any more runs[0m
 [[34m2024-11-27T14:56:45.921+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 2 tasks up for execution:
 	<TaskInstance: capstone_andres_dag_.load_polygon_data_task scheduled__2024-09-25T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.load_treasury_data_task scheduled__2024-09-24T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:56:45.921+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 5/16 running and queued tasks[0m
 [[34m2024-11-27T14:56:45.921+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 6/16 running and queued tasks[0m
 [[34m2024-11-27T14:56:45.921+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
 	<TaskInstance: capstone_andres_dag_.load_polygon_data_task scheduled__2024-09-25T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.load_treasury_data_task scheduled__2024-09-24T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:56:45.923+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='load_polygon_data_task', run_id='scheduled__2024-09-25T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
 [[34m2024-11-27T14:56:45.923+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'load_polygon_data_task', 'scheduled__2024-09-25T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:56:45.923+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='load_treasury_data_task', run_id='scheduled__2024-09-24T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 5 and queue default[0m
 [[34m2024-11-27T14:56:45.924+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'load_treasury_data_task', 'scheduled__2024-09-24T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:56:45.925+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'load_polygon_data_task', 'scheduled__2024-09-25T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:56:45.926+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'load_treasury_data_task', 'scheduled__2024-09-24T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:56:45.928+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run', run_id='scheduled__2024-09-22T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:56:45.928+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.stg_daily_bars_run', run_id='scheduled__2024-09-22T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:56:45.928+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='load_polygon_data_task', run_id='scheduled__2024-09-24T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:56:45.933+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run, run_id=scheduled__2024-09-22T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:56:37.378413+00:00, run_end_date=2024-11-27 14:56:45.435512+00:00, run_duration=8.057099, state=success, executor_state=success, try_number=1, max_tries=0, job_id=898, pool=default_pool, queue=default, priority_weight=2, operator=DbtRunLocalOperator, queued_dttm=2024-11-27 14:56:26.714804+00:00, queued_by_job_id=803, pid=10105[0m
 [[34m2024-11-27T14:56:45.933+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.stg_daily_bars_run, run_id=scheduled__2024-09-22T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:56:37.078077+00:00, run_end_date=2024-11-27 14:56:45.522143+00:00, run_duration=8.444066, state=success, executor_state=success, try_number=1, max_tries=0, job_id=897, pool=default_pool, queue=default, priority_weight=2, operator=DbtRunLocalOperator, queued_dttm=2024-11-27 14:56:26.714804+00:00, queued_by_job_id=803, pid=10103[0m
 [[34m2024-11-27T14:56:45.934+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=load_polygon_data_task, run_id=scheduled__2024-09-24T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:56:37.748893+00:00, run_end_date=2024-11-27 14:56:45.534498+00:00, run_duration=7.785605, state=success, executor_state=success, try_number=1, max_tries=0, job_id=900, pool=default_pool, queue=default, priority_weight=6, operator=PythonOperator, queued_dttm=2024-11-27 14:56:26.714804+00:00, queued_by_job_id=803, pid=10116[0m
 [[34m2024-11-27T14:56:45.952+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:56:45.952+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:56:46.282+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:56:46.484+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:56:46.592+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T14:56:46.654+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:56:46.736+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 /usr/local/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked semaphore objects to clean up at shutdown
   warnings.warn('resource_tracker: There appear to be %d '
 [[34m2024-11-27T14:56:46.775+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 [[34m2024-11-27T14:56:46.845+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:56:46.932+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:56:47.064+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 [[34m2024-11-27T14:56:47.070+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:56:47.071+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:56:47.072+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:56:47.161+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:56:47.162+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:56:47.163+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 Data loaded successfully
 [[34m2024-11-27T14:56:47.340+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:56:47.352+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 Data loaded successfully
 [[34m2024-11-27T14:56:47.472+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:56:47.479+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.weekly_metrics_run scheduled__2024-09-22T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:56:47.480+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 6/16 running and queued tasks[0m
 [[34m2024-11-27T14:56:47.480+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.weekly_metrics_run scheduled__2024-09-22T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:56:47.481+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.weekly_metrics_run', run_id='scheduled__2024-09-22T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
 [[34m2024-11-27T14:56:47.482+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.weekly_metrics_run', 'scheduled__2024-09-22T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:56:47.483+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 [[34m2024-11-27T14:56:47.483+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.weekly_metrics_run', 'scheduled__2024-09-22T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:56:47.484+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.dim_date_run', run_id='scheduled__2024-09-22T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:56:47.488+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.dim_date_run, run_id=scheduled__2024-09-22T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:56:36.871100+00:00, run_end_date=2024-11-27 14:56:46.729782+00:00, run_duration=9.858682, state=success, executor_state=success, try_number=1, max_tries=0, job_id=896, pool=default_pool, queue=default, priority_weight=2, operator=DbtRunLocalOperator, queued_dttm=2024-11-27 14:56:26.714804+00:00, queued_by_job_id=803, pid=10101[0m
 [[34m2024-11-27T14:56:47.510+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:56:47.548+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412096674,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T14:56:47.677+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412096678,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T14:56:47.837+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:56:47.986+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:56:48.091+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:56:48.234+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:56:48.464+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:56:48.465+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:56:48.465+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 [[34m2024-11-27T14:56:48.511+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.5779254960361868[0m
 [[34m2024-11-27T14:56:48.511+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|10125]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:56:48.511+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:56:48.512+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:56:48.512+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:56:48.512+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|10125]: It took 1.17s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:56:48.514+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|10125]: It took 0.00221s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:56:48.521+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.4598408208694309[0m
 [[34m2024-11-27T14:56:48.521+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|10126]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:56:48.522+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:56:48.521+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:56:48.522+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:56:48.522+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:56:48.522+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|10126]: It took 1.05s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:56:48.526+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|10126]: It took 0.00318s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:56:48.566+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run scheduled__2024-09-23T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [[34m2024-11-27T14:56:48.575+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_daily_bars_run scheduled__2024-09-23T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [[34m2024-11-27T14:56:48.675+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 Data loaded successfully
 [[34m2024-11-27T14:56:48.892+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:56:48.908+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 [[34m2024-11-27T14:56:48.928+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:56:49.000+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412096682,
 "os.name" : Linux
 [0m
 [2024-11-27T14:56:49.143+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run, dagrun_id=scheduled__2024-09-23T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:56:48.659904+00:00, try_number=1, job_id=902, op_classpath=cosmos.operators.local.DbtRunLocalOperator, cosmos.operators.base.DbtRunMixin, cosmos.operators.local.DbtLocalBaseOperator, cosmos.operators.base.AbstractDbtBaseOperator
 [2024-11-27T14:56:49.149+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.stg_daily_bars_run, dagrun_id=scheduled__2024-09-23T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:56:48.668069+00:00, try_number=1, job_id=903, op_classpath=cosmos.operators.local.DbtRunLocalOperator, cosmos.operators.base.DbtRunMixin, cosmos.operators.local.DbtLocalBaseOperator, cosmos.operators.base.AbstractDbtBaseOperator
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:56:49.746+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:56:49.821+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.32260002195835114[0m
 [[34m2024-11-27T14:56:49.821+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|10124]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:56:49.821+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:56:49.822+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:56:49.822+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:56:49.822+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|10124]: It took 0.93s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:56:49.824+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|10124]: It took 0.00217s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:56:49.856+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.dim_date_run scheduled__2024-09-23T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 /usr/local/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked semaphore objects to clean up at shutdown
   warnings.warn('resource_tracker: There appear to be %d '
 [2024-11-27T14:56:50.333+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.dim_date_run, dagrun_id=scheduled__2024-09-23T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:56:49.933989+00:00, try_number=1, job_id=904, op_classpath=cosmos.operators.local.DbtRunLocalOperator, cosmos.operators.base.DbtRunMixin, cosmos.operators.local.DbtLocalBaseOperator, cosmos.operators.base.AbstractDbtBaseOperator
 [[34m2024-11-27T14:56:50.431+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:56:50.563+0000[0m] {[34mdagrun.py:[0m851} INFO[0m - Marking run <DagRun capstone_andres_dag_ @ 2024-09-21 01:00:00+00:00: scheduled__2024-09-21T01:00:00+00:00, state:running, queued_at: 2024-11-27 14:52:07.241689+00:00. externally triggered: False> successful[0m
 [[34m2024-11-27T14:56:50.563+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=capstone_andres_dag_, execution_date=2024-09-21 01:00:00+00:00, run_id=scheduled__2024-09-21T01:00:00+00:00, run_start_date=2024-11-27 14:52:07.255001+00:00, run_end_date=2024-11-27 14:56:50.563740+00:00, run_duration=283.308739, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-09-21 01:00:00+00:00, data_interval_end=2024-09-22 01:00:00+00:00, dag_hash=77fa8f64adac9391cccaaae88e2be5d8[0m
 [[34m2024-11-27T14:56:50.569+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-09-22 01:00:00+00:00, run_after=2024-09-23 01:00:00+00:00[0m
 [[34m2024-11-27T14:56:50.782+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.weekly_metrics_run', run_id='scheduled__2024-09-21T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:56:50.786+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.weekly_metrics_run, run_id=scheduled__2024-09-21T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:56:43.069816+00:00, run_end_date=2024-11-27 14:56:50.247731+00:00, run_duration=7.177915, state=success, executor_state=success, try_number=1, max_tries=0, job_id=901, pool=default_pool, queue=default, priority_weight=1, operator=DbtRunLocalOperator, queued_dttm=2024-11-27 14:56:33.453122+00:00, queued_by_job_id=803, pid=10146[0m
 [[34m2024-11-27T14:56:51.334+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:56:51.635+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T14:56:51.699+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:56:51.807+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-09-23 01:00:00+00:00, run_after=2024-09-24 01:00:00+00:00[0m
 [[34m2024-11-27T14:56:51.950+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:56:52.031+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T14:56:52.303+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:56:53.024+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:56:53.040+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-09-24 01:00:00+00:00, run_after=2024-09-25 01:00:00+00:00[0m
 [[34m2024-11-27T14:56:53.080+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:56:53.226+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:56:53.331+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 [[34m2024-11-27T14:56:53.475+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T14:56:53.638+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:56:53.822+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:56:53.850+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:56:53.947+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 Data loaded successfully
 [[34m2024-11-27T14:56:54.136+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:56:54.148+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 [[34m2024-11-27T14:56:54.245+0000[0m] {[34mscheduler_job_runner.py:[0m1328} INFO[0m - DAG capstone_andres_dag_ is at (or above) max_active_runs (16 of 16), not creating any more runs[0m
 Data loaded successfully
 [[34m2024-11-27T14:56:54.660+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:56:54.673+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 [[34m2024-11-27T14:56:54.901+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:56:55.066+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:56:55.112+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.4293519319035113[0m
 [[34m2024-11-27T14:56:55.113+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|10200]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:56:55.113+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:56:55.114+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:56:55.114+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:56:55.114+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|10200]: It took 0.977s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:56:55.120+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|10200]: It took 0.00637s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:56:55.177+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 [[34m2024-11-27T14:56:55.179+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.load_treasury_data_task scheduled__2024-09-24T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 /usr/local/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked semaphore objects to clean up at shutdown
   warnings.warn('resource_tracker: There appear to be %d '
 /usr/local/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked semaphore objects to clean up at shutdown
   warnings.warn('resource_tracker: There appear to be %d '
 [2024-11-27T14:56:55.611+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=load_treasury_data_task, dagrun_id=scheduled__2024-09-24T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:56:55.281523+00:00, try_number=1, job_id=905, op_classpath=airflow.operators.python.PythonOperator
 [[34m2024-11-27T14:56:55.624+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.3881990930531174[0m
 [[34m2024-11-27T14:56:55.625+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|10199]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:56:55.625+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:56:55.625+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:56:55.626+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:56:55.626+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|10199]: It took 0.964s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:56:55.628+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|10199]: It took 0.00255s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:56:55.662+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.load_polygon_data_task scheduled__2024-09-25T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 Data loaded successfully
 [[34m2024-11-27T14:56:55.782+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:56:55.796+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 [[34m2024-11-27T14:56:56.032+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run', run_id='scheduled__2024-09-23T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:56:56.033+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.stg_daily_bars_run', run_id='scheduled__2024-09-23T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:56:56.036+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run, run_id=scheduled__2024-09-23T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:56:48.659904+00:00, run_end_date=2024-11-27 14:56:55.271342+00:00, run_duration=6.611438, state=success, executor_state=success, try_number=1, max_tries=0, job_id=902, pool=default_pool, queue=default, priority_weight=2, operator=DbtRunLocalOperator, queued_dttm=2024-11-27 14:56:40.108824+00:00, queued_by_job_id=803, pid=10229[0m
 [[34m2024-11-27T14:56:56.037+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.stg_daily_bars_run, run_id=scheduled__2024-09-23T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:56:48.668069+00:00, run_end_date=2024-11-27 14:56:55.292326+00:00, run_duration=6.624257, state=success, executor_state=success, try_number=1, max_tries=0, job_id=903, pool=default_pool, queue=default, priority_weight=2, operator=DbtRunLocalOperator, queued_dttm=2024-11-27 14:56:40.108824+00:00, queued_by_job_id=803, pid=10233[0m
 [2024-11-27T14:56:56.056+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=load_polygon_data_task, dagrun_id=scheduled__2024-09-25T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:56:55.735323+00:00, try_number=1, job_id=906, op_classpath=airflow.operators.python.PythonOperator
 [[34m2024-11-27T14:56:56.641+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.2759946009609848[0m
 [[34m2024-11-27T14:56:56.642+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|10212]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:56:56.642+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:56:56.642+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:56:56.643+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:56:56.643+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|10212]: It took 0.861s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:56:56.645+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|10212]: It took 0.00221s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:56:56.677+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.weekly_metrics_run scheduled__2024-09-22T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [2024-11-27T14:56:57.189+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.weekly_metrics_run, dagrun_id=scheduled__2024-09-22T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:56:56.751542+00:00, try_number=1, job_id=907, op_classpath=cosmos.operators.local.DbtRunLocalOperator, cosmos.operators.base.DbtRunMixin, cosmos.operators.local.DbtLocalBaseOperator, cosmos.operators.base.AbstractDbtBaseOperator
 /usr/local/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked semaphore objects to clean up at shutdown
   warnings.warn('resource_tracker: There appear to be %d '
 [[34m2024-11-27T14:56:58.514+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 4 tasks up for execution:
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_daily_bars_run scheduled__2024-09-24T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run scheduled__2024-09-24T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.dim_date_run scheduled__2024-09-24T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.weekly_metrics_run scheduled__2024-09-23T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:56:58.514+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 2/16 running and queued tasks[0m
 [[34m2024-11-27T14:56:58.515+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 3/16 running and queued tasks[0m
 [[34m2024-11-27T14:56:58.515+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 4/16 running and queued tasks[0m
 [[34m2024-11-27T14:56:58.515+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 5/16 running and queued tasks[0m
 [[34m2024-11-27T14:56:58.515+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_daily_bars_run scheduled__2024-09-24T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run scheduled__2024-09-24T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.dim_date_run scheduled__2024-09-24T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.weekly_metrics_run scheduled__2024-09-23T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:56:58.517+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.stg_daily_bars_run', run_id='scheduled__2024-09-24T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
 [[34m2024-11-27T14:56:58.518+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.stg_daily_bars_run', 'scheduled__2024-09-24T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:56:58.518+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run', run_id='scheduled__2024-09-24T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
 [[34m2024-11-27T14:56:58.518+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run', 'scheduled__2024-09-24T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:56:58.518+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.dim_date_run', run_id='scheduled__2024-09-24T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
 [[34m2024-11-27T14:56:58.518+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.dim_date_run', 'scheduled__2024-09-24T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:56:58.519+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.weekly_metrics_run', run_id='scheduled__2024-09-23T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
 [[34m2024-11-27T14:56:58.519+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.weekly_metrics_run', 'scheduled__2024-09-23T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:56:58.521+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.stg_daily_bars_run', 'scheduled__2024-09-24T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:56:58.521+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run', 'scheduled__2024-09-24T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:56:58.521+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.dim_date_run', 'scheduled__2024-09-24T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:56:58.522+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.weekly_metrics_run', 'scheduled__2024-09-23T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:56:58.523+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.dim_date_run', run_id='scheduled__2024-09-23T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:56:58.523+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='load_treasury_data_task', run_id='scheduled__2024-09-24T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:56:58.530+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.dim_date_run, run_id=scheduled__2024-09-23T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:56:49.933989+00:00, run_end_date=2024-11-27 14:56:57.312385+00:00, run_duration=7.378396, state=success, executor_state=success, try_number=1, max_tries=0, job_id=904, pool=default_pool, queue=default, priority_weight=2, operator=DbtRunLocalOperator, queued_dttm=2024-11-27 14:56:40.108824+00:00, queued_by_job_id=803, pid=10256[0m
 [[34m2024-11-27T14:56:58.531+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=load_treasury_data_task, run_id=scheduled__2024-09-24T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:56:55.281523+00:00, run_end_date=2024-11-27 14:56:57.936291+00:00, run_duration=2.654768, state=success, executor_state=success, try_number=1, max_tries=0, job_id=905, pool=default_pool, queue=default, priority_weight=5, operator=PythonOperator, queued_dttm=2024-11-27 14:56:45.922278+00:00, queued_by_job_id=803, pid=10319[0m
 [[34m2024-11-27T14:56:58.552+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:56:58.553+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:56:58.554+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:56:58.555+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:56:59.521+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:56:59.522+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:56:59.522+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:56:59.522+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:56:59.522+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:56:59.523+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:56:59.525+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:56:59.525+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:56:59.525+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:56:59.537+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:56:59.538+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:56:59.538+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:57:00.035+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412095286,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T14:57:00.036+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412095294,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T14:57:00.037+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412095290,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T14:57:00.053+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412095298,
 "os.name" : Linux
 [0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:57:00.755+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:57:00.755+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:57:00.776+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:57:00.841+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 [[34m2024-11-27T14:57:01.254+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:57:01.254+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:57:01.255+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:57:01.255+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:57:02.658+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:57:02.847+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.weekly_metrics'[0m
 [[34m2024-11-27T14:57:02.847+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections'[0m
 [[34m2024-11-27T14:57:02.847+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_payments'[0m
 [[34m2024-11-27T14:57:02.848+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_orders'[0m
 [[34m2024-11-27T14:57:02.848+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_daily_bars'[0m
 [[34m2024-11-27T14:57:02.995+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T14:57:03.194+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 /usr/local/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked semaphore objects to clean up at shutdown
   warnings.warn('resource_tracker: There appear to be %d '
 [[34m2024-11-27T14:57:03.819+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:57:03.935+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:57:03.986+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:57:04.123+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T14:57:04.219+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T14:57:04.298+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T14:57:04.303+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:57:04.331+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:57:04.440+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:57:04.465+0000[0m] {[34mdagrun.py:[0m851} INFO[0m - Marking run <DagRun capstone_andres_dag_ @ 2024-09-22 01:00:00+00:00: scheduled__2024-09-22T01:00:00+00:00, state:running, queued_at: 2024-11-27 14:52:08.037093+00:00. externally triggered: False> successful[0m
 [[34m2024-11-27T14:57:04.465+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=capstone_andres_dag_, execution_date=2024-09-22 01:00:00+00:00, run_id=scheduled__2024-09-22T01:00:00+00:00, run_start_date=2024-11-27 14:52:08.049674+00:00, run_end_date=2024-11-27 14:57:04.465641+00:00, run_duration=296.415967, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-09-22 01:00:00+00:00, data_interval_end=2024-09-23 01:00:00+00:00, dag_hash=77fa8f64adac9391cccaaae88e2be5d8[0m
 [[34m2024-11-27T14:57:04.466+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:57:04.470+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-09-23 01:00:00+00:00, run_after=2024-09-24 01:00:00+00:00[0m
 [[34m2024-11-27T14:57:04.536+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:57:04.552+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 [[34m2024-11-27T14:57:04.613+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 2 tasks up for execution:
 	<TaskInstance: capstone_andres_dag_.load_polygon_data_task scheduled__2024-09-26T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.load_treasury_data_task scheduled__2024-09-25T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:57:04.613+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 4/16 running and queued tasks[0m
 [[34m2024-11-27T14:57:04.613+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 5/16 running and queued tasks[0m
 [[34m2024-11-27T14:57:04.613+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
 	<TaskInstance: capstone_andres_dag_.load_polygon_data_task scheduled__2024-09-26T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.load_treasury_data_task scheduled__2024-09-25T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:57:04.615+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='load_polygon_data_task', run_id='scheduled__2024-09-26T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
 [[34m2024-11-27T14:57:04.615+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'load_polygon_data_task', 'scheduled__2024-09-26T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:57:04.615+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='load_treasury_data_task', run_id='scheduled__2024-09-25T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 5 and queue default[0m
 [[34m2024-11-27T14:57:04.615+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'load_treasury_data_task', 'scheduled__2024-09-25T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:57:04.617+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'load_polygon_data_task', 'scheduled__2024-09-26T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:57:04.618+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'load_treasury_data_task', 'scheduled__2024-09-25T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:57:04.619+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.weekly_metrics_run', run_id='scheduled__2024-09-22T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:57:04.620+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='load_polygon_data_task', run_id='scheduled__2024-09-25T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:57:04.624+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.weekly_metrics_run, run_id=scheduled__2024-09-22T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:56:56.751542+00:00, run_end_date=2024-11-27 14:57:03.289113+00:00, run_duration=6.537571, state=success, executor_state=success, try_number=1, max_tries=0, job_id=907, pool=default_pool, queue=default, priority_weight=1, operator=DbtRunLocalOperator, queued_dttm=2024-11-27 14:56:47.480798+00:00, queued_by_job_id=803, pid=10321[0m
 [[34m2024-11-27T14:57:04.624+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=load_polygon_data_task, run_id=scheduled__2024-09-25T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:56:55.735323+00:00, run_end_date=2024-11-27 14:57:03.632352+00:00, run_duration=7.897029, state=success, executor_state=success, try_number=1, max_tries=0, job_id=906, pool=default_pool, queue=default, priority_weight=6, operator=PythonOperator, queued_dttm=2024-11-27 14:56:45.922278+00:00, queued_by_job_id=803, pid=10320[0m
 [[34m2024-11-27T14:57:04.643+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-09-24 01:00:00+00:00, run_after=2024-09-25 01:00:00+00:00[0m
 [[34m2024-11-27T14:57:04.643+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:57:04.644+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:57:05.067+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-09-25 01:00:00+00:00, run_after=2024-09-26 01:00:00+00:00[0m
 Data loaded successfully
 [[34m2024-11-27T14:57:05.194+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:57:05.207+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 [[34m2024-11-27T14:57:05.489+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:57:05.495+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:57:05.495+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:57:05.496+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:57:05.496+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:57:05.496+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:57:05.496+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:57:05.668+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:57:05.668+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:57:05.728+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:57:05.770+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 [[34m2024-11-27T14:57:05.921+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:57:05.969+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:57:05.974+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412095314,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T14:57:05.976+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412095310,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T14:57:06.034+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 [[34m2024-11-27T14:57:06.079+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.3238602769561112[0m
 [[34m2024-11-27T14:57:06.079+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 [[34m2024-11-27T14:57:06.079+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|10329]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:57:06.080+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:57:06.080+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:57:06.080+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:57:06.080+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|10329]: It took 0.886s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:57:06.083+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|10329]: It took 0.00228s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:57:06.113+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run scheduled__2024-09-24T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 Data loaded successfully
 [[34m2024-11-27T14:57:06.327+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:57:06.333+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-09-26 01:00:00+00:00, run_after=2024-09-27 01:00:00+00:00[0m
 [[34m2024-11-27T14:57:06.340+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 Data loaded successfully
 [[34m2024-11-27T14:57:06.544+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:57:06.556+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 Data loaded successfully
 [[34m2024-11-27T14:57:06.562+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:57:06.575+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 [2024-11-27T14:57:06.588+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run, dagrun_id=scheduled__2024-09-24T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:57:06.181125+00:00, try_number=1, job_id=908, op_classpath=cosmos.operators.local.DbtRunLocalOperator, cosmos.operators.base.DbtRunMixin, cosmos.operators.local.DbtLocalBaseOperator, cosmos.operators.base.AbstractDbtBaseOperator
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:57:06.700+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:57:06.702+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:57:07.201+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.30765467998571694[0m
 [[34m2024-11-27T14:57:07.202+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|10326]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:57:07.202+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:57:07.202+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:57:07.203+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:57:07.203+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|10326]: It took 0.876s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:57:07.207+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|10326]: It took 0.00456s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:57:07.243+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_daily_bars_run scheduled__2024-09-24T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 [[34m2024-11-27T14:57:07.419+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.3270934741012752[0m
 [[34m2024-11-27T14:57:07.419+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|10328]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:57:07.420+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:57:07.420+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:57:07.420+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:57:07.420+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|10328]: It took 0.876s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:57:07.423+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|10328]: It took 0.00247s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:57:07.461+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.weekly_metrics_run scheduled__2024-09-23T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 [[34m2024-11-27T14:57:07.497+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.36742262402549386[0m
 [[34m2024-11-27T14:57:07.498+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|10327]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:57:07.498+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:57:07.499+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:57:07.499+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:57:07.499+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|10327]: It took 0.937s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:57:07.502+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|10327]: It took 0.00328s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:57:07.506+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:57:07.552+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.dim_date_run scheduled__2024-09-24T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [[34m2024-11-27T14:57:07.595+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-09-27 01:00:00+00:00, run_after=2024-09-28 01:00:00+00:00[0m
 [[34m2024-11-27T14:57:07.629+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [2024-11-27T14:57:07.731+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.stg_daily_bars_run, dagrun_id=scheduled__2024-09-24T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:57:07.309338+00:00, try_number=1, job_id=909, op_classpath=cosmos.operators.local.DbtRunLocalOperator, cosmos.operators.base.DbtRunMixin, cosmos.operators.local.DbtLocalBaseOperator, cosmos.operators.base.AbstractDbtBaseOperator
 [2024-11-27T14:57:08.040+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.weekly_metrics_run, dagrun_id=scheduled__2024-09-23T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:57:07.556794+00:00, try_number=1, job_id=910, op_classpath=cosmos.operators.local.DbtRunLocalOperator, cosmos.operators.base.DbtRunMixin, cosmos.operators.local.DbtLocalBaseOperator, cosmos.operators.base.AbstractDbtBaseOperator
 [2024-11-27T14:57:08.128+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.dim_date_run, dagrun_id=scheduled__2024-09-24T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:57:07.657967+00:00, try_number=1, job_id=911, op_classpath=cosmos.operators.local.DbtRunLocalOperator, cosmos.operators.base.DbtRunMixin, cosmos.operators.local.DbtLocalBaseOperator, cosmos.operators.base.AbstractDbtBaseOperator
 [[34m2024-11-27T14:57:08.891+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-09-28 01:00:00+00:00, run_after=2024-09-29 01:00:00+00:00[0m
 [[34m2024-11-27T14:57:10.196+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:57:10.211+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-09-29 01:00:00+00:00, run_after=2024-09-30 01:00:00+00:00[0m
 [[34m2024-11-27T14:57:10.250+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:57:10.511+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T14:57:10.570+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T14:57:10.731+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:57:10.824+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:57:11.524+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-09-30 01:00:00+00:00, run_after=2024-10-01 01:00:00+00:00[0m
 [[34m2024-11-27T14:57:11.945+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:57:11.970+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:57:12.105+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:57:12.109+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:57:12.217+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 [[34m2024-11-27T14:57:12.217+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 /usr/local/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked semaphore objects to clean up at shutdown
   warnings.warn('resource_tracker: There appear to be %d '
 [[34m2024-11-27T14:57:12.759+0000[0m] {[34mscheduler_job_runner.py:[0m1328} INFO[0m - DAG capstone_andres_dag_ is at (or above) max_active_runs (16 of 16), not creating any more runs[0m
 Data loaded successfully
 [[34m2024-11-27T14:57:12.859+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 Data loaded successfully
 [[34m2024-11-27T14:57:12.865+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:57:12.879+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 [[34m2024-11-27T14:57:12.886+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 [[34m2024-11-27T14:57:13.089+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run', run_id='scheduled__2024-09-24T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:57:13.093+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run, run_id=scheduled__2024-09-24T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:57:06.181125+00:00, run_end_date=2024-11-27 14:57:12.392894+00:00, run_duration=6.211769, state=success, executor_state=success, try_number=1, max_tries=0, job_id=908, pool=default_pool, queue=default, priority_weight=2, operator=DbtRunLocalOperator, queued_dttm=2024-11-27 14:56:58.516305+00:00, queued_by_job_id=803, pid=10403[0m
 [[34m2024-11-27T14:57:13.974+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.519788458943367[0m
 [[34m2024-11-27T14:57:13.975+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|10400]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:57:13.975+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:57:13.976+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:57:13.976+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:57:13.976+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|10400]: It took 1.11s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:57:13.977+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.5086949828546494[0m
 [[34m2024-11-27T14:57:13.978+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|10399]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:57:13.978+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:57:13.979+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:57:13.979+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:57:13.979+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|10399]: It took 1.12s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:57:13.980+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|10400]: It took 0.00409s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:57:13.981+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|10399]: It took 0.00248s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:57:14.028+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.load_treasury_data_task scheduled__2024-09-25T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [[34m2024-11-27T14:57:14.031+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.load_polygon_data_task scheduled__2024-09-26T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 /usr/local/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked semaphore objects to clean up at shutdown
   warnings.warn('resource_tracker: There appear to be %d '
 /usr/local/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked semaphore objects to clean up at shutdown
   warnings.warn('resource_tracker: There appear to be %d '
 [2024-11-27T14:57:14.591+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=load_treasury_data_task, dagrun_id=scheduled__2024-09-25T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:57:14.121441+00:00, try_number=1, job_id=912, op_classpath=airflow.operators.python.PythonOperator
 [2024-11-27T14:57:14.595+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=load_polygon_data_task, dagrun_id=scheduled__2024-09-26T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:57:14.128364+00:00, try_number=1, job_id=913, op_classpath=airflow.operators.python.PythonOperator
 [[34m2024-11-27T14:57:14.723+0000[0m] {[34mdagrun.py:[0m851} INFO[0m - Marking run <DagRun capstone_andres_dag_ @ 2024-09-23 01:00:00+00:00: scheduled__2024-09-23T01:00:00+00:00, state:running, queued_at: 2024-11-27 14:52:09.333584+00:00. externally triggered: False> successful[0m
 [[34m2024-11-27T14:57:14.723+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=capstone_andres_dag_, execution_date=2024-09-23 01:00:00+00:00, run_id=scheduled__2024-09-23T01:00:00+00:00, run_start_date=2024-11-27 14:52:09.352341+00:00, run_end_date=2024-11-27 14:57:14.723382+00:00, run_duration=305.371041, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-09-23 01:00:00+00:00, data_interval_end=2024-09-24 01:00:00+00:00, dag_hash=77fa8f64adac9391cccaaae88e2be5d8[0m
 [[34m2024-11-27T14:57:14.726+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-09-24 01:00:00+00:00, run_after=2024-09-25 01:00:00+00:00[0m
 [[34m2024-11-27T14:57:14.880+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.stg_daily_bars_run', run_id='scheduled__2024-09-24T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:57:14.880+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.weekly_metrics_run', run_id='scheduled__2024-09-23T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:57:14.883+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.weekly_metrics_run, run_id=scheduled__2024-09-23T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:57:07.556794+00:00, run_end_date=2024-11-27 14:57:14.312634+00:00, run_duration=6.75584, state=success, executor_state=success, try_number=1, max_tries=0, job_id=910, pool=default_pool, queue=default, priority_weight=1, operator=DbtRunLocalOperator, queued_dttm=2024-11-27 14:56:58.516305+00:00, queued_by_job_id=803, pid=10411[0m
 [[34m2024-11-27T14:57:14.884+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.stg_daily_bars_run, run_id=scheduled__2024-09-24T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:57:07.309338+00:00, run_end_date=2024-11-27 14:57:14.220680+00:00, run_duration=6.911342, state=success, executor_state=success, try_number=1, max_tries=0, job_id=909, pool=default_pool, queue=default, priority_weight=2, operator=DbtRunLocalOperator, queued_dttm=2024-11-27 14:56:58.516305+00:00, queued_by_job_id=803, pid=10404[0m
 /usr/local/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked semaphore objects to clean up at shutdown
   warnings.warn('resource_tracker: There appear to be %d '
 [[34m2024-11-27T14:57:15.527+0000[0m] {[34mscheduler_job_runner.py:[0m1328} INFO[0m - DAG capstone_andres_dag_ is at (or above) max_active_runs (16 of 16), not creating any more runs[0m
 [[34m2024-11-27T14:57:15.768+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.weekly_metrics_run scheduled__2024-09-24T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:57:15.768+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 2/16 running and queued tasks[0m
 [[34m2024-11-27T14:57:15.768+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.weekly_metrics_run scheduled__2024-09-24T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:57:15.770+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.weekly_metrics_run', run_id='scheduled__2024-09-24T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
 [[34m2024-11-27T14:57:15.771+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.weekly_metrics_run', 'scheduled__2024-09-24T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:57:15.774+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.dim_date_run', run_id='scheduled__2024-09-24T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:57:15.774+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.weekly_metrics_run', 'scheduled__2024-09-24T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:57:15.779+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.dim_date_run, run_id=scheduled__2024-09-24T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:57:07.657967+00:00, run_end_date=2024-11-27 14:57:15.481249+00:00, run_duration=7.823282, state=success, executor_state=success, try_number=1, max_tries=0, job_id=911, pool=default_pool, queue=default, priority_weight=2, operator=DbtRunLocalOperator, queued_dttm=2024-11-27 14:56:58.516305+00:00, queued_by_job_id=803, pid=10420[0m
 [[34m2024-11-27T14:57:15.807+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:57:16.598+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:57:16.599+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:57:16.599+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:57:17.112+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412096782,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T14:57:17.665+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run scheduled__2024-09-25T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.dim_date_run scheduled__2024-09-25T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_daily_bars_run scheduled__2024-09-25T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:57:17.665+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 2/16 running and queued tasks[0m
 [[34m2024-11-27T14:57:17.665+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 3/16 running and queued tasks[0m
 [[34m2024-11-27T14:57:17.665+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 4/16 running and queued tasks[0m
 [[34m2024-11-27T14:57:17.666+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run scheduled__2024-09-25T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.dim_date_run scheduled__2024-09-25T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_daily_bars_run scheduled__2024-09-25T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:57:17.668+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run', run_id='scheduled__2024-09-25T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
 [[34m2024-11-27T14:57:17.669+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run', 'scheduled__2024-09-25T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:57:17.669+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.dim_date_run', run_id='scheduled__2024-09-25T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
 [[34m2024-11-27T14:57:17.669+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.dim_date_run', 'scheduled__2024-09-25T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:57:17.669+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.stg_daily_bars_run', run_id='scheduled__2024-09-25T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
 [[34m2024-11-27T14:57:17.670+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.stg_daily_bars_run', 'scheduled__2024-09-25T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:57:17.673+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run', 'scheduled__2024-09-25T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:57:17.673+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.dim_date_run', 'scheduled__2024-09-25T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:57:17.675+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.stg_daily_bars_run', 'scheduled__2024-09-25T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:57:17.675+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='load_treasury_data_task', run_id='scheduled__2024-09-25T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:57:17.681+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=load_treasury_data_task, run_id=scheduled__2024-09-25T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:57:14.121441+00:00, run_end_date=2024-11-27 14:57:16.701967+00:00, run_duration=2.580526, state=success, executor_state=success, try_number=1, max_tries=0, job_id=912, pool=default_pool, queue=default, priority_weight=5, operator=PythonOperator, queued_dttm=2024-11-27 14:57:04.614217+00:00, queued_by_job_id=803, pid=10476[0m
 [[34m2024-11-27T14:57:17.708+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:57:17.710+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:57:17.711+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:57:17.913+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 [[34m2024-11-27T14:57:18.646+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:57:18.647+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:57:18.647+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:57:18.649+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:57:18.650+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:57:18.650+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:57:18.652+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:57:18.653+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:57:18.654+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:57:18.663+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:57:19.090+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412096786,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T14:57:19.090+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412096790,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T14:57:19.125+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412096794,
 "os.name" : Linux
 [0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:57:19.793+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:57:19.820+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:57:19.822+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 [[34m2024-11-27T14:57:20.415+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 2 tasks up for execution:
 	<TaskInstance: capstone_andres_dag_.load_polygon_data_task scheduled__2024-09-27T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.load_treasury_data_task scheduled__2024-09-26T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:57:20.415+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 4/16 running and queued tasks[0m
 [[34m2024-11-27T14:57:20.416+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 5/16 running and queued tasks[0m
 [[34m2024-11-27T14:57:20.416+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
 	<TaskInstance: capstone_andres_dag_.load_polygon_data_task scheduled__2024-09-27T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.load_treasury_data_task scheduled__2024-09-26T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:57:20.416+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:57:20.416+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:57:20.418+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:57:20.418+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='load_polygon_data_task', run_id='scheduled__2024-09-27T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
 [[34m2024-11-27T14:57:20.418+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'load_polygon_data_task', 'scheduled__2024-09-27T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:57:20.419+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='load_treasury_data_task', run_id='scheduled__2024-09-26T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 5 and queue default[0m
 [[34m2024-11-27T14:57:20.419+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'load_treasury_data_task', 'scheduled__2024-09-26T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:57:20.421+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'load_polygon_data_task', 'scheduled__2024-09-27T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:57:20.421+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'load_treasury_data_task', 'scheduled__2024-09-26T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:57:20.422+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='load_polygon_data_task', run_id='scheduled__2024-09-26T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:57:20.426+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=load_polygon_data_task, run_id=scheduled__2024-09-26T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:57:14.128364+00:00, run_end_date=2024-11-27 14:57:20.205630+00:00, run_duration=6.077266, state=success, executor_state=success, try_number=1, max_tries=0, job_id=913, pool=default_pool, queue=default, priority_weight=6, operator=PythonOperator, queued_dttm=2024-11-27 14:57:04.614217+00:00, queued_by_job_id=803, pid=10477[0m
 [[34m2024-11-27T14:57:20.451+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:57:20.452+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:57:21.315+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:57:21.315+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:57:21.316+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:57:21.316+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:57:21.316+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:57:21.316+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:57:21.448+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:57:21.763+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T14:57:21.799+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:57:21.898+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412095346,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T14:57:21.900+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412095350,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T14:57:22.014+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:57:22.118+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T14:57:22.384+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:57:22.677+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:57:22.994+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 [[34m2024-11-27T14:57:23.007+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:57:23.083+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:57:23.164+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:57:23.169+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:57:23.244+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:57:23.274+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 [[34m2024-11-27T14:57:23.311+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T14:57:23.368+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T14:57:23.491+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 [[34m2024-11-27T14:57:23.606+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:57:23.606+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:57:23.632+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:57:23.702+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:57:23.742+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 Data loaded successfully
 [[34m2024-11-27T14:57:23.836+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:57:23.848+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 Data loaded successfully
 [[34m2024-11-27T14:57:24.331+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:57:24.344+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 [[34m2024-11-27T14:57:24.585+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:57:24.646+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:57:24.682+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.29704978805966675[0m
 [[34m2024-11-27T14:57:24.682+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|10483]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:57:24.683+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:57:24.683+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:57:24.683+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:57:24.683+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|10483]: It took 0.847s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:57:24.685+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|10483]: It took 0.00236s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:57:24.719+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.weekly_metrics_run scheduled__2024-09-24T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [[34m2024-11-27T14:57:24.725+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:57:24.795+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:57:24.831+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 [[34m2024-11-27T14:57:24.898+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 [2024-11-27T14:57:25.167+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.weekly_metrics_run, dagrun_id=scheduled__2024-09-24T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:57:24.783485+00:00, try_number=1, job_id=914, op_classpath=cosmos.operators.local.DbtRunLocalOperator, cosmos.operators.base.DbtRunMixin, cosmos.operators.local.DbtLocalBaseOperator, cosmos.operators.base.AbstractDbtBaseOperator
 [[34m2024-11-27T14:57:25.211+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.33838644507341087[0m
 [[34m2024-11-27T14:57:25.211+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|10499]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:57:25.212+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:57:25.212+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:57:25.212+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:57:25.212+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|10499]: It took 0.88s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:57:25.215+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|10499]: It took 0.00231s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:57:25.246+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.dim_date_run scheduled__2024-09-25T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 Data loaded successfully
 [[34m2024-11-27T14:57:25.359+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:57:25.371+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 Data loaded successfully
 [[34m2024-11-27T14:57:25.412+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:57:25.428+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 [[34m2024-11-27T14:57:25.715+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [2024-11-27T14:57:25.838+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.dim_date_run, dagrun_id=scheduled__2024-09-25T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:57:25.310000+00:00, try_number=1, job_id=915, op_classpath=cosmos.operators.local.DbtRunLocalOperator, cosmos.operators.base.DbtRunMixin, cosmos.operators.local.DbtLocalBaseOperator, cosmos.operators.base.AbstractDbtBaseOperator
 [[34m2024-11-27T14:57:26.019+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T14:57:26.262+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:57:26.483+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.48132099909707904[0m
 [[34m2024-11-27T14:57:26.484+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|10500]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:57:26.484+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:57:26.485+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:57:26.485+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:57:26.485+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|10500]: It took 1.13s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:57:26.487+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|10500]: It took 0.00226s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:57:26.508+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:57:26.530+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_daily_bars_run scheduled__2024-09-25T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [[34m2024-11-27T14:57:26.604+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.5175841099116951[0m
 [[34m2024-11-27T14:57:26.604+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|10498]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:57:26.605+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:57:26.605+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:57:26.605+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:57:26.606+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|10498]: It took 1.19s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:57:26.609+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|10498]: It took 0.00301s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:57:26.654+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run scheduled__2024-09-25T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [[34m2024-11-27T14:57:26.932+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [2024-11-27T14:57:27.150+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.stg_daily_bars_run, dagrun_id=scheduled__2024-09-25T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:57:26.618274+00:00, try_number=1, job_id=916, op_classpath=cosmos.operators.local.DbtRunLocalOperator, cosmos.operators.base.DbtRunMixin, cosmos.operators.local.DbtLocalBaseOperator, cosmos.operators.base.AbstractDbtBaseOperator
 [[34m2024-11-27T14:57:27.181+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [2024-11-27T14:57:27.281+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run, dagrun_id=scheduled__2024-09-25T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:57:26.756518+00:00, try_number=1, job_id=917, op_classpath=cosmos.operators.local.DbtRunLocalOperator, cosmos.operators.base.DbtRunMixin, cosmos.operators.local.DbtLocalBaseOperator, cosmos.operators.base.AbstractDbtBaseOperator
 [[34m2024-11-27T14:57:27.539+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:57:27.709+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:57:27.813+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 [[34m2024-11-27T14:57:28.108+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:57:28.302+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:57:28.410+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 Data loaded successfully
 [[34m2024-11-27T14:57:28.468+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:57:28.486+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 Data loaded successfully
 [[34m2024-11-27T14:57:28.996+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:57:29.006+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 [[34m2024-11-27T14:57:29.448+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.41695172619074583[0m
 [[34m2024-11-27T14:57:29.448+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|10526]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:57:29.449+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:57:29.449+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:57:29.449+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:57:29.450+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|10526]: It took 0.981s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:57:29.453+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|10526]: It took 0.00336s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:57:29.493+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.load_polygon_data_task scheduled__2024-09-27T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [2024-11-27T14:57:30.007+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=load_polygon_data_task, dagrun_id=scheduled__2024-09-27T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:57:29.570064+00:00, try_number=1, job_id=918, op_classpath=airflow.operators.python.PythonOperator
 [[34m2024-11-27T14:57:30.137+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.5438478961586952[0m
 [[34m2024-11-27T14:57:30.137+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|10525]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:57:30.138+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:57:30.138+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:57:30.138+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:57:30.138+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|10525]: It took 1.14s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:57:30.141+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|10525]: It took 0.00249s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:57:30.178+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.load_treasury_data_task scheduled__2024-09-26T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [2024-11-27T14:57:30.663+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=load_treasury_data_task, dagrun_id=scheduled__2024-09-26T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:57:30.270127+00:00, try_number=1, job_id=919, op_classpath=airflow.operators.python.PythonOperator
 /usr/local/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked semaphore objects to clean up at shutdown
   warnings.warn('resource_tracker: There appear to be %d '
 [[34m2024-11-27T14:57:31.448+0000[0m] {[34mdagrun.py:[0m851} INFO[0m - Marking run <DagRun capstone_andres_dag_ @ 2024-09-24 01:00:00+00:00: scheduled__2024-09-24T01:00:00+00:00, state:running, queued_at: 2024-11-27 14:52:09.654928+00:00. externally triggered: False> successful[0m
 [[34m2024-11-27T14:57:31.448+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=capstone_andres_dag_, execution_date=2024-09-24 01:00:00+00:00, run_id=scheduled__2024-09-24T01:00:00+00:00, run_start_date=2024-11-27 14:52:09.664345+00:00, run_end_date=2024-11-27 14:57:31.448506+00:00, run_duration=321.784161, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-09-24 01:00:00+00:00, data_interval_end=2024-09-25 01:00:00+00:00, dag_hash=77fa8f64adac9391cccaaae88e2be5d8[0m
 [[34m2024-11-27T14:57:31.451+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-09-25 01:00:00+00:00, run_after=2024-09-26 01:00:00+00:00[0m
 [[34m2024-11-27T14:57:31.617+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.weekly_metrics_run', run_id='scheduled__2024-09-24T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:57:31.620+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.weekly_metrics_run, run_id=scheduled__2024-09-24T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:57:24.783485+00:00, run_end_date=2024-11-27 14:57:31.338813+00:00, run_duration=6.555328, state=success, executor_state=success, try_number=1, max_tries=0, job_id=914, pool=default_pool, queue=default, priority_weight=1, operator=DbtRunLocalOperator, queued_dttm=2024-11-27 14:57:15.769460+00:00, queued_by_job_id=803, pid=10586[0m
 [[34m2024-11-27T14:57:32.642+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-09-26 01:00:00+00:00, run_after=2024-09-27 01:00:00+00:00[0m
 /usr/local/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked semaphore objects to clean up at shutdown
   warnings.warn('resource_tracker: There appear to be %d '
 /usr/local/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked semaphore objects to clean up at shutdown
   warnings.warn('resource_tracker: There appear to be %d '
 /usr/local/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked semaphore objects to clean up at shutdown
   warnings.warn('resource_tracker: There appear to be %d '
 [[34m2024-11-27T14:57:33.852+0000[0m] {[34mscheduler_job_runner.py:[0m1328} INFO[0m - DAG capstone_andres_dag_ is at (or above) max_active_runs (16 of 16), not creating any more runs[0m
 [[34m2024-11-27T14:57:34.072+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 4 tasks up for execution:
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.dim_date_run scheduled__2024-09-26T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_daily_bars_run scheduled__2024-09-26T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run scheduled__2024-09-26T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.weekly_metrics_run scheduled__2024-09-25T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:57:34.072+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 1/16 running and queued tasks[0m
 [[34m2024-11-27T14:57:34.073+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 2/16 running and queued tasks[0m
 [[34m2024-11-27T14:57:34.073+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 3/16 running and queued tasks[0m
 [[34m2024-11-27T14:57:34.073+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 4/16 running and queued tasks[0m
 [[34m2024-11-27T14:57:34.073+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.dim_date_run scheduled__2024-09-26T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_daily_bars_run scheduled__2024-09-26T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run scheduled__2024-09-26T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.weekly_metrics_run scheduled__2024-09-25T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:57:34.076+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.dim_date_run', run_id='scheduled__2024-09-26T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
 [[34m2024-11-27T14:57:34.076+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.dim_date_run', 'scheduled__2024-09-26T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:57:34.076+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.stg_daily_bars_run', run_id='scheduled__2024-09-26T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
 [[34m2024-11-27T14:57:34.077+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.stg_daily_bars_run', 'scheduled__2024-09-26T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:57:34.077+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run', run_id='scheduled__2024-09-26T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
 [[34m2024-11-27T14:57:34.077+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run', 'scheduled__2024-09-26T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:57:34.078+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.weekly_metrics_run', run_id='scheduled__2024-09-25T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
 [[34m2024-11-27T14:57:34.078+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.weekly_metrics_run', 'scheduled__2024-09-25T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:57:34.080+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.stg_daily_bars_run', 'scheduled__2024-09-26T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:57:34.080+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run', 'scheduled__2024-09-26T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:57:34.081+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.weekly_metrics_run', 'scheduled__2024-09-25T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:57:34.085+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='load_treasury_data_task', run_id='scheduled__2024-09-26T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:57:34.086+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.dim_date_run', run_id='scheduled__2024-09-25T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:57:34.086+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.stg_daily_bars_run', run_id='scheduled__2024-09-25T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:57:34.087+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run', run_id='scheduled__2024-09-25T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:57:34.083+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.dim_date_run', 'scheduled__2024-09-26T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:57:34.096+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run, run_id=scheduled__2024-09-25T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:57:26.756518+00:00, run_end_date=2024-11-27 14:57:33.258544+00:00, run_duration=6.502026, state=success, executor_state=success, try_number=1, max_tries=0, job_id=917, pool=default_pool, queue=default, priority_weight=2, operator=DbtRunLocalOperator, queued_dttm=2024-11-27 14:57:17.666698+00:00, queued_by_job_id=803, pid=10604[0m
 [[34m2024-11-27T14:57:34.097+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.dim_date_run, run_id=scheduled__2024-09-25T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:57:25.310000+00:00, run_end_date=2024-11-27 14:57:32.923563+00:00, run_duration=7.613563, state=success, executor_state=success, try_number=1, max_tries=0, job_id=915, pool=default_pool, queue=default, priority_weight=2, operator=DbtRunLocalOperator, queued_dttm=2024-11-27 14:57:17.666698+00:00, queued_by_job_id=803, pid=10587[0m
 [[34m2024-11-27T14:57:34.097+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.stg_daily_bars_run, run_id=scheduled__2024-09-25T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:57:26.618274+00:00, run_end_date=2024-11-27 14:57:33.012464+00:00, run_duration=6.39419, state=success, executor_state=success, try_number=1, max_tries=0, job_id=916, pool=default_pool, queue=default, priority_weight=2, operator=DbtRunLocalOperator, queued_dttm=2024-11-27 14:57:17.666698+00:00, queued_by_job_id=803, pid=10600[0m
 [[34m2024-11-27T14:57:34.098+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=load_treasury_data_task, run_id=scheduled__2024-09-26T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:57:30.270127+00:00, run_end_date=2024-11-27 14:57:32.861842+00:00, run_duration=2.591715, state=success, executor_state=success, try_number=1, max_tries=0, job_id=919, pool=default_pool, queue=default, priority_weight=5, operator=PythonOperator, queued_dttm=2024-11-27 14:57:20.416885+00:00, queued_by_job_id=803, pid=10640[0m
 [[34m2024-11-27T14:57:34.127+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:57:34.127+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:57:34.128+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:57:34.154+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:57:35.224+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:57:35.225+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:57:35.225+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:57:35.225+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:57:35.226+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:57:35.227+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:57:35.233+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:57:35.234+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:57:35.234+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:57:35.253+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:57:35.254+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:57:35.254+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:57:35.721+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412096830,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T14:57:35.729+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412096834,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T14:57:35.732+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412095398,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T14:57:35.739+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412095402,
 "os.name" : Linux
 [0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:57:36.514+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:57:36.514+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:57:36.514+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:57:36.516+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 [[34m2024-11-27T14:57:37.607+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 [[34m2024-11-27T14:57:37.710+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 [[34m2024-11-27T14:57:37.842+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:57:37.910+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 2 tasks up for execution:
 	<TaskInstance: capstone_andres_dag_.load_polygon_data_task scheduled__2024-09-28T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.load_treasury_data_task scheduled__2024-09-27T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:57:37.910+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 4/16 running and queued tasks[0m
 [[34m2024-11-27T14:57:37.910+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 5/16 running and queued tasks[0m
 [[34m2024-11-27T14:57:37.911+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
 	<TaskInstance: capstone_andres_dag_.load_polygon_data_task scheduled__2024-09-28T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.load_treasury_data_task scheduled__2024-09-27T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:57:37.912+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='load_polygon_data_task', run_id='scheduled__2024-09-28T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
 [[34m2024-11-27T14:57:37.913+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'load_polygon_data_task', 'scheduled__2024-09-28T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:57:37.913+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='load_treasury_data_task', run_id='scheduled__2024-09-27T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 5 and queue default[0m
 [[34m2024-11-27T14:57:37.913+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'load_treasury_data_task', 'scheduled__2024-09-27T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:57:37.915+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'load_polygon_data_task', 'scheduled__2024-09-28T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:57:37.915+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'load_treasury_data_task', 'scheduled__2024-09-27T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:57:37.916+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='load_polygon_data_task', run_id='scheduled__2024-09-27T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:57:37.920+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=load_polygon_data_task, run_id=scheduled__2024-09-27T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:57:29.570064+00:00, run_end_date=2024-11-27 14:57:36.871231+00:00, run_duration=7.301167, state=success, executor_state=success, try_number=1, max_tries=0, job_id=918, pool=default_pool, queue=default, priority_weight=6, operator=PythonOperator, queued_dttm=2024-11-27 14:57:20.416885+00:00, queued_by_job_id=803, pid=10624[0m
 [[34m2024-11-27T14:57:37.945+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:57:37.945+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:57:37.973+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:57:39.421+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:57:39.424+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:57:39.425+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:57:39.425+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:57:39.427+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:57:39.428+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:57:39.911+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412095414,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T14:57:39.923+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412095410,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T14:57:40.264+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:57:40.380+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:57:40.594+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T14:57:40.619+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:57:40.671+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T14:57:40.691+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:57:40.721+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:57:40.780+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:57:40.891+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:57:40.901+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:57:40.904+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 [[34m2024-11-27T14:57:41.007+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 [[34m2024-11-27T14:57:41.125+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:57:41.145+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:57:41.231+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:57:41.251+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:57:41.914+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:57:41.969+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:57:42.048+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:57:42.118+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:57:42.172+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 [[34m2024-11-27T14:57:42.218+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 [[34m2024-11-27T14:57:42.357+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:57:42.499+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:57:42.531+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:57:42.657+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 [[34m2024-11-27T14:57:42.669+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 Data loaded successfully
 [[34m2024-11-27T14:57:42.679+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:57:42.691+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 [[34m2024-11-27T14:57:42.786+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 Data loaded successfully
 [[34m2024-11-27T14:57:42.799+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:57:42.811+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 Data loaded successfully
 [[34m2024-11-27T14:57:43.193+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:57:43.207+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 Data loaded successfully
 [[34m2024-11-27T14:57:43.359+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:57:43.380+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 [[34m2024-11-27T14:57:43.636+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.38283578399568796[0m
 [[34m2024-11-27T14:57:43.637+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|10661]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:57:43.637+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:57:43.637+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:57:43.638+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:57:43.638+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|10661]: It took 0.959s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:57:43.641+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|10661]: It took 0.00325s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:57:43.679+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_daily_bars_run scheduled__2024-09-26T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [[34m2024-11-27T14:57:43.766+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.3588598230853677[0m
 [[34m2024-11-27T14:57:43.767+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|10663]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:57:43.767+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:57:43.767+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:57:43.768+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:57:43.768+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|10663]: It took 0.969s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:57:43.770+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|10663]: It took 0.00257s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:57:43.837+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.weekly_metrics_run scheduled__2024-09-25T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [[34m2024-11-27T14:57:44.019+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:57:44.034+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [2024-11-27T14:57:44.230+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.stg_daily_bars_run, dagrun_id=scheduled__2024-09-26T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:57:43.762370+00:00, try_number=1, job_id=920, op_classpath=cosmos.operators.local.DbtRunLocalOperator, cosmos.operators.base.DbtRunMixin, cosmos.operators.local.DbtLocalBaseOperator, cosmos.operators.base.AbstractDbtBaseOperator
 [[34m2024-11-27T14:57:44.248+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.4050340740941465[0m
 [[34m2024-11-27T14:57:44.248+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|10662]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:57:44.249+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:57:44.250+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:57:44.250+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:57:44.250+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|10662]: It took 1.06s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:57:44.253+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|10662]: It took 0.00317s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:57:44.300+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run scheduled__2024-09-26T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [[34m2024-11-27T14:57:44.320+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [2024-11-27T14:57:44.391+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.weekly_metrics_run, dagrun_id=scheduled__2024-09-25T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:57:43.936681+00:00, try_number=1, job_id=921, op_classpath=cosmos.operators.local.DbtRunLocalOperator, cosmos.operators.base.DbtRunMixin, cosmos.operators.local.DbtLocalBaseOperator, cosmos.operators.base.AbstractDbtBaseOperator
 [[34m2024-11-27T14:57:44.394+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T14:57:44.581+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:57:44.665+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.4546967949718237[0m
 [[34m2024-11-27T14:57:44.666+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|10664]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:57:44.666+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:57:44.667+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:57:44.667+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:57:44.667+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|10664]: It took 1.31s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:57:44.671+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|10664]: It took 0.00346s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:57:44.681+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:57:44.730+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.dim_date_run scheduled__2024-09-26T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [2024-11-27T14:57:44.843+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run, dagrun_id=scheduled__2024-09-26T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:57:44.398334+00:00, try_number=1, job_id=922, op_classpath=cosmos.operators.local.DbtRunLocalOperator, cosmos.operators.base.DbtRunMixin, cosmos.operators.local.DbtLocalBaseOperator, cosmos.operators.base.AbstractDbtBaseOperator
 [2024-11-27T14:57:45.321+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.dim_date_run, dagrun_id=scheduled__2024-09-26T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:57:44.832301+00:00, try_number=1, job_id=923, op_classpath=cosmos.operators.local.DbtRunLocalOperator, cosmos.operators.base.DbtRunMixin, cosmos.operators.local.DbtLocalBaseOperator, cosmos.operators.base.AbstractDbtBaseOperator
 [[34m2024-11-27T14:57:45.578+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:57:45.699+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:57:45.758+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:57:45.892+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:57:45.894+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 [[34m2024-11-27T14:57:46.002+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 Data loaded successfully
 [[34m2024-11-27T14:57:46.425+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:57:46.441+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 Data loaded successfully
 [[34m2024-11-27T14:57:46.509+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:57:46.522+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 [[34m2024-11-27T14:57:47.910+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.7180468149017543[0m
 [[34m2024-11-27T14:57:47.910+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|10699]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:57:47.911+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:57:47.911+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:57:47.912+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:57:47.912+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|10699]: It took 1.49s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:57:47.915+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|10699]: It took 0.00291s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:57:47.942+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.6644145371392369[0m
 [[34m2024-11-27T14:57:47.942+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|10700]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:57:47.943+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:57:47.943+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:57:47.943+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:57:47.943+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|10700]: It took 1.43s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:57:47.947+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|10700]: It took 0.00349s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:57:47.964+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.load_treasury_data_task scheduled__2024-09-27T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [[34m2024-11-27T14:57:47.998+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.load_polygon_data_task scheduled__2024-09-28T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [2024-11-27T14:57:48.535+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=load_treasury_data_task, dagrun_id=scheduled__2024-09-27T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:57:48.085390+00:00, try_number=1, job_id=924, op_classpath=airflow.operators.python.PythonOperator
 [2024-11-27T14:57:48.598+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=load_polygon_data_task, dagrun_id=scheduled__2024-09-28T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:57:48.124098+00:00, try_number=1, job_id=925, op_classpath=airflow.operators.python.PythonOperator
 [[34m2024-11-27T14:57:51.115+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.dim_date_run scheduled__2024-09-27T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_daily_bars_run scheduled__2024-09-27T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run scheduled__2024-09-27T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:57:51.115+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 5/16 running and queued tasks[0m
 [[34m2024-11-27T14:57:51.115+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 6/16 running and queued tasks[0m
 [[34m2024-11-27T14:57:51.115+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 7/16 running and queued tasks[0m
 [[34m2024-11-27T14:57:51.115+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.dim_date_run scheduled__2024-09-27T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_daily_bars_run scheduled__2024-09-27T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run scheduled__2024-09-27T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:57:51.117+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.dim_date_run', run_id='scheduled__2024-09-27T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
 [[34m2024-11-27T14:57:51.118+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.dim_date_run', 'scheduled__2024-09-27T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:57:51.118+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.stg_daily_bars_run', run_id='scheduled__2024-09-27T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
 [[34m2024-11-27T14:57:51.118+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.stg_daily_bars_run', 'scheduled__2024-09-27T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:57:51.118+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run', run_id='scheduled__2024-09-27T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
 [[34m2024-11-27T14:57:51.118+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run', 'scheduled__2024-09-27T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:57:51.120+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.dim_date_run', 'scheduled__2024-09-27T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:57:51.121+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run', 'scheduled__2024-09-27T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:57:51.121+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='load_treasury_data_task', run_id='scheduled__2024-09-27T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:57:51.120+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.stg_daily_bars_run', 'scheduled__2024-09-27T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:57:51.126+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=load_treasury_data_task, run_id=scheduled__2024-09-27T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:57:48.085390+00:00, run_end_date=2024-11-27 14:57:50.568668+00:00, run_duration=2.483278, state=success, executor_state=success, try_number=1, max_tries=0, job_id=924, pool=default_pool, queue=default, priority_weight=5, operator=PythonOperator, queued_dttm=2024-11-27 14:57:37.911609+00:00, queued_by_job_id=803, pid=10790[0m
 [[34m2024-11-27T14:57:51.149+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:57:51.150+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:57:51.149+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 /usr/local/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked semaphore objects to clean up at shutdown
   warnings.warn('resource_tracker: There appear to be %d '
 /usr/local/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked semaphore objects to clean up at shutdown
   warnings.warn('resource_tracker: There appear to be %d '
 [[34m2024-11-27T14:57:52.070+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:57:52.070+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:57:52.070+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:57:52.070+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:57:52.070+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:57:52.070+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:57:52.087+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:57:52.088+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:57:52.088+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 /usr/local/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked semaphore objects to clean up at shutdown
   warnings.warn('resource_tracker: There appear to be %d '
 [[34m2024-11-27T14:57:52.318+0000[0m] {[34mdagrun.py:[0m851} INFO[0m - Marking run <DagRun capstone_andres_dag_ @ 2024-09-25 01:00:00+00:00: scheduled__2024-09-25T01:00:00+00:00, state:running, queued_at: 2024-11-27 14:53:23.818061+00:00. externally triggered: False> successful[0m
 [[34m2024-11-27T14:57:52.319+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=capstone_andres_dag_, execution_date=2024-09-25 01:00:00+00:00, run_id=scheduled__2024-09-25T01:00:00+00:00, run_start_date=2024-11-27 14:53:23.873122+00:00, run_end_date=2024-11-27 14:57:52.319098+00:00, run_duration=268.445976, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-09-25 01:00:00+00:00, data_interval_end=2024-09-26 01:00:00+00:00, dag_hash=77fa8f64adac9391cccaaae88e2be5d8[0m
 [[34m2024-11-27T14:57:52.323+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-09-26 01:00:00+00:00, run_after=2024-09-27 01:00:00+00:00[0m
 [[34m2024-11-27T14:57:52.485+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.stg_daily_bars_run', run_id='scheduled__2024-09-26T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:57:52.485+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.weekly_metrics_run', run_id='scheduled__2024-09-25T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:57:52.485+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run', run_id='scheduled__2024-09-26T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:57:52.489+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.stg_daily_bars_run, run_id=scheduled__2024-09-26T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:57:43.762370+00:00, run_end_date=2024-11-27 14:57:51.607918+00:00, run_duration=7.845548, state=success, executor_state=success, try_number=1, max_tries=0, job_id=920, pool=default_pool, queue=default, priority_weight=2, operator=DbtRunLocalOperator, queued_dttm=2024-11-27 14:57:34.074369+00:00, queued_by_job_id=803, pid=10767[0m
 [[34m2024-11-27T14:57:52.489+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.weekly_metrics_run, run_id=scheduled__2024-09-25T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:57:43.936681+00:00, run_end_date=2024-11-27 14:57:51.788590+00:00, run_duration=7.851909, state=success, executor_state=success, try_number=1, max_tries=0, job_id=921, pool=default_pool, queue=default, priority_weight=1, operator=DbtRunLocalOperator, queued_dttm=2024-11-27 14:57:34.074369+00:00, queued_by_job_id=803, pid=10770[0m
 [[34m2024-11-27T14:57:52.490+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run, run_id=scheduled__2024-09-26T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:57:44.398334+00:00, run_end_date=2024-11-27 14:57:52.078991+00:00, run_duration=7.680657, state=success, executor_state=success, try_number=1, max_tries=0, job_id=922, pool=default_pool, queue=default, priority_weight=2, operator=DbtRunLocalOperator, queued_dttm=2024-11-27 14:57:34.074369+00:00, queued_by_job_id=803, pid=10771[0m
 [[34m2024-11-27T14:57:52.525+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412096882,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T14:57:52.531+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412095438,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T14:57:52.537+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412095442,
 "os.name" : Linux
 [0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:57:53.231+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:57:53.233+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:57:53.332+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:57:53.520+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-09-27 01:00:00+00:00, run_after=2024-09-28 01:00:00+00:00[0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 /usr/local/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked semaphore objects to clean up at shutdown
   warnings.warn('resource_tracker: There appear to be %d '
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 [[34m2024-11-27T14:57:53.839+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 [[34m2024-11-27T14:57:54.026+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:57:54.189+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:57:54.424+0000[0m] {[34mscheduler_job_runner.py:[0m1328} INFO[0m - DAG capstone_andres_dag_ is at (or above) max_active_runs (16 of 16), not creating any more runs[0m
 [[34m2024-11-27T14:57:54.848+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.weekly_metrics_run scheduled__2024-09-26T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:57:54.848+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 4/16 running and queued tasks[0m
 [[34m2024-11-27T14:57:54.848+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.weekly_metrics_run scheduled__2024-09-26T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:57:54.850+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.weekly_metrics_run', run_id='scheduled__2024-09-26T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
 [[34m2024-11-27T14:57:54.850+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.weekly_metrics_run', 'scheduled__2024-09-26T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:57:54.852+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.weekly_metrics_run', 'scheduled__2024-09-26T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:57:54.852+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.dim_date_run', run_id='scheduled__2024-09-26T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:57:54.856+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.dim_date_run, run_id=scheduled__2024-09-26T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:57:44.832301+00:00, run_end_date=2024-11-27 14:57:53.706088+00:00, run_duration=8.873787, state=success, executor_state=success, try_number=1, max_tries=0, job_id=923, pool=default_pool, queue=default, priority_weight=2, operator=DbtRunLocalOperator, queued_dttm=2024-11-27 14:57:34.074369+00:00, queued_by_job_id=803, pid=10772[0m
 [[34m2024-11-27T14:57:54.879+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:57:55.734+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:57:55.734+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:57:55.735+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:57:56.331+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412095446,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T14:57:56.383+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:57:56.662+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:57:56.749+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:57:56.750+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T14:57:56.758+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 2 tasks up for execution:
 	<TaskInstance: capstone_andres_dag_.load_polygon_data_task scheduled__2024-09-29T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.load_treasury_data_task scheduled__2024-09-28T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:57:56.758+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 4/16 running and queued tasks[0m
 [[34m2024-11-27T14:57:56.758+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 5/16 running and queued tasks[0m
 [[34m2024-11-27T14:57:56.759+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
 	<TaskInstance: capstone_andres_dag_.load_polygon_data_task scheduled__2024-09-29T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.load_treasury_data_task scheduled__2024-09-28T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:57:56.761+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='load_polygon_data_task', run_id='scheduled__2024-09-29T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
 [[34m2024-11-27T14:57:56.761+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'load_polygon_data_task', 'scheduled__2024-09-29T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:57:56.762+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='load_treasury_data_task', run_id='scheduled__2024-09-28T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 5 and queue default[0m
 [[34m2024-11-27T14:57:56.762+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'load_treasury_data_task', 'scheduled__2024-09-28T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:57:56.764+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'load_polygon_data_task', 'scheduled__2024-09-29T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:57:56.764+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'load_treasury_data_task', 'scheduled__2024-09-28T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:57:56.766+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='load_polygon_data_task', run_id='scheduled__2024-09-28T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:57:56.770+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=load_polygon_data_task, run_id=scheduled__2024-09-28T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:57:48.124098+00:00, run_end_date=2024-11-27 14:57:55.451302+00:00, run_duration=7.327204, state=success, executor_state=success, try_number=1, max_tries=0, job_id=925, pool=default_pool, queue=default, priority_weight=6, operator=PythonOperator, queued_dttm=2024-11-27 14:57:37.911609+00:00, queued_by_job_id=803, pid=10791[0m
 [[34m2024-11-27T14:57:56.801+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:57:56.805+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:57:56.972+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T14:57:56.995+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:57:57.115+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T14:57:57.268+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:57:57.364+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:57:57.488+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 [[34m2024-11-27T14:57:58.051+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:57:58.166+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:57:58.364+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:57:58.448+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:57:58.459+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:57:58.470+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 [[34m2024-11-27T14:57:58.627+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:57:58.644+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:57:58.646+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:57:58.647+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:57:58.648+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:57:58.649+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:57:58.650+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:57:58.651+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:57:58.741+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 [[34m2024-11-27T14:57:58.755+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 Data loaded successfully
 [[34m2024-11-27T14:57:59.023+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:57:59.041+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 [[34m2024-11-27T14:57:59.144+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412096886,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T14:57:59.197+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412095450,
 "os.name" : Linux
 [0m
 Data loaded successfully
 [[34m2024-11-27T14:57:59.239+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:57:59.257+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 Data loaded successfully
 [[34m2024-11-27T14:57:59.296+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:57:59.334+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:57:59.899+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:57:59.977+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:58:00.160+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.46729538193903863[0m
 [[34m2024-11-27T14:58:00.161+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|10834]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:58:00.162+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:58:00.162+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:58:00.163+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:58:00.163+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|10834]: It took 1.14s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:58:00.171+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|10834]: It took 0.00772s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:58:00.247+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_daily_bars_run scheduled__2024-09-27T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 [[34m2024-11-27T14:58:00.430+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.523452450055629[0m
 [[34m2024-11-27T14:58:00.431+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|10835]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:58:00.432+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:58:00.433+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:58:00.433+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:58:00.433+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|10835]: It took 1.19s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:58:00.439+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|10835]: It took 0.00537s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:58:00.501+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run scheduled__2024-09-27T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 [[34m2024-11-27T14:58:00.587+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:58:00.715+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.6859385780990124[0m
 [[34m2024-11-27T14:58:00.716+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|10833]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:58:00.717+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:58:00.718+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:58:00.718+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:58:00.719+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|10833]: It took 1.42s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:58:00.723+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|10833]: It took 0.00382s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:58:00.781+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.dim_date_run scheduled__2024-09-27T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [[34m2024-11-27T14:58:00.789+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:58:00.855+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [2024-11-27T14:58:00.933+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.stg_daily_bars_run, dagrun_id=scheduled__2024-09-27T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:58:00.369338+00:00, try_number=1, job_id=926, op_classpath=cosmos.operators.local.DbtRunLocalOperator, cosmos.operators.base.DbtRunMixin, cosmos.operators.local.DbtLocalBaseOperator, cosmos.operators.base.AbstractDbtBaseOperator
 [[34m2024-11-27T14:58:01.190+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [2024-11-27T14:58:01.231+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run, dagrun_id=scheduled__2024-09-27T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:58:00.634267+00:00, try_number=1, job_id=927, op_classpath=cosmos.operators.local.DbtRunLocalOperator, cosmos.operators.base.DbtRunMixin, cosmos.operators.local.DbtLocalBaseOperator, cosmos.operators.base.AbstractDbtBaseOperator
 [2024-11-27T14:58:01.493+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.dim_date_run, dagrun_id=scheduled__2024-09-27T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:58:00.924761+00:00, try_number=1, job_id=928, op_classpath=cosmos.operators.local.DbtRunLocalOperator, cosmos.operators.base.DbtRunMixin, cosmos.operators.local.DbtLocalBaseOperator, cosmos.operators.base.AbstractDbtBaseOperator
 [[34m2024-11-27T14:58:01.533+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:58:02.547+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
 [[34m2024-11-27T14:58:02.756+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:58:02.900+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:58:03.013+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 [[34m2024-11-27T14:58:03.087+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.weekly_metrics'[0m
 [[34m2024-11-27T14:58:03.087+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections'[0m
 [[34m2024-11-27T14:58:03.087+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_payments'[0m
 [[34m2024-11-27T14:58:03.087+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_orders'[0m
 [[34m2024-11-27T14:58:03.088+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_daily_bars'[0m
 [[34m2024-11-27T14:58:03.317+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:58:03.460+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:58:03.624+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 Data loaded successfully
 [[34m2024-11-27T14:58:03.655+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:58:03.670+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 [[34m2024-11-27T14:58:03.776+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T14:58:03.895+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:58:03.985+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:58:04.905+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.3760624909773469[0m
 [[34m2024-11-27T14:58:04.905+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|10873]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:58:04.906+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:58:04.906+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:58:04.906+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:58:04.907+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|10873]: It took 1.25s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:58:04.909+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|10873]: It took 0.00248s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:58:04.942+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:58:04.953+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.weekly_metrics_run scheduled__2024-09-26T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [[34m2024-11-27T14:58:05.040+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:58:05.085+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:58:05.202+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 [[34m2024-11-27T14:58:05.213+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:58:05.336+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 [2024-11-27T14:58:05.622+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.weekly_metrics_run, dagrun_id=scheduled__2024-09-26T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:58:05.057279+00:00, try_number=1, job_id=929, op_classpath=cosmos.operators.local.DbtRunLocalOperator, cosmos.operators.base.DbtRunMixin, cosmos.operators.local.DbtLocalBaseOperator, cosmos.operators.base.AbstractDbtBaseOperator
 Data loaded successfully
 [[34m2024-11-27T14:58:05.746+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:58:05.759+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 Data loaded successfully
 [[34m2024-11-27T14:58:05.860+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:58:05.881+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 [[34m2024-11-27T14:58:07.418+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 1.0682102930732071[0m
 [[34m2024-11-27T14:58:07.419+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|10884]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:58:07.420+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:58:07.420+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:58:07.420+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:58:07.421+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|10884]: It took 1.67s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:58:07.432+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|10884]: It took 0.0107s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:58:07.515+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.load_polygon_data_task scheduled__2024-09-29T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [[34m2024-11-27T14:58:07.719+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 1.047526012873277[0m
 [[34m2024-11-27T14:58:07.719+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|10883]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:58:07.720+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:58:07.721+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:58:07.722+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:58:07.722+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|10883]: It took 1.86s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:58:07.728+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|10883]: It took 0.00625s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:58:07.858+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.load_treasury_data_task scheduled__2024-09-28T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 /usr/local/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked semaphore objects to clean up at shutdown
   warnings.warn('resource_tracker: There appear to be %d '
 /usr/local/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked semaphore objects to clean up at shutdown
   warnings.warn('resource_tracker: There appear to be %d '
 [2024-11-27T14:58:08.344+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=load_polygon_data_task, dagrun_id=scheduled__2024-09-29T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:58:07.667319+00:00, try_number=1, job_id=930, op_classpath=airflow.operators.python.PythonOperator
 [2024-11-27T14:58:08.536+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=load_treasury_data_task, dagrun_id=scheduled__2024-09-28T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:58:08.041084+00:00, try_number=1, job_id=931, op_classpath=airflow.operators.python.PythonOperator
 [[34m2024-11-27T14:58:08.733+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.stg_daily_bars_run', run_id='scheduled__2024-09-27T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:58:08.733+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run', run_id='scheduled__2024-09-27T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:58:08.738+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.stg_daily_bars_run, run_id=scheduled__2024-09-27T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:58:00.369338+00:00, run_end_date=2024-11-27 14:58:08.048698+00:00, run_duration=7.67936, state=success, executor_state=success, try_number=1, max_tries=0, job_id=926, pool=default_pool, queue=default, priority_weight=2, operator=DbtRunLocalOperator, queued_dttm=2024-11-27 14:57:51.116300+00:00, queued_by_job_id=803, pid=10910[0m
 [[34m2024-11-27T14:58:08.738+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run, run_id=scheduled__2024-09-27T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:58:00.634267+00:00, run_end_date=2024-11-27 14:58:08.092069+00:00, run_duration=7.457802, state=success, executor_state=success, try_number=1, max_tries=0, job_id=927, pool=default_pool, queue=default, priority_weight=2, operator=DbtRunLocalOperator, queued_dttm=2024-11-27 14:57:51.116300+00:00, queued_by_job_id=803, pid=10919[0m
 /usr/local/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked semaphore objects to clean up at shutdown
   warnings.warn('resource_tracker: There appear to be %d '
 [[34m2024-11-27T14:58:09.980+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 2 tasks up for execution:
 	<TaskInstance: capstone_andres_dag_.load_polygon_data_task scheduled__2024-09-30T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.weekly_metrics_run scheduled__2024-09-27T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:58:09.980+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 2/16 running and queued tasks[0m
 [[34m2024-11-27T14:58:09.980+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 3/16 running and queued tasks[0m
 [[34m2024-11-27T14:58:09.981+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
 	<TaskInstance: capstone_andres_dag_.load_polygon_data_task scheduled__2024-09-30T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.weekly_metrics_run scheduled__2024-09-27T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:58:09.983+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='load_polygon_data_task', run_id='scheduled__2024-09-30T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
 [[34m2024-11-27T14:58:09.983+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'load_polygon_data_task', 'scheduled__2024-09-30T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:58:09.983+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.weekly_metrics_run', run_id='scheduled__2024-09-27T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
 [[34m2024-11-27T14:58:09.983+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.weekly_metrics_run', 'scheduled__2024-09-27T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:58:09.985+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.weekly_metrics_run', 'scheduled__2024-09-27T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:58:09.985+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'load_polygon_data_task', 'scheduled__2024-09-30T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:58:09.986+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='load_polygon_data_task', run_id='scheduled__2024-09-29T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:58:09.986+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.dim_date_run', run_id='scheduled__2024-09-27T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:58:09.990+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=load_polygon_data_task, run_id=scheduled__2024-09-29T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:58:07.667319+00:00, run_end_date=2024-11-27 14:58:09.004094+00:00, run_duration=1.336775, state=success, executor_state=success, try_number=1, max_tries=0, job_id=930, pool=default_pool, queue=default, priority_weight=6, operator=PythonOperator, queued_dttm=2024-11-27 14:57:56.759885+00:00, queued_by_job_id=803, pid=10967[0m
 [[34m2024-11-27T14:58:09.991+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.dim_date_run, run_id=scheduled__2024-09-27T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:58:00.924761+00:00, run_end_date=2024-11-27 14:58:09.287830+00:00, run_duration=8.363069, state=success, executor_state=success, try_number=1, max_tries=0, job_id=928, pool=default_pool, queue=default, priority_weight=2, operator=DbtRunLocalOperator, queued_dttm=2024-11-27 14:57:51.116300+00:00, queued_by_job_id=803, pid=10929[0m
 [[34m2024-11-27T14:58:10.022+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:58:10.023+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:58:10.903+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:58:10.904+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:58:10.904+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:58:10.905+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:58:10.906+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:58:10.906+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:58:11.360+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412095510,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T14:58:11.412+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412096910,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T14:58:11.510+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 4 tasks up for execution:
 	<TaskInstance: capstone_andres_dag_.load_treasury_data_task scheduled__2024-09-29T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run scheduled__2024-09-28T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.dim_date_run scheduled__2024-09-28T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_daily_bars_run scheduled__2024-09-28T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:58:11.511+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 3/16 running and queued tasks[0m
 [[34m2024-11-27T14:58:11.511+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 4/16 running and queued tasks[0m
 [[34m2024-11-27T14:58:11.511+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 5/16 running and queued tasks[0m
 [[34m2024-11-27T14:58:11.512+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 6/16 running and queued tasks[0m
 [[34m2024-11-27T14:58:11.512+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
 	<TaskInstance: capstone_andres_dag_.load_treasury_data_task scheduled__2024-09-29T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run scheduled__2024-09-28T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.dim_date_run scheduled__2024-09-28T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_daily_bars_run scheduled__2024-09-28T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:58:11.514+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='load_treasury_data_task', run_id='scheduled__2024-09-29T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 5 and queue default[0m
 [[34m2024-11-27T14:58:11.515+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'load_treasury_data_task', 'scheduled__2024-09-29T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:58:11.515+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run', run_id='scheduled__2024-09-28T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
 [[34m2024-11-27T14:58:11.516+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run', 'scheduled__2024-09-28T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:58:11.516+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.dim_date_run', run_id='scheduled__2024-09-28T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
 [[34m2024-11-27T14:58:11.516+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.dim_date_run', 'scheduled__2024-09-28T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:58:11.517+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.stg_daily_bars_run', run_id='scheduled__2024-09-28T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
 [[34m2024-11-27T14:58:11.517+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.stg_daily_bars_run', 'scheduled__2024-09-28T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:58:11.519+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'load_treasury_data_task', 'scheduled__2024-09-29T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:58:11.519+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run', 'scheduled__2024-09-28T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:58:11.520+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.dim_date_run', 'scheduled__2024-09-28T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:58:11.521+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='load_treasury_data_task', run_id='scheduled__2024-09-28T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:58:11.521+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.stg_daily_bars_run', 'scheduled__2024-09-28T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:58:11.527+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=load_treasury_data_task, run_id=scheduled__2024-09-28T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:58:08.041084+00:00, run_end_date=2024-11-27 14:58:10.672615+00:00, run_duration=2.631531, state=success, executor_state=success, try_number=1, max_tries=0, job_id=931, pool=default_pool, queue=default, priority_weight=5, operator=PythonOperator, queued_dttm=2024-11-27 14:57:56.759885+00:00, queued_by_job_id=803, pid=10968[0m
 [[34m2024-11-27T14:58:11.564+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:58:11.564+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:58:11.568+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:58:11.568+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:58:12.161+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:58:12.172+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 [[34m2024-11-27T14:58:12.719+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:58:12.825+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:58:12.825+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:58:12.826+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:58:12.826+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:58:12.827+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:58:12.828+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:58:12.828+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:58:12.828+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:58:12.829+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:58:12.830+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:58:12.835+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:58:12.837+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:58:12.837+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 /usr/local/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked semaphore objects to clean up at shutdown
   warnings.warn('resource_tracker: There appear to be %d '
 [[34m2024-11-27T14:58:13.347+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412096918,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T14:58:13.348+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412096922,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T14:58:13.352+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412095514,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T14:58:13.359+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412095518,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T14:58:13.806+0000[0m] {[34mdagrun.py:[0m851} INFO[0m - Marking run <DagRun capstone_andres_dag_ @ 2024-09-26 01:00:00+00:00: scheduled__2024-09-26T01:00:00+00:00, state:running, queued_at: 2024-11-27 14:54:01.623075+00:00. externally triggered: False> successful[0m
 [[34m2024-11-27T14:58:13.807+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=capstone_andres_dag_, execution_date=2024-09-26 01:00:00+00:00, run_id=scheduled__2024-09-26T01:00:00+00:00, run_start_date=2024-11-27 14:54:01.638624+00:00, run_end_date=2024-11-27 14:58:13.807022+00:00, run_duration=252.168398, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-09-26 01:00:00+00:00, data_interval_end=2024-09-27 01:00:00+00:00, dag_hash=77fa8f64adac9391cccaaae88e2be5d8[0m
 [[34m2024-11-27T14:58:13.815+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-09-27 01:00:00+00:00, run_after=2024-09-28 01:00:00+00:00[0m
 [[34m2024-11-27T14:58:14.325+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.weekly_metrics_run', run_id='scheduled__2024-09-26T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:58:14.338+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.weekly_metrics_run, run_id=scheduled__2024-09-26T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:58:05.057279+00:00, run_end_date=2024-11-27 14:58:13.180636+00:00, run_duration=8.123357, state=success, executor_state=success, try_number=1, max_tries=0, job_id=929, pool=default_pool, queue=default, priority_weight=1, operator=DbtRunLocalOperator, queued_dttm=2024-11-27 14:57:54.849073+00:00, queued_by_job_id=803, pid=10947[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:58:14.369+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:58:14.389+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:58:14.389+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:58:14.389+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 [[34m2024-11-27T14:58:14.905+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:58:14.905+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 [[34m2024-11-27T14:58:14.980+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:58:15.068+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:58:15.222+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-09-28 01:00:00+00:00, run_after=2024-09-29 01:00:00+00:00[0m
 [[34m2024-11-27T14:58:15.522+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:58:15.542+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:58:15.817+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T14:58:15.852+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T14:58:16.085+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:58:16.146+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:58:16.510+0000[0m] {[34mscheduler_job_runner.py:[0m1328} INFO[0m - DAG capstone_andres_dag_ is at (or above) max_active_runs (16 of 16), not creating any more runs[0m
 [[34m2024-11-27T14:58:17.214+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:58:17.266+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:58:17.369+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:58:17.419+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:58:17.481+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 [[34m2024-11-27T14:58:17.526+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 [[34m2024-11-27T14:58:17.797+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:58:17.849+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:58:17.859+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:58:17.953+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 Data loaded successfully
 [[34m2024-11-27T14:58:18.073+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:58:18.085+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 Data loaded successfully
 [[34m2024-11-27T14:58:18.132+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:58:18.139+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T14:58:18.146+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 [[34m2024-11-27T14:58:18.198+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T14:58:18.198+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T14:58:18.258+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T14:58:18.397+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:58:18.431+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:58:18.431+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:58:18.494+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:58:19.073+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.4542700839228928[0m
 [[34m2024-11-27T14:58:19.073+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|10977]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:58:19.074+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:58:19.074+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:58:19.075+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:58:19.075+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|10977]: It took 1.0s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:58:19.080+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|10977]: It took 0.00539s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:58:19.132+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.42139260401017964[0m
 [[34m2024-11-27T14:58:19.133+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|10978]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:58:19.133+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:58:19.134+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:58:19.134+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:58:19.134+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|10978]: It took 1.0s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:58:19.134+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.weekly_metrics_run scheduled__2024-09-27T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [[34m2024-11-27T14:58:19.139+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|10978]: It took 0.00477s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:58:19.201+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.load_polygon_data_task scheduled__2024-09-30T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [[34m2024-11-27T14:58:19.410+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:58:19.467+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:58:19.558+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:58:19.593+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:58:19.593+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:58:19.625+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:58:19.701+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 [[34m2024-11-27T14:58:19.727+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:58:19.734+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 [[34m2024-11-27T14:58:19.736+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:58:19.853+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 [[34m2024-11-27T14:58:19.851+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 [2024-11-27T14:58:19.868+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.weekly_metrics_run, dagrun_id=scheduled__2024-09-27T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:58:19.244362+00:00, try_number=1, job_id=932, op_classpath=cosmos.operators.local.DbtRunLocalOperator, cosmos.operators.base.DbtRunMixin, cosmos.operators.local.DbtLocalBaseOperator, cosmos.operators.base.AbstractDbtBaseOperator
 [2024-11-27T14:58:19.870+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=load_polygon_data_task, dagrun_id=scheduled__2024-09-30T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:58:19.349157+00:00, try_number=1, job_id=933, op_classpath=airflow.operators.python.PythonOperator
 Data loaded successfully
 [[34m2024-11-27T14:58:20.198+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:58:20.209+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 Data loaded successfully
 [[34m2024-11-27T14:58:20.244+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:58:20.260+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 Data loaded successfully
 [[34m2024-11-27T14:58:20.394+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:58:20.405+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 Data loaded successfully
 [[34m2024-11-27T14:58:20.596+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:58:20.609+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 [[34m2024-11-27T14:58:21.189+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
 	<TaskInstance: capstone_andres_dag_.load_polygon_data_task scheduled__2024-10-01T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:58:21.189+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 5/16 running and queued tasks[0m
 [[34m2024-11-27T14:58:21.189+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
 	<TaskInstance: capstone_andres_dag_.load_polygon_data_task scheduled__2024-10-01T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:58:21.192+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='load_polygon_data_task', run_id='scheduled__2024-10-01T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
 [[34m2024-11-27T14:58:21.193+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'load_polygon_data_task', 'scheduled__2024-10-01T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:58:21.198+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='load_polygon_data_task', run_id='scheduled__2024-09-30T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:58:21.198+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'load_polygon_data_task', 'scheduled__2024-10-01T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:58:21.205+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=load_polygon_data_task, run_id=scheduled__2024-09-30T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:58:19.349157+00:00, run_end_date=2024-11-27 14:58:20.752445+00:00, run_duration=1.403288, state=success, executor_state=success, try_number=1, max_tries=0, job_id=933, pool=default_pool, queue=default, priority_weight=6, operator=PythonOperator, queued_dttm=2024-11-27 14:58:09.981456+00:00, queued_by_job_id=803, pid=11085[0m
 [[34m2024-11-27T14:58:21.231+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:58:21.437+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.6796246729791164[0m
 [[34m2024-11-27T14:58:21.437+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|10990]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:58:21.437+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:58:21.438+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:58:21.438+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:58:21.438+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|10990]: It took 1.24s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:58:21.441+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|10990]: It took 0.00254s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:58:21.481+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run scheduled__2024-09-28T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [[34m2024-11-27T14:58:21.527+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.7086876269895583[0m
 [[34m2024-11-27T14:58:21.528+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|10989]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:58:21.528+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:58:21.529+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:58:21.529+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:58:21.529+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|10989]: It took 1.29s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:58:21.532+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|10989]: It took 0.00272s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:58:21.584+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.dim_date_run scheduled__2024-09-28T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [[34m2024-11-27T14:58:21.731+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.7611347660422325[0m
 [[34m2024-11-27T14:58:21.732+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|10988]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:58:21.733+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:58:21.733+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:58:21.734+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:58:21.734+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|10988]: It took 1.34s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:58:21.738+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|10988]: It took 0.00413s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:58:21.797+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.load_treasury_data_task scheduled__2024-09-29T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [[34m2024-11-27T14:58:22.107+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.8610902598593384[0m
 [[34m2024-11-27T14:58:22.108+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|10991]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:58:22.109+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:58:22.110+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:58:22.110+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:58:22.110+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|10991]: It took 1.51s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:58:22.115+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|10991]: It took 0.0049s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:58:22.286+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_daily_bars_run scheduled__2024-09-28T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [2024-11-27T14:58:22.335+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run, dagrun_id=scheduled__2024-09-28T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:58:21.572958+00:00, try_number=1, job_id=934, op_classpath=cosmos.operators.local.DbtRunLocalOperator, cosmos.operators.base.DbtRunMixin, cosmos.operators.local.DbtLocalBaseOperator, cosmos.operators.base.AbstractDbtBaseOperator
 [2024-11-27T14:58:22.575+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.dim_date_run, dagrun_id=scheduled__2024-09-28T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:58:21.694521+00:00, try_number=1, job_id=935, op_classpath=cosmos.operators.local.DbtRunLocalOperator, cosmos.operators.base.DbtRunMixin, cosmos.operators.local.DbtLocalBaseOperator, cosmos.operators.base.AbstractDbtBaseOperator
 [[34m2024-11-27T14:58:22.760+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [2024-11-27T14:58:22.761+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=load_treasury_data_task, dagrun_id=scheduled__2024-09-29T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:58:22.019181+00:00, try_number=1, job_id=936, op_classpath=airflow.operators.python.PythonOperator
 [[34m2024-11-27T14:58:22.762+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:58:22.763+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [2024-11-27T14:58:23.162+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.stg_daily_bars_run, dagrun_id=scheduled__2024-09-28T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:58:22.465866+00:00, try_number=1, job_id=937, op_classpath=cosmos.operators.local.DbtRunLocalOperator, cosmos.operators.base.DbtRunMixin, cosmos.operators.local.DbtLocalBaseOperator, cosmos.operators.base.AbstractDbtBaseOperator
 [[34m2024-11-27T14:58:23.308+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412096934,
 "os.name" : Linux
 [0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:58:24.146+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:58:24.304+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 4 tasks up for execution:
 	<TaskInstance: capstone_andres_dag_.load_treasury_data_task scheduled__2024-09-30T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run scheduled__2024-09-29T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_daily_bars_run scheduled__2024-09-29T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.dim_date_run scheduled__2024-09-29T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:58:24.305+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 5/16 running and queued tasks[0m
 [[34m2024-11-27T14:58:24.305+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 6/16 running and queued tasks[0m
 [[34m2024-11-27T14:58:24.305+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 7/16 running and queued tasks[0m
 [[34m2024-11-27T14:58:24.306+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 8/16 running and queued tasks[0m
 [[34m2024-11-27T14:58:24.306+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
 	<TaskInstance: capstone_andres_dag_.load_treasury_data_task scheduled__2024-09-30T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run scheduled__2024-09-29T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_daily_bars_run scheduled__2024-09-29T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.dim_date_run scheduled__2024-09-29T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:58:24.310+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='load_treasury_data_task', run_id='scheduled__2024-09-30T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 5 and queue default[0m
 [[34m2024-11-27T14:58:24.310+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'load_treasury_data_task', 'scheduled__2024-09-30T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:58:24.311+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run', run_id='scheduled__2024-09-29T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
 [[34m2024-11-27T14:58:24.311+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run', 'scheduled__2024-09-29T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:58:24.312+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.stg_daily_bars_run', run_id='scheduled__2024-09-29T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
 [[34m2024-11-27T14:58:24.312+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.stg_daily_bars_run', 'scheduled__2024-09-29T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:58:24.312+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.dim_date_run', run_id='scheduled__2024-09-29T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
 [[34m2024-11-27T14:58:24.312+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.dim_date_run', 'scheduled__2024-09-29T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:58:24.315+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'load_treasury_data_task', 'scheduled__2024-09-30T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:58:24.315+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run', 'scheduled__2024-09-29T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:58:24.318+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.dim_date_run', 'scheduled__2024-09-29T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:58:24.321+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='load_treasury_data_task', run_id='scheduled__2024-09-29T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:58:24.328+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=load_treasury_data_task, run_id=scheduled__2024-09-29T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:58:22.019181+00:00, run_end_date=2024-11-27 14:58:23.783636+00:00, run_duration=1.764455, state=success, executor_state=success, try_number=1, max_tries=0, job_id=936, pool=default_pool, queue=default, priority_weight=5, operator=PythonOperator, queued_dttm=2024-11-27 14:58:11.512974+00:00, queued_by_job_id=803, pid=11113[0m
 [[34m2024-11-27T14:58:24.322+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.stg_daily_bars_run', 'scheduled__2024-09-29T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:58:24.363+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:58:24.364+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:58:24.367+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:58:24.394+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 [[34m2024-11-27T14:58:24.885+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:58:25.666+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:58:25.666+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:58:25.667+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:58:25.671+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:58:25.672+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:58:25.673+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:58:25.680+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:58:25.681+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:58:25.682+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:58:25.707+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:58:25.707+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:58:25.708+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:58:26.142+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412095530,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T14:58:26.148+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412095534,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T14:58:26.171+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412095542,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T14:58:26.171+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412095538,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T14:58:26.484+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:58:26.775+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:58:26.958+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:58:26.971+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:58:26.986+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:58:27.100+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:58:27.162+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 /usr/local/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked semaphore objects to clean up at shutdown
   warnings.warn('resource_tracker: There appear to be %d '
 [[34m2024-11-27T14:58:27.468+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 [[34m2024-11-27T14:58:27.723+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 [[34m2024-11-27T14:58:27.825+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:58:27.954+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:58:28.174+0000[0m] {[34mdagrun.py:[0m851} INFO[0m - Marking run <DagRun capstone_andres_dag_ @ 2024-09-27 01:00:00+00:00: scheduled__2024-09-27T01:00:00+00:00, state:running, queued_at: 2024-11-27 14:54:18.686683+00:00. externally triggered: False> successful[0m
 [[34m2024-11-27T14:58:28.175+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=capstone_andres_dag_, execution_date=2024-09-27 01:00:00+00:00, run_id=scheduled__2024-09-27T01:00:00+00:00, run_start_date=2024-11-27 14:54:18.700790+00:00, run_end_date=2024-11-27 14:58:28.175258+00:00, run_duration=249.474468, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-09-27 01:00:00+00:00, data_interval_end=2024-09-28 01:00:00+00:00, dag_hash=77fa8f64adac9391cccaaae88e2be5d8[0m
 [[34m2024-11-27T14:58:28.176+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:58:28.179+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-09-28 01:00:00+00:00, run_after=2024-09-29 01:00:00+00:00[0m
 [[34m2024-11-27T14:58:28.356+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:58:28.409+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.weekly_metrics_run', run_id='scheduled__2024-09-27T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:58:28.414+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.weekly_metrics_run, run_id=scheduled__2024-09-27T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:58:19.244362+00:00, run_end_date=2024-11-27 14:58:27.431335+00:00, run_duration=8.186973, state=success, executor_state=success, try_number=1, max_tries=0, job_id=932, pool=default_pool, queue=default, priority_weight=1, operator=DbtRunLocalOperator, queued_dttm=2024-11-27 14:58:09.981456+00:00, queued_by_job_id=803, pid=11079[0m
 [[34m2024-11-27T14:58:28.467+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 Data loaded successfully
 [[34m2024-11-27T14:58:29.091+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:58:29.105+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 [[34m2024-11-27T14:58:29.435+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-09-29 01:00:00+00:00, run_after=2024-09-30 01:00:00+00:00[0m
 /usr/local/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked semaphore objects to clean up at shutdown
   warnings.warn('resource_tracker: There appear to be %d '
 [[34m2024-11-27T14:58:29.968+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.34974926616996527[0m
 [[34m2024-11-27T14:58:29.968+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|11099]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:58:29.969+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:58:29.969+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:58:29.969+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:58:29.969+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|11099]: It took 0.878s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:58:29.971+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|11099]: It took 0.0022s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:58:30.003+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.load_polygon_data_task scheduled__2024-10-01T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [[34m2024-11-27T14:58:30.075+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 /usr/local/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked semaphore objects to clean up at shutdown
   warnings.warn('resource_tracker: There appear to be %d '
 [[34m2024-11-27T14:58:30.264+0000[0m] {[34mscheduler_job_runner.py:[0m1328} INFO[0m - DAG capstone_andres_dag_ is at (or above) max_active_runs (16 of 16), not creating any more runs[0m
 [[34m2024-11-27T14:58:30.399+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:58:30.403+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [2024-11-27T14:58:30.479+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=load_polygon_data_task, dagrun_id=scheduled__2024-10-01T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:58:30.099745+00:00, try_number=1, job_id=938, op_classpath=airflow.operators.python.PythonOperator
 [[34m2024-11-27T14:58:30.511+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run', run_id='scheduled__2024-09-28T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:58:30.511+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.stg_daily_bars_run', run_id='scheduled__2024-09-28T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:58:30.514+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run, run_id=scheduled__2024-09-28T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:58:21.572958+00:00, run_end_date=2024-11-27 14:58:29.669095+00:00, run_duration=8.096137, state=success, executor_state=success, try_number=1, max_tries=0, job_id=934, pool=default_pool, queue=default, priority_weight=2, operator=DbtRunLocalOperator, queued_dttm=2024-11-27 14:58:11.512974+00:00, queued_by_job_id=803, pid=11106[0m
 [[34m2024-11-27T14:58:30.514+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.stg_daily_bars_run, run_id=scheduled__2024-09-28T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:58:22.465866+00:00, run_end_date=2024-11-27 14:58:30.130762+00:00, run_duration=7.664896, state=success, executor_state=success, try_number=1, max_tries=0, job_id=937, pool=default_pool, queue=default, priority_weight=2, operator=DbtRunLocalOperator, queued_dttm=2024-11-27 14:58:11.512974+00:00, queued_by_job_id=803, pid=11114[0m
 [[34m2024-11-27T14:58:30.548+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:58:30.667+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:58:30.691+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:58:30.692+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T14:58:30.833+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T14:58:30.997+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T14:58:31.012+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:58:31.170+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 /usr/local/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked semaphore objects to clean up at shutdown
   warnings.warn('resource_tracker: There appear to be %d '
 [[34m2024-11-27T14:58:31.362+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:58:31.717+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.weekly_metrics_run scheduled__2024-09-28T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:58:31.717+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 5/16 running and queued tasks[0m
 [[34m2024-11-27T14:58:31.717+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.weekly_metrics_run scheduled__2024-09-28T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:58:31.719+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.weekly_metrics_run', run_id='scheduled__2024-09-28T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
 [[34m2024-11-27T14:58:31.719+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.weekly_metrics_run', 'scheduled__2024-09-28T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:58:31.720+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.weekly_metrics_run', 'scheduled__2024-09-28T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:58:31.721+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.dim_date_run', run_id='scheduled__2024-09-28T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:58:31.724+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.dim_date_run, run_id=scheduled__2024-09-28T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:58:21.694521+00:00, run_end_date=2024-11-27 14:58:31.146438+00:00, run_duration=9.451917, state=success, executor_state=success, try_number=1, max_tries=0, job_id=935, pool=default_pool, queue=default, priority_weight=2, operator=DbtRunLocalOperator, queued_dttm=2024-11-27 14:58:11.512974+00:00, queued_by_job_id=803, pid=11112[0m
 [[34m2024-11-27T14:58:31.747+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:58:31.915+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:58:32.091+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:58:32.199+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 [[34m2024-11-27T14:58:32.313+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:58:32.411+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:58:32.486+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:58:32.597+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 [[34m2024-11-27T14:58:32.602+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:58:32.650+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:58:32.651+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:58:32.651+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:58:32.696+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:58:32.708+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 Data loaded successfully
 [[34m2024-11-27T14:58:32.852+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:58:32.864+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 [[34m2024-11-27T14:58:32.877+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:58:32.987+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 [[34m2024-11-27T14:58:33.149+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412095570,
 "os.name" : Linux
 [0m
 Data loaded successfully
 [[34m2024-11-27T14:58:33.161+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:58:33.173+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 Data loaded successfully
 [[34m2024-11-27T14:58:33.381+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:58:33.394+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 Data loaded successfully
 [[34m2024-11-27T14:58:33.500+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:58:33.514+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 [[34m2024-11-27T14:58:33.790+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.41469023283571005[0m
 [[34m2024-11-27T14:58:33.790+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|11119]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:58:33.791+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:58:33.791+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:58:33.791+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:58:33.791+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|11119]: It took 0.94s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:58:33.794+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|11119]: It took 0.00315s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:58:33.832+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.load_treasury_data_task scheduled__2024-09-30T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:58:33.991+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:58:34.203+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.49998128204606473[0m
 [[34m2024-11-27T14:58:34.203+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|11121]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:58:34.204+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:58:34.204+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:58:34.204+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:58:34.205+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|11121]: It took 1.04s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:58:34.208+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|11121]: It took 0.00337s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:58:34.245+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.dim_date_run scheduled__2024-09-29T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [2024-11-27T14:58:34.275+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=load_treasury_data_task, dagrun_id=scheduled__2024-09-30T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:58:33.933241+00:00, try_number=1, job_id=939, op_classpath=airflow.operators.python.PythonOperator
 [[34m2024-11-27T14:58:34.328+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.378853120142594[0m
 [[34m2024-11-27T14:58:34.329+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|11120]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:58:34.329+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:58:34.329+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:58:34.330+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:58:34.330+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|11120]: It took 0.949s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:58:34.332+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|11120]: It took 0.0025s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:58:34.370+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run scheduled__2024-09-29T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [[34m2024-11-27T14:58:34.461+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.4136832950171083[0m
 [[34m2024-11-27T14:58:34.461+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|11122]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:58:34.462+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:58:34.462+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:58:34.462+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:58:34.462+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|11122]: It took 0.962s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:58:34.466+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|11122]: It took 0.00363s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:58:34.516+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_daily_bars_run scheduled__2024-09-29T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 [2024-11-27T14:58:34.764+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.dim_date_run, dagrun_id=scheduled__2024-09-29T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:58:34.323469+00:00, try_number=1, job_id=940, op_classpath=cosmos.operators.local.DbtRunLocalOperator, cosmos.operators.base.DbtRunMixin, cosmos.operators.local.DbtLocalBaseOperator, cosmos.operators.base.AbstractDbtBaseOperator
 [[34m2024-11-27T14:58:34.775+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [2024-11-27T14:58:34.922+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run, dagrun_id=scheduled__2024-09-29T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:58:34.450645+00:00, try_number=1, job_id=941, op_classpath=cosmos.operators.local.DbtRunLocalOperator, cosmos.operators.base.DbtRunMixin, cosmos.operators.local.DbtLocalBaseOperator, cosmos.operators.base.AbstractDbtBaseOperator
 [2024-11-27T14:58:35.054+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.stg_daily_bars_run, dagrun_id=scheduled__2024-09-29T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:58:34.609344+00:00, try_number=1, job_id=942, op_classpath=cosmos.operators.local.DbtRunLocalOperator, cosmos.operators.base.DbtRunMixin, cosmos.operators.local.DbtLocalBaseOperator, cosmos.operators.base.AbstractDbtBaseOperator
 [[34m2024-11-27T14:58:35.730+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_daily_bars_run scheduled__2024-09-30T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run scheduled__2024-09-30T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.dim_date_run scheduled__2024-09-30T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:58:35.730+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 5/16 running and queued tasks[0m
 [[34m2024-11-27T14:58:35.730+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 6/16 running and queued tasks[0m
 [[34m2024-11-27T14:58:35.731+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 7/16 running and queued tasks[0m
 [[34m2024-11-27T14:58:35.731+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_daily_bars_run scheduled__2024-09-30T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run scheduled__2024-09-30T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.dim_date_run scheduled__2024-09-30T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:58:35.733+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.stg_daily_bars_run', run_id='scheduled__2024-09-30T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
 [[34m2024-11-27T14:58:35.733+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.stg_daily_bars_run', 'scheduled__2024-09-30T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:58:35.733+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run', run_id='scheduled__2024-09-30T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
 [[34m2024-11-27T14:58:35.734+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run', 'scheduled__2024-09-30T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:58:35.734+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.dim_date_run', run_id='scheduled__2024-09-30T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
 [[34m2024-11-27T14:58:35.734+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.dim_date_run', 'scheduled__2024-09-30T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:58:35.737+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run', 'scheduled__2024-09-30T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:58:35.738+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='load_treasury_data_task', run_id='scheduled__2024-09-30T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:58:35.744+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=load_treasury_data_task, run_id=scheduled__2024-09-30T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:58:33.933241+00:00, run_end_date=2024-11-27 14:58:35.250694+00:00, run_duration=1.317453, state=success, executor_state=success, try_number=1, max_tries=0, job_id=939, pool=default_pool, queue=default, priority_weight=5, operator=PythonOperator, queued_dttm=2024-11-27 14:58:24.307442+00:00, queued_by_job_id=803, pid=11241[0m
 [[34m2024-11-27T14:58:35.743+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.stg_daily_bars_run', 'scheduled__2024-09-30T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:58:35.746+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.dim_date_run', 'scheduled__2024-09-30T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:58:35.774+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:58:35.790+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:58:35.801+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:58:37.252+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:58:37.252+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:58:37.253+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:58:37.255+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:58:37.256+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:58:37.256+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:58:37.270+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:58:37.271+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:58:37.271+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:58:37.409+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:58:37.775+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T14:58:37.782+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412095582,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T14:58:37.786+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412096978,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T14:58:37.791+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412095586,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T14:58:38.038+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:58:38.604+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:58:38.610+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:58:38.704+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 [[34m2024-11-27T14:58:38.898+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 2 tasks up for execution:
 	<TaskInstance: capstone_andres_dag_.load_polygon_data_task scheduled__2024-10-02T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.load_treasury_data_task scheduled__2024-10-01T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:58:38.898+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 7/16 running and queued tasks[0m
 [[34m2024-11-27T14:58:38.899+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 8/16 running and queued tasks[0m
 [[34m2024-11-27T14:58:38.899+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
 	<TaskInstance: capstone_andres_dag_.load_polygon_data_task scheduled__2024-10-02T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.load_treasury_data_task scheduled__2024-10-01T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:58:38.902+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='load_polygon_data_task', run_id='scheduled__2024-10-02T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
 [[34m2024-11-27T14:58:38.902+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'load_polygon_data_task', 'scheduled__2024-10-02T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:58:38.903+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='load_treasury_data_task', run_id='scheduled__2024-10-01T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 5 and queue default[0m
 [[34m2024-11-27T14:58:38.903+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'load_treasury_data_task', 'scheduled__2024-10-01T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:58:38.905+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'load_polygon_data_task', 'scheduled__2024-10-02T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:58:38.906+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='load_polygon_data_task', run_id='scheduled__2024-10-01T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:58:38.905+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'load_treasury_data_task', 'scheduled__2024-10-01T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:58:38.911+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=load_polygon_data_task, run_id=scheduled__2024-10-01T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:58:30.099745+00:00, run_end_date=2024-11-27 14:58:37.405564+00:00, run_duration=7.305819, state=success, executor_state=success, try_number=1, max_tries=0, job_id=938, pool=default_pool, queue=default, priority_weight=6, operator=PythonOperator, queued_dttm=2024-11-27 14:58:21.190920+00:00, queued_by_job_id=803, pid=11221[0m
 [[34m2024-11-27T14:58:38.938+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:58:38.938+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 [[34m2024-11-27T14:58:39.047+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:58:39.047+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:58:39.062+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:58:39.082+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:58:39.201+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:58:39.311+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 Data loaded successfully
 [[34m2024-11-27T14:58:39.857+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:58:39.873+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 [[34m2024-11-27T14:58:40.071+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:58:40.072+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:58:40.072+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:58:40.083+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:58:40.084+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:58:40.084+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:58:40.559+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412095606,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T14:58:40.583+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412096990,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T14:58:40.596+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:58:40.704+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.30789627390913665[0m
 [[34m2024-11-27T14:58:40.705+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|11230]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:58:40.705+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:58:40.706+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:58:40.706+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:58:40.706+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|11230]: It took 0.849s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:58:40.709+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|11230]: It took 0.00261s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:58:40.741+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.weekly_metrics_run scheduled__2024-09-28T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [[34m2024-11-27T14:58:40.913+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T14:58:41.192+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [2024-11-27T14:58:41.265+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.weekly_metrics_run, dagrun_id=scheduled__2024-09-28T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:58:40.819966+00:00, try_number=1, job_id=943, op_classpath=cosmos.operators.local.DbtRunLocalOperator, cosmos.operators.base.DbtRunMixin, cosmos.operators.local.DbtLocalBaseOperator, cosmos.operators.base.AbstractDbtBaseOperator
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:58:41.369+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:58:41.371+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:58:41.784+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:58:41.807+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 /usr/local/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked semaphore objects to clean up at shutdown
   warnings.warn('resource_tracker: There appear to be %d '
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 [[34m2024-11-27T14:58:42.078+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T14:58:42.111+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 /usr/local/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked semaphore objects to clean up at shutdown
   warnings.warn('resource_tracker: There appear to be %d '
 [[34m2024-11-27T14:58:42.166+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 [[34m2024-11-27T14:58:42.233+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run', run_id='scheduled__2024-09-29T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:58:42.233+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.stg_daily_bars_run', run_id='scheduled__2024-09-29T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:58:42.237+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.stg_daily_bars_run, run_id=scheduled__2024-09-29T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:58:34.609344+00:00, run_end_date=2024-11-27 14:58:42.083614+00:00, run_duration=7.47427, state=success, executor_state=success, try_number=1, max_tries=0, job_id=942, pool=default_pool, queue=default, priority_weight=2, operator=DbtRunLocalOperator, queued_dttm=2024-11-27 14:58:24.307442+00:00, queued_by_job_id=803, pid=11244[0m
 [[34m2024-11-27T14:58:42.237+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run, run_id=scheduled__2024-09-29T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:58:34.450645+00:00, run_end_date=2024-11-27 14:58:41.812327+00:00, run_duration=7.361682, state=success, executor_state=success, try_number=1, max_tries=0, job_id=941, pool=default_pool, queue=default, priority_weight=2, operator=DbtRunLocalOperator, queued_dttm=2024-11-27 14:58:24.307442+00:00, queued_by_job_id=803, pid=11243[0m
 [[34m2024-11-27T14:58:42.347+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:58:42.359+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:58:42.362+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:58:42.373+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:58:42.529+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:58:42.632+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 /usr/local/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked semaphore objects to clean up at shutdown
   warnings.warn('resource_tracker: There appear to be %d '
 Data loaded successfully
 [[34m2024-11-27T14:58:43.207+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:58:43.220+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 [[34m2024-11-27T14:58:43.377+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:58:43.424+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.weekly_metrics_run scheduled__2024-09-29T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:58:43.424+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 6/16 running and queued tasks[0m
 [[34m2024-11-27T14:58:43.425+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.weekly_metrics_run scheduled__2024-09-29T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:58:43.426+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.weekly_metrics_run', run_id='scheduled__2024-09-29T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
 [[34m2024-11-27T14:58:43.426+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.weekly_metrics_run', 'scheduled__2024-09-29T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:58:43.428+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.dim_date_run', run_id='scheduled__2024-09-29T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:58:43.428+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.weekly_metrics_run', 'scheduled__2024-09-29T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:58:43.431+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.dim_date_run, run_id=scheduled__2024-09-29T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:58:34.323469+00:00, run_end_date=2024-11-27 14:58:42.800206+00:00, run_duration=8.476737, state=success, executor_state=success, try_number=1, max_tries=0, job_id=940, pool=default_pool, queue=default, priority_weight=2, operator=DbtRunLocalOperator, queued_dttm=2024-11-27 14:58:24.307442+00:00, queued_by_job_id=803, pid=11242[0m
 [[34m2024-11-27T14:58:43.445+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:58:43.454+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:58:43.587+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:58:43.599+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:58:43.693+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 [[34m2024-11-27T14:58:43.719+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:58:43.723+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 [[34m2024-11-27T14:58:44.062+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T14:58:44.098+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.34388884995132685[0m
 [[34m2024-11-27T14:58:44.098+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|11254]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:58:44.098+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:58:44.099+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:58:44.099+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:58:44.099+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|11254]: It took 0.893s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:58:44.102+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|11254]: It took 0.00317s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:58:44.138+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_daily_bars_run scheduled__2024-09-30T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 Data loaded successfully
 [[34m2024-11-27T14:58:44.219+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:58:44.232+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 [[34m2024-11-27T14:58:44.338+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:58:44.338+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:58:44.338+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:58:44.339+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 Data loaded successfully
 [[34m2024-11-27T14:58:44.355+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:58:44.367+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 [2024-11-27T14:58:44.650+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.stg_daily_bars_run, dagrun_id=scheduled__2024-09-30T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:58:44.216881+00:00, try_number=1, job_id=944, op_classpath=cosmos.operators.local.DbtRunLocalOperator, cosmos.operators.base.DbtRunMixin, cosmos.operators.local.DbtLocalBaseOperator, cosmos.operators.base.AbstractDbtBaseOperator
 [[34m2024-11-27T14:58:44.799+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:58:44.827+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412095626,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T14:58:45.112+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T14:58:45.128+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.35810686298646033[0m
 [[34m2024-11-27T14:58:45.128+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|11253]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:58:45.129+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:58:45.129+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:58:45.129+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:58:45.129+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|11253]: It took 0.91s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:58:45.132+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|11253]: It took 0.00231s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:58:45.171+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run scheduled__2024-09-30T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [[34m2024-11-27T14:58:45.331+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.422665833029896[0m
 [[34m2024-11-27T14:58:45.332+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|11255]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:58:45.332+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:58:45.333+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:58:45.333+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:58:45.333+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|11255]: It took 0.978s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:58:45.336+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|11255]: It took 0.00268s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:58:45.368+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:58:45.382+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.dim_date_run scheduled__2024-09-30T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [[34m2024-11-27T14:58:45.534+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:58:45.575+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [2024-11-27T14:58:45.704+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run, dagrun_id=scheduled__2024-09-30T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:58:45.250238+00:00, try_number=1, job_id=945, op_classpath=cosmos.operators.local.DbtRunLocalOperator, cosmos.operators.base.DbtRunMixin, cosmos.operators.local.DbtLocalBaseOperator, cosmos.operators.base.AbstractDbtBaseOperator
 [[34m2024-11-27T14:58:45.710+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:58:45.820+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 [2024-11-27T14:58:46.054+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.dim_date_run, dagrun_id=scheduled__2024-09-30T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:58:45.467184+00:00, try_number=1, job_id=946, op_classpath=cosmos.operators.local.DbtRunLocalOperator, cosmos.operators.base.DbtRunMixin, cosmos.operators.local.DbtLocalBaseOperator, cosmos.operators.base.AbstractDbtBaseOperator
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 Data loaded successfully
 [[34m2024-11-27T14:58:46.344+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:58:46.361+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 [[34m2024-11-27T14:58:46.454+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:58:46.479+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:58:46.685+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:58:46.819+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 Data loaded successfully
 [[34m2024-11-27T14:58:47.925+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:58:48.029+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 [[34m2024-11-27T14:58:48.273+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 /usr/local/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked semaphore objects to clean up at shutdown
   warnings.warn('resource_tracker: There appear to be %d '
 [[34m2024-11-27T14:58:48.572+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T14:58:48.848+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 1.7455726559273899[0m
 [[34m2024-11-27T14:58:48.849+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|11270]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:58:48.850+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:58:48.851+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:58:48.852+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:58:48.853+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|11270]: It took 2.51s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:58:48.858+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:58:48.862+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|11270]: It took 0.0091s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:58:48.956+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.load_polygon_data_task scheduled__2024-10-02T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [[34m2024-11-27T14:58:49.447+0000[0m] {[34mdagrun.py:[0m851} INFO[0m - Marking run <DagRun capstone_andres_dag_ @ 2024-09-28 01:00:00+00:00: scheduled__2024-09-28T01:00:00+00:00, state:running, queued_at: 2024-11-27 14:54:37.495431+00:00. externally triggered: False> successful[0m
 [[34m2024-11-27T14:58:49.448+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=capstone_andres_dag_, execution_date=2024-09-28 01:00:00+00:00, run_id=scheduled__2024-09-28T01:00:00+00:00, run_start_date=2024-11-27 14:54:37.547014+00:00, run_end_date=2024-11-27 14:58:49.448241+00:00, run_duration=251.901227, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-09-28 01:00:00+00:00, data_interval_end=2024-09-29 01:00:00+00:00, dag_hash=77fa8f64adac9391cccaaae88e2be5d8[0m
 [[34m2024-11-27T14:58:49.456+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-09-29 01:00:00+00:00, run_after=2024-09-30 01:00:00+00:00[0m
 [[34m2024-11-27T14:58:49.713+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.8899064450524747[0m
 [[34m2024-11-27T14:58:49.714+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|11271]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:58:49.715+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:58:49.715+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:58:49.715+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:58:49.716+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|11271]: It took 1.79s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:58:49.720+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|11271]: It took 0.00478s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:58:49.789+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.load_treasury_data_task scheduled__2024-10-01T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [2024-11-27T14:58:49.797+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=load_polygon_data_task, dagrun_id=scheduled__2024-10-02T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:58:49.117160+00:00, try_number=1, job_id=947, op_classpath=airflow.operators.python.PythonOperator
 [[34m2024-11-27T14:58:49.826+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.weekly_metrics_run', run_id='scheduled__2024-09-28T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:58:49.834+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.weekly_metrics_run, run_id=scheduled__2024-09-28T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:58:40.819966+00:00, run_end_date=2024-11-27 14:58:48.133120+00:00, run_duration=7.313154, state=success, executor_state=success, try_number=1, max_tries=0, job_id=943, pool=default_pool, queue=default, priority_weight=1, operator=DbtRunLocalOperator, queued_dttm=2024-11-27 14:58:31.718107+00:00, queued_by_job_id=803, pid=11317[0m
 [[34m2024-11-27T14:58:49.888+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:58:50.055+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:58:50.174+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 [2024-11-27T14:58:50.530+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=load_treasury_data_task, dagrun_id=scheduled__2024-10-01T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:58:49.924953+00:00, try_number=1, job_id=948, op_classpath=airflow.operators.python.PythonOperator
 Data loaded successfully
 [[34m2024-11-27T14:58:50.688+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:58:50.714+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 [[34m2024-11-27T14:58:50.859+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-09-30 01:00:00+00:00, run_after=2024-10-01 01:00:00+00:00[0m
 [[34m2024-11-27T14:58:51.393+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-10-01 01:00:00+00:00, run_after=2024-10-02 01:00:00+00:00[0m
 [[34m2024-11-27T14:58:51.965+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.5554929890204221[0m
 [[34m2024-11-27T14:58:51.965+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|11337]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:58:51.966+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:58:51.966+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:58:51.966+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:58:51.967+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|11337]: It took 1.28s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:58:51.969+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|11337]: It took 0.00243s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:58:52.008+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.weekly_metrics_run scheduled__2024-09-29T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [2024-11-27T14:58:52.587+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.weekly_metrics_run, dagrun_id=scheduled__2024-09-29T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:58:52.106730+00:00, try_number=1, job_id=949, op_classpath=cosmos.operators.local.DbtRunLocalOperator, cosmos.operators.base.DbtRunMixin, cosmos.operators.local.DbtLocalBaseOperator, cosmos.operators.base.AbstractDbtBaseOperator
 [[34m2024-11-27T14:58:52.711+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-10-02 01:00:00+00:00, run_after=2024-10-03 01:00:00+00:00[0m
 [[34m2024-11-27T14:58:52.938+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.dim_date_run scheduled__2024-10-01T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_daily_bars_run scheduled__2024-10-01T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run scheduled__2024-10-01T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:58:52.938+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 5/16 running and queued tasks[0m
 [[34m2024-11-27T14:58:52.939+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 6/16 running and queued tasks[0m
 [[34m2024-11-27T14:58:52.939+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 7/16 running and queued tasks[0m
 [[34m2024-11-27T14:58:52.939+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.dim_date_run scheduled__2024-10-01T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_daily_bars_run scheduled__2024-10-01T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run scheduled__2024-10-01T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:58:52.941+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.dim_date_run', run_id='scheduled__2024-10-01T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
 [[34m2024-11-27T14:58:52.941+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.dim_date_run', 'scheduled__2024-10-01T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:58:52.942+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.stg_daily_bars_run', run_id='scheduled__2024-10-01T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
 [[34m2024-11-27T14:58:52.942+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.stg_daily_bars_run', 'scheduled__2024-10-01T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:58:52.942+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run', run_id='scheduled__2024-10-01T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
 [[34m2024-11-27T14:58:52.942+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run', 'scheduled__2024-10-01T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:58:52.945+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='load_treasury_data_task', run_id='scheduled__2024-10-01T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:58:52.945+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.dim_date_run', 'scheduled__2024-10-01T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:58:52.946+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.stg_daily_bars_run', 'scheduled__2024-10-01T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:58:52.946+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run', 'scheduled__2024-10-01T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:58:52.951+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=load_treasury_data_task, run_id=scheduled__2024-10-01T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:58:49.924953+00:00, run_end_date=2024-11-27 14:58:52.662836+00:00, run_duration=2.737883, state=success, executor_state=success, try_number=1, max_tries=0, job_id=948, pool=default_pool, queue=default, priority_weight=5, operator=PythonOperator, queued_dttm=2024-11-27 14:58:38.900142+00:00, queued_by_job_id=803, pid=11388[0m
 [[34m2024-11-27T14:58:52.972+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-10-03 01:00:00+00:00, run_after=2024-10-04 01:00:00+00:00[0m
 [[34m2024-11-27T14:58:52.984+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:58:52.989+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:58:52.991+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 /usr/local/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked semaphore objects to clean up at shutdown
   warnings.warn('resource_tracker: There appear to be %d '
 [[34m2024-11-27T14:58:54.017+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:58:54.017+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:58:54.018+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:58:54.018+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:58:54.019+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:58:54.019+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:58:54.036+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:58:54.037+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:58:54.037+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:58:54.327+0000[0m] {[34mscheduler_job_runner.py:[0m1328} INFO[0m - DAG capstone_andres_dag_ is at (or above) max_active_runs (16 of 16), not creating any more runs[0m
 [[34m2024-11-27T14:58:54.466+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412095670,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T14:58:54.467+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412095666,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T14:58:54.475+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412095674,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T14:58:54.551+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.stg_daily_bars_run', run_id='scheduled__2024-09-30T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:58:54.555+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.stg_daily_bars_run, run_id=scheduled__2024-09-30T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:58:44.216881+00:00, run_end_date=2024-11-27 14:58:53.230200+00:00, run_duration=9.013319, state=success, executor_state=success, try_number=1, max_tries=0, job_id=944, pool=default_pool, queue=default, priority_weight=2, operator=DbtRunLocalOperator, queued_dttm=2024-11-27 14:58:35.731683+00:00, queued_by_job_id=803, pid=11340[0m
 /usr/local/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked semaphore objects to clean up at shutdown
   warnings.warn('resource_tracker: There appear to be %d '
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:58:55.134+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:58:55.145+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:58:55.154+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 [[34m2024-11-27T14:58:55.563+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:58:55.596+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:58:55.600+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:58:55.831+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run', run_id='scheduled__2024-09-30T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:58:55.835+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run, run_id=scheduled__2024-09-30T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:58:45.250238+00:00, run_end_date=2024-11-27 14:58:54.558581+00:00, run_duration=9.308343, state=success, executor_state=success, try_number=1, max_tries=0, job_id=945, pool=default_pool, queue=default, priority_weight=2, operator=DbtRunLocalOperator, queued_dttm=2024-11-27 14:58:35.731683+00:00, queued_by_job_id=803, pid=11345[0m
 /usr/local/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked semaphore objects to clean up at shutdown
   warnings.warn('resource_tracker: There appear to be %d '
 [[34m2024-11-27T14:58:56.565+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.weekly_metrics_run scheduled__2024-09-30T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:58:56.565+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 5/16 running and queued tasks[0m
 [[34m2024-11-27T14:58:56.566+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.weekly_metrics_run scheduled__2024-09-30T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:58:56.567+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.weekly_metrics_run', run_id='scheduled__2024-09-30T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
 [[34m2024-11-27T14:58:56.568+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.weekly_metrics_run', 'scheduled__2024-09-30T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:58:56.571+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.dim_date_run', run_id='scheduled__2024-09-30T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:58:56.570+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.weekly_metrics_run', 'scheduled__2024-09-30T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:58:56.576+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.dim_date_run, run_id=scheduled__2024-09-30T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:58:45.467184+00:00, run_end_date=2024-11-27 14:58:56.080870+00:00, run_duration=10.613686, state=success, executor_state=success, try_number=1, max_tries=0, job_id=946, pool=default_pool, queue=default, priority_weight=2, operator=DbtRunLocalOperator, queued_dttm=2024-11-27 14:58:35.731683+00:00, queued_by_job_id=803, pid=11346[0m
 [[34m2024-11-27T14:58:56.602+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:58:57.473+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:58:57.474+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:58:57.474+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:58:57.937+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412095694,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T14:58:58.042+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 2 tasks up for execution:
 	<TaskInstance: capstone_andres_dag_.load_polygon_data_task scheduled__2024-10-03T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.load_treasury_data_task scheduled__2024-10-02T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:58:58.042+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 5/16 running and queued tasks[0m
 [[34m2024-11-27T14:58:58.043+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 6/16 running and queued tasks[0m
 [[34m2024-11-27T14:58:58.043+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
 	<TaskInstance: capstone_andres_dag_.load_polygon_data_task scheduled__2024-10-03T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.load_treasury_data_task scheduled__2024-10-02T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:58:58.045+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='load_polygon_data_task', run_id='scheduled__2024-10-03T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
 [[34m2024-11-27T14:58:58.045+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'load_polygon_data_task', 'scheduled__2024-10-03T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:58:58.045+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='load_treasury_data_task', run_id='scheduled__2024-10-02T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 5 and queue default[0m
 [[34m2024-11-27T14:58:58.045+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'load_treasury_data_task', 'scheduled__2024-10-02T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:58:58.047+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'load_polygon_data_task', 'scheduled__2024-10-03T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:58:58.049+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='load_polygon_data_task', run_id='scheduled__2024-10-02T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:58:58.049+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'load_treasury_data_task', 'scheduled__2024-10-02T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:58:58.053+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=load_polygon_data_task, run_id=scheduled__2024-10-02T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:58:49.117160+00:00, run_end_date=2024-11-27 14:58:56.841969+00:00, run_duration=7.724809, state=success, executor_state=success, try_number=1, max_tries=0, job_id=947, pool=default_pool, queue=default, priority_weight=6, operator=PythonOperator, queued_dttm=2024-11-27 14:58:38.900142+00:00, queued_by_job_id=803, pid=11379[0m
 [[34m2024-11-27T14:58:58.075+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:58:58.080+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:58:58.176+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:58:58.201+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:58:58.392+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:58:58.502+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T14:58:58.539+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T14:58:58.703+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:58:58.772+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:58:58.822+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 /usr/local/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked semaphore objects to clean up at shutdown
   warnings.warn('resource_tracker: There appear to be %d '
 [[34m2024-11-27T14:58:58.921+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:58:58.933+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:58:58.934+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:58:58.934+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:58:58.952+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:58:58.986+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:58:58.987+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:58:58.987+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 [[34m2024-11-27T14:58:59.177+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:58:59.319+0000[0m] {[34mdagrun.py:[0m851} INFO[0m - Marking run <DagRun capstone_andres_dag_ @ 2024-09-29 01:00:00+00:00: scheduled__2024-09-29T01:00:00+00:00, state:running, queued_at: 2024-11-27 14:54:41.496084+00:00. externally triggered: False> successful[0m
 [[34m2024-11-27T14:58:59.319+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=capstone_andres_dag_, execution_date=2024-09-29 01:00:00+00:00, run_id=scheduled__2024-09-29T01:00:00+00:00, run_start_date=2024-11-27 14:54:41.518359+00:00, run_end_date=2024-11-27 14:58:59.319447+00:00, run_duration=257.801088, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-09-29 01:00:00+00:00, data_interval_end=2024-09-30 01:00:00+00:00, dag_hash=77fa8f64adac9391cccaaae88e2be5d8[0m
 [[34m2024-11-27T14:58:59.322+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-09-30 01:00:00+00:00, run_after=2024-10-01 01:00:00+00:00[0m
 [[34m2024-11-27T14:58:59.361+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412095698,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T14:58:59.424+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412095702,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T14:58:59.502+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.weekly_metrics_run', run_id='scheduled__2024-09-29T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:58:59.506+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.weekly_metrics_run, run_id=scheduled__2024-09-29T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:58:52.106730+00:00, run_end_date=2024-11-27 14:58:58.847200+00:00, run_duration=6.74047, state=success, executor_state=success, try_number=1, max_tries=0, job_id=949, pool=default_pool, queue=default, priority_weight=1, operator=DbtRunLocalOperator, queued_dttm=2024-11-27 14:58:43.425482+00:00, queued_by_job_id=803, pid=11406[0m
 [[34m2024-11-27T14:58:59.811+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:58:59.915+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:58:59.947+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:59:00.058+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:59:00.059+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:59:00.069+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 [[34m2024-11-27T14:59:00.070+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:59:00.115+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:59:00.180+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 [[34m2024-11-27T14:59:00.191+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:59:00.307+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 [[34m2024-11-27T14:59:00.525+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-10-01 01:00:00+00:00, run_after=2024-10-02 01:00:00+00:00[0m
 [[34m2024-11-27T14:59:00.554+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:59:00.604+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 Data loaded successfully
 [[34m2024-11-27T14:59:00.655+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 Data loaded successfully
 [[34m2024-11-27T14:59:00.659+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:59:00.678+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 [[34m2024-11-27T14:59:00.682+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 Data loaded successfully
 [[34m2024-11-27T14:59:00.949+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:59:00.962+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 [[34m2024-11-27T14:59:01.525+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.3292347320821136[0m
 [[34m2024-11-27T14:59:01.526+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|11416]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:59:01.527+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:59:01.527+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:59:01.527+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:59:01.527+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|11416]: It took 0.868s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:59:01.532+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|11416]: It took 0.00482s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:59:01.563+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.3586235938128084[0m
 [[34m2024-11-27T14:59:01.564+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|11418]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:59:01.564+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:59:01.565+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:59:01.565+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:59:01.565+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|11418]: It took 0.91s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:59:01.568+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|11418]: It took 0.00276s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:59:01.575+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.dim_date_run scheduled__2024-10-01T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [[34m2024-11-27T14:59:01.624+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run scheduled__2024-10-01T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [[34m2024-11-27T14:59:01.733+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-10-02 01:00:00+00:00, run_after=2024-10-03 01:00:00+00:00[0m
 [[34m2024-11-27T14:59:01.795+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:59:01.975+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.48550311499275267[0m
 [[34m2024-11-27T14:59:01.976+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|11417]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:59:01.976+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:59:01.977+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:59:01.977+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:59:01.977+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|11417]: It took 1.03s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:59:01.981+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|11417]: It took 0.00336s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:59:02.030+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_daily_bars_run scheduled__2024-10-01T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [2024-11-27T14:59:02.125+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.dim_date_run, dagrun_id=scheduled__2024-10-01T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:59:01.671168+00:00, try_number=1, job_id=950, op_classpath=cosmos.operators.local.DbtRunLocalOperator, cosmos.operators.base.DbtRunMixin, cosmos.operators.local.DbtLocalBaseOperator, cosmos.operators.base.AbstractDbtBaseOperator
 [[34m2024-11-27T14:59:02.132+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [2024-11-27T14:59:02.166+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run, dagrun_id=scheduled__2024-10-01T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:59:01.721692+00:00, try_number=1, job_id=951, op_classpath=cosmos.operators.local.DbtRunLocalOperator, cosmos.operators.base.DbtRunMixin, cosmos.operators.local.DbtLocalBaseOperator, cosmos.operators.base.AbstractDbtBaseOperator
 [[34m2024-11-27T14:59:02.411+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [2024-11-27T14:59:02.524+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.stg_daily_bars_run, dagrun_id=scheduled__2024-10-01T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:59:02.121662+00:00, try_number=1, job_id=952, op_classpath=cosmos.operators.local.DbtRunLocalOperator, cosmos.operators.base.DbtRunMixin, cosmos.operators.local.DbtLocalBaseOperator, cosmos.operators.base.AbstractDbtBaseOperator
 [[34m2024-11-27T14:59:02.849+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-10-03 01:00:00+00:00, run_after=2024-10-04 01:00:00+00:00[0m
 [[34m2024-11-27T14:59:03.108+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections'[0m
 [[34m2024-11-27T14:59:03.108+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.dim_date'[0m
 [[34m2024-11-27T14:59:03.108+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_payments'[0m
 [[34m2024-11-27T14:59:03.109+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_orders'[0m
 [[34m2024-11-27T14:59:03.109+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_daily_bars'[0m
 [[34m2024-11-27T14:59:03.311+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:59:03.358+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:59:03.409+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-10-04 01:00:00+00:00, run_after=2024-10-05 01:00:00+00:00[0m
 [[34m2024-11-27T14:59:03.602+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:59:03.637+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T14:59:03.658+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T14:59:03.746+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:59:03.854+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 [[34m2024-11-27T14:59:03.905+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:59:03.915+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:59:04.698+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-10-05 01:00:00+00:00, run_after=2024-10-06 01:00:00+00:00[0m
 Data loaded successfully
 [[34m2024-11-27T14:59:04.774+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:59:04.793+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 [[34m2024-11-27T14:59:05.102+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:59:05.286+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:59:05.312+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:59:05.424+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 [[34m2024-11-27T14:59:05.431+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:59:05.541+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 [[34m2024-11-27T14:59:05.670+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.3028670640196651[0m
 [[34m2024-11-27T14:59:05.670+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|11485]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:59:05.671+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:59:05.671+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:59:05.671+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:59:05.671+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|11485]: It took 0.897s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:59:05.674+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|11485]: It took 0.00216s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:59:05.711+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.weekly_metrics_run scheduled__2024-09-30T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [[34m2024-11-27T14:59:06.033+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-10-06 01:00:00+00:00, run_after=2024-10-07 01:00:00+00:00[0m
 Data loaded successfully
 [[34m2024-11-27T14:59:06.130+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:59:06.142+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 [2024-11-27T14:59:06.147+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.weekly_metrics_run, dagrun_id=scheduled__2024-09-30T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:59:05.785753+00:00, try_number=1, job_id=953, op_classpath=cosmos.operators.local.DbtRunLocalOperator, cosmos.operators.base.DbtRunMixin, cosmos.operators.local.DbtLocalBaseOperator, cosmos.operators.base.AbstractDbtBaseOperator
 Data loaded successfully
 [[34m2024-11-27T14:59:06.170+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:59:06.189+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 [[34m2024-11-27T14:59:06.600+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-10-07 01:00:00+00:00, run_after=2024-10-08 01:00:00+00:00[0m
 [[34m2024-11-27T14:59:07.021+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-10-08 01:00:00+00:00, run_after=2024-10-09 01:00:00+00:00[0m
 [[34m2024-11-27T14:59:07.113+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.43612192291766405[0m
 [[34m2024-11-27T14:59:07.113+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|11492]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:59:07.114+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:59:07.114+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:59:07.114+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:59:07.114+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|11492]: It took 0.984s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:59:07.118+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|11492]: It took 0.00379s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:59:07.167+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.load_polygon_data_task scheduled__2024-10-03T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [[34m2024-11-27T14:59:07.175+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.4669770519249141[0m
 [[34m2024-11-27T14:59:07.176+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|11493]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:59:07.177+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:59:07.177+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:59:07.177+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:59:07.178+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|11493]: It took 1.01s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:59:07.181+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|11493]: It took 0.00335s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:59:07.236+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.load_treasury_data_task scheduled__2024-10-02T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [2024-11-27T14:59:07.597+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=load_polygon_data_task, dagrun_id=scheduled__2024-10-03T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:59:07.280444+00:00, try_number=1, job_id=954, op_classpath=airflow.operators.python.PythonOperator
 [2024-11-27T14:59:07.643+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=load_treasury_data_task, dagrun_id=scheduled__2024-10-02T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:59:07.336858+00:00, try_number=1, job_id=955, op_classpath=airflow.operators.python.PythonOperator
 /usr/local/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked semaphore objects to clean up at shutdown
   warnings.warn('resource_tracker: There appear to be %d '
 [[34m2024-11-27T14:59:08.305+0000[0m] {[34mscheduler_job_runner.py:[0m1328} INFO[0m - DAG capstone_andres_dag_ is at (or above) max_active_runs (16 of 16), not creating any more runs[0m
 [[34m2024-11-27T14:59:08.491+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run', run_id='scheduled__2024-10-01T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:59:08.495+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run, run_id=scheduled__2024-10-01T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:59:01.721692+00:00, run_end_date=2024-11-27 14:59:08.183562+00:00, run_duration=6.46187, state=success, executor_state=success, try_number=1, max_tries=0, job_id=951, pool=default_pool, queue=default, priority_weight=2, operator=DbtRunLocalOperator, queued_dttm=2024-11-27 14:58:52.940003+00:00, queued_by_job_id=803, pid=11548[0m
 /usr/local/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked semaphore objects to clean up at shutdown
   warnings.warn('resource_tracker: There appear to be %d '
 /usr/local/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked semaphore objects to clean up at shutdown
   warnings.warn('resource_tracker: There appear to be %d '
 [[34m2024-11-27T14:59:09.710+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.weekly_metrics_run scheduled__2024-10-01T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:59:09.711+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 2/16 running and queued tasks[0m
 [[34m2024-11-27T14:59:09.711+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.weekly_metrics_run scheduled__2024-10-01T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:59:09.712+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.weekly_metrics_run', run_id='scheduled__2024-10-01T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
 [[34m2024-11-27T14:59:09.713+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.weekly_metrics_run', 'scheduled__2024-10-01T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:59:09.715+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.weekly_metrics_run', 'scheduled__2024-10-01T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:59:09.715+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.stg_daily_bars_run', run_id='scheduled__2024-10-01T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:59:09.716+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.dim_date_run', run_id='scheduled__2024-10-01T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:59:09.719+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.dim_date_run, run_id=scheduled__2024-10-01T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:59:01.671168+00:00, run_end_date=2024-11-27 14:59:09.215142+00:00, run_duration=7.543974, state=success, executor_state=success, try_number=1, max_tries=0, job_id=950, pool=default_pool, queue=default, priority_weight=2, operator=DbtRunLocalOperator, queued_dttm=2024-11-27 14:58:52.940003+00:00, queued_by_job_id=803, pid=11547[0m
 [[34m2024-11-27T14:59:09.720+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.stg_daily_bars_run, run_id=scheduled__2024-10-01T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:59:02.121662+00:00, run_end_date=2024-11-27 14:59:08.574208+00:00, run_duration=6.452546, state=success, executor_state=success, try_number=1, max_tries=0, job_id=952, pool=default_pool, queue=default, priority_weight=2, operator=DbtRunLocalOperator, queued_dttm=2024-11-27 14:58:52.940003+00:00, queued_by_job_id=803, pid=11549[0m
 [[34m2024-11-27T14:59:09.741+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:59:09.955+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.dim_date_run scheduled__2024-10-02T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_daily_bars_run scheduled__2024-10-02T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run scheduled__2024-10-02T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:59:09.955+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 3/16 running and queued tasks[0m
 [[34m2024-11-27T14:59:09.955+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 4/16 running and queued tasks[0m
 [[34m2024-11-27T14:59:09.955+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 5/16 running and queued tasks[0m
 [[34m2024-11-27T14:59:09.955+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.dim_date_run scheduled__2024-10-02T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_daily_bars_run scheduled__2024-10-02T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run scheduled__2024-10-02T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:59:09.957+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.dim_date_run', run_id='scheduled__2024-10-02T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
 [[34m2024-11-27T14:59:09.957+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.dim_date_run', 'scheduled__2024-10-02T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:59:09.958+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.stg_daily_bars_run', run_id='scheduled__2024-10-02T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
 [[34m2024-11-27T14:59:09.958+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.stg_daily_bars_run', 'scheduled__2024-10-02T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:59:09.958+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run', run_id='scheduled__2024-10-02T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
 [[34m2024-11-27T14:59:09.958+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run', 'scheduled__2024-10-02T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:59:09.960+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.stg_daily_bars_run', 'scheduled__2024-10-02T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:59:09.961+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='load_treasury_data_task', run_id='scheduled__2024-10-02T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:59:09.961+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run', 'scheduled__2024-10-02T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:59:09.961+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.dim_date_run', 'scheduled__2024-10-02T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:59:09.965+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=load_treasury_data_task, run_id=scheduled__2024-10-02T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:59:07.336858+00:00, run_end_date=2024-11-27 14:59:09.640861+00:00, run_duration=2.304003, state=success, executor_state=success, try_number=1, max_tries=0, job_id=955, pool=default_pool, queue=default, priority_weight=5, operator=PythonOperator, queued_dttm=2024-11-27 14:58:58.043851+00:00, queued_by_job_id=803, pid=11592[0m
 [[34m2024-11-27T14:59:09.989+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:59:09.990+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:59:09.990+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:59:10.684+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:59:10.685+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:59:10.685+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:59:10.915+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:59:10.915+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:59:10.916+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:59:10.916+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:59:10.916+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:59:10.916+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:59:10.917+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:59:10.917+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:59:10.917+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:59:11.142+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412097050,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T14:59:11.380+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412097054,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T14:59:11.381+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412095746,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T14:59:11.452+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412095750,
 "os.name" : Linux
 [0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:59:11.812+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 /usr/local/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked semaphore objects to clean up at shutdown
   warnings.warn('resource_tracker: There appear to be %d '
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:59:12.097+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:59:12.108+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:59:12.224+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:59:12.315+0000[0m] {[34mdagrun.py:[0m851} INFO[0m - Marking run <DagRun capstone_andres_dag_ @ 2024-09-30 01:00:00+00:00: scheduled__2024-09-30T01:00:00+00:00, state:running, queued_at: 2024-11-27 14:55:06.096418+00:00. externally triggered: False> successful[0m
 [[34m2024-11-27T14:59:12.315+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=capstone_andres_dag_, execution_date=2024-09-30 01:00:00+00:00, run_id=scheduled__2024-09-30T01:00:00+00:00, run_start_date=2024-11-27 14:55:06.112729+00:00, run_end_date=2024-11-27 14:59:12.315545+00:00, run_duration=246.202816, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-09-30 01:00:00+00:00, data_interval_end=2024-10-01 01:00:00+00:00, dag_hash=77fa8f64adac9391cccaaae88e2be5d8[0m
 [[34m2024-11-27T14:59:12.319+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-10-01 01:00:00+00:00, run_after=2024-10-02 01:00:00+00:00[0m
 [[34m2024-11-27T14:59:12.465+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.weekly_metrics_run', run_id='scheduled__2024-09-30T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:59:12.468+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.weekly_metrics_run, run_id=scheduled__2024-09-30T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:59:05.785753+00:00, run_end_date=2024-11-27 14:59:11.807690+00:00, run_duration=6.021937, state=success, executor_state=success, try_number=1, max_tries=0, job_id=953, pool=default_pool, queue=default, priority_weight=1, operator=DbtRunLocalOperator, queued_dttm=2024-11-27 14:58:56.566652+00:00, queued_by_job_id=803, pid=11575[0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 [[34m2024-11-27T14:59:12.685+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 [[34m2024-11-27T14:59:12.991+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 [[34m2024-11-27T14:59:13.168+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:59:13.353+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:59:13.497+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-10-02 01:00:00+00:00, run_after=2024-10-03 01:00:00+00:00[0m
 [[34m2024-11-27T14:59:14.704+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-10-03 01:00:00+00:00, run_after=2024-10-04 01:00:00+00:00[0m
 [[34m2024-11-27T14:59:14.922+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 2 tasks up for execution:
 	<TaskInstance: capstone_andres_dag_.load_polygon_data_task scheduled__2024-10-04T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.load_treasury_data_task scheduled__2024-10-03T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:59:14.922+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 4/16 running and queued tasks[0m
 [[34m2024-11-27T14:59:14.923+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 5/16 running and queued tasks[0m
 [[34m2024-11-27T14:59:14.923+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
 	<TaskInstance: capstone_andres_dag_.load_polygon_data_task scheduled__2024-10-04T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.load_treasury_data_task scheduled__2024-10-03T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:59:14.925+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='load_polygon_data_task', run_id='scheduled__2024-10-04T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
 [[34m2024-11-27T14:59:14.925+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'load_polygon_data_task', 'scheduled__2024-10-04T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:59:14.925+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='load_treasury_data_task', run_id='scheduled__2024-10-03T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 5 and queue default[0m
 [[34m2024-11-27T14:59:14.925+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'load_treasury_data_task', 'scheduled__2024-10-03T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:59:14.927+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'load_polygon_data_task', 'scheduled__2024-10-04T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:59:14.927+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'load_treasury_data_task', 'scheduled__2024-10-03T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:59:14.928+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='load_polygon_data_task', run_id='scheduled__2024-10-03T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:59:14.932+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=load_polygon_data_task, run_id=scheduled__2024-10-03T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:59:07.280444+00:00, run_end_date=2024-11-27 14:59:14.603303+00:00, run_duration=7.322859, state=success, executor_state=success, try_number=1, max_tries=0, job_id=954, pool=default_pool, queue=default, priority_weight=6, operator=PythonOperator, queued_dttm=2024-11-27 14:58:58.043851+00:00, queued_by_job_id=803, pid=11591[0m
 [[34m2024-11-27T14:59:14.952+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-10-04 01:00:00+00:00, run_after=2024-10-05 01:00:00+00:00[0m
 [[34m2024-11-27T14:59:14.955+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:59:14.955+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:59:15.316+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:59:15.661+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T14:59:15.674+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:59:15.727+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:59:15.826+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:59:15.826+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:59:15.826+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:59:15.826+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:59:15.826+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:59:15.827+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:59:15.903+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:59:15.971+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T14:59:16.025+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T14:59:16.184+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:59:16.214+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-10-05 01:00:00+00:00, run_after=2024-10-06 01:00:00+00:00[0m
 [[34m2024-11-27T14:59:16.264+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:59:16.275+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412097058,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T14:59:16.276+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:59:16.276+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412095754,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T14:59:16.502+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T14:59:16.750+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:59:16.824+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-10-06 01:00:00+00:00, run_after=2024-10-07 01:00:00+00:00[0m
 [[34m2024-11-27T14:59:16.894+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:59:16.964+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:59:17.078+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:59:17.183+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 [[34m2024-11-27T14:59:17.381+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:59:17.428+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:59:17.479+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:59:17.495+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-10-07 01:00:00+00:00, run_after=2024-10-08 01:00:00+00:00[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:59:17.522+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:59:17.539+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:59:17.625+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:59:17.651+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 Data loaded successfully
 [[34m2024-11-27T14:59:17.667+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:59:17.679+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 [[34m2024-11-27T14:59:17.755+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:59:17.757+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 [[34m2024-11-27T14:59:17.927+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:59:17.928+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:59:18.042+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 Data loaded successfully
 [[34m2024-11-27T14:59:18.245+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 Data loaded successfully
 [[34m2024-11-27T14:59:18.257+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:59:18.260+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 [[34m2024-11-27T14:59:18.270+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 [[34m2024-11-27T14:59:18.555+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.36132096382789314[0m
 [[34m2024-11-27T14:59:18.555+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|11606]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:59:18.556+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:59:18.556+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:59:18.556+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:59:18.556+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|11606]: It took 0.889s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:59:18.560+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|11606]: It took 0.00317s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:59:18.605+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.weekly_metrics_run scheduled__2024-10-01T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [[34m2024-11-27T14:59:18.692+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-10-08 01:00:00+00:00, run_after=2024-10-09 01:00:00+00:00[0m
 Data loaded successfully
 [[34m2024-11-27T14:59:18.762+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:59:18.775+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 [2024-11-27T14:59:19.127+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.weekly_metrics_run, dagrun_id=scheduled__2024-10-01T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:59:18.686507+00:00, try_number=1, job_id=956, op_classpath=cosmos.operators.local.DbtRunLocalOperator, cosmos.operators.base.DbtRunMixin, cosmos.operators.local.DbtLocalBaseOperator, cosmos.operators.base.AbstractDbtBaseOperator
 [[34m2024-11-27T14:59:19.209+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.44438534299843013[0m
 [[34m2024-11-27T14:59:19.209+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|11611]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:59:19.210+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:59:19.210+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:59:19.210+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:59:19.210+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|11611]: It took 0.965s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:59:19.214+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|11611]: It took 0.00334s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:59:19.256+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.43882355093955994[0m
 [[34m2024-11-27T14:59:19.257+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|11612]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:59:19.257+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:59:19.257+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:59:19.258+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:59:19.258+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|11612]: It took 1.0s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:59:19.258+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run scheduled__2024-10-02T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [[34m2024-11-27T14:59:19.261+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|11612]: It took 0.00277s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:59:19.309+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.dim_date_run scheduled__2024-10-02T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [[34m2024-11-27T14:59:19.827+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.4659489030018449[0m
 [[34m2024-11-27T14:59:19.828+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|11610]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:59:19.828+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:59:19.829+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:59:19.829+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:59:19.829+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|11610]: It took 1.07s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [2024-11-27T14:59:19.829+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run, dagrun_id=scheduled__2024-10-02T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:59:19.356290+00:00, try_number=1, job_id=957, op_classpath=cosmos.operators.local.DbtRunLocalOperator, cosmos.operators.base.DbtRunMixin, cosmos.operators.local.DbtLocalBaseOperator, cosmos.operators.base.AbstractDbtBaseOperator
 [[34m2024-11-27T14:59:19.833+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|11610]: It took 0.0032s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:59:19.875+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_daily_bars_run scheduled__2024-10-02T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [2024-11-27T14:59:19.884+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.dim_date_run, dagrun_id=scheduled__2024-10-02T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:59:19.425692+00:00, try_number=1, job_id=958, op_classpath=cosmos.operators.local.DbtRunLocalOperator, cosmos.operators.base.DbtRunMixin, cosmos.operators.local.DbtLocalBaseOperator, cosmos.operators.base.AbstractDbtBaseOperator
 [[34m2024-11-27T14:59:19.954+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-10-09 01:00:00+00:00, run_after=2024-10-10 01:00:00+00:00[0m
 [[34m2024-11-27T14:59:20.190+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [2024-11-27T14:59:20.385+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.stg_daily_bars_run, dagrun_id=scheduled__2024-10-02T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:59:19.955100+00:00, try_number=1, job_id=959, op_classpath=cosmos.operators.local.DbtRunLocalOperator, cosmos.operators.base.DbtRunMixin, cosmos.operators.local.DbtLocalBaseOperator, cosmos.operators.base.AbstractDbtBaseOperator
 [[34m2024-11-27T14:59:20.505+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T14:59:20.620+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:59:20.875+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:59:20.949+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T14:59:21.198+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-10-10 01:00:00+00:00, run_after=2024-10-11 01:00:00+00:00[0m
 [[34m2024-11-27T14:59:21.280+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:59:22.001+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:59:22.164+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:59:22.274+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 [[34m2024-11-27T14:59:22.342+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:59:22.424+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-10-11 01:00:00+00:00, run_after=2024-10-12 01:00:00+00:00[0m
 [[34m2024-11-27T14:59:22.503+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:59:22.613+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 Data loaded successfully
 [[34m2024-11-27T14:59:22.901+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:59:22.917+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 Data loaded successfully
 [[34m2024-11-27T14:59:23.167+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:59:23.190+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 [[34m2024-11-27T14:59:23.734+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-10-12 01:00:00+00:00, run_after=2024-10-13 01:00:00+00:00[0m
 [[34m2024-11-27T14:59:23.937+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.48051119898445904[0m
 [[34m2024-11-27T14:59:23.937+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|11659]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:59:23.938+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:59:23.938+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:59:23.939+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:59:23.939+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|11659]: It took 1.04s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:59:23.945+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|11659]: It took 0.00604s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:59:24.010+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.load_treasury_data_task scheduled__2024-10-03T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [[34m2024-11-27T14:59:24.325+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.6145952250808477[0m
 [[34m2024-11-27T14:59:24.325+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|11658]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:59:24.326+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:59:24.326+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:59:24.327+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:59:24.327+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|11658]: It took 1.16s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:59:24.330+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|11658]: It took 0.00274s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:59:24.372+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.load_polygon_data_task scheduled__2024-10-04T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [2024-11-27T14:59:24.481+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=load_treasury_data_task, dagrun_id=scheduled__2024-10-03T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:59:24.101551+00:00, try_number=1, job_id=960, op_classpath=airflow.operators.python.PythonOperator
 [2024-11-27T14:59:24.900+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=load_polygon_data_task, dagrun_id=scheduled__2024-10-04T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:59:24.481983+00:00, try_number=1, job_id=961, op_classpath=airflow.operators.python.PythonOperator
 [[34m2024-11-27T14:59:25.064+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-10-13 01:00:00+00:00, run_after=2024-10-14 01:00:00+00:00[0m
 /usr/local/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked semaphore objects to clean up at shutdown
   warnings.warn('resource_tracker: There appear to be %d '
 /usr/local/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked semaphore objects to clean up at shutdown
   warnings.warn('resource_tracker: There appear to be %d '
 [[34m2024-11-27T14:59:26.354+0000[0m] {[34mdagrun.py:[0m851} INFO[0m - Marking run <DagRun capstone_andres_dag_ @ 2024-10-01 01:00:00+00:00: scheduled__2024-10-01T01:00:00+00:00, state:running, queued_at: 2024-11-27 14:55:19.019177+00:00. externally triggered: False> successful[0m
 [[34m2024-11-27T14:59:26.355+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=capstone_andres_dag_, execution_date=2024-10-01 01:00:00+00:00, run_id=scheduled__2024-10-01T01:00:00+00:00, run_start_date=2024-11-27 14:55:19.030295+00:00, run_end_date=2024-11-27 14:59:26.354989+00:00, run_duration=247.324694, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-10-01 01:00:00+00:00, data_interval_end=2024-10-02 01:00:00+00:00, dag_hash=77fa8f64adac9391cccaaae88e2be5d8[0m
 [[34m2024-11-27T14:59:26.359+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-10-02 01:00:00+00:00, run_after=2024-10-03 01:00:00+00:00[0m
 [[34m2024-11-27T14:59:26.512+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.weekly_metrics_run', run_id='scheduled__2024-10-01T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:59:26.512+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run', run_id='scheduled__2024-10-02T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:59:26.517+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.weekly_metrics_run, run_id=scheduled__2024-10-01T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:59:18.686507+00:00, run_end_date=2024-11-27 14:59:25.298182+00:00, run_duration=6.611675, state=success, executor_state=success, try_number=1, max_tries=0, job_id=956, pool=default_pool, queue=default, priority_weight=1, operator=DbtRunLocalOperator, queued_dttm=2024-11-27 14:59:09.711763+00:00, queued_by_job_id=803, pid=11694[0m
 [[34m2024-11-27T14:59:26.517+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run, run_id=scheduled__2024-10-02T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:59:19.356290+00:00, run_end_date=2024-11-27 14:59:25.588130+00:00, run_duration=6.23184, state=success, executor_state=success, try_number=1, max_tries=0, job_id=957, pool=default_pool, queue=default, priority_weight=2, operator=DbtRunLocalOperator, queued_dttm=2024-11-27 14:59:09.956312+00:00, queued_by_job_id=803, pid=11697[0m
 /usr/local/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked semaphore objects to clean up at shutdown
   warnings.warn('resource_tracker: There appear to be %d '
 /usr/local/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked semaphore objects to clean up at shutdown
   warnings.warn('resource_tracker: There appear to be %d '
 [[34m2024-11-27T14:59:27.035+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-10-17 01:00:00+00:00, run_after=2024-10-18 01:00:00+00:00[0m
 [[34m2024-11-27T14:59:27.224+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 4 tasks up for execution:
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.dim_date_run scheduled__2024-10-03T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_daily_bars_run scheduled__2024-10-03T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run scheduled__2024-10-03T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.weekly_metrics_run scheduled__2024-10-02T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:59:27.224+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 1/16 running and queued tasks[0m
 [[34m2024-11-27T14:59:27.224+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 2/16 running and queued tasks[0m
 [[34m2024-11-27T14:59:27.224+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 3/16 running and queued tasks[0m
 [[34m2024-11-27T14:59:27.225+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 4/16 running and queued tasks[0m
 [[34m2024-11-27T14:59:27.225+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.dim_date_run scheduled__2024-10-03T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_daily_bars_run scheduled__2024-10-03T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run scheduled__2024-10-03T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.weekly_metrics_run scheduled__2024-10-02T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:59:27.227+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.dim_date_run', run_id='scheduled__2024-10-03T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
 [[34m2024-11-27T14:59:27.227+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.dim_date_run', 'scheduled__2024-10-03T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:59:27.227+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.stg_daily_bars_run', run_id='scheduled__2024-10-03T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
 [[34m2024-11-27T14:59:27.227+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.stg_daily_bars_run', 'scheduled__2024-10-03T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:59:27.228+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run', run_id='scheduled__2024-10-03T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
 [[34m2024-11-27T14:59:27.228+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run', 'scheduled__2024-10-03T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:59:27.228+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.weekly_metrics_run', run_id='scheduled__2024-10-02T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
 [[34m2024-11-27T14:59:27.228+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.weekly_metrics_run', 'scheduled__2024-10-02T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:59:27.231+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.stg_daily_bars_run', 'scheduled__2024-10-03T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:59:27.231+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.dim_date_run', 'scheduled__2024-10-03T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:59:27.231+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run', 'scheduled__2024-10-03T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:59:27.232+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.weekly_metrics_run', 'scheduled__2024-10-02T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:59:27.233+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.stg_daily_bars_run', run_id='scheduled__2024-10-02T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:59:27.234+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='load_treasury_data_task', run_id='scheduled__2024-10-03T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:59:27.234+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.dim_date_run', run_id='scheduled__2024-10-02T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:59:27.241+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=load_treasury_data_task, run_id=scheduled__2024-10-03T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:59:24.101551+00:00, run_end_date=2024-11-27 14:59:26.584552+00:00, run_duration=2.483001, state=success, executor_state=success, try_number=1, max_tries=0, job_id=960, pool=default_pool, queue=default, priority_weight=5, operator=PythonOperator, queued_dttm=2024-11-27 14:59:14.923769+00:00, queued_by_job_id=803, pid=11733[0m
 [[34m2024-11-27T14:59:27.242+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.stg_daily_bars_run, run_id=scheduled__2024-10-02T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:59:19.955100+00:00, run_end_date=2024-11-27 14:59:26.399743+00:00, run_duration=6.444643, state=success, executor_state=success, try_number=1, max_tries=0, job_id=959, pool=default_pool, queue=default, priority_weight=2, operator=DbtRunLocalOperator, queued_dttm=2024-11-27 14:59:09.956312+00:00, queued_by_job_id=803, pid=11699[0m
 [[34m2024-11-27T14:59:27.242+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.dim_date_run, run_id=scheduled__2024-10-02T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:59:19.425692+00:00, run_end_date=2024-11-27 14:59:26.955643+00:00, run_duration=7.529951, state=success, executor_state=success, try_number=1, max_tries=0, job_id=958, pool=default_pool, queue=default, priority_weight=2, operator=DbtRunLocalOperator, queued_dttm=2024-11-27 14:59:09.956312+00:00, queued_by_job_id=803, pid=11698[0m
 [[34m2024-11-27T14:59:27.268+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:59:27.268+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:59:27.269+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:59:27.270+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:59:27.279+0000[0m] {[34mscheduler_job_runner.py:[0m1328} INFO[0m - DAG capstone_andres_dag_ is at (or above) max_active_runs (16 of 16), not creating any more runs[0m
 [[34m2024-11-27T14:59:28.249+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:59:28.249+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:59:28.249+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:59:28.249+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:59:28.250+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:59:28.250+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:59:28.254+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:59:28.255+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:59:28.255+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:59:28.265+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:59:28.265+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:59:28.265+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:59:28.816+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412095790,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T14:59:28.843+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412097094,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T14:59:28.878+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412097098,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T14:59:28.880+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412097102,
 "os.name" : Linux
 [0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:59:29.590+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:59:29.597+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:59:29.599+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:59:29.627+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 [[34m2024-11-27T14:59:30.060+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 [[34m2024-11-27T14:59:30.243+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 [[34m2024-11-27T14:59:30.482+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:59:30.541+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:59:32.622+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:59:32.889+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:59:32.907+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 2 tasks up for execution:
 	<TaskInstance: capstone_andres_dag_.load_polygon_data_task scheduled__2024-10-05T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.load_treasury_data_task scheduled__2024-10-04T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:59:32.907+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 4/16 running and queued tasks[0m
 [[34m2024-11-27T14:59:32.908+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 5/16 running and queued tasks[0m
 [[34m2024-11-27T14:59:32.908+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
 	<TaskInstance: capstone_andres_dag_.load_polygon_data_task scheduled__2024-10-05T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.load_treasury_data_task scheduled__2024-10-04T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:59:32.910+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='load_polygon_data_task', run_id='scheduled__2024-10-05T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
 [[34m2024-11-27T14:59:32.910+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'load_polygon_data_task', 'scheduled__2024-10-05T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:59:32.910+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='load_treasury_data_task', run_id='scheduled__2024-10-04T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 5 and queue default[0m
 [[34m2024-11-27T14:59:32.910+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'load_treasury_data_task', 'scheduled__2024-10-04T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:59:32.912+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'load_polygon_data_task', 'scheduled__2024-10-05T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:59:32.913+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='load_polygon_data_task', run_id='scheduled__2024-10-04T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:59:32.912+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'load_treasury_data_task', 'scheduled__2024-10-04T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:59:32.917+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=load_polygon_data_task, run_id=scheduled__2024-10-04T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:59:24.481983+00:00, run_end_date=2024-11-27 14:59:31.622947+00:00, run_duration=7.140964, state=success, executor_state=success, try_number=1, max_tries=0, job_id=961, pool=default_pool, queue=default, priority_weight=6, operator=PythonOperator, queued_dttm=2024-11-27 14:59:14.923769+00:00, queued_by_job_id=803, pid=11748[0m
 [[34m2024-11-27T14:59:32.920+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T14:59:32.950+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:59:32.950+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:59:33.161+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:59:33.227+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:59:33.243+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:59:33.244+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T14:59:33.464+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T14:59:33.490+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:59:33.505+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T14:59:33.844+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:59:33.844+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:59:33.845+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:59:33.852+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:59:33.853+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:59:33.853+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:59:33.898+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:59:33.901+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:59:34.389+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412095794,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T14:59:34.421+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412095798,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T14:59:34.515+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:59:34.589+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:59:34.676+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:59:34.776+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:59:34.817+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 [[34m2024-11-27T14:59:34.930+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 [[34m2024-11-27T14:59:34.999+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:59:35.002+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:59:35.138+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:59:35.140+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:59:35.149+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:59:35.185+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:59:35.244+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 Data loaded successfully
 [[34m2024-11-27T14:59:35.274+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:59:35.284+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 [[34m2024-11-27T14:59:35.286+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 Data loaded successfully
 [[34m2024-11-27T14:59:35.532+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:59:35.552+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 [[34m2024-11-27T14:59:35.565+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:59:35.595+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 Data loaded successfully
 [[34m2024-11-27T14:59:35.815+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:59:35.827+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 Data loaded successfully
 [[34m2024-11-27T14:59:35.888+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:59:35.903+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 [[34m2024-11-27T14:59:36.292+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.4514410770498216[0m
 [[34m2024-11-27T14:59:36.292+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|11768]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:59:36.292+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:59:36.293+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:59:36.293+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:59:36.293+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|11768]: It took 1.02s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:59:36.297+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|11768]: It took 0.00359s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:59:36.339+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.weekly_metrics_run scheduled__2024-10-02T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [[34m2024-11-27T14:59:36.552+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.4805170048493892[0m
 [[34m2024-11-27T14:59:36.552+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|11767]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:59:36.553+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:59:36.553+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:59:36.554+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:59:36.554+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|11767]: It took 1.02s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:59:36.558+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|11767]: It took 0.00347s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:59:36.606+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.dim_date_run scheduled__2024-10-03T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [[34m2024-11-27T14:59:36.825+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.4574025191832334[0m
 [[34m2024-11-27T14:59:36.826+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|11769]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:59:36.826+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:59:36.826+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:59:36.827+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:59:36.827+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|11769]: It took 1.01s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:59:36.830+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|11769]: It took 0.00295s to build the Airflow DAG.[0m
 [2024-11-27T14:59:36.844+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.weekly_metrics_run, dagrun_id=scheduled__2024-10-02T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:59:36.429420+00:00, try_number=1, job_id=962, op_classpath=cosmos.operators.local.DbtRunLocalOperator, cosmos.operators.base.DbtRunMixin, cosmos.operators.local.DbtLocalBaseOperator, cosmos.operators.base.AbstractDbtBaseOperator
 [[34m2024-11-27T14:59:36.883+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_daily_bars_run scheduled__2024-10-03T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [[34m2024-11-27T14:59:36.982+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.5106590918730944[0m
 [[34m2024-11-27T14:59:36.983+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|11770]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:59:36.984+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:59:36.985+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:59:36.985+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:59:36.986+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|11770]: It took 1.1s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:59:36.992+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|11770]: It took 0.00618s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:59:37.006+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:59:37.063+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run scheduled__2024-10-03T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [[34m2024-11-27T14:59:37.083+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [2024-11-27T14:59:37.186+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.dim_date_run, dagrun_id=scheduled__2024-10-03T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:59:36.690335+00:00, try_number=1, job_id=963, op_classpath=cosmos.operators.local.DbtRunLocalOperator, cosmos.operators.base.DbtRunMixin, cosmos.operators.local.DbtLocalBaseOperator, cosmos.operators.base.AbstractDbtBaseOperator
 [[34m2024-11-27T14:59:37.371+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T14:59:37.396+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [2024-11-27T14:59:37.516+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.stg_daily_bars_run, dagrun_id=scheduled__2024-10-03T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:59:37.027742+00:00, try_number=1, job_id=964, op_classpath=cosmos.operators.local.DbtRunLocalOperator, cosmos.operators.base.DbtRunMixin, cosmos.operators.local.DbtLocalBaseOperator, cosmos.operators.base.AbstractDbtBaseOperator
 [[34m2024-11-27T14:59:37.608+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [2024-11-27T14:59:37.615+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run, dagrun_id=scheduled__2024-10-03T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:59:37.148456+00:00, try_number=1, job_id=965, op_classpath=cosmos.operators.local.DbtRunLocalOperator, cosmos.operators.base.DbtRunMixin, cosmos.operators.local.DbtLocalBaseOperator, cosmos.operators.base.AbstractDbtBaseOperator
 [[34m2024-11-27T14:59:37.697+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:59:38.769+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:59:38.796+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:59:38.940+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:59:38.946+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:59:39.046+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 [[34m2024-11-27T14:59:39.054+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 Data loaded successfully
 [[34m2024-11-27T14:59:39.523+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:59:39.545+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 Data loaded successfully
 [[34m2024-11-27T14:59:39.552+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:59:39.574+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 [[34m2024-11-27T14:59:40.587+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.47051890613511205[0m
 [[34m2024-11-27T14:59:40.587+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|11822]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:59:40.587+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:59:40.588+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:59:40.588+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:59:40.588+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|11822]: It took 1.06s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:59:40.592+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|11822]: It took 0.00347s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:59:40.624+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.4618954479228705[0m
 [[34m2024-11-27T14:59:40.625+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|11821]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:59:40.625+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:59:40.625+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:59:40.626+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:59:40.626+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|11821]: It took 1.07s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:59:40.628+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|11821]: It took 0.00253s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:59:40.632+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.load_treasury_data_task scheduled__2024-10-04T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [[34m2024-11-27T14:59:40.682+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.load_polygon_data_task scheduled__2024-10-05T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [2024-11-27T14:59:41.143+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=load_treasury_data_task, dagrun_id=scheduled__2024-10-04T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:59:40.742553+00:00, try_number=1, job_id=966, op_classpath=airflow.operators.python.PythonOperator
 [2024-11-27T14:59:41.213+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=load_polygon_data_task, dagrun_id=scheduled__2024-10-05T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:59:40.789098+00:00, try_number=1, job_id=967, op_classpath=airflow.operators.python.PythonOperator
 /usr/local/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked semaphore objects to clean up at shutdown
   warnings.warn('resource_tracker: There appear to be %d '
 [[34m2024-11-27T14:59:43.327+0000[0m] {[34mdagrun.py:[0m851} INFO[0m - Marking run <DagRun capstone_andres_dag_ @ 2024-10-02 01:00:00+00:00: scheduled__2024-10-02T01:00:00+00:00, state:running, queued_at: 2024-11-27 14:55:31.847848+00:00. externally triggered: False> successful[0m
 [[34m2024-11-27T14:59:43.327+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=capstone_andres_dag_, execution_date=2024-10-02 01:00:00+00:00, run_id=scheduled__2024-10-02T01:00:00+00:00, run_start_date=2024-11-27 14:55:31.864551+00:00, run_end_date=2024-11-27 14:59:43.327318+00:00, run_duration=251.462767, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-10-02 01:00:00+00:00, data_interval_end=2024-10-03 01:00:00+00:00, dag_hash=77fa8f64adac9391cccaaae88e2be5d8[0m
 [[34m2024-11-27T14:59:43.330+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-10-03 01:00:00+00:00, run_after=2024-10-04 01:00:00+00:00[0m
 [[34m2024-11-27T14:59:43.512+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.dim_date_run scheduled__2024-10-04T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_daily_bars_run scheduled__2024-10-04T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run scheduled__2024-10-04T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:59:43.512+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 3/16 running and queued tasks[0m
 [[34m2024-11-27T14:59:43.512+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 4/16 running and queued tasks[0m
 [[34m2024-11-27T14:59:43.512+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 5/16 running and queued tasks[0m
 [[34m2024-11-27T14:59:43.513+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.dim_date_run scheduled__2024-10-04T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_daily_bars_run scheduled__2024-10-04T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run scheduled__2024-10-04T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:59:43.514+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.dim_date_run', run_id='scheduled__2024-10-04T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
 [[34m2024-11-27T14:59:43.515+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.dim_date_run', 'scheduled__2024-10-04T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:59:43.515+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.stg_daily_bars_run', run_id='scheduled__2024-10-04T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
 [[34m2024-11-27T14:59:43.515+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.stg_daily_bars_run', 'scheduled__2024-10-04T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:59:43.515+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run', run_id='scheduled__2024-10-04T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
 [[34m2024-11-27T14:59:43.516+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run', 'scheduled__2024-10-04T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:59:43.518+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.dim_date_run', 'scheduled__2024-10-04T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:59:43.518+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.stg_daily_bars_run', 'scheduled__2024-10-04T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:59:43.519+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run', 'scheduled__2024-10-04T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:59:43.520+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.weekly_metrics_run', run_id='scheduled__2024-10-02T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:59:43.521+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='load_treasury_data_task', run_id='scheduled__2024-10-04T01:00:00+00:00', try_number=1, map_index=-1)[0m
 /usr/local/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked semaphore objects to clean up at shutdown
   warnings.warn('resource_tracker: There appear to be %d '
 [[34m2024-11-27T14:59:43.528+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=load_treasury_data_task, run_id=scheduled__2024-10-04T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:59:40.742553+00:00, run_end_date=2024-11-27 14:59:43.097521+00:00, run_duration=2.354968, state=success, executor_state=success, try_number=1, max_tries=0, job_id=966, pool=default_pool, queue=default, priority_weight=5, operator=PythonOperator, queued_dttm=2024-11-27 14:59:32.908918+00:00, queued_by_job_id=803, pid=11897[0m
 [[34m2024-11-27T14:59:43.528+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.weekly_metrics_run, run_id=scheduled__2024-10-02T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:59:36.429420+00:00, run_end_date=2024-11-27 14:59:42.991636+00:00, run_duration=6.562216, state=success, executor_state=success, try_number=1, max_tries=0, job_id=962, pool=default_pool, queue=default, priority_weight=1, operator=DbtRunLocalOperator, queued_dttm=2024-11-27 14:59:27.225918+00:00, queued_by_job_id=803, pid=11873[0m
 /usr/local/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked semaphore objects to clean up at shutdown
   warnings.warn('resource_tracker: There appear to be %d '
 [[34m2024-11-27T14:59:43.553+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:59:43.553+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:59:43.554+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:59:43.561+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-10-04 01:00:00+00:00, run_after=2024-10-05 01:00:00+00:00[0m
 [[34m2024-11-27T14:59:43.826+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.stg_daily_bars_run', run_id='scheduled__2024-10-03T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:59:43.826+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run', run_id='scheduled__2024-10-03T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:59:43.831+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run, run_id=scheduled__2024-10-03T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:59:37.148456+00:00, run_end_date=2024-11-27 14:59:43.411996+00:00, run_duration=6.26354, state=success, executor_state=success, try_number=1, max_tries=0, job_id=965, pool=default_pool, queue=default, priority_weight=2, operator=DbtRunLocalOperator, queued_dttm=2024-11-27 14:59:27.225918+00:00, queued_by_job_id=803, pid=11876[0m
 [[34m2024-11-27T14:59:43.832+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.stg_daily_bars_run, run_id=scheduled__2024-10-03T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:59:37.027742+00:00, run_end_date=2024-11-27 14:59:43.462296+00:00, run_duration=6.434554, state=success, executor_state=success, try_number=1, max_tries=0, job_id=964, pool=default_pool, queue=default, priority_weight=2, operator=DbtRunLocalOperator, queued_dttm=2024-11-27 14:59:27.225918+00:00, queued_by_job_id=803, pid=11875[0m
 /usr/local/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked semaphore objects to clean up at shutdown
   warnings.warn('resource_tracker: There appear to be %d '
 [[34m2024-11-27T14:59:44.483+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:59:44.484+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:59:44.485+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:59:44.485+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:59:44.486+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:59:44.486+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:59:44.509+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:59:44.510+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:59:44.510+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:59:44.857+0000[0m] {[34mscheduler_job_runner.py:[0m1328} INFO[0m - DAG capstone_andres_dag_ is at (or above) max_active_runs (16 of 16), not creating any more runs[0m
 [[34m2024-11-27T14:59:44.964+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412097130,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T14:59:45.040+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412095846,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T14:59:45.048+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.weekly_metrics_run scheduled__2024-10-03T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:59:45.048+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 4/16 running and queued tasks[0m
 [[34m2024-11-27T14:59:45.048+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.weekly_metrics_run scheduled__2024-10-03T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:59:45.050+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.weekly_metrics_run', run_id='scheduled__2024-10-03T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
 [[34m2024-11-27T14:59:45.050+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.weekly_metrics_run', 'scheduled__2024-10-03T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:59:45.052+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.weekly_metrics_run', 'scheduled__2024-10-03T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:59:45.052+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.dim_date_run', run_id='scheduled__2024-10-03T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:59:45.056+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.dim_date_run, run_id=scheduled__2024-10-03T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:59:36.690335+00:00, run_end_date=2024-11-27 14:59:44.413837+00:00, run_duration=7.723502, state=success, executor_state=success, try_number=1, max_tries=0, job_id=963, pool=default_pool, queue=default, priority_weight=2, operator=DbtRunLocalOperator, queued_dttm=2024-11-27 14:59:27.225918+00:00, queued_by_job_id=803, pid=11874[0m
 [[34m2024-11-27T14:59:45.077+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412095850,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T14:59:45.078+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:59:46.025+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:59:46.039+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 [[34m2024-11-27T14:59:46.951+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:59:47.101+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:59:47.870+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:59:47.871+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:59:47.871+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:59:47.926+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 [[34m2024-11-27T14:59:48.359+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412095854,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T14:59:48.444+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:59:48.752+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 2 tasks up for execution:
 	<TaskInstance: capstone_andres_dag_.load_polygon_data_task scheduled__2024-10-06T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.load_treasury_data_task scheduled__2024-10-05T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:59:48.753+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 4/16 running and queued tasks[0m
 [[34m2024-11-27T14:59:48.753+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 5/16 running and queued tasks[0m
 [[34m2024-11-27T14:59:48.753+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
 	<TaskInstance: capstone_andres_dag_.load_polygon_data_task scheduled__2024-10-06T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.load_treasury_data_task scheduled__2024-10-05T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:59:48.755+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='load_polygon_data_task', run_id='scheduled__2024-10-06T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
 [[34m2024-11-27T14:59:48.756+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'load_polygon_data_task', 'scheduled__2024-10-06T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:59:48.756+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='load_treasury_data_task', run_id='scheduled__2024-10-05T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 5 and queue default[0m
 [[34m2024-11-27T14:59:48.756+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'load_treasury_data_task', 'scheduled__2024-10-05T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:59:48.758+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'load_polygon_data_task', 'scheduled__2024-10-06T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:59:48.759+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='load_polygon_data_task', run_id='scheduled__2024-10-05T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:59:48.759+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'load_treasury_data_task', 'scheduled__2024-10-05T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:59:48.763+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=load_polygon_data_task, run_id=scheduled__2024-10-05T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:59:40.789098+00:00, run_end_date=2024-11-27 14:59:47.471031+00:00, run_duration=6.681933, state=success, executor_state=success, try_number=1, max_tries=0, job_id=967, pool=default_pool, queue=default, priority_weight=6, operator=PythonOperator, queued_dttm=2024-11-27 14:59:32.908918+00:00, queued_by_job_id=803, pid=11898[0m
 [[34m2024-11-27T14:59:48.793+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:59:48.794+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T14:59:49.704+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:59:49.704+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:59:49.704+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:59:49.724+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T14:59:49.725+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T14:59:49.725+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T14:59:50.232+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412095858,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T14:59:50.265+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412097134,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T14:59:50.420+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:59:50.480+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:59:50.719+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T14:59:50.800+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T14:59:50.938+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:59:51.005+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:59:51.043+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:59:51.044+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:59:51.147+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 [[34m2024-11-27T14:59:51.338+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 [[34m2024-11-27T14:59:51.438+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:59:51.566+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:59:51.577+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:59:51.905+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:59:52.028+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:59:52.088+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:59:52.167+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:59:52.207+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 [[34m2024-11-27T14:59:52.282+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 [[34m2024-11-27T14:59:52.776+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 Data loaded successfully
 [[34m2024-11-27T14:59:52.824+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:59:52.841+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 Data loaded successfully
 [[34m2024-11-27T14:59:52.874+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:59:52.888+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 [[34m2024-11-27T14:59:52.925+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:59:52.967+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:59:53.051+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 [[34m2024-11-27T14:59:53.331+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T14:59:53.696+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 Data loaded successfully
 [[34m2024-11-27T14:59:53.754+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:59:53.773+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 [[34m2024-11-27T14:59:53.921+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.4512495610397309[0m
 [[34m2024-11-27T14:59:53.921+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|11933]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:59:53.922+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:59:53.922+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:59:53.923+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:59:53.923+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|11933]: It took 1.1s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:59:53.926+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|11933]: It took 0.0035s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:59:53.933+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.4371607410721481[0m
 [[34m2024-11-27T14:59:53.934+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|11935]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:59:53.935+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:59:53.935+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:59:53.935+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:59:53.936+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|11935]: It took 1.06s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:59:53.939+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|11935]: It took 0.0036s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:59:53.982+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run scheduled__2024-10-04T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [[34m2024-11-27T14:59:53.993+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.dim_date_run scheduled__2024-10-04T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [[34m2024-11-27T14:59:54.236+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:59:54.559+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [2024-11-27T14:59:54.756+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run, dagrun_id=scheduled__2024-10-04T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:59:54.095500+00:00, try_number=1, job_id=968, op_classpath=cosmos.operators.local.DbtRunLocalOperator, cosmos.operators.base.DbtRunMixin, cosmos.operators.local.DbtLocalBaseOperator, cosmos.operators.base.AbstractDbtBaseOperator
 [2024-11-27T14:59:54.775+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.dim_date_run, dagrun_id=scheduled__2024-10-04T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:59:54.115055+00:00, try_number=1, job_id=969, op_classpath=cosmos.operators.local.DbtRunLocalOperator, cosmos.operators.base.DbtRunMixin, cosmos.operators.local.DbtLocalBaseOperator, cosmos.operators.base.AbstractDbtBaseOperator
 [[34m2024-11-27T14:59:54.817+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:59:54.817+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:59:54.991+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:59:55.004+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.608514062827453[0m
 [[34m2024-11-27T14:59:55.005+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|11934]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:59:55.006+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:59:55.006+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:59:55.006+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:59:55.007+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|11934]: It took 1.25s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T14:59:55.012+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:59:55.013+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|11934]: It took 0.00549s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:59:55.139+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_daily_bars_run scheduled__2024-10-04T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [[34m2024-11-27T14:59:55.144+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 Data loaded successfully
 [[34m2024-11-27T14:59:55.599+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:59:55.617+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 [[34m2024-11-27T14:59:55.712+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:59:55.811+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [2024-11-27T14:59:55.894+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.stg_daily_bars_run, dagrun_id=scheduled__2024-10-04T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:59:55.280809+00:00, try_number=1, job_id=970, op_classpath=cosmos.operators.local.DbtRunLocalOperator, cosmos.operators.base.DbtRunMixin, cosmos.operators.local.DbtLocalBaseOperator, cosmos.operators.base.AbstractDbtBaseOperator
 [[34m2024-11-27T14:59:55.982+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:59:56.094+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 [[34m2024-11-27T14:59:56.699+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.5314847719855607[0m
 [[34m2024-11-27T14:59:56.699+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|11980]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:59:56.700+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:59:56.700+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:59:56.701+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:59:56.701+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|11980]: It took 1.1s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:59:56.704+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|11980]: It took 0.00289s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:59:56.755+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.load_polygon_data_task scheduled__2024-10-06T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 Data loaded successfully
 [[34m2024-11-27T14:59:56.757+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T14:59:56.776+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 [2024-11-27T14:59:57.335+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=load_polygon_data_task, dagrun_id=scheduled__2024-10-06T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:59:56.882352+00:00, try_number=1, job_id=971, op_classpath=airflow.operators.python.PythonOperator
 [[34m2024-11-27T14:59:58.016+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.6127682889346033[0m
 [[34m2024-11-27T14:59:58.016+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|11981]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T14:59:58.017+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T14:59:58.017+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T14:59:58.017+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T14:59:58.018+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|11981]: It took 1.26s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T14:59:58.021+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|11981]: It took 0.00305s to build the Airflow DAG.[0m
 [[34m2024-11-27T14:59:58.065+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.load_treasury_data_task scheduled__2024-10-05T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [2024-11-27T14:59:58.491+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=load_treasury_data_task, dagrun_id=scheduled__2024-10-05T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:59:58.170215+00:00, try_number=1, job_id=972, op_classpath=airflow.operators.python.PythonOperator
 [[34m2024-11-27T14:59:58.583+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:59:58.889+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T14:59:59.150+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T14:59:59.215+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
 	<TaskInstance: capstone_andres_dag_.load_polygon_data_task scheduled__2024-10-07T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:59:59.215+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 5/16 running and queued tasks[0m
 [[34m2024-11-27T14:59:59.215+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
 	<TaskInstance: capstone_andres_dag_.load_polygon_data_task scheduled__2024-10-07T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T14:59:59.217+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='load_polygon_data_task', run_id='scheduled__2024-10-07T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
 [[34m2024-11-27T14:59:59.218+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'load_polygon_data_task', 'scheduled__2024-10-07T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:59:59.221+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='load_polygon_data_task', run_id='scheduled__2024-10-06T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T14:59:59.220+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'load_polygon_data_task', 'scheduled__2024-10-07T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T14:59:59.225+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=load_polygon_data_task, run_id=scheduled__2024-10-06T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:59:56.882352+00:00, run_end_date=2024-11-27 14:59:58.044563+00:00, run_duration=1.162211, state=success, executor_state=success, try_number=1, max_tries=0, job_id=971, pool=default_pool, queue=default, priority_weight=6, operator=PythonOperator, queued_dttm=2024-11-27 14:59:48.754452+00:00, queued_by_job_id=803, pid=12054[0m
 [[34m2024-11-27T14:59:59.257+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T15:00:00.160+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T15:00:00.160+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T15:00:00.161+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T15:00:00.214+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:00:00.361+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:00:00.479+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 [[34m2024-11-27T15:00:00.590+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412095874,
 "os.name" : Linux
 [0m
 Data loaded successfully
 [[34m2024-11-27T15:00:01.037+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T15:00:01.052+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 /usr/local/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked semaphore objects to clean up at shutdown
   warnings.warn('resource_tracker: There appear to be %d '
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T15:00:01.363+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:00:01.926+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.3421815631445497[0m
 [[34m2024-11-27T15:00:01.926+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|11943]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T15:00:01.927+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T15:00:01.927+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T15:00:01.927+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T15:00:01.927+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|11943]: It took 0.89s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T15:00:01.930+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|11943]: It took 0.00244s to build the Airflow DAG.[0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 [[34m2024-11-27T15:00:01.969+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.weekly_metrics_run scheduled__2024-10-03T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [[34m2024-11-27T15:00:02.031+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 4 tasks up for execution:
 	<TaskInstance: capstone_andres_dag_.load_treasury_data_task scheduled__2024-10-06T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.dim_date_run scheduled__2024-10-05T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_daily_bars_run scheduled__2024-10-05T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run scheduled__2024-10-05T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T15:00:02.031+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 4/16 running and queued tasks[0m
 [[34m2024-11-27T15:00:02.031+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 5/16 running and queued tasks[0m
 [[34m2024-11-27T15:00:02.031+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 6/16 running and queued tasks[0m
 [[34m2024-11-27T15:00:02.031+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 7/16 running and queued tasks[0m
 [[34m2024-11-27T15:00:02.032+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
 	<TaskInstance: capstone_andres_dag_.load_treasury_data_task scheduled__2024-10-06T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.dim_date_run scheduled__2024-10-05T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_daily_bars_run scheduled__2024-10-05T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run scheduled__2024-10-05T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T15:00:02.034+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='load_treasury_data_task', run_id='scheduled__2024-10-06T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 5 and queue default[0m
 [[34m2024-11-27T15:00:02.034+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'load_treasury_data_task', 'scheduled__2024-10-06T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T15:00:02.034+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.dim_date_run', run_id='scheduled__2024-10-05T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
 [[34m2024-11-27T15:00:02.035+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.dim_date_run', 'scheduled__2024-10-05T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T15:00:02.035+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.stg_daily_bars_run', run_id='scheduled__2024-10-05T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
 [[34m2024-11-27T15:00:02.035+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.stg_daily_bars_run', 'scheduled__2024-10-05T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T15:00:02.035+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run', run_id='scheduled__2024-10-05T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
 [[34m2024-11-27T15:00:02.036+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run', 'scheduled__2024-10-05T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T15:00:02.039+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run', 'scheduled__2024-10-05T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T15:00:02.038+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'load_treasury_data_task', 'scheduled__2024-10-06T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T15:00:02.038+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.stg_daily_bars_run', 'scheduled__2024-10-05T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T15:00:02.040+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='load_treasury_data_task', run_id='scheduled__2024-10-05T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T15:00:02.040+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run', run_id='scheduled__2024-10-04T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T15:00:02.042+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.dim_date_run', 'scheduled__2024-10-05T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T15:00:02.047+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=load_treasury_data_task, run_id=scheduled__2024-10-05T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:59:58.170215+00:00, run_end_date=2024-11-27 15:00:00.555373+00:00, run_duration=2.385158, state=success, executor_state=success, try_number=1, max_tries=0, job_id=972, pool=default_pool, queue=default, priority_weight=5, operator=PythonOperator, queued_dttm=2024-11-27 14:59:48.754452+00:00, queued_by_job_id=803, pid=12060[0m
 [[34m2024-11-27T15:00:02.047+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run, run_id=scheduled__2024-10-04T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:59:54.095500+00:00, run_end_date=2024-11-27 15:00:01.282585+00:00, run_duration=7.187085, state=success, executor_state=success, try_number=1, max_tries=0, job_id=968, pool=default_pool, queue=default, priority_weight=2, operator=DbtRunLocalOperator, queued_dttm=2024-11-27 14:59:43.513610+00:00, queued_by_job_id=803, pid=12017[0m
 [[34m2024-11-27T15:00:02.074+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T15:00:02.075+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T15:00:02.076+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T15:00:02.085+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T15:00:02.211+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 /usr/local/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked semaphore objects to clean up at shutdown
   warnings.warn('resource_tracker: There appear to be %d '
 /usr/local/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked semaphore objects to clean up at shutdown
   warnings.warn('resource_tracker: There appear to be %d '
 [2024-11-27T15:00:02.949+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.weekly_metrics_run, dagrun_id=scheduled__2024-10-03T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 15:00:02.062269+00:00, try_number=1, job_id=973, op_classpath=cosmos.operators.local.DbtRunLocalOperator, cosmos.operators.base.DbtRunMixin, cosmos.operators.local.DbtLocalBaseOperator, cosmos.operators.base.AbstractDbtBaseOperator
 [[34m2024-11-27T15:00:03.224+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T15:00:03.225+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T15:00:03.225+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T15:00:03.229+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T15:00:03.230+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T15:00:03.230+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T15:00:03.230+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T15:00:03.230+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T15:00:03.231+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T15:00:03.251+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T15:00:03.251+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T15:00:03.252+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T15:00:03.359+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.weekly_metrics_run scheduled__2024-10-04T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T15:00:03.360+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 6/16 running and queued tasks[0m
 [[34m2024-11-27T15:00:03.360+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.weekly_metrics_run scheduled__2024-10-04T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T15:00:03.361+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.weekly_metrics_run', run_id='scheduled__2024-10-04T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
 [[34m2024-11-27T15:00:03.362+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.weekly_metrics_run', 'scheduled__2024-10-04T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T15:00:03.363+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.weekly_metrics_run', 'scheduled__2024-10-04T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T15:00:03.364+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.stg_daily_bars_run', run_id='scheduled__2024-10-04T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T15:00:03.364+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.dim_date_run', run_id='scheduled__2024-10-04T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T15:00:03.368+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.dim_date_run, run_id=scheduled__2024-10-04T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:59:54.115055+00:00, run_end_date=2024-11-27 15:00:02.404737+00:00, run_duration=8.289682, state=success, executor_state=success, try_number=1, max_tries=0, job_id=969, pool=default_pool, queue=default, priority_weight=2, operator=DbtRunLocalOperator, queued_dttm=2024-11-27 14:59:43.513610+00:00, queued_by_job_id=803, pid=12018[0m
 [[34m2024-11-27T15:00:03.368+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.stg_daily_bars_run, run_id=scheduled__2024-10-04T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 14:59:55.280809+00:00, run_end_date=2024-11-27 15:00:02.173948+00:00, run_duration=6.893139, state=success, executor_state=success, try_number=1, max_tries=0, job_id=970, pool=default_pool, queue=default, priority_weight=2, operator=DbtRunLocalOperator, queued_dttm=2024-11-27 14:59:43.513610+00:00, queued_by_job_id=803, pid=12025[0m
 [[34m2024-11-27T15:00:03.387+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.weekly_metrics'[0m
 [[34m2024-11-27T15:00:03.388+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections'[0m
 [[34m2024-11-27T15:00:03.388+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_payments'[0m
 [[34m2024-11-27T15:00:03.388+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_orders'[0m
 [[34m2024-11-27T15:00:03.389+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_daily_bars'[0m
 [[34m2024-11-27T15:00:03.390+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T15:00:03.696+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412097182,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T15:00:03.702+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412095878,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T15:00:03.706+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412097178,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T15:00:03.768+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412095882,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T15:00:03.780+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:00:04.059+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T15:00:04.232+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T15:00:04.232+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T15:00:04.233+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T15:00:04.276+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T15:00:04.406+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T15:00:04.462+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T15:00:04.630+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:00:04.723+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412095886,
 "os.name" : Linux
 [0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T15:00:04.726+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 [[34m2024-11-27T15:00:05.066+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 [[34m2024-11-27T15:00:05.148+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 [[34m2024-11-27T15:00:05.352+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:00:05.393+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:00:05.438+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T15:00:05.477+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:00:05.635+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:00:05.744+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 [[34m2024-11-27T15:00:05.900+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 Data loaded successfully
 [[34m2024-11-27T15:00:06.366+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T15:00:06.377+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 [[34m2024-11-27T15:00:07.192+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.28952528606168926[0m
 [[34m2024-11-27T15:00:07.192+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|12064]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T15:00:07.193+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T15:00:07.193+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T15:00:07.193+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T15:00:07.194+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|12064]: It took 0.828s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T15:00:07.196+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|12064]: It took 0.00223s to build the Airflow DAG.[0m
 [[34m2024-11-27T15:00:07.228+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.load_polygon_data_task scheduled__2024-10-07T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [2024-11-27T15:00:07.669+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=load_polygon_data_task, dagrun_id=scheduled__2024-10-07T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 15:00:07.305375+00:00, try_number=1, job_id=974, op_classpath=airflow.operators.python.PythonOperator
 [[34m2024-11-27T15:00:07.709+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:00:07.709+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:00:07.995+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:00:08.016+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T15:00:08.021+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T15:00:08.142+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:00:08.277+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:00:08.301+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:00:08.306+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T15:00:08.416+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T15:00:08.648+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:00:08.666+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:00:08.708+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:00:08.965+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 /usr/local/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked semaphore objects to clean up at shutdown
   warnings.warn('resource_tracker: There appear to be %d '
 [[34m2024-11-27T15:00:09.229+0000[0m] {[34mdagrun.py:[0m851} INFO[0m - Marking run <DagRun capstone_andres_dag_ @ 2024-10-03 01:00:00+00:00: scheduled__2024-10-03T01:00:00+00:00, state:running, queued_at: 2024-11-27 14:55:55.018201+00:00. externally triggered: False> successful[0m
 [[34m2024-11-27T15:00:09.229+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=capstone_andres_dag_, execution_date=2024-10-03 01:00:00+00:00, run_id=scheduled__2024-10-03T01:00:00+00:00, run_start_date=2024-11-27 14:55:55.031812+00:00, run_end_date=2024-11-27 15:00:09.229811+00:00, run_duration=254.197999, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-10-03 01:00:00+00:00, data_interval_end=2024-10-04 01:00:00+00:00, dag_hash=77fa8f64adac9391cccaaae88e2be5d8[0m
 [[34m2024-11-27T15:00:09.232+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-10-04 01:00:00+00:00, run_after=2024-10-05 01:00:00+00:00[0m
 [[34m2024-11-27T15:00:09.327+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:00:09.330+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:00:09.395+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
 	<TaskInstance: capstone_andres_dag_.load_polygon_data_task scheduled__2024-10-08T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T15:00:09.396+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 5/16 running and queued tasks[0m
 [[34m2024-11-27T15:00:09.396+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
 	<TaskInstance: capstone_andres_dag_.load_polygon_data_task scheduled__2024-10-08T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T15:00:09.398+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='load_polygon_data_task', run_id='scheduled__2024-10-08T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
 [[34m2024-11-27T15:00:09.398+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'load_polygon_data_task', 'scheduled__2024-10-08T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T15:00:09.400+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'load_polygon_data_task', 'scheduled__2024-10-08T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T15:00:09.400+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='load_polygon_data_task', run_id='scheduled__2024-10-07T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T15:00:09.401+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.weekly_metrics_run', run_id='scheduled__2024-10-03T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T15:00:09.405+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.weekly_metrics_run, run_id=scheduled__2024-10-03T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 15:00:02.062269+00:00, run_end_date=2024-11-27 15:00:08.995521+00:00, run_duration=6.933252, state=success, executor_state=success, try_number=1, max_tries=0, job_id=973, pool=default_pool, queue=default, priority_weight=1, operator=DbtRunLocalOperator, queued_dttm=2024-11-27 14:59:45.049217+00:00, queued_by_job_id=803, pid=12099[0m
 [[34m2024-11-27T15:00:09.405+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=load_polygon_data_task, run_id=scheduled__2024-10-07T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 15:00:07.305375+00:00, run_end_date=2024-11-27 15:00:08.346460+00:00, run_duration=1.041085, state=success, executor_state=success, try_number=1, max_tries=0, job_id=974, pool=default_pool, queue=default, priority_weight=6, operator=PythonOperator, queued_dttm=2024-11-27 14:59:59.216365+00:00, queued_by_job_id=803, pid=12175[0m
 [[34m2024-11-27T15:00:09.425+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-10-05 01:00:00+00:00, run_after=2024-10-06 01:00:00+00:00[0m
 [[34m2024-11-27T15:00:09.427+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T15:00:09.438+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:00:09.483+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:00:09.570+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:00:09.589+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 [[34m2024-11-27T15:00:09.673+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 [[34m2024-11-27T15:00:09.772+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:00:09.796+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:00:09.914+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:00:09.954+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:00:10.041+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 [[34m2024-11-27T15:00:10.057+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 Data loaded successfully
 [[34m2024-11-27T15:00:10.194+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T15:00:10.208+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 [[34m2024-11-27T15:00:10.262+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T15:00:10.263+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T15:00:10.263+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 Data loaded successfully
 [[34m2024-11-27T15:00:10.304+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T15:00:10.315+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 [[34m2024-11-27T15:00:10.649+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-10-06 01:00:00+00:00, run_after=2024-10-07 01:00:00+00:00[0m
 [[34m2024-11-27T15:00:10.708+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 Data loaded successfully
 [[34m2024-11-27T15:00:10.728+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T15:00:10.740+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 Data loaded successfully
 [[34m2024-11-27T15:00:10.748+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T15:00:10.760+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 [[34m2024-11-27T15:00:10.771+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412097194,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T15:00:10.862+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:00:10.980+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 [[34m2024-11-27T15:00:11.054+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.3272783989086747[0m
 [[34m2024-11-27T15:00:11.055+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|12084]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T15:00:11.055+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T15:00:11.055+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T15:00:11.056+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T15:00:11.056+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|12084]: It took 0.862s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T15:00:11.058+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|12084]: It took 0.00228s to build the Airflow DAG.[0m
 [[34m2024-11-27T15:00:11.102+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.load_treasury_data_task scheduled__2024-10-06T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [[34m2024-11-27T15:00:11.231+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.396980440011248[0m
 [[34m2024-11-27T15:00:11.232+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|12086]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T15:00:11.232+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T15:00:11.233+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T15:00:11.233+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T15:00:11.233+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|12086]: It took 0.93s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T15:00:11.236+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|12086]: It took 0.00311s to build the Airflow DAG.[0m
 [[34m2024-11-27T15:00:11.283+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run scheduled__2024-10-05T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [2024-11-27T15:00:11.546+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=load_treasury_data_task, dagrun_id=scheduled__2024-10-06T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 15:00:11.188844+00:00, try_number=1, job_id=975, op_classpath=airflow.operators.python.PythonOperator
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T15:00:11.563+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 Data loaded successfully
 [[34m2024-11-27T15:00:11.658+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T15:00:11.672+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 [[34m2024-11-27T15:00:11.741+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.4501002039760351[0m
 [[34m2024-11-27T15:00:11.741+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|12085]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T15:00:11.741+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T15:00:11.742+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T15:00:11.742+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T15:00:11.742+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|12085]: It took 1.01s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T15:00:11.747+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|12085]: It took 0.00438s to build the Airflow DAG.[0m
 [[34m2024-11-27T15:00:11.801+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_daily_bars_run scheduled__2024-10-05T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [[34m2024-11-27T15:00:11.809+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.488036833005026[0m
 [[34m2024-11-27T15:00:11.809+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|12087]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T15:00:11.810+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T15:00:11.810+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T15:00:11.810+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T15:00:11.811+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|12087]: It took 1.06s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T15:00:11.814+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|12087]: It took 0.00329s to build the Airflow DAG.[0m
 [2024-11-27T15:00:11.819+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run, dagrun_id=scheduled__2024-10-05T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 15:00:11.379266+00:00, try_number=1, job_id=976, op_classpath=cosmos.operators.local.DbtRunLocalOperator, cosmos.operators.base.DbtRunMixin, cosmos.operators.local.DbtLocalBaseOperator, cosmos.operators.base.AbstractDbtBaseOperator
 [[34m2024-11-27T15:00:11.869+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.dim_date_run scheduled__2024-10-05T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [[34m2024-11-27T15:00:11.881+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-10-07 01:00:00+00:00, run_after=2024-10-08 01:00:00+00:00[0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 [[34m2024-11-27T15:00:12.298+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [2024-11-27T15:00:12.416+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.stg_daily_bars_run, dagrun_id=scheduled__2024-10-05T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 15:00:11.914443+00:00, try_number=1, job_id=977, op_classpath=cosmos.operators.local.DbtRunLocalOperator, cosmos.operators.base.DbtRunMixin, cosmos.operators.local.DbtLocalBaseOperator, cosmos.operators.base.AbstractDbtBaseOperator
 [2024-11-27T15:00:12.457+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.dim_date_run, dagrun_id=scheduled__2024-10-05T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 15:00:11.969434+00:00, try_number=1, job_id=978, op_classpath=cosmos.operators.local.DbtRunLocalOperator, cosmos.operators.base.DbtRunMixin, cosmos.operators.local.DbtLocalBaseOperator, cosmos.operators.base.AbstractDbtBaseOperator
 [[34m2024-11-27T15:00:12.807+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.5857696428429335[0m
 [[34m2024-11-27T15:00:12.808+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|12102]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T15:00:12.808+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T15:00:12.808+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T15:00:12.809+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T15:00:12.809+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|12102]: It took 1.15s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T15:00:12.811+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|12102]: It took 0.00257s to build the Airflow DAG.[0m
 [[34m2024-11-27T15:00:12.849+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.weekly_metrics_run scheduled__2024-10-04T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [[34m2024-11-27T15:00:13.191+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-10-08 01:00:00+00:00, run_after=2024-10-09 01:00:00+00:00[0m
 [2024-11-27T15:00:13.360+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.weekly_metrics_run, dagrun_id=scheduled__2024-10-04T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 15:00:12.926349+00:00, try_number=1, job_id=979, op_classpath=cosmos.operators.local.DbtRunLocalOperator, cosmos.operators.base.DbtRunMixin, cosmos.operators.local.DbtLocalBaseOperator, cosmos.operators.base.AbstractDbtBaseOperator
 [[34m2024-11-27T15:00:13.463+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 4 tasks up for execution:
 	<TaskInstance: capstone_andres_dag_.load_treasury_data_task scheduled__2024-10-07T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.dim_date_run scheduled__2024-10-06T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_daily_bars_run scheduled__2024-10-06T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run scheduled__2024-10-06T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T15:00:13.463+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 5/16 running and queued tasks[0m
 [[34m2024-11-27T15:00:13.463+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 6/16 running and queued tasks[0m
 [[34m2024-11-27T15:00:13.463+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 7/16 running and queued tasks[0m
 [[34m2024-11-27T15:00:13.464+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 8/16 running and queued tasks[0m
 [[34m2024-11-27T15:00:13.464+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
 	<TaskInstance: capstone_andres_dag_.load_treasury_data_task scheduled__2024-10-07T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.dim_date_run scheduled__2024-10-06T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_daily_bars_run scheduled__2024-10-06T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run scheduled__2024-10-06T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T15:00:13.467+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='load_treasury_data_task', run_id='scheduled__2024-10-07T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 5 and queue default[0m
 [[34m2024-11-27T15:00:13.467+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'load_treasury_data_task', 'scheduled__2024-10-07T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T15:00:13.468+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.dim_date_run', run_id='scheduled__2024-10-06T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
 [[34m2024-11-27T15:00:13.468+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.dim_date_run', 'scheduled__2024-10-06T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T15:00:13.468+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.stg_daily_bars_run', run_id='scheduled__2024-10-06T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
 [[34m2024-11-27T15:00:13.469+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.stg_daily_bars_run', 'scheduled__2024-10-06T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T15:00:13.469+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run', run_id='scheduled__2024-10-06T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
 [[34m2024-11-27T15:00:13.469+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run', 'scheduled__2024-10-06T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T15:00:13.472+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'load_treasury_data_task', 'scheduled__2024-10-07T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T15:00:13.472+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.dim_date_run', 'scheduled__2024-10-06T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T15:00:13.473+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.stg_daily_bars_run', 'scheduled__2024-10-06T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T15:00:13.473+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='load_treasury_data_task', run_id='scheduled__2024-10-06T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T15:00:13.473+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run', 'scheduled__2024-10-06T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T15:00:13.479+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=load_treasury_data_task, run_id=scheduled__2024-10-06T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 15:00:11.188844+00:00, run_end_date=2024-11-27 15:00:12.503673+00:00, run_duration=1.314829, state=success, executor_state=success, try_number=1, max_tries=0, job_id=975, pool=default_pool, queue=default, priority_weight=5, operator=PythonOperator, queued_dttm=2024-11-27 15:00:02.032750+00:00, queued_by_job_id=803, pid=12182[0m
 [[34m2024-11-27T15:00:13.504+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-10-09 01:00:00+00:00, run_after=2024-10-10 01:00:00+00:00[0m
 [[34m2024-11-27T15:00:13.517+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T15:00:13.518+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T15:00:13.532+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T15:00:13.533+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T15:00:14.652+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-10-10 01:00:00+00:00, run_after=2024-10-11 01:00:00+00:00[0m
 [[34m2024-11-27T15:00:14.934+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:00:15.008+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T15:00:15.009+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T15:00:15.009+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T15:00:15.010+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T15:00:15.010+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T15:00:15.011+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T15:00:15.016+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T15:00:15.017+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T15:00:15.018+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T15:00:15.050+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T15:00:15.051+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T15:00:15.052+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T15:00:15.231+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T15:00:15.541+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:00:15.552+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412097210,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T15:00:15.564+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412097206,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T15:00:15.569+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412095906,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T15:00:15.614+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412097202,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T15:00:16.031+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-10-11 01:00:00+00:00, run_after=2024-10-12 01:00:00+00:00[0m
 [[34m2024-11-27T15:00:16.795+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T15:00:16.889+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:00:17.057+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:00:17.256+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T15:00:17.731+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T15:00:17.770+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:00:17.788+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:00:17.860+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-10-12 01:00:00+00:00, run_after=2024-10-13 01:00:00+00:00[0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 Data loaded successfully
 [[34m2024-11-27T15:00:18.179+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T15:00:18.212+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 [[34m2024-11-27T15:00:18.376+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:00:18.412+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:00:19.522+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-10-13 01:00:00+00:00, run_after=2024-10-14 01:00:00+00:00[0m
 [[34m2024-11-27T15:00:19.671+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.6271884909365326[0m
 [[34m2024-11-27T15:00:19.671+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|12180]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T15:00:19.672+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T15:00:19.672+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T15:00:19.672+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T15:00:19.673+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|12180]: It took 1.49s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T15:00:19.677+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|12180]: It took 0.00388s to build the Airflow DAG.[0m
 [[34m2024-11-27T15:00:19.747+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.load_polygon_data_task scheduled__2024-10-08T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [[34m2024-11-27T15:00:20.040+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-10-14 01:00:00+00:00, run_after=2024-10-15 01:00:00+00:00[0m
 /usr/local/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked semaphore objects to clean up at shutdown
   warnings.warn('resource_tracker: There appear to be %d '
 [2024-11-27T15:00:20.756+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=load_polygon_data_task, dagrun_id=scheduled__2024-10-08T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 15:00:19.903761+00:00, try_number=1, job_id=980, op_classpath=airflow.operators.python.PythonOperator
 [[34m2024-11-27T15:00:20.961+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:00:21.220+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:00:21.246+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:00:21.284+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T15:00:21.467+0000[0m] {[34mscheduler_job_runner.py:[0m1328} INFO[0m - DAG capstone_andres_dag_ is at (or above) max_active_runs (16 of 16), not creating any more runs[0m
 [[34m2024-11-27T15:00:21.525+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T15:00:21.615+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T15:00:21.668+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:00:21.671+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run', run_id='scheduled__2024-10-05T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T15:00:21.677+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run, run_id=scheduled__2024-10-05T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 15:00:11.379266+00:00, run_end_date=2024-11-27 15:00:20.584562+00:00, run_duration=9.205296, state=success, executor_state=success, try_number=1, max_tries=0, job_id=976, pool=default_pool, queue=default, priority_weight=2, operator=DbtRunLocalOperator, queued_dttm=2024-11-27 15:00:02.032750+00:00, queued_by_job_id=803, pid=12183[0m
 /usr/local/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked semaphore objects to clean up at shutdown
   warnings.warn('resource_tracker: There appear to be %d '
 [[34m2024-11-27T15:00:21.894+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:00:21.982+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T15:00:22.192+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:00:22.949+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.stg_daily_bars_run', run_id='scheduled__2024-10-05T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T15:00:22.953+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.stg_daily_bars_run, run_id=scheduled__2024-10-05T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 15:00:11.914443+00:00, run_end_date=2024-11-27 15:00:21.564384+00:00, run_duration=9.649941, state=success, executor_state=success, try_number=1, max_tries=0, job_id=977, pool=default_pool, queue=default, priority_weight=2, operator=DbtRunLocalOperator, queued_dttm=2024-11-27 15:00:02.032750+00:00, queued_by_job_id=803, pid=12188[0m
 [[34m2024-11-27T15:00:22.961+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 [[34m2024-11-27T15:00:23.130+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:00:23.215+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 /usr/local/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked semaphore objects to clean up at shutdown
   warnings.warn('resource_tracker: There appear to be %d '
 [[34m2024-11-27T15:00:23.222+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:00:23.223+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:00:23.237+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 [[34m2024-11-27T15:00:23.397+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:00:23.397+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:00:23.501+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 [[34m2024-11-27T15:00:23.525+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 Data loaded successfully
 [[34m2024-11-27T15:00:23.800+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T15:00:23.821+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 /usr/local/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked semaphore objects to clean up at shutdown
   warnings.warn('resource_tracker: There appear to be %d '
 [[34m2024-11-27T15:00:23.999+0000[0m] {[34mdagrun.py:[0m851} INFO[0m - Marking run <DagRun capstone_andres_dag_ @ 2024-10-04 01:00:00+00:00: scheduled__2024-10-04T01:00:00+00:00, state:running, queued_at: 2024-11-27 14:56:13.337371+00:00. externally triggered: False> successful[0m
 [[34m2024-11-27T15:00:24.000+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=capstone_andres_dag_, execution_date=2024-10-04 01:00:00+00:00, run_id=scheduled__2024-10-04T01:00:00+00:00, run_start_date=2024-11-27 14:56:13.367407+00:00, run_end_date=2024-11-27 15:00:23.999987+00:00, run_duration=250.63258, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-10-04 01:00:00+00:00, data_interval_end=2024-10-05 01:00:00+00:00, dag_hash=77fa8f64adac9391cccaaae88e2be5d8[0m
 [[34m2024-11-27T15:00:24.006+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-10-05 01:00:00+00:00, run_after=2024-10-06 01:00:00+00:00[0m
 Data loaded successfully
 [[34m2024-11-27T15:00:24.069+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 Data loaded successfully
 [[34m2024-11-27T15:00:24.076+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T15:00:24.088+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 [[34m2024-11-27T15:00:24.097+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 [[34m2024-11-27T15:00:24.339+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.weekly_metrics_run scheduled__2024-10-05T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T15:00:24.339+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 5/16 running and queued tasks[0m
 [[34m2024-11-27T15:00:24.339+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.weekly_metrics_run scheduled__2024-10-05T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T15:00:24.345+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.weekly_metrics_run', run_id='scheduled__2024-10-05T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
 [[34m2024-11-27T15:00:24.345+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.weekly_metrics_run', 'scheduled__2024-10-05T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T15:00:24.352+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.dim_date_run', run_id='scheduled__2024-10-05T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T15:00:24.353+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.weekly_metrics_run', run_id='scheduled__2024-10-04T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T15:00:24.353+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.weekly_metrics_run', 'scheduled__2024-10-05T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T15:00:24.362+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.weekly_metrics_run, run_id=scheduled__2024-10-04T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 15:00:12.926349+00:00, run_end_date=2024-11-27 15:00:23.752320+00:00, run_duration=10.825971, state=success, executor_state=success, try_number=1, max_tries=0, job_id=979, pool=default_pool, queue=default, priority_weight=1, operator=DbtRunLocalOperator, queued_dttm=2024-11-27 15:00:03.360782+00:00, queued_by_job_id=803, pid=12197[0m
 [[34m2024-11-27T15:00:24.363+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.dim_date_run, run_id=scheduled__2024-10-05T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 15:00:11.969434+00:00, run_end_date=2024-11-27 15:00:23.159203+00:00, run_duration=11.189769, state=success, executor_state=success, try_number=1, max_tries=0, job_id=978, pool=default_pool, queue=default, priority_weight=2, operator=DbtRunLocalOperator, queued_dttm=2024-11-27 15:00:02.032750+00:00, queued_by_job_id=803, pid=12189[0m
 [[34m2024-11-27T15:00:24.386+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-10-06 01:00:00+00:00, run_after=2024-10-07 01:00:00+00:00[0m
 [[34m2024-11-27T15:00:24.436+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T15:00:25.385+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.8606242551468313[0m
 [[34m2024-11-27T15:00:25.394+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|12202]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T15:00:25.407+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T15:00:25.410+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T15:00:25.411+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T15:00:25.412+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|12202]: It took 1.61s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T15:00:25.430+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|12202]: It took 0.0175s to build the Airflow DAG.[0m
 [[34m2024-11-27T15:00:25.510+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-10-07 01:00:00+00:00, run_after=2024-10-08 01:00:00+00:00[0m
 [[34m2024-11-27T15:00:25.627+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.8302462080027908[0m
 [[34m2024-11-27T15:00:25.636+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|12204]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T15:00:25.653+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T15:00:25.656+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T15:00:25.661+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T15:00:25.663+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|12204]: It took 1.6s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T15:00:25.685+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.8746364030521363[0m
 [[34m2024-11-27T15:00:25.690+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|12203]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T15:00:25.691+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|12204]: It took 0.0285s to build the Airflow DAG.[0m
 [[34m2024-11-27T15:00:25.694+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T15:00:25.696+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T15:00:25.696+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T15:00:25.697+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|12203]: It took 1.62s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T15:00:25.713+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|12203]: It took 0.0163s to build the Airflow DAG.[0m
 [[34m2024-11-27T15:00:25.714+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run scheduled__2024-10-06T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [[34m2024-11-27T15:00:26.137+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:00:26.304+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.load_treasury_data_task scheduled__2024-10-07T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [[34m2024-11-27T15:00:26.306+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_daily_bars_run scheduled__2024-10-06T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [[34m2024-11-27T15:00:26.496+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T15:00:26.852+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:00:27.493+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T15:00:27.494+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T15:00:27.495+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [2024-11-27T15:00:27.532+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=load_treasury_data_task, dagrun_id=scheduled__2024-10-07T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 15:00:26.625262+00:00, try_number=1, job_id=982, op_classpath=airflow.operators.python.PythonOperator
 [2024-11-27T15:00:27.653+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run, dagrun_id=scheduled__2024-10-06T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 15:00:26.419139+00:00, try_number=1, job_id=981, op_classpath=cosmos.operators.local.DbtRunLocalOperator, cosmos.operators.base.DbtRunMixin, cosmos.operators.local.DbtLocalBaseOperator, cosmos.operators.base.AbstractDbtBaseOperator
 [2024-11-27T15:00:27.743+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.stg_daily_bars_run, dagrun_id=scheduled__2024-10-06T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 15:00:26.616535+00:00, try_number=1, job_id=983, op_classpath=cosmos.operators.local.DbtRunLocalOperator, cosmos.operators.base.DbtRunMixin, cosmos.operators.local.DbtLocalBaseOperator, cosmos.operators.base.AbstractDbtBaseOperator
 [[34m2024-11-27T15:00:27.801+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-10-08 01:00:00+00:00, run_after=2024-10-09 01:00:00+00:00[0m
 [[34m2024-11-27T15:00:28.008+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:00:28.054+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412095946,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T15:00:28.173+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:00:28.280+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 [[34m2024-11-27T15:00:28.518+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-10-09 01:00:00+00:00, run_after=2024-10-10 01:00:00+00:00[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T15:00:28.987+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 Data loaded successfully
 [[34m2024-11-27T15:00:29.149+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T15:00:29.184+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 [[34m2024-11-27T15:00:29.827+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:00:30.013+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-10-10 01:00:00+00:00, run_after=2024-10-11 01:00:00+00:00[0m
 [[34m2024-11-27T15:00:30.785+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.dim_date_run scheduled__2024-10-07T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_daily_bars_run scheduled__2024-10-07T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run scheduled__2024-10-07T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T15:00:30.789+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 4/16 running and queued tasks[0m
 [[34m2024-11-27T15:00:30.792+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 5/16 running and queued tasks[0m
 [[34m2024-11-27T15:00:30.795+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 6/16 running and queued tasks[0m
 [[34m2024-11-27T15:00:30.797+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.dim_date_run scheduled__2024-10-07T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_daily_bars_run scheduled__2024-10-07T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run scheduled__2024-10-07T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T15:00:30.800+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.dim_date_run', run_id='scheduled__2024-10-07T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
 [[34m2024-11-27T15:00:30.801+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.dim_date_run', 'scheduled__2024-10-07T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T15:00:30.801+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.stg_daily_bars_run', run_id='scheduled__2024-10-07T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
 [[34m2024-11-27T15:00:30.802+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.stg_daily_bars_run', 'scheduled__2024-10-07T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T15:00:30.802+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run', run_id='scheduled__2024-10-07T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
 [[34m2024-11-27T15:00:30.802+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run', 'scheduled__2024-10-07T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T15:00:30.807+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='load_treasury_data_task', run_id='scheduled__2024-10-07T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T15:00:30.806+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run', 'scheduled__2024-10-07T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T15:00:30.811+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.stg_daily_bars_run', 'scheduled__2024-10-07T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T15:00:30.816+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=load_treasury_data_task, run_id=scheduled__2024-10-07T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 15:00:26.625262+00:00, run_end_date=2024-11-27 15:00:28.546306+00:00, run_duration=1.921044, state=success, executor_state=success, try_number=1, max_tries=0, job_id=982, pool=default_pool, queue=default, priority_weight=5, operator=PythonOperator, queued_dttm=2024-11-27 15:00:13.465266+00:00, queued_by_job_id=803, pid=12331[0m
 [[34m2024-11-27T15:00:30.810+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.dim_date_run', 'scheduled__2024-10-07T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T15:00:30.846+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-10-11 01:00:00+00:00, run_after=2024-10-12 01:00:00+00:00[0m
 [[34m2024-11-27T15:00:30.938+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T15:00:30.943+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T15:00:30.945+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T15:00:31.432+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 2 tasks up for execution:
 	<TaskInstance: capstone_andres_dag_.load_polygon_data_task scheduled__2024-10-09T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.load_treasury_data_task scheduled__2024-10-08T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T15:00:31.432+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 7/16 running and queued tasks[0m
 [[34m2024-11-27T15:00:31.432+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 8/16 running and queued tasks[0m
 [[34m2024-11-27T15:00:31.433+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
 	<TaskInstance: capstone_andres_dag_.load_polygon_data_task scheduled__2024-10-09T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.load_treasury_data_task scheduled__2024-10-08T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T15:00:31.437+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='load_polygon_data_task', run_id='scheduled__2024-10-09T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
 [[34m2024-11-27T15:00:31.437+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'load_polygon_data_task', 'scheduled__2024-10-09T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T15:00:31.437+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='load_treasury_data_task', run_id='scheduled__2024-10-08T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 5 and queue default[0m
 [[34m2024-11-27T15:00:31.438+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'load_treasury_data_task', 'scheduled__2024-10-08T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T15:00:31.442+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='load_polygon_data_task', run_id='scheduled__2024-10-08T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T15:00:31.445+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'load_treasury_data_task', 'scheduled__2024-10-08T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T15:00:31.453+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=load_polygon_data_task, run_id=scheduled__2024-10-08T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 15:00:19.903761+00:00, run_end_date=2024-11-27 15:00:30.269417+00:00, run_duration=10.365656, state=success, executor_state=success, try_number=1, max_tries=0, job_id=980, pool=default_pool, queue=default, priority_weight=6, operator=PythonOperator, queued_dttm=2024-11-27 15:00:09.396890+00:00, queued_by_job_id=803, pid=12280[0m
 [[34m2024-11-27T15:00:31.445+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'load_polygon_data_task', 'scheduled__2024-10-09T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T15:00:31.511+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-10-12 01:00:00+00:00, run_after=2024-10-13 01:00:00+00:00[0m
 [[34m2024-11-27T15:00:31.554+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T15:00:31.592+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T15:00:32.315+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 1.624936186010018[0m
 [[34m2024-11-27T15:00:32.320+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|12205]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T15:00:32.323+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T15:00:32.326+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T15:00:32.327+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T15:00:32.328+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|12205]: It took 3.18s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T15:00:32.351+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|12205]: It took 0.0228s to build the Airflow DAG.[0m
 [[34m2024-11-27T15:00:32.570+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.dim_date_run scheduled__2024-10-06T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [[34m2024-11-27T15:00:33.244+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-10-13 01:00:00+00:00, run_after=2024-10-14 01:00:00+00:00[0m
 [[34m2024-11-27T15:00:33.553+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T15:00:33.557+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T15:00:33.558+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T15:00:33.563+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T15:00:33.566+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T15:00:33.567+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T15:00:33.691+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T15:00:33.693+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T15:00:33.693+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T15:00:33.862+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:00:34.171+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T15:00:34.248+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T15:00:34.250+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T15:00:34.250+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T15:00:34.260+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T15:00:34.261+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T15:00:34.262+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [2024-11-27T15:00:34.317+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.dim_date_run, dagrun_id=scheduled__2024-10-06T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 15:00:32.850652+00:00, try_number=1, job_id=984, op_classpath=cosmos.operators.local.DbtRunLocalOperator, cosmos.operators.base.DbtRunMixin, cosmos.operators.local.DbtLocalBaseOperator, cosmos.operators.base.AbstractDbtBaseOperator
 [[34m2024-11-27T15:00:34.415+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:00:34.539+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412095962,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T15:00:34.567+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412097250,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T15:00:34.568+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412095966,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T15:00:34.847+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-10-14 01:00:00+00:00, run_after=2024-10-15 01:00:00+00:00[0m
 [[34m2024-11-27T15:00:34.883+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412095970,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T15:00:34.898+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412097254,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T15:00:35.442+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T15:00:35.650+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T15:00:35.655+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:00:35.671+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T15:00:35.693+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:00:35.783+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T15:00:35.964+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T15:00:35.971+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 [[34m2024-11-27T15:00:36.101+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:00:36.101+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:00:36.137+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 Data loaded successfully
 [[34m2024-11-27T15:00:36.376+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T15:00:36.393+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 [[34m2024-11-27T15:00:36.442+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:00:36.485+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:00:36.499+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-10-15 01:00:00+00:00, run_after=2024-10-16 01:00:00+00:00[0m
 [[34m2024-11-27T15:00:37.251+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-10-16 01:00:00+00:00, run_after=2024-10-17 01:00:00+00:00[0m
 [[34m2024-11-27T15:00:37.769+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:00:38.166+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T15:00:38.624+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:00:38.891+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 1.7970647350884974[0m
 [[34m2024-11-27T15:00:38.895+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|12320]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T15:00:38.898+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T15:00:38.900+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T15:00:38.900+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T15:00:38.901+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|12320]: It took 2.53s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T15:00:38.910+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|12320]: It took 0.00952s to build the Airflow DAG.[0m
 [[34m2024-11-27T15:00:38.948+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-10-17 01:00:00+00:00, run_after=2024-10-18 01:00:00+00:00[0m
 [[34m2024-11-27T15:00:39.028+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:00:39.050+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.weekly_metrics_run scheduled__2024-10-05T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [[34m2024-11-27T15:00:39.089+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:00:39.230+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:00:39.297+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:00:39.353+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T15:00:39.433+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T15:00:39.550+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T15:00:39.664+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:00:39.665+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T15:00:39.715+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:00:39.846+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 /usr/local/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked semaphore objects to clean up at shutdown
   warnings.warn('resource_tracker: There appear to be %d '
 [[34m2024-11-27T15:00:39.965+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:00:40.041+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:00:40.196+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:00:40.307+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 /usr/local/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked semaphore objects to clean up at shutdown
   warnings.warn('resource_tracker: There appear to be %d '
 [2024-11-27T15:00:40.548+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.weekly_metrics_run, dagrun_id=scheduled__2024-10-05T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 15:00:39.280053+00:00, try_number=1, job_id=985, op_classpath=cosmos.operators.local.DbtRunLocalOperator, cosmos.operators.base.DbtRunMixin, cosmos.operators.local.DbtLocalBaseOperator, cosmos.operators.base.AbstractDbtBaseOperator
 [[34m2024-11-27T15:00:40.560+0000[0m] {[34mscheduler_job_runner.py:[0m1328} INFO[0m - DAG capstone_andres_dag_ is at (or above) max_active_runs (16 of 16), not creating any more runs[0m
 Data loaded successfully
 [[34m2024-11-27T15:00:40.823+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T15:00:40.823+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:00:40.824+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:00:40.850+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 [[34m2024-11-27T15:00:41.003+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:00:41.012+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:00:41.040+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:00:41.075+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:00:41.126+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 [[34m2024-11-27T15:00:41.147+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 [[34m2024-11-27T15:00:41.187+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:00:41.240+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:00:41.267+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run', run_id='scheduled__2024-10-06T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T15:00:41.269+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.stg_daily_bars_run', run_id='scheduled__2024-10-06T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T15:00:41.286+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.stg_daily_bars_run, run_id=scheduled__2024-10-06T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 15:00:26.616535+00:00, run_end_date=2024-11-27 15:00:40.261723+00:00, run_duration=13.645188, state=success, executor_state=success, try_number=1, max_tries=0, job_id=983, pool=default_pool, queue=default, priority_weight=2, operator=DbtRunLocalOperator, queued_dttm=2024-11-27 15:00:13.465266+00:00, queued_by_job_id=803, pid=12330[0m
 [[34m2024-11-27T15:00:41.287+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run, run_id=scheduled__2024-10-06T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 15:00:26.419139+00:00, run_end_date=2024-11-27 15:00:39.852681+00:00, run_duration=13.433542, state=success, executor_state=success, try_number=1, max_tries=0, job_id=981, pool=default_pool, queue=default, priority_weight=2, operator=DbtRunLocalOperator, queued_dttm=2024-11-27 15:00:13.465266+00:00, queued_by_job_id=803, pid=12329[0m
 [[34m2024-11-27T15:00:41.316+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 [[34m2024-11-27T15:00:41.383+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 Data loaded successfully
 [[34m2024-11-27T15:00:41.931+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T15:00:41.995+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 Data loaded successfully
 [[34m2024-11-27T15:00:42.039+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T15:00:42.068+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 Data loaded successfully
 [[34m2024-11-27T15:00:42.195+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T15:00:42.244+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 Data loaded successfully
 [[34m2024-11-27T15:00:42.394+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T15:00:42.436+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 [[34m2024-11-27T15:00:42.945+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 1.2361364958342165[0m
 [[34m2024-11-27T15:00:42.945+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|12358]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T15:00:42.947+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T15:00:42.947+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T15:00:42.948+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T15:00:42.948+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|12358]: It took 2.12s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T15:00:42.954+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|12358]: It took 0.0056s to build the Airflow DAG.[0m
 [[34m2024-11-27T15:00:43.027+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run scheduled__2024-10-07T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [[34m2024-11-27T15:00:43.542+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.7419763479847461[0m
 [[34m2024-11-27T15:00:43.543+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|12360]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T15:00:43.544+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T15:00:43.545+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T15:00:43.546+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T15:00:43.546+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|12360]: It took 1.62s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T15:00:43.554+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|12360]: It took 0.00771s to build the Airflow DAG.[0m
 [[34m2024-11-27T15:00:43.592+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.6994188309181482[0m
 [[34m2024-11-27T15:00:43.593+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|12359]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T15:00:43.594+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T15:00:43.594+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T15:00:43.595+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T15:00:43.595+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|12359]: It took 1.56s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T15:00:43.602+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|12359]: It took 0.00689s to build the Airflow DAG.[0m
 [[34m2024-11-27T15:00:43.663+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.dim_date_run scheduled__2024-10-07T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [[34m2024-11-27T15:00:43.706+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_daily_bars_run scheduled__2024-10-07T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [[34m2024-11-27T15:00:43.844+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.7739189369603992[0m
 [[34m2024-11-27T15:00:43.844+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|12366]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T15:00:43.846+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T15:00:43.846+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T15:00:43.848+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T15:00:43.850+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|12366]: It took 1.66s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T15:00:43.862+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|12366]: It took 0.0121s to build the Airflow DAG.[0m
 [[34m2024-11-27T15:00:43.990+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.load_treasury_data_task scheduled__2024-10-08T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [2024-11-27T15:00:44.163+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run, dagrun_id=scheduled__2024-10-07T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 15:00:43.142480+00:00, try_number=1, job_id=986, op_classpath=cosmos.operators.local.DbtRunLocalOperator, cosmos.operators.base.DbtRunMixin, cosmos.operators.local.DbtLocalBaseOperator, cosmos.operators.base.AbstractDbtBaseOperator
 [[34m2024-11-27T15:00:44.370+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 1.1286764990072697[0m
 [[34m2024-11-27T15:00:44.371+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|12367]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T15:00:44.373+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T15:00:44.374+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T15:00:44.374+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T15:00:44.374+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|12367]: It took 1.98s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T15:00:44.382+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|12367]: It took 0.00796s to build the Airflow DAG.[0m
 [[34m2024-11-27T15:00:44.505+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.load_polygon_data_task scheduled__2024-10-09T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [2024-11-27T15:00:45.203+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.dim_date_run, dagrun_id=scheduled__2024-10-07T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 15:00:43.906203+00:00, try_number=1, job_id=987, op_classpath=cosmos.operators.local.DbtRunLocalOperator, cosmos.operators.base.DbtRunMixin, cosmos.operators.local.DbtLocalBaseOperator, cosmos.operators.base.AbstractDbtBaseOperator
 [2024-11-27T15:00:45.239+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.stg_daily_bars_run, dagrun_id=scheduled__2024-10-07T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 15:00:43.972264+00:00, try_number=1, job_id=988, op_classpath=cosmos.operators.local.DbtRunLocalOperator, cosmos.operators.base.DbtRunMixin, cosmos.operators.local.DbtLocalBaseOperator, cosmos.operators.base.AbstractDbtBaseOperator
 [2024-11-27T15:00:45.379+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=load_treasury_data_task, dagrun_id=scheduled__2024-10-08T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 15:00:44.233145+00:00, try_number=1, job_id=989, op_classpath=airflow.operators.python.PythonOperator
 [2024-11-27T15:00:45.862+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=load_polygon_data_task, dagrun_id=scheduled__2024-10-09T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 15:00:44.773990+00:00, try_number=1, job_id=990, op_classpath=airflow.operators.python.PythonOperator
 /usr/local/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked semaphore objects to clean up at shutdown
   warnings.warn('resource_tracker: There appear to be %d '
 [[34m2024-11-27T15:00:49.040+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 4 tasks up for execution:
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.dim_date_run scheduled__2024-10-08T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_daily_bars_run scheduled__2024-10-08T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run scheduled__2024-10-08T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.weekly_metrics_run scheduled__2024-10-06T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T15:00:49.040+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 5/16 running and queued tasks[0m
 [[34m2024-11-27T15:00:49.041+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 6/16 running and queued tasks[0m
 [[34m2024-11-27T15:00:49.041+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 7/16 running and queued tasks[0m
 [[34m2024-11-27T15:00:49.041+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 8/16 running and queued tasks[0m
 [[34m2024-11-27T15:00:49.042+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.dim_date_run scheduled__2024-10-08T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_daily_bars_run scheduled__2024-10-08T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run scheduled__2024-10-08T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.weekly_metrics_run scheduled__2024-10-06T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T15:00:49.049+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.dim_date_run', run_id='scheduled__2024-10-08T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
 [[34m2024-11-27T15:00:49.049+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.dim_date_run', 'scheduled__2024-10-08T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T15:00:49.050+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.stg_daily_bars_run', run_id='scheduled__2024-10-08T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
 [[34m2024-11-27T15:00:49.050+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.stg_daily_bars_run', 'scheduled__2024-10-08T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T15:00:49.051+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run', run_id='scheduled__2024-10-08T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
 [[34m2024-11-27T15:00:49.051+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run', 'scheduled__2024-10-08T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T15:00:49.051+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.weekly_metrics_run', run_id='scheduled__2024-10-06T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
 [[34m2024-11-27T15:00:49.052+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.weekly_metrics_run', 'scheduled__2024-10-06T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T15:00:49.060+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='load_treasury_data_task', run_id='scheduled__2024-10-08T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T15:00:49.061+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.dim_date_run', run_id='scheduled__2024-10-06T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T15:00:49.063+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.dim_date_run', 'scheduled__2024-10-08T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T15:00:49.063+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.weekly_metrics_run', 'scheduled__2024-10-06T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T15:00:49.064+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.stg_daily_bars_run', 'scheduled__2024-10-08T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T15:00:49.071+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.dim_date_run, run_id=scheduled__2024-10-06T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 15:00:32.850652+00:00, run_end_date=2024-11-27 15:00:48.086982+00:00, run_duration=15.23633, state=success, executor_state=success, try_number=1, max_tries=0, job_id=984, pool=default_pool, queue=default, priority_weight=2, operator=DbtRunLocalOperator, queued_dttm=2024-11-27 15:00:13.465266+00:00, queued_by_job_id=803, pid=12377[0m
 [[34m2024-11-27T15:00:49.075+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=load_treasury_data_task, run_id=scheduled__2024-10-08T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 15:00:44.233145+00:00, run_end_date=2024-11-27 15:00:47.882387+00:00, run_duration=3.649242, state=success, executor_state=success, try_number=1, max_tries=0, job_id=989, pool=default_pool, queue=default, priority_weight=5, operator=PythonOperator, queued_dttm=2024-11-27 15:00:31.434526+00:00, queued_by_job_id=803, pid=12483[0m
 [[34m2024-11-27T15:00:49.072+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run', 'scheduled__2024-10-08T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T15:00:49.132+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T15:00:49.134+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T15:00:49.174+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T15:00:49.199+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T15:00:50.723+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T15:00:50.724+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T15:00:50.725+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T15:00:50.775+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T15:00:50.775+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T15:00:50.776+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T15:00:50.796+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T15:00:50.797+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T15:00:50.797+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T15:00:50.846+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T15:00:50.847+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T15:00:50.847+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T15:00:51.225+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412097294,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T15:00:51.226+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412097298,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T15:00:51.259+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412096018,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T15:00:51.318+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412096022,
 "os.name" : Linux
 [0m
 /usr/local/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked semaphore objects to clean up at shutdown
   warnings.warn('resource_tracker: There appear to be %d '
 [[34m2024-11-27T15:00:51.969+0000[0m] {[34mdagrun.py:[0m851} INFO[0m - Marking run <DagRun capstone_andres_dag_ @ 2024-10-05 01:00:00+00:00: scheduled__2024-10-05T01:00:00+00:00, state:running, queued_at: 2024-11-27 14:56:32.545520+00:00. externally triggered: False> successful[0m
 [[34m2024-11-27T15:00:51.970+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=capstone_andres_dag_, execution_date=2024-10-05 01:00:00+00:00, run_id=scheduled__2024-10-05T01:00:00+00:00, run_start_date=2024-11-27 14:56:32.559096+00:00, run_end_date=2024-11-27 15:00:51.970204+00:00, run_duration=259.411108, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-10-05 01:00:00+00:00, data_interval_end=2024-10-06 01:00:00+00:00, dag_hash=77fa8f64adac9391cccaaae88e2be5d8[0m
 [[34m2024-11-27T15:00:51.984+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-10-06 01:00:00+00:00, run_after=2024-10-07 01:00:00+00:00[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T15:00:52.283+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T15:00:52.370+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T15:00:52.371+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:00:52.444+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.weekly_metrics_run', run_id='scheduled__2024-10-05T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T15:00:52.451+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.weekly_metrics_run, run_id=scheduled__2024-10-05T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 15:00:39.280053+00:00, run_end_date=2024-11-27 15:00:51.773080+00:00, run_duration=12.493027, state=success, executor_state=success, try_number=1, max_tries=0, job_id=985, pool=default_pool, queue=default, priority_weight=1, operator=DbtRunLocalOperator, queued_dttm=2024-11-27 15:00:24.340806+00:00, queued_by_job_id=803, pid=12463[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T15:00:52.507+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 [[34m2024-11-27T15:00:53.070+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 [[34m2024-11-27T15:00:53.248+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 [[34m2024-11-27T15:00:53.404+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:00:53.481+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-10-07 01:00:00+00:00, run_after=2024-10-08 01:00:00+00:00[0m
 [[34m2024-11-27T15:00:53.542+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 /usr/local/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked semaphore objects to clean up at shutdown
   warnings.warn('resource_tracker: There appear to be %d '
 [[34m2024-11-27T15:00:54.554+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:00:54.797+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 2 tasks up for execution:
 	<TaskInstance: capstone_andres_dag_.load_polygon_data_task scheduled__2024-10-10T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.load_treasury_data_task scheduled__2024-10-09T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T15:00:54.797+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 6/16 running and queued tasks[0m
 [[34m2024-11-27T15:00:54.797+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 7/16 running and queued tasks[0m
 [[34m2024-11-27T15:00:54.797+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
 	<TaskInstance: capstone_andres_dag_.load_polygon_data_task scheduled__2024-10-10T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.load_treasury_data_task scheduled__2024-10-09T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T15:00:54.799+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='load_polygon_data_task', run_id='scheduled__2024-10-10T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
 [[34m2024-11-27T15:00:54.800+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'load_polygon_data_task', 'scheduled__2024-10-10T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T15:00:54.800+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='load_treasury_data_task', run_id='scheduled__2024-10-09T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 5 and queue default[0m
 [[34m2024-11-27T15:00:54.800+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'load_treasury_data_task', 'scheduled__2024-10-09T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T15:00:54.803+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'load_treasury_data_task', 'scheduled__2024-10-09T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T15:00:54.804+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='load_polygon_data_task', run_id='scheduled__2024-10-09T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T15:00:54.804+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run', run_id='scheduled__2024-10-07T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T15:00:54.810+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=load_polygon_data_task, run_id=scheduled__2024-10-09T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 15:00:44.773990+00:00, run_end_date=2024-11-27 15:00:53.881576+00:00, run_duration=9.107586, state=success, executor_state=success, try_number=1, max_tries=0, job_id=990, pool=default_pool, queue=default, priority_weight=6, operator=PythonOperator, queued_dttm=2024-11-27 15:00:31.434526+00:00, queued_by_job_id=803, pid=12484[0m
 [[34m2024-11-27T15:00:54.810+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run, run_id=scheduled__2024-10-07T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 15:00:43.142480+00:00, run_end_date=2024-11-27 15:00:54.145225+00:00, run_duration=11.002745, state=success, executor_state=success, try_number=1, max_tries=0, job_id=986, pool=default_pool, queue=default, priority_weight=2, operator=DbtRunLocalOperator, queued_dttm=2024-11-27 15:00:30.798022+00:00, queued_by_job_id=803, pid=12475[0m
 [[34m2024-11-27T15:00:54.807+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'load_polygon_data_task', 'scheduled__2024-10-10T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 /usr/local/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked semaphore objects to clean up at shutdown
   warnings.warn('resource_tracker: There appear to be %d '
 [[34m2024-11-27T15:00:54.834+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T15:00:54.845+0000[0m] {[34mscheduler_job_runner.py:[0m1328} INFO[0m - DAG capstone_andres_dag_ is at (or above) max_active_runs (16 of 16), not creating any more runs[0m
 [[34m2024-11-27T15:00:54.860+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T15:00:54.910+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T15:00:55.134+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.stg_daily_bars_run', run_id='scheduled__2024-10-07T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T15:00:55.138+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.stg_daily_bars_run, run_id=scheduled__2024-10-07T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 15:00:43.972264+00:00, run_end_date=2024-11-27 15:00:54.681437+00:00, run_duration=10.709173, state=success, executor_state=success, try_number=1, max_tries=0, job_id=988, pool=default_pool, queue=default, priority_weight=2, operator=DbtRunLocalOperator, queued_dttm=2024-11-27 15:00:30.798022+00:00, queued_by_job_id=803, pid=12478[0m
 [[34m2024-11-27T15:00:55.225+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:00:55.816+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T15:00:55.817+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T15:00:55.817+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T15:00:55.835+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T15:00:55.836+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T15:00:55.836+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T15:00:55.906+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 /usr/local/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked semaphore objects to clean up at shutdown
   warnings.warn('resource_tracker: There appear to be %d '
 [[34m2024-11-27T15:00:56.175+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:00:56.226+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T15:00:56.266+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:00:56.293+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:00:56.304+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412097318,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T15:00:56.346+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:00:56.363+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412097322,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T15:00:56.440+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.weekly_metrics_run scheduled__2024-10-07T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T15:00:56.441+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 6/16 running and queued tasks[0m
 [[34m2024-11-27T15:00:56.441+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.weekly_metrics_run scheduled__2024-10-07T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T15:00:56.443+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.weekly_metrics_run', run_id='scheduled__2024-10-07T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
 [[34m2024-11-27T15:00:56.443+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.weekly_metrics_run', 'scheduled__2024-10-07T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T15:00:56.445+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.dim_date_run', run_id='scheduled__2024-10-07T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T15:00:56.445+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 [[34m2024-11-27T15:00:56.450+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.dim_date_run, run_id=scheduled__2024-10-07T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 15:00:43.906203+00:00, run_end_date=2024-11-27 15:00:55.977610+00:00, run_duration=12.071407, state=success, executor_state=success, try_number=1, max_tries=0, job_id=987, pool=default_pool, queue=default, priority_weight=2, operator=DbtRunLocalOperator, queued_dttm=2024-11-27 15:00:30.798022+00:00, queued_by_job_id=803, pid=12477[0m
 [[34m2024-11-27T15:00:56.447+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.weekly_metrics_run', 'scheduled__2024-10-07T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T15:00:56.498+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T15:00:56.513+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:00:56.568+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T15:00:56.630+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T15:00:56.934+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:00:56.936+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T15:00:57.064+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 Data loaded successfully
 [[34m2024-11-27T15:00:57.133+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T15:00:57.145+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:00:57.148+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 [[34m2024-11-27T15:00:57.428+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T15:00:57.428+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T15:00:57.429+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T15:00:57.617+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 [[34m2024-11-27T15:00:57.791+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 [[34m2024-11-27T15:00:57.859+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:00:57.893+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 [[34m2024-11-27T15:00:57.915+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412096046,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T15:00:57.998+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:00:58.017+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:00:58.068+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.4053161218762398[0m
 [[34m2024-11-27T15:00:58.068+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|12504]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T15:00:58.068+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T15:00:58.069+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T15:00:58.069+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T15:00:58.069+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|12504]: It took 0.936s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T15:00:58.071+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|12504]: It took 0.00224s to build the Airflow DAG.[0m
 [[34m2024-11-27T15:00:58.099+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:00:58.107+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.weekly_metrics_run scheduled__2024-10-06T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [[34m2024-11-27T15:00:58.145+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:00:58.268+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 [[34m2024-11-27T15:00:58.309+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:00:58.418+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 Data loaded successfully
 [[34m2024-11-27T15:00:58.554+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T15:00:58.566+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 [2024-11-27T15:00:58.590+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.weekly_metrics_run, dagrun_id=scheduled__2024-10-06T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 15:00:58.186890+00:00, try_number=1, job_id=991, op_classpath=cosmos.operators.local.DbtRunLocalOperator, cosmos.operators.base.DbtRunMixin, cosmos.operators.local.DbtLocalBaseOperator, cosmos.operators.base.AbstractDbtBaseOperator
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T15:00:58.693+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 Data loaded successfully
 [[34m2024-11-27T15:00:58.840+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T15:00:58.850+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 Data loaded successfully
 [[34m2024-11-27T15:00:58.946+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T15:00:58.958+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 [[34m2024-11-27T15:00:59.458+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.36914113513194025[0m
 [[34m2024-11-27T15:00:59.458+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|12505]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T15:00:59.459+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T15:00:59.459+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T15:00:59.459+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T15:00:59.459+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|12505]: It took 0.906s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T15:00:59.462+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|12505]: It took 0.00242s to build the Airflow DAG.[0m
 [[34m2024-11-27T15:00:59.508+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_daily_bars_run scheduled__2024-10-08T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [[34m2024-11-27T15:00:59.641+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:00:59.970+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.6097857470158488[0m
 [[34m2024-11-27T15:00:59.970+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|12506]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T15:00:59.972+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T15:00:59.972+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T15:00:59.972+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T15:00:59.973+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|12506]: It took 1.13s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T15:00:59.982+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|12506]: It took 0.00928s to build the Airflow DAG.[0m
 [[34m2024-11-27T15:01:00.054+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run scheduled__2024-10-08T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [[34m2024-11-27T15:01:00.157+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.6512622039299458[0m
 [[34m2024-11-27T15:01:00.157+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|12503]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T15:01:00.158+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T15:01:00.158+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T15:01:00.159+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T15:01:00.159+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|12503]: It took 1.21s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T15:01:00.162+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|12503]: It took 0.00273s to build the Airflow DAG.[0m
 [2024-11-27T15:01:00.167+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.stg_daily_bars_run, dagrun_id=scheduled__2024-10-08T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 15:00:59.600588+00:00, try_number=1, job_id=992, op_classpath=cosmos.operators.local.DbtRunLocalOperator, cosmos.operators.base.DbtRunMixin, cosmos.operators.local.DbtLocalBaseOperator, cosmos.operators.base.AbstractDbtBaseOperator
 [[34m2024-11-27T15:01:00.214+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.dim_date_run scheduled__2024-10-08T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [[34m2024-11-27T15:01:00.478+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:01:00.534+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [2024-11-27T15:01:00.630+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run, dagrun_id=scheduled__2024-10-08T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 15:01:00.158660+00:00, try_number=1, job_id=993, op_classpath=cosmos.operators.local.DbtRunLocalOperator, cosmos.operators.base.DbtRunMixin, cosmos.operators.local.DbtLocalBaseOperator, cosmos.operators.base.AbstractDbtBaseOperator
 [2024-11-27T15:01:00.783+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.dim_date_run, dagrun_id=scheduled__2024-10-08T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 15:01:00.309969+00:00, try_number=1, job_id=994, op_classpath=cosmos.operators.local.DbtRunLocalOperator, cosmos.operators.base.DbtRunMixin, cosmos.operators.local.DbtLocalBaseOperator, cosmos.operators.base.AbstractDbtBaseOperator
 [[34m2024-11-27T15:01:00.839+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T15:01:01.004+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T15:01:01.115+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:01:01.243+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:01:02.387+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:01:02.446+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:01:02.580+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:01:02.586+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:01:02.678+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:01:02.696+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 [[34m2024-11-27T15:01:02.783+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 [[34m2024-11-27T15:01:02.908+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T15:01:03.231+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 Data loaded successfully
 [[34m2024-11-27T15:01:03.347+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T15:01:03.371+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 [[34m2024-11-27T15:01:03.451+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.weekly_metrics'[0m
 [[34m2024-11-27T15:01:03.451+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections'[0m
 [[34m2024-11-27T15:01:03.452+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.dim_date'[0m
 [[34m2024-11-27T15:01:03.452+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_payments'[0m
 [[34m2024-11-27T15:01:03.452+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_orders'[0m
 [[34m2024-11-27T15:01:03.453+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_daily_bars'[0m
 Data loaded successfully
 [[34m2024-11-27T15:01:03.466+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T15:01:03.482+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 [[34m2024-11-27T15:01:04.432+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.4239238740410656[0m
 [[34m2024-11-27T15:01:04.432+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|12583]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T15:01:04.433+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T15:01:04.433+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T15:01:04.433+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T15:01:04.434+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|12583]: It took 1.09s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T15:01:04.439+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|12583]: It took 0.00531s to build the Airflow DAG.[0m
 [[34m2024-11-27T15:01:04.488+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.load_treasury_data_task scheduled__2024-10-09T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [[34m2024-11-27T15:01:04.533+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.4407964260317385[0m
 [[34m2024-11-27T15:01:04.533+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|12584]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T15:01:04.534+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T15:01:04.534+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T15:01:04.535+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T15:01:04.535+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|12584]: It took 1.07s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T15:01:04.540+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|12584]: It took 0.00515s to build the Airflow DAG.[0m
 [[34m2024-11-27T15:01:04.574+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:01:04.588+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.load_polygon_data_task scheduled__2024-10-10T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [[34m2024-11-27T15:01:04.793+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:01:04.907+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 [2024-11-27T15:01:05.109+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=load_treasury_data_task, dagrun_id=scheduled__2024-10-09T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 15:01:04.596661+00:00, try_number=1, job_id=995, op_classpath=airflow.operators.python.PythonOperator
 [2024-11-27T15:01:05.198+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=load_polygon_data_task, dagrun_id=scheduled__2024-10-10T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 15:01:04.710531+00:00, try_number=1, job_id=996, op_classpath=airflow.operators.python.PythonOperator
 /usr/local/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked semaphore objects to clean up at shutdown
   warnings.warn('resource_tracker: There appear to be %d '
 Data loaded successfully
 [[34m2024-11-27T15:01:05.595+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T15:01:05.610+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 [[34m2024-11-27T15:01:05.891+0000[0m] {[34mdagrun.py:[0m851} INFO[0m - Marking run <DagRun capstone_andres_dag_ @ 2024-10-06 01:00:00+00:00: scheduled__2024-10-06T01:00:00+00:00, state:running, queued_at: 2024-11-27 14:56:45.738460+00:00. externally triggered: False> successful[0m
 [[34m2024-11-27T15:01:05.892+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=capstone_andres_dag_, execution_date=2024-10-06 01:00:00+00:00, run_id=scheduled__2024-10-06T01:00:00+00:00, run_start_date=2024-11-27 14:56:45.748192+00:00, run_end_date=2024-11-27 15:01:05.892003+00:00, run_duration=260.143811, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-10-06 01:00:00+00:00, data_interval_end=2024-10-07 01:00:00+00:00, dag_hash=77fa8f64adac9391cccaaae88e2be5d8[0m
 [[34m2024-11-27T15:01:05.897+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-10-07 01:00:00+00:00, run_after=2024-10-08 01:00:00+00:00[0m
 [[34m2024-11-27T15:01:06.069+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.weekly_metrics_run', run_id='scheduled__2024-10-06T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T15:01:06.072+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.weekly_metrics_run, run_id=scheduled__2024-10-06T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 15:00:58.186890+00:00, run_end_date=2024-11-27 15:01:05.102715+00:00, run_duration=6.915825, state=success, executor_state=success, try_number=1, max_tries=0, job_id=991, pool=default_pool, queue=default, priority_weight=1, operator=DbtRunLocalOperator, queued_dttm=2024-11-27 15:00:49.044106+00:00, queued_by_job_id=803, pid=12622[0m
 /usr/local/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked semaphore objects to clean up at shutdown
   warnings.warn('resource_tracker: There appear to be %d '
 [[34m2024-11-27T15:01:06.623+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.49933943897485733[0m
 [[34m2024-11-27T15:01:06.623+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|12590]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T15:01:06.624+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T15:01:06.624+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T15:01:06.624+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T15:01:06.624+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|12590]: It took 1.03s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T15:01:06.627+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|12590]: It took 0.00264s to build the Airflow DAG.[0m
 [[34m2024-11-27T15:01:06.665+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.weekly_metrics_run scheduled__2024-10-07T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 /usr/local/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked semaphore objects to clean up at shutdown
   warnings.warn('resource_tracker: There appear to be %d '
 [[34m2024-11-27T15:01:07.105+0000[0m] {[34mscheduler_job_runner.py:[0m1328} INFO[0m - DAG capstone_andres_dag_ is at (or above) max_active_runs (16 of 16), not creating any more runs[0m
 [2024-11-27T15:01:07.185+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.weekly_metrics_run, dagrun_id=scheduled__2024-10-07T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 15:01:06.745919+00:00, try_number=1, job_id=997, op_classpath=cosmos.operators.local.DbtRunLocalOperator, cosmos.operators.base.DbtRunMixin, cosmos.operators.local.DbtLocalBaseOperator, cosmos.operators.base.AbstractDbtBaseOperator
 [[34m2024-11-27T15:01:07.309+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.stg_daily_bars_run', run_id='scheduled__2024-10-08T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T15:01:07.309+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run', run_id='scheduled__2024-10-08T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T15:01:07.312+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.stg_daily_bars_run, run_id=scheduled__2024-10-08T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 15:00:59.600588+00:00, run_end_date=2024-11-27 15:01:06.467160+00:00, run_duration=6.866572, state=success, executor_state=success, try_number=1, max_tries=0, job_id=992, pool=default_pool, queue=default, priority_weight=2, operator=DbtRunLocalOperator, queued_dttm=2024-11-27 15:00:49.044106+00:00, queued_by_job_id=803, pid=12633[0m
 [[34m2024-11-27T15:01:07.313+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run, run_id=scheduled__2024-10-08T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 15:01:00.158660+00:00, run_end_date=2024-11-27 15:01:06.636794+00:00, run_duration=6.478134, state=success, executor_state=success, try_number=1, max_tries=0, job_id=993, pool=default_pool, queue=default, priority_weight=2, operator=DbtRunLocalOperator, queued_dttm=2024-11-27 15:00:49.044106+00:00, queued_by_job_id=803, pid=12642[0m
 /usr/local/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked semaphore objects to clean up at shutdown
   warnings.warn('resource_tracker: There appear to be %d '
 [[34m2024-11-27T15:01:08.564+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 4 tasks up for execution:
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_daily_bars_run scheduled__2024-10-09T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run scheduled__2024-10-09T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.dim_date_run scheduled__2024-10-09T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.weekly_metrics_run scheduled__2024-10-08T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T15:01:08.564+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 2/16 running and queued tasks[0m
 [[34m2024-11-27T15:01:08.565+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 3/16 running and queued tasks[0m
 [[34m2024-11-27T15:01:08.565+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 4/16 running and queued tasks[0m
 [[34m2024-11-27T15:01:08.565+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 5/16 running and queued tasks[0m
 [[34m2024-11-27T15:01:08.565+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_daily_bars_run scheduled__2024-10-09T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run scheduled__2024-10-09T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.dim_date_run scheduled__2024-10-09T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.weekly_metrics_run scheduled__2024-10-08T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T15:01:08.568+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.stg_daily_bars_run', run_id='scheduled__2024-10-09T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
 [[34m2024-11-27T15:01:08.569+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.stg_daily_bars_run', 'scheduled__2024-10-09T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T15:01:08.569+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run', run_id='scheduled__2024-10-09T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
 [[34m2024-11-27T15:01:08.569+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run', 'scheduled__2024-10-09T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T15:01:08.570+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.dim_date_run', run_id='scheduled__2024-10-09T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
 [[34m2024-11-27T15:01:08.570+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.dim_date_run', 'scheduled__2024-10-09T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T15:01:08.570+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.weekly_metrics_run', run_id='scheduled__2024-10-08T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
 [[34m2024-11-27T15:01:08.570+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.weekly_metrics_run', 'scheduled__2024-10-08T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T15:01:08.576+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='load_treasury_data_task', run_id='scheduled__2024-10-09T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T15:01:08.573+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.stg_daily_bars_run', 'scheduled__2024-10-09T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T15:01:08.576+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.dim_date_run', run_id='scheduled__2024-10-08T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T15:01:08.574+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.dim_date_run', 'scheduled__2024-10-09T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T15:01:08.575+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run', 'scheduled__2024-10-09T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T15:01:08.584+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=load_treasury_data_task, run_id=scheduled__2024-10-09T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 15:01:04.596661+00:00, run_end_date=2024-11-27 15:01:07.414705+00:00, run_duration=2.818044, state=success, executor_state=success, try_number=1, max_tries=0, job_id=995, pool=default_pool, queue=default, priority_weight=5, operator=PythonOperator, queued_dttm=2024-11-27 15:00:54.798375+00:00, queued_by_job_id=803, pid=12680[0m
 [[34m2024-11-27T15:01:08.585+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.dim_date_run, run_id=scheduled__2024-10-08T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 15:01:00.309969+00:00, run_end_date=2024-11-27 15:01:08.267090+00:00, run_duration=7.957121, state=success, executor_state=success, try_number=1, max_tries=0, job_id=994, pool=default_pool, queue=default, priority_weight=2, operator=DbtRunLocalOperator, queued_dttm=2024-11-27 15:00:49.044106+00:00, queued_by_job_id=803, pid=12644[0m
 [[34m2024-11-27T15:01:08.580+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.weekly_metrics_run', 'scheduled__2024-10-08T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T15:01:08.620+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T15:01:08.623+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T15:01:08.632+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T15:01:08.640+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T15:01:09.735+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T15:01:09.736+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T15:01:09.736+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T15:01:09.742+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T15:01:09.743+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T15:01:09.743+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T15:01:09.757+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T15:01:09.758+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T15:01:09.758+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T15:01:09.777+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T15:01:09.777+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T15:01:09.778+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T15:01:10.237+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412096094,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T15:01:10.237+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412096086,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T15:01:10.243+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412097358,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T15:01:10.243+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412096090,
 "os.name" : Linux
 [0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T15:01:11.079+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T15:01:11.079+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T15:01:11.151+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T15:01:11.177+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 [[34m2024-11-27T15:01:11.543+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:01:11.761+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 [[34m2024-11-27T15:01:12.002+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:01:12.094+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 /usr/local/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked semaphore objects to clean up at shutdown
   warnings.warn('resource_tracker: There appear to be %d '
 [[34m2024-11-27T15:01:14.211+0000[0m] {[34mdagrun.py:[0m851} INFO[0m - Marking run <DagRun capstone_andres_dag_ @ 2024-10-07 01:00:00+00:00: scheduled__2024-10-07T01:00:00+00:00, state:running, queued_at: 2024-11-27 14:56:54.240174+00:00. externally triggered: False> successful[0m
 [[34m2024-11-27T15:01:14.211+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=capstone_andres_dag_, execution_date=2024-10-07 01:00:00+00:00, run_id=scheduled__2024-10-07T01:00:00+00:00, run_start_date=2024-11-27 14:56:54.251934+00:00, run_end_date=2024-11-27 15:01:14.211581+00:00, run_duration=259.959647, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-10-07 01:00:00+00:00, data_interval_end=2024-10-08 01:00:00+00:00, dag_hash=77fa8f64adac9391cccaaae88e2be5d8[0m
 [[34m2024-11-27T15:01:14.215+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-10-08 01:00:00+00:00, run_after=2024-10-09 01:00:00+00:00[0m
 [[34m2024-11-27T15:01:14.291+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:01:14.567+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:01:14.622+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.weekly_metrics_run', run_id='scheduled__2024-10-07T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T15:01:14.627+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.weekly_metrics_run, run_id=scheduled__2024-10-07T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 15:01:06.745919+00:00, run_end_date=2024-11-27 15:01:13.711812+00:00, run_duration=6.965893, state=success, executor_state=success, try_number=1, max_tries=0, job_id=997, pool=default_pool, queue=default, priority_weight=1, operator=DbtRunLocalOperator, queued_dttm=2024-11-27 15:00:56.441901+00:00, queued_by_job_id=803, pid=12704[0m
 [[34m2024-11-27T15:01:14.631+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T15:01:14.720+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:01:14.889+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T15:01:14.932+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:01:14.972+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:01:15.025+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T15:01:15.229+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:01:15.259+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T15:01:15.381+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:01:15.555+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:01:15.618+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-10-09 01:00:00+00:00, run_after=2024-10-10 01:00:00+00:00[0m
 [[34m2024-11-27T15:01:16.310+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:01:16.311+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:01:16.466+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:01:16.468+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:01:16.487+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:01:16.570+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 [[34m2024-11-27T15:01:16.573+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 [[34m2024-11-27T15:01:16.670+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:01:16.788+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 [[34m2024-11-27T15:01:16.813+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:01:16.943+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-10-10 01:00:00+00:00, run_after=2024-10-11 01:00:00+00:00[0m
 [[34m2024-11-27T15:01:16.995+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:01:17.112+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 Data loaded successfully
 [[34m2024-11-27T15:01:17.139+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T15:01:17.159+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 Data loaded successfully
 [[34m2024-11-27T15:01:17.379+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T15:01:17.421+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 Data loaded successfully
 [[34m2024-11-27T15:01:17.520+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T15:01:17.552+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 Data loaded successfully
 [[34m2024-11-27T15:01:17.701+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T15:01:17.722+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 [[34m2024-11-27T15:01:18.289+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-10-11 01:00:00+00:00, run_after=2024-10-12 01:00:00+00:00[0m
 [[34m2024-11-27T15:01:18.408+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.5107793491333723[0m
 [[34m2024-11-27T15:01:18.409+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|12718]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T15:01:18.409+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T15:01:18.410+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T15:01:18.410+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T15:01:18.410+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|12718]: It took 1.27s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T15:01:18.416+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|12718]: It took 0.00592s to build the Airflow DAG.[0m
 [[34m2024-11-27T15:01:18.485+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.weekly_metrics_run scheduled__2024-10-08T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [[34m2024-11-27T15:01:18.668+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.5373854250647128[0m
 [[34m2024-11-27T15:01:18.669+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|12716]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T15:01:18.670+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T15:01:18.670+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T15:01:18.670+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T15:01:18.671+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|12716]: It took 1.29s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T15:01:18.677+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|12716]: It took 0.0061s to build the Airflow DAG.[0m
 [[34m2024-11-27T15:01:18.739+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.dim_date_run scheduled__2024-10-09T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [[34m2024-11-27T15:01:18.761+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.5628868751227856[0m
 [[34m2024-11-27T15:01:18.761+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|12717]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T15:01:18.762+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T15:01:18.762+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T15:01:18.763+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T15:01:18.763+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|12717]: It took 1.24s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T15:01:18.770+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|12717]: It took 0.00742s to build the Airflow DAG.[0m
 [[34m2024-11-27T15:01:18.837+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run scheduled__2024-10-09T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [[34m2024-11-27T15:01:18.984+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.6943495019804686[0m
 [[34m2024-11-27T15:01:18.984+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|12715]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T15:01:18.985+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T15:01:18.985+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T15:01:18.986+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T15:01:18.986+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|12715]: It took 1.29s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T15:01:18.991+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|12715]: It took 0.00454s to build the Airflow DAG.[0m
 [[34m2024-11-27T15:01:19.076+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_daily_bars_run scheduled__2024-10-09T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [2024-11-27T15:01:19.268+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.weekly_metrics_run, dagrun_id=scheduled__2024-10-08T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 15:01:18.615517+00:00, try_number=1, job_id=998, op_classpath=cosmos.operators.local.DbtRunLocalOperator, cosmos.operators.base.DbtRunMixin, cosmos.operators.local.DbtLocalBaseOperator, cosmos.operators.base.AbstractDbtBaseOperator
 [2024-11-27T15:01:19.569+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.dim_date_run, dagrun_id=scheduled__2024-10-09T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 15:01:18.866052+00:00, try_number=1, job_id=999, op_classpath=cosmos.operators.local.DbtRunLocalOperator, cosmos.operators.base.DbtRunMixin, cosmos.operators.local.DbtLocalBaseOperator, cosmos.operators.base.AbstractDbtBaseOperator
 [[34m2024-11-27T15:01:19.644+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-10-12 01:00:00+00:00, run_after=2024-10-13 01:00:00+00:00[0m
 [2024-11-27T15:01:19.645+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run, dagrun_id=scheduled__2024-10-09T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 15:01:18.999760+00:00, try_number=1, job_id=1000, op_classpath=cosmos.operators.local.DbtRunLocalOperator, cosmos.operators.base.DbtRunMixin, cosmos.operators.local.DbtLocalBaseOperator, cosmos.operators.base.AbstractDbtBaseOperator
 [2024-11-27T15:01:19.804+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.stg_daily_bars_run, dagrun_id=scheduled__2024-10-09T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 15:01:19.215007+00:00, try_number=1, job_id=1001, op_classpath=cosmos.operators.local.DbtRunLocalOperator, cosmos.operators.base.DbtRunMixin, cosmos.operators.local.DbtLocalBaseOperator, cosmos.operators.base.AbstractDbtBaseOperator
 [[34m2024-11-27T15:01:20.277+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-10-13 01:00:00+00:00, run_after=2024-10-14 01:00:00+00:00[0m
 [[34m2024-11-27T15:01:20.943+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-10-14 01:00:00+00:00, run_after=2024-10-15 01:00:00+00:00[0m
 [[34m2024-11-27T15:01:22.234+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-10-15 01:00:00+00:00, run_after=2024-10-16 01:00:00+00:00[0m
 [[34m2024-11-27T15:01:23.699+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-10-16 01:00:00+00:00, run_after=2024-10-17 01:00:00+00:00[0m
 [[34m2024-11-27T15:01:25.413+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-10-17 01:00:00+00:00, run_after=2024-10-18 01:00:00+00:00[0m
 /usr/local/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked semaphore objects to clean up at shutdown
   warnings.warn('resource_tracker: There appear to be %d '
 [[34m2024-11-27T15:01:26.227+0000[0m] {[34mscheduler_job_runner.py:[0m1328} INFO[0m - DAG capstone_andres_dag_ is at (or above) max_active_runs (16 of 16), not creating any more runs[0m
 /usr/local/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked semaphore objects to clean up at shutdown
   warnings.warn('resource_tracker: There appear to be %d '
 [[34m2024-11-27T15:01:26.258+0000[0m] {[34mdagrun.py:[0m851} INFO[0m - Marking run <DagRun capstone_andres_dag_ @ 2024-10-08 01:00:00+00:00: scheduled__2024-10-08T01:00:00+00:00, state:running, queued_at: 2024-11-27 14:57:12.752362+00:00. externally triggered: False> successful[0m
 [[34m2024-11-27T15:01:26.259+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=capstone_andres_dag_, execution_date=2024-10-08 01:00:00+00:00, run_id=scheduled__2024-10-08T01:00:00+00:00, run_start_date=2024-11-27 14:57:12.774284+00:00, run_end_date=2024-11-27 15:01:26.259218+00:00, run_duration=253.484934, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-10-08 01:00:00+00:00, data_interval_end=2024-10-09 01:00:00+00:00, dag_hash=77fa8f64adac9391cccaaae88e2be5d8[0m
 [[34m2024-11-27T15:01:26.263+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-10-09 01:00:00+00:00, run_after=2024-10-10 01:00:00+00:00[0m
 [[34m2024-11-27T15:01:26.444+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.weekly_metrics_run', run_id='scheduled__2024-10-08T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T15:01:26.445+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run', run_id='scheduled__2024-10-09T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T15:01:26.450+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run, run_id=scheduled__2024-10-09T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 15:01:18.999760+00:00, run_end_date=2024-11-27 15:01:26.200726+00:00, run_duration=7.200966, state=success, executor_state=success, try_number=1, max_tries=0, job_id=1000, pool=default_pool, queue=default, priority_weight=2, operator=DbtRunLocalOperator, queued_dttm=2024-11-27 15:01:08.566743+00:00, queued_by_job_id=803, pid=12799[0m
 [[34m2024-11-27T15:01:26.450+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.weekly_metrics_run, run_id=scheduled__2024-10-08T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 15:01:18.615517+00:00, run_end_date=2024-11-27 15:01:25.979921+00:00, run_duration=7.364404, state=success, executor_state=success, try_number=1, max_tries=0, job_id=998, pool=default_pool, queue=default, priority_weight=1, operator=DbtRunLocalOperator, queued_dttm=2024-11-27 15:01:08.566743+00:00, queued_by_job_id=803, pid=12797[0m
 /usr/local/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked semaphore objects to clean up at shutdown
   warnings.warn('resource_tracker: There appear to be %d '
 [[34m2024-11-27T15:01:27.480+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-10-10 01:00:00+00:00, run_after=2024-10-11 01:00:00+00:00[0m
 /usr/local/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked semaphore objects to clean up at shutdown
   warnings.warn('resource_tracker: There appear to be %d '
 [[34m2024-11-27T15:01:27.723+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.stg_daily_bars_run', run_id='scheduled__2024-10-09T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T15:01:27.723+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.dim_date_run', run_id='scheduled__2024-10-09T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T15:01:27.726+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.stg_daily_bars_run, run_id=scheduled__2024-10-09T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 15:01:19.215007+00:00, run_end_date=2024-11-27 15:01:26.452206+00:00, run_duration=7.237199, state=success, executor_state=success, try_number=1, max_tries=0, job_id=1001, pool=default_pool, queue=default, priority_weight=2, operator=DbtRunLocalOperator, queued_dttm=2024-11-27 15:01:08.566743+00:00, queued_by_job_id=803, pid=12800[0m
 [[34m2024-11-27T15:01:27.727+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.dim_date_run, run_id=scheduled__2024-10-09T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 15:01:18.866052+00:00, run_end_date=2024-11-27 15:01:27.558850+00:00, run_duration=8.692798, state=success, executor_state=success, try_number=1, max_tries=0, job_id=999, pool=default_pool, queue=default, priority_weight=2, operator=DbtRunLocalOperator, queued_dttm=2024-11-27 15:01:08.566743+00:00, queued_by_job_id=803, pid=12798[0m
 [[34m2024-11-27T15:01:28.751+0000[0m] {[34mscheduler_job_runner.py:[0m1328} INFO[0m - DAG capstone_andres_dag_ is at (or above) max_active_runs (16 of 16), not creating any more runs[0m
 [[34m2024-11-27T15:01:29.015+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.weekly_metrics_run scheduled__2024-10-09T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T15:01:29.015+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 1/16 running and queued tasks[0m
 [[34m2024-11-27T15:01:29.015+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.weekly_metrics_run scheduled__2024-10-09T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T15:01:29.017+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.weekly_metrics_run', run_id='scheduled__2024-10-09T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
 [[34m2024-11-27T15:01:29.017+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.weekly_metrics_run', 'scheduled__2024-10-09T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T15:01:29.022+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.weekly_metrics_run', 'scheduled__2024-10-09T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T15:01:29.059+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T15:01:30.027+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T15:01:30.027+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T15:01:30.027+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T15:01:30.518+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412097390,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T15:01:31.713+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 2 tasks up for execution:
 	<TaskInstance: capstone_andres_dag_.load_polygon_data_task scheduled__2024-10-11T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.load_treasury_data_task scheduled__2024-10-10T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T15:01:31.713+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 1/16 running and queued tasks[0m
 [[34m2024-11-27T15:01:31.713+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 2/16 running and queued tasks[0m
 [[34m2024-11-27T15:01:31.714+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
 	<TaskInstance: capstone_andres_dag_.load_polygon_data_task scheduled__2024-10-11T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.load_treasury_data_task scheduled__2024-10-10T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T15:01:31.715+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='load_polygon_data_task', run_id='scheduled__2024-10-11T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
 [[34m2024-11-27T15:01:31.716+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'load_polygon_data_task', 'scheduled__2024-10-11T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T15:01:31.716+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='load_treasury_data_task', run_id='scheduled__2024-10-10T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 5 and queue default[0m
 [[34m2024-11-27T15:01:31.716+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'load_treasury_data_task', 'scheduled__2024-10-10T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T15:01:31.719+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='load_polygon_data_task', run_id='scheduled__2024-10-10T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T15:01:31.719+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'load_treasury_data_task', 'scheduled__2024-10-10T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T15:01:31.723+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=load_polygon_data_task, run_id=scheduled__2024-10-10T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 15:01:04.710531+00:00, run_end_date=2024-11-27 15:01:31.065856+00:00, run_duration=26.355325, state=success, executor_state=success, try_number=1, max_tries=0, job_id=996, pool=default_pool, queue=default, priority_weight=6, operator=PythonOperator, queued_dttm=2024-11-27 15:00:54.798375+00:00, queued_by_job_id=803, pid=12681[0m
 [[34m2024-11-27T15:01:31.723+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'load_polygon_data_task', 'scheduled__2024-10-11T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T15:01:31.773+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T15:01:31.783+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T15:01:33.070+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T15:01:33.070+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T15:01:33.071+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T15:01:33.086+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T15:01:33.086+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T15:01:33.087+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T15:01:33.641+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412097394,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T15:01:33.667+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412096134,
 "os.name" : Linux
 [0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T15:01:34.434+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T15:01:34.442+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 [[34m2024-11-27T15:01:35.270+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:01:35.439+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:01:37.968+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:01:38.215+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:01:38.308+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T15:01:38.538+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:01:38.538+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T15:01:38.850+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:01:39.722+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:01:39.919+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:01:39.935+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:01:40.051+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 [[34m2024-11-27T15:01:40.101+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:01:40.208+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 Data loaded successfully
 [[34m2024-11-27T15:01:40.692+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T15:01:40.702+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 Data loaded successfully
 [[34m2024-11-27T15:01:40.975+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T15:01:40.986+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 [[34m2024-11-27T15:01:41.537+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.3165982239879668[0m
 [[34m2024-11-27T15:01:41.537+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|12862]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T15:01:41.538+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T15:01:41.538+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T15:01:41.538+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T15:01:41.539+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|12862]: It took 0.847s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T15:01:41.541+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|12862]: It took 0.00232s to build the Airflow DAG.[0m
 [[34m2024-11-27T15:01:41.578+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.load_polygon_data_task scheduled__2024-10-11T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [[34m2024-11-27T15:01:42.018+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.5012560919858515[0m
 [[34m2024-11-27T15:01:42.019+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|12861]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T15:01:42.020+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T15:01:42.020+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T15:01:42.020+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T15:01:42.020+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|12861]: It took 1.05s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T15:01:42.023+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|12861]: It took 0.00267s to build the Airflow DAG.[0m
 [2024-11-27T15:01:42.046+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=load_polygon_data_task, dagrun_id=scheduled__2024-10-11T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 15:01:41.667916+00:00, try_number=1, job_id=1002, op_classpath=airflow.operators.python.PythonOperator
 [[34m2024-11-27T15:01:42.061+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.load_treasury_data_task scheduled__2024-10-10T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [2024-11-27T15:01:42.506+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=load_treasury_data_task, dagrun_id=scheduled__2024-10-10T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 15:01:42.139252+00:00, try_number=1, job_id=1003, op_classpath=airflow.operators.python.PythonOperator
 [[34m2024-11-27T15:01:45.873+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.dim_date_run scheduled__2024-10-10T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_daily_bars_run scheduled__2024-10-10T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run scheduled__2024-10-10T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T15:01:45.874+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 2/16 running and queued tasks[0m
 [[34m2024-11-27T15:01:45.874+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 3/16 running and queued tasks[0m
 [[34m2024-11-27T15:01:45.874+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 4/16 running and queued tasks[0m
 [[34m2024-11-27T15:01:45.874+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.dim_date_run scheduled__2024-10-10T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_daily_bars_run scheduled__2024-10-10T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run scheduled__2024-10-10T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T15:01:45.876+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.dim_date_run', run_id='scheduled__2024-10-10T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
 [[34m2024-11-27T15:01:45.876+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.dim_date_run', 'scheduled__2024-10-10T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T15:01:45.876+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.stg_daily_bars_run', run_id='scheduled__2024-10-10T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
 [[34m2024-11-27T15:01:45.877+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.stg_daily_bars_run', 'scheduled__2024-10-10T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T15:01:45.877+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run', run_id='scheduled__2024-10-10T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
 [[34m2024-11-27T15:01:45.877+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run', 'scheduled__2024-10-10T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T15:01:45.881+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='load_treasury_data_task', run_id='scheduled__2024-10-10T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T15:01:45.880+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run', 'scheduled__2024-10-10T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T15:01:45.885+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=load_treasury_data_task, run_id=scheduled__2024-10-10T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 15:01:42.139252+00:00, run_end_date=2024-11-27 15:01:44.755981+00:00, run_duration=2.616729, state=success, executor_state=success, try_number=1, max_tries=0, job_id=1003, pool=default_pool, queue=default, priority_weight=5, operator=PythonOperator, queued_dttm=2024-11-27 15:01:31.714640+00:00, queued_by_job_id=803, pid=12916[0m
 [[34m2024-11-27T15:01:45.883+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.stg_daily_bars_run', 'scheduled__2024-10-10T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T15:01:45.886+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.dim_date_run', 'scheduled__2024-10-10T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T15:01:45.922+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T15:01:45.923+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T15:01:45.937+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T15:01:49.923+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T15:01:49.925+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T15:01:49.927+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T15:01:49.928+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T15:01:49.929+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T15:01:49.931+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T15:01:49.931+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T15:01:49.934+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T15:01:49.936+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T15:01:50.502+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412096166,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T15:01:50.564+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412097406,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T15:01:50.597+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412096170,
 "os.name" : Linux
 [0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T15:01:51.289+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T15:01:51.325+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T15:01:51.475+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 [[34m2024-11-27T15:01:52.153+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 [[34m2024-11-27T15:01:52.200+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T15:01:52.296+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:01:52.390+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 [[34m2024-11-27T15:01:52.801+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:01:54.251+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:01:54.574+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T15:01:54.834+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:01:54.864+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:01:54.921+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:01:55.088+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:01:55.137+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T15:01:55.248+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T15:01:55.394+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:01:55.400+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T15:01:55.602+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:01:55.708+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:01:56.095+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:01:56.277+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:01:56.382+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 [[34m2024-11-27T15:01:56.435+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:01:56.610+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:01:56.717+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 [[34m2024-11-27T15:01:56.915+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:01:56.954+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 Data loaded successfully
 [[34m2024-11-27T15:01:56.965+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T15:01:56.978+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 [[34m2024-11-27T15:01:57.069+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:01:57.106+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:01:57.196+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 [[34m2024-11-27T15:01:57.212+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 Data loaded successfully
 [[34m2024-11-27T15:01:57.273+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T15:01:57.293+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 [[34m2024-11-27T15:01:57.769+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 2 tasks up for execution:
 	<TaskInstance: capstone_andres_dag_.load_polygon_data_task scheduled__2024-10-12T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.load_treasury_data_task scheduled__2024-10-11T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T15:01:57.770+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 4/16 running and queued tasks[0m
 [[34m2024-11-27T15:01:57.770+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 5/16 running and queued tasks[0m
 [[34m2024-11-27T15:01:57.770+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
 	<TaskInstance: capstone_andres_dag_.load_polygon_data_task scheduled__2024-10-12T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.load_treasury_data_task scheduled__2024-10-11T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T15:01:57.772+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='load_polygon_data_task', run_id='scheduled__2024-10-12T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
 [[34m2024-11-27T15:01:57.772+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'load_polygon_data_task', 'scheduled__2024-10-12T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T15:01:57.772+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='load_treasury_data_task', run_id='scheduled__2024-10-11T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 5 and queue default[0m
 [[34m2024-11-27T15:01:57.772+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'load_treasury_data_task', 'scheduled__2024-10-11T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T15:01:57.778+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='load_polygon_data_task', run_id='scheduled__2024-10-11T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T15:01:57.781+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=load_polygon_data_task, run_id=scheduled__2024-10-11T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 15:01:41.667916+00:00, run_end_date=2024-11-27 15:01:57.532431+00:00, run_duration=15.864515, state=success, executor_state=success, try_number=1, max_tries=0, job_id=1002, pool=default_pool, queue=default, priority_weight=6, operator=PythonOperator, queued_dttm=2024-11-27 15:01:31.714640+00:00, queued_by_job_id=803, pid=12915[0m
 [[34m2024-11-27T15:01:57.781+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'load_polygon_data_task', 'scheduled__2024-10-12T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T15:01:57.785+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'load_treasury_data_task', 'scheduled__2024-10-11T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T15:01:57.825+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T15:01:57.832+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 Data loaded successfully
 [[34m2024-11-27T15:01:57.839+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T15:01:57.852+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 Data loaded successfully
 [[34m2024-11-27T15:01:57.860+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T15:01:57.884+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 [[34m2024-11-27T15:01:58.189+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.5912339701317251[0m
 [[34m2024-11-27T15:01:58.189+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|12854]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T15:01:58.190+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T15:01:58.190+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T15:01:58.191+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T15:01:58.191+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|12854]: It took 1.23s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T15:01:58.197+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|12854]: It took 0.00586s to build the Airflow DAG.[0m
 [[34m2024-11-27T15:01:58.253+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.weekly_metrics_run scheduled__2024-10-09T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [[34m2024-11-27T15:01:58.511+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.6396735399030149[0m
 [[34m2024-11-27T15:01:58.511+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|12925]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T15:01:58.513+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T15:01:58.513+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T15:01:58.514+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T15:01:58.514+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|12925]: It took 1.24s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T15:01:58.523+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|12925]: It took 0.0085s to build the Airflow DAG.[0m
 [[34m2024-11-27T15:01:58.600+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_daily_bars_run scheduled__2024-10-10T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [2024-11-27T15:01:59.129+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.weekly_metrics_run, dagrun_id=scheduled__2024-10-09T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 15:01:58.383526+00:00, try_number=1, job_id=1004, op_classpath=cosmos.operators.local.DbtRunLocalOperator, cosmos.operators.base.DbtRunMixin, cosmos.operators.local.DbtLocalBaseOperator, cosmos.operators.base.AbstractDbtBaseOperator
 [[34m2024-11-27T15:01:59.159+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.742120087845251[0m
 [[34m2024-11-27T15:01:59.160+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|12926]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T15:01:59.161+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T15:01:59.161+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T15:01:59.162+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T15:01:59.162+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|12926]: It took 1.32s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T15:01:59.171+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|12926]: It took 0.0087s to build the Airflow DAG.[0m
 [[34m2024-11-27T15:01:59.178+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T15:01:59.179+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T15:01:59.180+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T15:01:59.204+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T15:01:59.206+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T15:01:59.206+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T15:01:59.207+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.7195072420872748[0m
 [[34m2024-11-27T15:01:59.208+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|12924]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T15:01:59.209+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T15:01:59.210+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T15:01:59.210+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T15:01:59.211+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|12924]: It took 1.35s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T15:01:59.215+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|12924]: It took 0.00461s to build the Airflow DAG.[0m
 [[34m2024-11-27T15:01:59.254+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.dim_date_run scheduled__2024-10-10T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [[34m2024-11-27T15:01:59.308+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run scheduled__2024-10-10T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [2024-11-27T15:01:59.568+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.stg_daily_bars_run, dagrun_id=scheduled__2024-10-10T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 15:01:58.721188+00:00, try_number=1, job_id=1005, op_classpath=cosmos.operators.local.DbtRunLocalOperator, cosmos.operators.base.DbtRunMixin, cosmos.operators.local.DbtLocalBaseOperator, cosmos.operators.base.AbstractDbtBaseOperator
 [[34m2024-11-27T15:01:59.906+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412096178,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T15:01:59.910+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412096174,
 "os.name" : Linux
 [0m
 [2024-11-27T15:02:00.324+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.dim_date_run, dagrun_id=scheduled__2024-10-10T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 15:01:59.484533+00:00, try_number=1, job_id=1006, op_classpath=cosmos.operators.local.DbtRunLocalOperator, cosmos.operators.base.DbtRunMixin, cosmos.operators.local.DbtLocalBaseOperator, cosmos.operators.base.AbstractDbtBaseOperator
 [2024-11-27T15:02:00.372+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run, dagrun_id=scheduled__2024-10-10T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 15:01:59.558302+00:00, try_number=1, job_id=1007, op_classpath=cosmos.operators.local.DbtRunLocalOperator, cosmos.operators.base.DbtRunMixin, cosmos.operators.local.DbtLocalBaseOperator, cosmos.operators.base.AbstractDbtBaseOperator
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T15:02:00.841+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T15:02:00.849+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 [[34m2024-11-27T15:02:01.746+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:02:02.048+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:02:03.901+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:02:03.931+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.weekly_metrics'[0m
 [[34m2024-11-27T15:02:03.932+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections'[0m
 [[34m2024-11-27T15:02:03.932+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_payments'[0m
 [[34m2024-11-27T15:02:03.933+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_orders'[0m
 [[34m2024-11-27T15:02:03.933+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_daily_bars'[0m
 [[34m2024-11-27T15:02:04.217+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 Process DagFileProcessor1535-Process:
 Traceback (most recent call last):
   File "/usr/local/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
     self.run()
   File "/usr/local/lib/python3.11/multiprocessing/process.py", line 108, in run
     self._target(*self._args, **self._kwargs)
   File "/usr/local/lib/python3.11/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
     _handle_dag_file_processing()
   File "/usr/local/lib/python3.11/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
     result: tuple[int, int] = dag_file_processor.process_file(
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "/usr/local/lib/python3.11/site-packages/airflow/utils/session.py", line 79, in wrapper
     return func(*args, session=session, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "/usr/local/lib/python3.11/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
     dagbag = DagFileProcessor._get_dagbag(file_path)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "/usr/local/lib/python3.11/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
     return DagBag(file_path, include_examples=False)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "/usr/local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 152, in __init__
     self.collect_dags(
   File "/usr/local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
     found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "/usr/local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 318, in process_file
     mods = self._load_modules_from_file(filepath, safe_mode)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "/usr/local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
     return parse(mod_name, filepath)
            ^^^^^^^^^^^^^^^^^^^^^^^^^
   File "/usr/local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
     loader.exec_module(new_module)
   File "<frozen importlib._bootstrap_external>", line 940, in exec_module
   File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
   File "/usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py", line 9, in <module>
     from dags.capstone.python_scripts.load_polygon_into_snowflake import load_polygon_main
   File "/usr/local/airflow/dags/capstone/python_scripts/load_polygon_into_snowflake.py", line 118, in <module>
     load_polygon_main()
   File "/usr/local/airflow/dags/capstone/python_scripts/load_polygon_into_snowflake.py", line 31, in load_polygon_main
     response = requests.get(url)
                ^^^^^^^^^^^^^^^^^
   File "/usr/local/lib/python3.11/site-packages/requests/api.py", line 73, in get
     return request("get", url, params=params, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "/usr/local/lib/python3.11/site-packages/requests/api.py", line 59, in request
     return session.request(method=method, url=url, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "/usr/local/lib/python3.11/site-packages/requests/sessions.py", line 589, in request
     resp = self.send(prep, **send_kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "/usr/local/lib/python3.11/site-packages/requests/sessions.py", line 703, in send
     r = adapter.send(request, **kwargs)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "/usr/local/lib/python3.11/site-packages/requests/adapters.py", line 486, in send
     resp = conn.urlopen(
            ^^^^^^^^^^^^^
   File "/usr/local/lib/python3.11/site-packages/urllib3/connectionpool.py", line 793, in urlopen
     response = self._make_request(
                ^^^^^^^^^^^^^^^^^^^
   File "/usr/local/lib/python3.11/site-packages/urllib3/connectionpool.py", line 537, in _make_request
     response = conn.getresponse()
                ^^^^^^^^^^^^^^^^^^
   File "/usr/local/lib/python3.11/site-packages/urllib3/connection.py", line 466, in getresponse
     httplib_response = super().getresponse()
                        ^^^^^^^^^^^^^^^^^^^^^
   File "/usr/local/lib/python3.11/http/client.py", line 1395, in getresponse
     response.begin()
   File "/usr/local/lib/python3.11/http/client.py", line 325, in begin
     version, status, reason = self._read_status()
                               ^^^^^^^^^^^^^^^^^^^
   File "/usr/local/lib/python3.11/http/client.py", line 286, in _read_status
     line = str(self.fp.readline(_MAXLINE + 1), "iso-8859-1")
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "/usr/local/lib/python3.11/socket.py", line 706, in readinto
     return self._sock.recv_into(b)
            ^^^^^^^^^^^^^^^^^^^^^^^
   File "/usr/local/lib/python3.11/ssl.py", line 1314, in recv_into
     return self.read(nbytes, buffer)
            ^^^^^^^^^^^^^^^^^^^^^^^^^
   File "/usr/local/lib/python3.11/ssl.py", line 1166, in read
     return self._sslobj.read(len, buffer)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "/usr/local/lib/python3.11/site-packages/airflow/utils/timeout.py", line 69, in handle_timeout
     raise AirflowTaskTimeout(self.error_message)
 airflow.exceptions.AirflowTaskTimeout: DagBag import timeout for /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py after 30.0s.
 Please take a look at these docs to improve your DAG import time:
 * http://apache-airflow-docs.s3-website.eu-central-1.amazonaws.com/docs/apache-airflow/stable/best-practices.html#top-level-python-code
 * http://apache-airflow-docs.s3-website.eu-central-1.amazonaws.com/docs/apache-airflow/stable/best-practices.html#reducing-dag-complexity, PID: 12865
 [[34m2024-11-27T15:02:04.559+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:02:04.592+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:02:04.918+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T15:02:05.209+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:02:06.078+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:02:06.308+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:02:06.414+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 [[34m2024-11-27T15:02:06.526+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:02:06.694+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:02:06.802+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 Data loaded successfully
 [[34m2024-11-27T15:02:07.054+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T15:02:07.076+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 /usr/local/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked semaphore objects to clean up at shutdown
   warnings.warn('resource_tracker: There appear to be %d '
 /usr/local/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked semaphore objects to clean up at shutdown
   warnings.warn('resource_tracker: There appear to be %d '
 Data loaded successfully
 [[34m2024-11-27T15:02:07.443+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T15:02:07.456+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 /usr/local/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked semaphore objects to clean up at shutdown
   warnings.warn('resource_tracker: There appear to be %d '
 [[34m2024-11-27T15:02:08.069+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.3933247660752386[0m
 [[34m2024-11-27T15:02:08.069+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|13016]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T15:02:08.070+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T15:02:08.070+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T15:02:08.070+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T15:02:08.070+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|13016]: It took 1.02s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T15:02:08.073+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|13016]: It took 0.00222s to build the Airflow DAG.[0m
 [[34m2024-11-27T15:02:08.110+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.load_treasury_data_task scheduled__2024-10-11T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [[34m2024-11-27T15:02:08.386+0000[0m] {[34mdagrun.py:[0m851} INFO[0m - Marking run <DagRun capstone_andres_dag_ @ 2024-10-09 01:00:00+00:00: scheduled__2024-10-09T01:00:00+00:00, state:running, queued_at: 2024-11-27 14:57:15.522667+00:00. externally triggered: False> successful[0m
 [[34m2024-11-27T15:02:08.387+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=capstone_andres_dag_, execution_date=2024-10-09 01:00:00+00:00, run_id=scheduled__2024-10-09T01:00:00+00:00, run_start_date=2024-11-27 14:57:15.534515+00:00, run_end_date=2024-11-27 15:02:08.387165+00:00, run_duration=292.85265, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-10-09 01:00:00+00:00, data_interval_end=2024-10-10 01:00:00+00:00, dag_hash=77fa8f64adac9391cccaaae88e2be5d8[0m
 [[34m2024-11-27T15:02:08.392+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-10-10 01:00:00+00:00, run_after=2024-10-11 01:00:00+00:00[0m
 [[34m2024-11-27T15:02:08.564+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.539491246920079[0m
 [[34m2024-11-27T15:02:08.564+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|13015]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T15:02:08.565+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T15:02:08.565+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T15:02:08.566+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T15:02:08.566+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|13015]: It took 1.12s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T15:02:08.569+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|13015]: It took 0.00299s to build the Airflow DAG.[0m
 [2024-11-27T15:02:08.589+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=load_treasury_data_task, dagrun_id=scheduled__2024-10-11T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 15:02:08.196575+00:00, try_number=1, job_id=1008, op_classpath=airflow.operators.python.PythonOperator
 [[34m2024-11-27T15:02:08.621+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.load_polygon_data_task scheduled__2024-10-12T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [[34m2024-11-27T15:02:08.624+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.weekly_metrics_run', run_id='scheduled__2024-10-09T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T15:02:08.624+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.stg_daily_bars_run', run_id='scheduled__2024-10-10T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T15:02:08.625+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run', run_id='scheduled__2024-10-10T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T15:02:08.631+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.weekly_metrics_run, run_id=scheduled__2024-10-09T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 15:01:58.383526+00:00, run_end_date=2024-11-27 15:02:07.150062+00:00, run_duration=8.766536, state=success, executor_state=success, try_number=1, max_tries=0, job_id=1004, pool=default_pool, queue=default, priority_weight=1, operator=DbtRunLocalOperator, queued_dttm=2024-11-27 15:01:29.016032+00:00, queued_by_job_id=803, pid=13024[0m
 [[34m2024-11-27T15:02:08.631+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.stg_daily_bars_run, run_id=scheduled__2024-10-10T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 15:01:58.721188+00:00, run_end_date=2024-11-27 15:02:07.269775+00:00, run_duration=8.548587, state=success, executor_state=success, try_number=1, max_tries=0, job_id=1005, pool=default_pool, queue=default, priority_weight=2, operator=DbtRunLocalOperator, queued_dttm=2024-11-27 15:01:45.875091+00:00, queued_by_job_id=803, pid=13030[0m
 [[34m2024-11-27T15:02:08.631+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run, run_id=scheduled__2024-10-10T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 15:01:59.558302+00:00, run_end_date=2024-11-27 15:02:07.803443+00:00, run_duration=8.245141, state=success, executor_state=success, try_number=1, max_tries=0, job_id=1007, pool=default_pool, queue=default, priority_weight=2, operator=DbtRunLocalOperator, queued_dttm=2024-11-27 15:01:45.875091+00:00, queued_by_job_id=803, pid=13032[0m
 [2024-11-27T15:02:09.072+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=load_polygon_data_task, dagrun_id=scheduled__2024-10-12T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 15:02:08.723201+00:00, try_number=1, job_id=1009, op_classpath=airflow.operators.python.PythonOperator
 /usr/local/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked semaphore objects to clean up at shutdown
   warnings.warn('resource_tracker: There appear to be %d '
 [[34m2024-11-27T15:02:09.658+0000[0m] {[34mscheduler_job_runner.py:[0m1328} INFO[0m - DAG capstone_andres_dag_ is at (or above) max_active_runs (16 of 16), not creating any more runs[0m
 [[34m2024-11-27T15:02:09.960+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.weekly_metrics_run scheduled__2024-10-10T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T15:02:09.961+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 2/16 running and queued tasks[0m
 [[34m2024-11-27T15:02:09.962+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.weekly_metrics_run scheduled__2024-10-10T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T15:02:09.968+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.weekly_metrics_run', run_id='scheduled__2024-10-10T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
 [[34m2024-11-27T15:02:09.969+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.weekly_metrics_run', 'scheduled__2024-10-10T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T15:02:09.978+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.dim_date_run', run_id='scheduled__2024-10-10T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T15:02:09.985+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.dim_date_run, run_id=scheduled__2024-10-10T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 15:01:59.484533+00:00, run_end_date=2024-11-27 15:02:09.097655+00:00, run_duration=9.613122, state=success, executor_state=success, try_number=1, max_tries=0, job_id=1006, pool=default_pool, queue=default, priority_weight=2, operator=DbtRunLocalOperator, queued_dttm=2024-11-27 15:01:45.875091+00:00, queued_by_job_id=803, pid=13031[0m
 [[34m2024-11-27T15:02:09.980+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.weekly_metrics_run', 'scheduled__2024-10-10T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T15:02:10.149+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T15:02:11.523+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T15:02:11.524+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T15:02:11.524+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T15:02:11.548+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.dim_date_run scheduled__2024-10-11T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_daily_bars_run scheduled__2024-10-11T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run scheduled__2024-10-11T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T15:02:11.548+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 2/16 running and queued tasks[0m
 [[34m2024-11-27T15:02:11.549+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 3/16 running and queued tasks[0m
 [[34m2024-11-27T15:02:11.549+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 4/16 running and queued tasks[0m
 [[34m2024-11-27T15:02:11.549+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.dim_date_run scheduled__2024-10-11T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_daily_bars_run scheduled__2024-10-11T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run scheduled__2024-10-11T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T15:02:11.551+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.dim_date_run', run_id='scheduled__2024-10-11T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
 [[34m2024-11-27T15:02:11.551+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.dim_date_run', 'scheduled__2024-10-11T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T15:02:11.551+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.stg_daily_bars_run', run_id='scheduled__2024-10-11T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
 [[34m2024-11-27T15:02:11.551+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.stg_daily_bars_run', 'scheduled__2024-10-11T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T15:02:11.552+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run', run_id='scheduled__2024-10-11T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
 [[34m2024-11-27T15:02:11.552+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run', 'scheduled__2024-10-11T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T15:02:11.556+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='load_treasury_data_task', run_id='scheduled__2024-10-11T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T15:02:11.559+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=load_treasury_data_task, run_id=scheduled__2024-10-11T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 15:02:08.196575+00:00, run_end_date=2024-11-27 15:02:10.731023+00:00, run_duration=2.534448, state=success, executor_state=success, try_number=1, max_tries=0, job_id=1008, pool=default_pool, queue=default, priority_weight=5, operator=PythonOperator, queued_dttm=2024-11-27 15:01:57.770950+00:00, queued_by_job_id=803, pid=13099[0m
 [[34m2024-11-27T15:02:11.557+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.dim_date_run', 'scheduled__2024-10-11T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T15:02:11.557+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run', 'scheduled__2024-10-11T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T15:02:11.558+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.stg_daily_bars_run', 'scheduled__2024-10-11T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T15:02:11.590+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T15:02:11.596+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T15:02:11.603+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T15:02:12.070+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412096206,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T15:02:12.579+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T15:02:12.580+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T15:02:12.580+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T15:02:12.597+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T15:02:12.598+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T15:02:12.598+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T15:02:12.614+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T15:02:12.615+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T15:02:12.615+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T15:02:12.863+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:02:13.136+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412096214,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T15:02:13.159+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412097450,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T15:02:13.159+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412097446,
 "os.name" : Linux
 [0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 [[34m2024-11-27T15:02:13.601+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T15:02:13.837+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T15:02:14.078+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 [[34m2024-11-27T15:02:14.455+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:02:14.714+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:02:16.320+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:02:16.651+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T15:02:16.975+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:02:16.991+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 2 tasks up for execution:
 	<TaskInstance: capstone_andres_dag_.load_polygon_data_task scheduled__2024-10-13T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.load_treasury_data_task scheduled__2024-10-12T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T15:02:16.991+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 4/16 running and queued tasks[0m
 [[34m2024-11-27T15:02:16.991+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 5/16 running and queued tasks[0m
 [[34m2024-11-27T15:02:16.991+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
 	<TaskInstance: capstone_andres_dag_.load_polygon_data_task scheduled__2024-10-13T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.load_treasury_data_task scheduled__2024-10-12T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T15:02:16.993+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='load_polygon_data_task', run_id='scheduled__2024-10-13T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default[0m
 [[34m2024-11-27T15:02:16.993+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'load_polygon_data_task', 'scheduled__2024-10-13T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T15:02:16.994+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='load_treasury_data_task', run_id='scheduled__2024-10-12T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 5 and queue default[0m
 [[34m2024-11-27T15:02:16.994+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'load_treasury_data_task', 'scheduled__2024-10-12T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T15:02:16.997+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='load_polygon_data_task', run_id='scheduled__2024-10-12T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T15:02:17.000+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=load_polygon_data_task, run_id=scheduled__2024-10-12T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 15:02:08.723201+00:00, run_end_date=2024-11-27 15:02:16.718399+00:00, run_duration=7.995198, state=success, executor_state=success, try_number=1, max_tries=0, job_id=1009, pool=default_pool, queue=default, priority_weight=6, operator=PythonOperator, queued_dttm=2024-11-27 15:01:57.770950+00:00, queued_by_job_id=803, pid=13102[0m
 [[34m2024-11-27T15:02:16.996+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'load_treasury_data_task', 'scheduled__2024-10-12T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T15:02:16.997+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'load_polygon_data_task', 'scheduled__2024-10-13T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T15:02:17.034+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T15:02:17.041+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T15:02:17.293+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:02:17.624+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T15:02:17.689+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:02:17.925+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T15:02:17.926+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T15:02:17.926+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T15:02:17.932+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T15:02:17.933+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T15:02:17.933+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T15:02:17.950+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:02:18.043+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T15:02:18.155+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:02:18.292+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:02:18.340+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:02:18.377+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412096218,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T15:02:18.401+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 [[34m2024-11-27T15:02:18.509+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412096222,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T15:02:19.011+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 Data loaded successfully
 [[34m2024-11-27T15:02:19.085+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T15:02:19.098+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T15:02:19.160+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:02:19.210+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:02:19.320+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 [[34m2024-11-27T15:02:19.518+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:02:19.724+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:02:19.833+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 Data loaded successfully
 [[34m2024-11-27T15:02:19.865+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T15:02:19.893+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 [[34m2024-11-27T15:02:20.179+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:02:20.536+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.7407754389569163[0m
 [[34m2024-11-27T15:02:20.537+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|13111]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T15:02:20.537+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T15:02:20.538+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T15:02:20.538+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T15:02:20.538+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|13111]: It took 1.45s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T15:02:20.541+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|13111]: It took 0.00322s to build the Airflow DAG.[0m
 [[34m2024-11-27T15:02:20.589+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.weekly_metrics_run scheduled__2024-10-10T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 Data loaded successfully
 [[34m2024-11-27T15:02:20.662+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T15:02:20.684+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 [[34m2024-11-27T15:02:21.223+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.6609520970378071[0m
 [[34m2024-11-27T15:02:21.223+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|13121]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T15:02:21.224+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T15:02:21.224+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T15:02:21.224+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T15:02:21.224+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|13121]: It took 1.36s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T15:02:21.227+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|13121]: It took 0.00299s to build the Airflow DAG.[0m
 [2024-11-27T15:02:21.259+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.weekly_metrics_run, dagrun_id=scheduled__2024-10-10T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 15:02:20.695051+00:00, try_number=1, job_id=1010, op_classpath=cosmos.operators.local.DbtRunLocalOperator, cosmos.operators.base.DbtRunMixin, cosmos.operators.local.DbtLocalBaseOperator, cosmos.operators.base.AbstractDbtBaseOperator
 [[34m2024-11-27T15:02:21.280+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.dim_date_run scheduled__2024-10-11T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [[34m2024-11-27T15:02:21.839+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.5400208518840373[0m
 [[34m2024-11-27T15:02:21.839+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|13123]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T15:02:21.840+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T15:02:21.841+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T15:02:21.841+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T15:02:21.842+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|13123]: It took 1.18s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T15:02:21.847+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|13123]: It took 0.00518s to build the Airflow DAG.[0m
 [[34m2024-11-27T15:02:21.930+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_daily_bars_run scheduled__2024-10-11T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [2024-11-27T15:02:22.005+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.dim_date_run, dagrun_id=scheduled__2024-10-11T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 15:02:21.380410+00:00, try_number=1, job_id=1011, op_classpath=cosmos.operators.local.DbtRunLocalOperator, cosmos.operators.base.DbtRunMixin, cosmos.operators.local.DbtLocalBaseOperator, cosmos.operators.base.AbstractDbtBaseOperator
 [2024-11-27T15:02:22.863+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.stg_daily_bars_run, dagrun_id=scheduled__2024-10-11T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 15:02:22.054463+00:00, try_number=1, job_id=1012, op_classpath=cosmos.operators.local.DbtRunLocalOperator, cosmos.operators.base.DbtRunMixin, cosmos.operators.local.DbtLocalBaseOperator, cosmos.operators.base.AbstractDbtBaseOperator
 [[34m2024-11-27T15:02:22.914+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:02:23.243+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T15:02:23.500+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:02:24.635+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:02:24.800+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:02:24.940+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 Data loaded successfully
 [[34m2024-11-27T15:02:25.373+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T15:02:25.390+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 [[34m2024-11-27T15:02:26.493+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.5418489100411534[0m
 [[34m2024-11-27T15:02:26.493+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|13155]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T15:02:26.494+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T15:02:26.495+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T15:02:26.495+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T15:02:26.495+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|13155]: It took 1.12s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T15:02:26.498+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|13155]: It took 0.00302s to build the Airflow DAG.[0m
 [[34m2024-11-27T15:02:26.554+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.load_treasury_data_task scheduled__2024-10-12T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [2024-11-27T15:02:27.023+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=load_treasury_data_task, dagrun_id=scheduled__2024-10-12T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 15:02:26.661024+00:00, try_number=1, job_id=1013, op_classpath=airflow.operators.python.PythonOperator
 /usr/local/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked semaphore objects to clean up at shutdown
   warnings.warn('resource_tracker: There appear to be %d '
 [[34m2024-11-27T15:02:28.814+0000[0m] {[34mdagrun.py:[0m851} INFO[0m - Marking run <DagRun capstone_andres_dag_ @ 2024-10-10 01:00:00+00:00: scheduled__2024-10-10T01:00:00+00:00, state:running, queued_at: 2024-11-27 14:57:33.848088+00:00. externally triggered: False> successful[0m
 [[34m2024-11-27T15:02:28.815+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=capstone_andres_dag_, execution_date=2024-10-10 01:00:00+00:00, run_id=scheduled__2024-10-10T01:00:00+00:00, run_start_date=2024-11-27 14:57:33.858749+00:00, run_end_date=2024-11-27 15:02:28.815223+00:00, run_duration=294.956474, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-10-10 01:00:00+00:00, data_interval_end=2024-10-11 01:00:00+00:00, dag_hash=77fa8f64adac9391cccaaae88e2be5d8[0m
 [[34m2024-11-27T15:02:28.819+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-10-11 01:00:00+00:00, run_after=2024-10-12 01:00:00+00:00[0m
 [[34m2024-11-27T15:02:28.994+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.weekly_metrics_run', run_id='scheduled__2024-10-10T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T15:02:28.997+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.weekly_metrics_run, run_id=scheduled__2024-10-10T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 15:02:20.695051+00:00, run_end_date=2024-11-27 15:02:28.428580+00:00, run_duration=7.733529, state=success, executor_state=success, try_number=1, max_tries=0, job_id=1010, pool=default_pool, queue=default, priority_weight=1, operator=DbtRunLocalOperator, queued_dttm=2024-11-27 15:02:09.963718+00:00, queued_by_job_id=803, pid=13176[0m
 /usr/local/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked semaphore objects to clean up at shutdown
   warnings.warn('resource_tracker: There appear to be %d '
 [[34m2024-11-27T15:02:30.023+0000[0m] {[34mscheduler_job_runner.py:[0m1328} INFO[0m - DAG capstone_andres_dag_ is at (or above) max_active_runs (16 of 16), not creating any more runs[0m
 /usr/local/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked semaphore objects to clean up at shutdown
   warnings.warn('resource_tracker: There appear to be %d '
 [[34m2024-11-27T15:02:30.283+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 3 tasks up for execution:
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.dim_date_run scheduled__2024-10-12T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_daily_bars_run scheduled__2024-10-12T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run scheduled__2024-10-12T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T15:02:30.284+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 2/16 running and queued tasks[0m
 [[34m2024-11-27T15:02:30.284+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 3/16 running and queued tasks[0m
 [[34m2024-11-27T15:02:30.284+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 4/16 running and queued tasks[0m
 [[34m2024-11-27T15:02:30.284+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.dim_date_run scheduled__2024-10-12T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_daily_bars_run scheduled__2024-10-12T01:00:00+00:00 [scheduled]>
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run scheduled__2024-10-12T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T15:02:30.286+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.dim_date_run', run_id='scheduled__2024-10-12T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
 [[34m2024-11-27T15:02:30.286+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.dim_date_run', 'scheduled__2024-10-12T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T15:02:30.287+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.stg_daily_bars_run', run_id='scheduled__2024-10-12T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
 [[34m2024-11-27T15:02:30.287+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.stg_daily_bars_run', 'scheduled__2024-10-12T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T15:02:30.287+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run', run_id='scheduled__2024-10-12T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
 [[34m2024-11-27T15:02:30.287+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run', 'scheduled__2024-10-12T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T15:02:30.291+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='load_treasury_data_task', run_id='scheduled__2024-10-12T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T15:02:30.291+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.stg_daily_bars_run', run_id='scheduled__2024-10-11T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T15:02:30.290+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.dim_date_run', 'scheduled__2024-10-12T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T15:02:30.292+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.dim_date_run', run_id='scheduled__2024-10-11T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T15:02:30.290+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.stg_daily_bars_run', 'scheduled__2024-10-12T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T15:02:30.297+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=load_treasury_data_task, run_id=scheduled__2024-10-12T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 15:02:26.661024+00:00, run_end_date=2024-11-27 15:02:29.079966+00:00, run_duration=2.418942, state=success, executor_state=success, try_number=1, max_tries=0, job_id=1013, pool=default_pool, queue=default, priority_weight=5, operator=PythonOperator, queued_dttm=2024-11-27 15:02:16.992381+00:00, queued_by_job_id=803, pid=13222[0m
 [[34m2024-11-27T15:02:30.297+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.dim_date_run, run_id=scheduled__2024-10-11T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 15:02:21.380410+00:00, run_end_date=2024-11-27 15:02:29.988859+00:00, run_duration=8.608449, state=success, executor_state=success, try_number=1, max_tries=0, job_id=1011, pool=default_pool, queue=default, priority_weight=2, operator=DbtRunLocalOperator, queued_dttm=2024-11-27 15:02:11.549920+00:00, queued_by_job_id=803, pid=13179[0m
 [[34m2024-11-27T15:02:30.297+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.stg_daily_bars_run, run_id=scheduled__2024-10-11T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 15:02:22.054463+00:00, run_end_date=2024-11-27 15:02:29.138694+00:00, run_duration=7.084231, state=success, executor_state=success, try_number=1, max_tries=0, job_id=1012, pool=default_pool, queue=default, priority_weight=2, operator=DbtRunLocalOperator, queued_dttm=2024-11-27 15:02:11.549920+00:00, queued_by_job_id=803, pid=13183[0m
 [[34m2024-11-27T15:02:30.294+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run', 'scheduled__2024-10-12T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T15:02:30.324+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T15:02:30.329+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T15:02:30.343+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T15:02:31.412+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T15:02:31.412+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T15:02:31.413+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T15:02:31.424+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T15:02:31.424+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T15:02:31.425+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T15:02:31.432+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T15:02:31.433+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T15:02:31.434+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T15:02:31.843+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412097502,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T15:02:31.876+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412096250,
 "os.name" : Linux
 [0m
 [[34m2024-11-27T15:02:31.876+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412097506,
 "os.name" : Linux
 [0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T15:02:32.703+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T15:02:32.702+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T15:02:32.707+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 [[34m2024-11-27T15:02:33.400+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 [[34m2024-11-27T15:02:33.581+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:02:33.694+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:02:36.199+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:02:36.407+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:02:36.501+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:02:36.530+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T15:02:36.711+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T15:02:36.804+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T15:02:36.838+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:02:36.998+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:02:37.100+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:02:38.059+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:02:38.113+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:02:38.218+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:02:38.285+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:02:38.296+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:02:38.379+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:02:38.403+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 [[34m2024-11-27T15:02:38.418+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 [[34m2024-11-27T15:02:38.497+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 Data loaded successfully
 [[34m2024-11-27T15:02:39.010+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T15:02:39.022+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 Data loaded successfully
 [[34m2024-11-27T15:02:39.078+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T15:02:39.090+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 Data loaded successfully
 [[34m2024-11-27T15:02:39.333+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T15:02:39.349+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 [[34m2024-11-27T15:02:39.859+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.3281154721044004[0m
 [[34m2024-11-27T15:02:39.859+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|13252]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T15:02:39.860+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T15:02:39.860+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T15:02:39.861+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T15:02:39.861+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|13252]: It took 0.851s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T15:02:39.866+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|13252]: It took 0.00494s to build the Airflow DAG.[0m
 [[34m2024-11-27T15:02:39.950+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_daily_bars_run scheduled__2024-10-12T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [[34m2024-11-27T15:02:40.057+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.4421930380631238[0m
 [[34m2024-11-27T15:02:40.058+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|13253]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T15:02:40.058+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T15:02:40.059+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T15:02:40.059+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T15:02:40.059+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|13253]: It took 0.981s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T15:02:40.063+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|13253]: It took 0.00413s to build the Airflow DAG.[0m
 [[34m2024-11-27T15:02:40.110+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run scheduled__2024-10-12T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [[34m2024-11-27T15:02:40.503+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.6052051510196179[0m
 [[34m2024-11-27T15:02:40.503+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|13251]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T15:02:40.504+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T15:02:40.505+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T15:02:40.505+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T15:02:40.505+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|13251]: It took 1.17s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T15:02:40.510+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|13251]: It took 0.00419s to build the Airflow DAG.[0m
 [[34m2024-11-27T15:02:40.563+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.dim_date_run scheduled__2024-10-12T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [2024-11-27T15:02:40.591+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.stg_daily_bars_run, dagrun_id=scheduled__2024-10-12T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 15:02:40.068449+00:00, try_number=1, job_id=1014, op_classpath=cosmos.operators.local.DbtRunLocalOperator, cosmos.operators.base.DbtRunMixin, cosmos.operators.local.DbtLocalBaseOperator, cosmos.operators.base.AbstractDbtBaseOperator
 [2024-11-27T15:02:40.720+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run, dagrun_id=scheduled__2024-10-12T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 15:02:40.199981+00:00, try_number=1, job_id=1015, op_classpath=cosmos.operators.local.DbtRunLocalOperator, cosmos.operators.base.DbtRunMixin, cosmos.operators.local.DbtLocalBaseOperator, cosmos.operators.base.AbstractDbtBaseOperator
 [2024-11-27T15:02:41.109+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.dim_date_run, dagrun_id=scheduled__2024-10-12T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 15:02:40.665610+00:00, try_number=1, job_id=1016, op_classpath=cosmos.operators.local.DbtRunLocalOperator, cosmos.operators.base.DbtRunMixin, cosmos.operators.local.DbtLocalBaseOperator, cosmos.operators.base.AbstractDbtBaseOperator
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 [[34m2024-11-27T15:02:41.606+0000[0m] {[34mtimeout.py:[0m68} ERROR[0m - Process timed out, PID: 13122[0m
 [[34m2024-11-27T15:02:41.973+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state failed for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run', run_id='scheduled__2024-10-11T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T15:02:41.976+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run, run_id=scheduled__2024-10-11T01:00:00+00:00, map_index=-1, run_start_date=None, run_end_date=None, run_duration=None, state=queued, executor_state=failed, try_number=1, max_tries=0, job_id=None, pool=default_pool, queue=default, priority_weight=2, operator=DbtRunLocalOperator, queued_dttm=2024-11-27 15:02:11.549920+00:00, queued_by_job_id=803, pid=None[0m
 [[34m2024-11-27T15:02:41.976+0000[0m] {[34mtask_context_logger.py:[0m91} ERROR[0m - Executor reports task instance <TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run scheduled__2024-10-11T01:00:00+00:00 [queued]> finished (failed) although the task says it's queued. (Info: None) Was the task killed externally?[0m
 [[34m2024-11-27T15:02:41.994+0000[0m] {[34mtaskinstance.py:[0m2907} ERROR[0m - Executor reports task instance <TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run scheduled__2024-10-11T01:00:00+00:00 [queued]> finished (failed) although the task says it's queued. (Info: None) Was the task killed externally?[0m
 [[34m2024-11-27T15:02:42.008+0000[0m] {[34mtaskinstance.py:[0m1206} INFO[0m - Marking task as FAILED. dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run, run_id=scheduled__2024-10-11T01:00:00+00:00, execution_date=20241011T010000, start_date=, end_date=20241127T150241[0m
 [[34m2024-11-27T15:02:43.473+0000[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun capstone_andres_dag_ @ 2024-10-11 01:00:00+00:00: scheduled__2024-10-11T01:00:00+00:00, state:running, queued_at: 2024-11-27 14:57:54.419817+00:00. externally triggered: False> failed[0m
 [[34m2024-11-27T15:02:43.474+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=capstone_andres_dag_, execution_date=2024-10-11 01:00:00+00:00, run_id=scheduled__2024-10-11T01:00:00+00:00, run_start_date=2024-11-27 14:57:54.431621+00:00, run_end_date=2024-11-27 15:02:43.474340+00:00, run_duration=289.042719, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2024-10-11 01:00:00+00:00, data_interval_end=2024-10-12 01:00:00+00:00, dag_hash=77fa8f64adac9391cccaaae88e2be5d8[0m
 [[34m2024-11-27T15:02:43.480+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-10-12 01:00:00+00:00, run_after=2024-10-13 01:00:00+00:00[0m
 [[34m2024-11-27T15:02:44.927+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-10-13 01:00:00+00:00, run_after=2024-10-14 01:00:00+00:00[0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T15:02:45.444+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 [[34m2024-11-27T15:02:46.282+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:02:46.304+0000[0m] {[34mscheduler_job_runner.py:[0m1328} INFO[0m - DAG capstone_andres_dag_ is at (or above) max_active_runs (16 of 16), not creating any more runs[0m
 /usr/local/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked semaphore objects to clean up at shutdown
   warnings.warn('resource_tracker: There appear to be %d '
 [[34m2024-11-27T15:02:47.054+0000[0m] {[34mtimeout.py:[0m68} ERROR[0m - Process timed out, PID: 13156[0m
 /usr/local/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked semaphore objects to clean up at shutdown
   warnings.warn('resource_tracker: There appear to be %d '
 [[34m2024-11-27T15:02:48.357+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run', run_id='scheduled__2024-10-12T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T15:02:48.357+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state failed for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='load_polygon_data_task', run_id='scheduled__2024-10-13T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T15:02:48.358+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.stg_daily_bars_run', run_id='scheduled__2024-10-12T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T15:02:48.364+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=load_polygon_data_task, run_id=scheduled__2024-10-13T01:00:00+00:00, map_index=-1, run_start_date=None, run_end_date=None, run_duration=None, state=queued, executor_state=failed, try_number=1, max_tries=0, job_id=None, pool=default_pool, queue=default, priority_weight=6, operator=PythonOperator, queued_dttm=2024-11-27 15:02:16.992381+00:00, queued_by_job_id=803, pid=None[0m
 [[34m2024-11-27T15:02:48.364+0000[0m] {[34mtask_context_logger.py:[0m91} ERROR[0m - Executor reports task instance <TaskInstance: capstone_andres_dag_.load_polygon_data_task scheduled__2024-10-13T01:00:00+00:00 [queued]> finished (failed) although the task says it's queued. (Info: None) Was the task killed externally?[0m
 [[34m2024-11-27T15:02:48.376+0000[0m] {[34mtaskinstance.py:[0m2907} ERROR[0m - Executor reports task instance <TaskInstance: capstone_andres_dag_.load_polygon_data_task scheduled__2024-10-13T01:00:00+00:00 [queued]> finished (failed) although the task says it's queued. (Info: None) Was the task killed externally?[0m
 [[34m2024-11-27T15:02:48.384+0000[0m] {[34mtaskinstance.py:[0m1206} INFO[0m - Marking task as FAILED. dag_id=capstone_andres_dag_, task_id=load_polygon_data_task, run_id=scheduled__2024-10-13T01:00:00+00:00, execution_date=20241013T010000, start_date=, end_date=20241127T150248[0m
 [[34m2024-11-27T15:02:48.392+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.stg_daily_bars_run, run_id=scheduled__2024-10-12T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 15:02:40.068449+00:00, run_end_date=2024-11-27 15:02:47.130083+00:00, run_duration=7.061634, state=success, executor_state=success, try_number=1, max_tries=0, job_id=1014, pool=default_pool, queue=default, priority_weight=2, operator=DbtRunLocalOperator, queued_dttm=2024-11-27 15:02:30.285290+00:00, queued_by_job_id=803, pid=13318[0m
 [[34m2024-11-27T15:02:48.392+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.stg_treasury_revenue_collections_run, run_id=scheduled__2024-10-12T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 15:02:40.199981+00:00, run_end_date=2024-11-27 15:02:46.709865+00:00, run_duration=6.509884, state=success, executor_state=success, try_number=1, max_tries=0, job_id=1015, pool=default_pool, queue=default, priority_weight=2, operator=DbtRunLocalOperator, queued_dttm=2024-11-27 15:02:30.285290+00:00, queued_by_job_id=803, pid=13319[0m
 /usr/local/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked semaphore objects to clean up at shutdown
   warnings.warn('resource_tracker: There appear to be %d '
 [[34m2024-11-27T15:02:49.630+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.weekly_metrics_run scheduled__2024-10-12T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T15:02:49.630+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG capstone_andres_dag_ has 0/16 running and queued tasks[0m
 [[34m2024-11-27T15:02:49.631+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
 	<TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.weekly_metrics_run scheduled__2024-10-12T01:00:00+00:00 [scheduled]>[0m
 [[34m2024-11-27T15:02:49.632+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.weekly_metrics_run', run_id='scheduled__2024-10-12T01:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
 [[34m2024-11-27T15:02:49.632+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.weekly_metrics_run', 'scheduled__2024-10-12T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T15:02:49.635+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.dim_date_run', run_id='scheduled__2024-10-12T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T15:02:49.637+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.dim_date_run, run_id=scheduled__2024-10-12T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 15:02:40.665610+00:00, run_end_date=2024-11-27 15:02:48.903356+00:00, run_duration=8.237746, state=success, executor_state=success, try_number=1, max_tries=0, job_id=1016, pool=default_pool, queue=default, priority_weight=2, operator=DbtRunLocalOperator, queued_dttm=2024-11-27 15:02:30.285290+00:00, queued_by_job_id=803, pid=13320[0m
 [[34m2024-11-27T15:02:49.636+0000[0m] {[34mlocal_executor.py:[0m90} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'capstone_andres_dag_', 'dbt_build_weekly_metrics_task.weekly_metrics_run', 'scheduled__2024-10-12T01:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/capstone/airflow_dags/capstone_andres_dag.py'][0m
 [[34m2024-11-27T15:02:49.678+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /usr/local/airflow/dags/capstone/airflow_dags/capstone_andres_dag.py[0m
 [[34m2024-11-27T15:02:50.559+0000[0m] {[34mconnection.py:[0m414} INFO[0m - Snowflake Connector for Python Version: 3.12.3, Python Version: 3.11.9, Platform: Linux-6.10.0-linuxkit-x86_64-with-glibc2.31[0m
 [[34m2024-11-27T15:02:50.560+0000[0m] {[34mconnection.py:[0m1197} INFO[0m - Connecting to GLOBAL Snowflake domain[0m
 [[34m2024-11-27T15:02:50.560+0000[0m] {[34mconnection.py:[0m1278} INFO[0m - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.[0m
 [[34m2024-11-27T15:02:51.184+0000[0m] {[34msession.py:[0m635} INFO[0m - Snowpark Session information: 
 "version" : 1.24.0,
 "python.version" : 3.11.9,
 "python.connector.version" : 3.12.3,
 "python.connector.session.id" : 57084412096266,
 "os.name" : Linux
 [0m
 the url is: https://api.polygon.io/v2/aggs/grouped/locale/us/market/stocks/2023-01-09?adjusted=true&apiKey=Em7xrXc5QX01uQqD29xxTrVZXfrrjC6Q
 the results dict is:
 type is:  <class 'list'>
 [{'T': 'TTMI', 'v': 394280, 'vw': 16.1078, 'o': 15.96, 'c': 16.08, 'h': 16.335, 'l': 15.96, 't': 1673298000000, 'n': 5416}, {'T': 'OEC', 'v': 485157, 'vw': 19.0627, 'o': 18.67, 'c': 18.98, 'h': 19.43, 'l': 18.45, 't': 1673298000000, 'n': 8130}, {'T': 'USDU', 'v': 642813, 'vw': 25.8745, 'o': 25.92, 'c': 25.88, 'h': 25.95, 'l': 25.8204, 't': 1673298000000, 'n': 2417}, {'T': 'BWXT', 'v': 389669, 'vw': 57.4657, 'o': 58.15, 'c': 57.28, 'h': 58.18, 'l': 57.025, 't': 1673298000000, 'n': 7325}, {'T': 'WRB', 'v': 1947042.0, 'vw': 48.8506, 'o': 49.5267, 'c': 48.6533, 'h': 49.5867, 'l': 48.3933, 't': 1673298000000, 'n': 15947}]
 the col TTMI is: <class 'str'>
 the col 394280 is: <class 'int'>
 the col 16.1078 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 16.08 is: <class 'float'>
 the col 16.335 is: <class 'float'>
 the col 15.96 is: <class 'float'>
 the col 1673298000000 is: <class 'int'>
 the col 5416 is: <class 'int'>
 [[34m2024-11-27T15:02:52.037+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:02:52.300+0000[0m] {[34mdagrun.py:[0m820} ERROR[0m - Marking run <DagRun capstone_andres_dag_ @ 2024-10-13 01:00:00+00:00: scheduled__2024-10-13T01:00:00+00:00, state:running, queued_at: 2024-11-27 14:58:30.258170+00:00. externally triggered: False> failed[0m
 [[34m2024-11-27T15:02:52.300+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=capstone_andres_dag_, execution_date=2024-10-13 01:00:00+00:00, run_id=scheduled__2024-10-13T01:00:00+00:00, run_start_date=2024-11-27 14:58:30.274372+00:00, run_end_date=2024-11-27 15:02:52.300686+00:00, run_duration=262.026314, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2024-10-13 01:00:00+00:00, data_interval_end=2024-10-14 01:00:00+00:00, dag_hash=77fa8f64adac9391cccaaae88e2be5d8[0m
 [[34m2024-11-27T15:02:52.307+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-10-14 01:00:00+00:00, run_after=2024-10-15 01:00:00+00:00[0m
 /usr/local/lib/python3.11/site-packages/snowflake/snowpark/session.py:2712 UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)
 [[34m2024-11-27T15:02:52.428+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-10-15 01:00:00+00:00, run_after=2024-10-16 01:00:00+00:00[0m
 [[34m2024-11-27T15:02:52.711+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:02:53.216+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-10-16 01:00:00+00:00, run_after=2024-10-17 01:00:00+00:00[0m
 [[34m2024-11-27T15:02:54.250+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:02:54.460+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-10-17 01:00:00+00:00, run_after=2024-10-18 01:00:00+00:00[0m
 [[34m2024-11-27T15:02:54.568+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 10[0m
 [[34m2024-11-27T15:02:54.919+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:02:55.657+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-10-18 01:00:00+00:00, run_after=2024-10-19 01:00:00+00:00[0m
 [[34m2024-11-27T15:02:55.989+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:02:56.157+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 1[0m
 [[34m2024-11-27T15:02:56.282+0000[0m] {[34mcursor.py:[0m1166} INFO[0m - Number of results in first chunk: 0[0m
 [[34m2024-11-27T15:02:56.853+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-10-19 01:00:00+00:00, run_after=2024-10-20 01:00:00+00:00[0m
 Data loaded successfully
 [[34m2024-11-27T15:02:56.997+0000[0m] {[34mgraph.py:[0m470} INFO[0m - Trying to parse the dbt project using dbt ls cache cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task...[0m
 [[34m2024-11-27T15:02:57.008+0000[0m] {[34mcredentials.py:[0m1147} INFO[0m - Found credentials in environment variables.[0m
 [[34m2024-11-27T15:02:57.881+0000[0m] {[34mcache.py:[0m320} INFO[0m - Cosmos performance: time to calculate cache identifier cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task for current version: 0.3046758121345192[0m
 [[34m2024-11-27T15:02:57.882+0000[0m] {[34mgraph.py:[0m487} INFO[0m - Cosmos performance [75994b6d2295|13368]: The cache size for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task is 4885[0m
 [[34m2024-11-27T15:02:57.882+0000[0m] {[34mgraph.py:[0m495} INFO[0m - Cosmos performance: Cache hit for cosmos_cache__capstone_andres_dag___dbt_build_weekly_metrics_task - 7d67f979e794cf5fe4e9bece86fa347c,94c5059a60432ad4d72f037d7eb8148e[0m
 [[34m2024-11-27T15:02:57.882+0000[0m] {[34mgraph.py:[0m412} INFO[0m - Total nodes: 6[0m
 [[34m2024-11-27T15:02:57.883+0000[0m] {[34mgraph.py:[0m413} INFO[0m - Total filtered nodes: 6[0m
 [[34m2024-11-27T15:02:57.883+0000[0m] {[34mconverter.py:[0m264} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) -  [75994b6d2295|13368]: It took 0.886s to parse the dbt project for DAG using LoadMode.DBT_LS_CACHE[0m
 [[34m2024-11-27T15:02:57.885+0000[0m] {[34mconverter.py:[0m308} INFO[0m - Cosmos performance (capstone_andres_dag___dbt_build_weekly_metrics_task) - [75994b6d2295|13368]: It took 0.00216s to build the Airflow DAG.[0m
 [[34m2024-11-27T15:02:57.927+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: capstone_andres_dag_.dbt_build_weekly_metrics_task.weekly_metrics_run scheduled__2024-10-12T01:00:00+00:00 [queued]> on host 75994b6d2295[0m
 [[34m2024-11-27T15:02:58.091+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-10-20 01:00:00+00:00, run_after=2024-10-21 01:00:00+00:00[0m
 [2024-11-27T15:02:58.417+0000] {listener.py:32} INFO - TaskInstance Details: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.weekly_metrics_run, dagrun_id=scheduled__2024-10-12T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 15:02:57.998039+00:00, try_number=1, job_id=1017, op_classpath=cosmos.operators.local.DbtRunLocalOperator, cosmos.operators.base.DbtRunMixin, cosmos.operators.local.DbtLocalBaseOperator, cosmos.operators.base.AbstractDbtBaseOperator
 [[34m2024-11-27T15:02:58.460+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-10-21 01:00:00+00:00, run_after=2024-10-22 01:00:00+00:00[0m
 [[34m2024-11-27T15:02:59.719+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-10-22 01:00:00+00:00, run_after=2024-10-23 01:00:00+00:00[0m
 [[34m2024-11-27T15:03:00.955+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-10-23 01:00:00+00:00, run_after=2024-10-24 01:00:00+00:00[0m
 [[34m2024-11-27T15:03:02.177+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-10-24 01:00:00+00:00, run_after=2024-10-25 01:00:00+00:00[0m
 [[34m2024-11-27T15:03:02.569+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-10-25 01:00:00+00:00, run_after=2024-10-26 01:00:00+00:00[0m
 [[34m2024-11-27T15:03:02.765+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
 [[34m2024-11-27T15:03:03.451+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-10-26 01:00:00+00:00, run_after=2024-10-27 01:00:00+00:00[0m
 [[34m2024-11-27T15:03:03.726+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-10-27 01:00:00+00:00, run_after=2024-10-28 01:00:00+00:00[0m
 [[34m2024-11-27T15:03:03.965+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.weekly_metrics'[0m
 [[34m2024-11-27T15:03:03.965+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections'[0m
 [[34m2024-11-27T15:03:03.966+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_payments'[0m
 [[34m2024-11-27T15:03:03.966+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_orders'[0m
 [[34m2024-11-27T15:03:03.966+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_daily_bars'[0m
 /usr/local/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 2 leaked semaphore objects to clean up at shutdown
   warnings.warn('resource_tracker: There appear to be %d '
 [[34m2024-11-27T15:03:05.107+0000[0m] {[34mscheduler_job_runner.py:[0m1328} INFO[0m - DAG capstone_andres_dag_ is at (or above) max_active_runs (16 of 16), not creating any more runs[0m
 [[34m2024-11-27T15:03:05.291+0000[0m] {[34mdagrun.py:[0m851} INFO[0m - Marking run <DagRun capstone_andres_dag_ @ 2024-10-12 01:00:00+00:00: scheduled__2024-10-12T01:00:00+00:00, state:running, queued_at: 2024-11-27 14:58:16.505998+00:00. externally triggered: False> successful[0m
 [[34m2024-11-27T15:03:05.292+0000[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=capstone_andres_dag_, execution_date=2024-10-12 01:00:00+00:00, run_id=scheduled__2024-10-12T01:00:00+00:00, run_start_date=2024-11-27 14:58:16.517665+00:00, run_end_date=2024-11-27 15:03:05.291953+00:00, run_duration=288.774288, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-10-12 01:00:00+00:00, data_interval_end=2024-10-13 01:00:00+00:00, dag_hash=77fa8f64adac9391cccaaae88e2be5d8[0m
 [[34m2024-11-27T15:03:05.340+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-10-13 01:00:00+00:00, run_after=2024-10-14 01:00:00+00:00[0m
 [[34m2024-11-27T15:03:07.373+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='capstone_andres_dag_', task_id='dbt_build_weekly_metrics_task.weekly_metrics_run', run_id='scheduled__2024-10-12T01:00:00+00:00', try_number=1, map_index=-1)[0m
 [[34m2024-11-27T15:03:07.384+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=capstone_andres_dag_, task_id=dbt_build_weekly_metrics_task.weekly_metrics_run, run_id=scheduled__2024-10-12T01:00:00+00:00, map_index=-1, run_start_date=2024-11-27 15:02:57.998039+00:00, run_end_date=2024-11-27 15:03:04.092172+00:00, run_duration=6.094133, state=success, executor_state=success, try_number=1, max_tries=0, job_id=1017, pool=default_pool, queue=default, priority_weight=1, operator=DbtRunLocalOperator, queued_dttm=2024-11-27 15:02:49.631358+00:00, queued_by_job_id=803, pid=13405[0m
 [[34m2024-11-27T15:03:08.411+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-10-14 01:00:00+00:00, run_after=2024-10-15 01:00:00+00:00[0m
 [[34m2024-11-27T15:03:08.966+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-10-15 01:00:00+00:00, run_after=2024-10-16 01:00:00+00:00[0m
 [[34m2024-11-27T15:03:10.364+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-10-16 01:00:00+00:00, run_after=2024-10-17 01:00:00+00:00[0m
 [[34m2024-11-27T15:03:11.676+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-10-17 01:00:00+00:00, run_after=2024-10-18 01:00:00+00:00[0m
 [[34m2024-11-27T15:03:12.779+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-10-18 01:00:00+00:00, run_after=2024-10-19 01:00:00+00:00[0m
 [[34m2024-11-27T15:03:14.028+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-10-19 01:00:00+00:00, run_after=2024-10-20 01:00:00+00:00[0m
 [[34m2024-11-27T15:03:14.360+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-10-20 01:00:00+00:00, run_after=2024-10-21 01:00:00+00:00[0m
 [[34m2024-11-27T15:03:15.615+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-10-21 01:00:00+00:00, run_after=2024-10-22 01:00:00+00:00[0m
 [[34m2024-11-27T15:03:16.860+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-10-22 01:00:00+00:00, run_after=2024-10-23 01:00:00+00:00[0m
 [[34m2024-11-27T15:03:18.108+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-10-23 01:00:00+00:00, run_after=2024-10-24 01:00:00+00:00[0m
 [[34m2024-11-27T15:03:19.325+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-10-24 01:00:00+00:00, run_after=2024-10-25 01:00:00+00:00[0m
 [[34m2024-11-27T15:03:19.616+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-10-25 01:00:00+00:00, run_after=2024-10-26 01:00:00+00:00[0m
 [[34m2024-11-27T15:03:20.905+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-10-26 01:00:00+00:00, run_after=2024-10-27 01:00:00+00:00[0m
 [[34m2024-11-27T15:03:22.346+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-10-27 01:00:00+00:00, run_after=2024-10-28 01:00:00+00:00[0m
 [[34m2024-11-27T15:03:23.028+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-10-28 01:00:00+00:00, run_after=2024-10-29 01:00:00+00:00[0m
 [[34m2024-11-27T15:03:23.799+0000[0m] {[34mdag.py:[0m3954} INFO[0m - Setting next_dagrun for capstone_andres_dag_ to 2024-10-29 01:00:00+00:00, run_after=2024-10-30 01:00:00+00:00[0m
 [[34m2024-11-27T15:03:24.909+0000[0m] {[34mscheduler_job_runner.py:[0m1328} INFO[0m - DAG capstone_andres_dag_ is at (or above) max_active_runs (16 of 16), not creating any more runs[0m
 [[34m2024-11-27T15:04:04.050+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.weekly_metrics'[0m
 [[34m2024-11-27T15:04:04.052+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections'[0m
 [[34m2024-11-27T15:04:04.052+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.dim_date'[0m
 [[34m2024-11-27T15:04:04.053+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_payments'[0m
 [[34m2024-11-27T15:04:04.053+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_orders'[0m
 [[34m2024-11-27T15:04:04.053+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_daily_bars'[0m
 [[34m2024-11-27T15:05:04.384+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.weekly_metrics'[0m
 [[34m2024-11-27T15:05:04.384+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections'[0m
 [[34m2024-11-27T15:05:04.384+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.dim_date'[0m
 [[34m2024-11-27T15:05:04.385+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_payments'[0m
 [[34m2024-11-27T15:05:04.385+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_orders'[0m
 [[34m2024-11-27T15:05:04.385+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_daily_bars'[0m
 [[34m2024-11-27T15:06:04.602+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.weekly_metrics'[0m
 [[34m2024-11-27T15:06:04.602+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections'[0m
 [[34m2024-11-27T15:06:04.603+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.dim_date'[0m
 [[34m2024-11-27T15:06:04.603+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_payments'[0m
 [[34m2024-11-27T15:06:04.603+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_orders'[0m
 [[34m2024-11-27T15:06:04.603+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_daily_bars'[0m
 [[34m2024-11-27T15:07:04.888+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.weekly_metrics'[0m
 [[34m2024-11-27T15:07:04.889+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections'[0m
 [[34m2024-11-27T15:07:04.889+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.dim_date'[0m
 [[34m2024-11-27T15:07:04.889+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_payments'[0m
 [[34m2024-11-27T15:07:04.889+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_orders'[0m
 [[34m2024-11-27T15:07:04.890+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_daily_bars'[0m
 [[34m2024-11-27T15:08:03.046+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
 [[34m2024-11-27T15:08:04.885+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.weekly_metrics'[0m
 [[34m2024-11-27T15:08:04.886+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections'[0m
 [[34m2024-11-27T15:08:04.886+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.dim_date'[0m
 [[34m2024-11-27T15:08:04.886+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_payments'[0m
 [[34m2024-11-27T15:08:04.886+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_orders'[0m
 [[34m2024-11-27T15:08:04.886+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_daily_bars'[0m
 [[34m2024-11-27T15:09:05.113+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.weekly_metrics'[0m
 [[34m2024-11-27T15:09:05.114+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections'[0m
 [[34m2024-11-27T15:09:05.114+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.dim_date'[0m
 [[34m2024-11-27T15:09:05.114+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_payments'[0m
 [[34m2024-11-27T15:09:05.114+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_orders'[0m
 [[34m2024-11-27T15:09:05.114+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_daily_bars'[0m
 [[34m2024-11-27T15:10:05.125+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.weekly_metrics'[0m
 [[34m2024-11-27T15:10:05.125+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections'[0m
 [[34m2024-11-27T15:10:05.125+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.dim_date'[0m
 [[34m2024-11-27T15:10:05.125+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_payments'[0m
 [[34m2024-11-27T15:10:05.126+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_orders'[0m
 [[34m2024-11-27T15:10:05.126+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_daily_bars'[0m
 [[34m2024-11-27T15:11:05.399+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.weekly_metrics'[0m
 [[34m2024-11-27T15:11:05.399+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections'[0m
 [[34m2024-11-27T15:11:05.400+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.dim_date'[0m
 [[34m2024-11-27T15:11:05.400+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_payments'[0m
 [[34m2024-11-27T15:11:05.400+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_orders'[0m
 [[34m2024-11-27T15:11:05.400+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_daily_bars'[0m
 [[34m2024-11-27T15:12:05.620+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.weekly_metrics'[0m
 [[34m2024-11-27T15:12:05.620+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections'[0m
 [[34m2024-11-27T15:12:05.621+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.dim_date'[0m
 [[34m2024-11-27T15:12:05.621+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_payments'[0m
 [[34m2024-11-27T15:12:05.621+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_orders'[0m
 [[34m2024-11-27T15:12:05.622+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_daily_bars'[0m
 [[34m2024-11-27T15:13:03.093+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
 [[34m2024-11-27T15:13:05.651+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.weekly_metrics'[0m
 [[34m2024-11-27T15:13:05.651+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections'[0m
 [[34m2024-11-27T15:13:05.652+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.dim_date'[0m
 [[34m2024-11-27T15:13:05.652+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_payments'[0m
 [[34m2024-11-27T15:13:05.652+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_orders'[0m
 [[34m2024-11-27T15:13:05.652+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_daily_bars'[0m
 [[34m2024-11-27T15:14:05.735+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.weekly_metrics'[0m
 [[34m2024-11-27T15:14:05.736+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections'[0m
 [[34m2024-11-27T15:14:05.736+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.dim_date'[0m
 [[34m2024-11-27T15:14:05.736+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_payments'[0m
 [[34m2024-11-27T15:14:05.737+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_orders'[0m
 [[34m2024-11-27T15:14:05.737+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_daily_bars'[0m
 [[34m2024-11-27T15:15:05.771+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.weekly_metrics'[0m
 [[34m2024-11-27T15:15:05.772+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections'[0m
 [[34m2024-11-27T15:15:05.772+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.dim_date'[0m
 [[34m2024-11-27T15:15:05.772+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_payments'[0m
 [[34m2024-11-27T15:15:05.772+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_orders'[0m
 [[34m2024-11-27T15:15:05.773+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_daily_bars'[0m
 [[34m2024-11-27T15:16:05.807+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.weekly_metrics'[0m
 [[34m2024-11-27T15:16:05.808+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections'[0m
 [[34m2024-11-27T15:16:05.808+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.dim_date'[0m
 [[34m2024-11-27T15:16:05.808+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_payments'[0m
 [[34m2024-11-27T15:16:05.808+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_orders'[0m
 [[34m2024-11-27T15:16:05.808+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_daily_bars'[0m
 [[34m2024-11-27T15:17:05.851+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.weekly_metrics'[0m
 [[34m2024-11-27T15:17:05.851+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections'[0m
 [[34m2024-11-27T15:17:05.851+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.dim_date'[0m
 [[34m2024-11-27T15:17:05.852+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_payments'[0m
 [[34m2024-11-27T15:17:05.852+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_orders'[0m
 [[34m2024-11-27T15:17:05.852+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_daily_bars'[0m
 [[34m2024-11-27T15:18:03.136+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
 [[34m2024-11-27T15:18:05.901+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.weekly_metrics'[0m
 [[34m2024-11-27T15:18:05.902+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections'[0m
 [[34m2024-11-27T15:18:05.902+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.dim_date'[0m
 [[34m2024-11-27T15:18:05.902+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_payments'[0m
 [[34m2024-11-27T15:18:05.902+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_orders'[0m
 [[34m2024-11-27T15:18:05.903+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_daily_bars'[0m
 [[34m2024-11-27T15:19:05.932+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.weekly_metrics'[0m
 [[34m2024-11-27T15:19:05.933+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections'[0m
 [[34m2024-11-27T15:19:05.933+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.dim_date'[0m
 [[34m2024-11-27T15:19:05.933+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_payments'[0m
 [[34m2024-11-27T15:19:05.933+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_orders'[0m
 [[34m2024-11-27T15:19:05.933+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_daily_bars'[0m
 [[34m2024-11-27T15:20:05.968+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.weekly_metrics'[0m
 [[34m2024-11-27T15:20:05.969+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections'[0m
 [[34m2024-11-27T15:20:05.969+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.dim_date'[0m
 [[34m2024-11-27T15:20:05.969+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_payments'[0m
 [[34m2024-11-27T15:20:05.969+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_orders'[0m
 [[34m2024-11-27T15:20:05.969+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_daily_bars'[0m
 [[34m2024-11-27T15:21:06.025+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.weekly_metrics'[0m
 [[34m2024-11-27T15:21:06.027+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections'[0m
 [[34m2024-11-27T15:21:06.027+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.dim_date'[0m
 [[34m2024-11-27T15:21:06.028+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_payments'[0m
 [[34m2024-11-27T15:21:06.028+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_orders'[0m
 [[34m2024-11-27T15:21:06.029+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_daily_bars'[0m
 [[34m2024-11-27T15:22:06.060+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.weekly_metrics'[0m
 [[34m2024-11-27T15:22:06.061+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections'[0m
 [[34m2024-11-27T15:22:06.061+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.dim_date'[0m
 [[34m2024-11-27T15:22:06.061+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_payments'[0m
 [[34m2024-11-27T15:22:06.062+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_orders'[0m
 [[34m2024-11-27T15:22:06.062+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_daily_bars'[0m
 [[34m2024-11-27T15:23:03.182+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
 [[34m2024-11-27T15:23:06.084+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.weekly_metrics'[0m
 [[34m2024-11-27T15:23:06.084+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections'[0m
 [[34m2024-11-27T15:23:06.084+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.dim_date'[0m
 [[34m2024-11-27T15:23:06.084+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_payments'[0m
 [[34m2024-11-27T15:23:06.084+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_orders'[0m
 [[34m2024-11-27T15:23:06.085+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_daily_bars'[0m
 [[34m2024-11-27T15:24:06.161+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.weekly_metrics'[0m
 [[34m2024-11-27T15:24:06.162+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections'[0m
 [[34m2024-11-27T15:24:06.162+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.dim_date'[0m
 [[34m2024-11-27T15:24:06.163+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_payments'[0m
 [[34m2024-11-27T15:24:06.163+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_orders'[0m
 [[34m2024-11-27T15:24:06.163+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_daily_bars'[0m
 [[34m2024-11-27T15:25:06.201+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.weekly_metrics'[0m
 [[34m2024-11-27T15:25:06.203+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections'[0m
 [[34m2024-11-27T15:25:06.203+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.dim_date'[0m
 [[34m2024-11-27T15:25:06.204+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_payments'[0m
 [[34m2024-11-27T15:25:06.204+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_orders'[0m
 [[34m2024-11-27T15:25:06.204+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_daily_bars'[0m
 [[34m2024-11-27T15:26:06.233+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.weekly_metrics'[0m
 [[34m2024-11-27T15:26:06.233+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections'[0m
 [[34m2024-11-27T15:26:06.233+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.dim_date'[0m
 [[34m2024-11-27T15:26:06.233+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_payments'[0m
 [[34m2024-11-27T15:26:06.234+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_orders'[0m
 [[34m2024-11-27T15:26:06.234+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_daily_bars'[0m
 [[34m2024-11-27T15:27:06.276+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.weekly_metrics'[0m
 [[34m2024-11-27T15:27:06.276+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections'[0m
 [[34m2024-11-27T15:27:06.277+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.dim_date'[0m
 [[34m2024-11-27T15:27:06.277+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_payments'[0m
 [[34m2024-11-27T15:27:06.277+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_orders'[0m
 [[34m2024-11-27T15:27:06.277+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_daily_bars'[0m
 [[34m2024-11-27T15:28:03.236+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
 [[34m2024-11-27T15:28:06.313+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.weekly_metrics'[0m
 [[34m2024-11-27T15:28:06.313+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections'[0m
 [[34m2024-11-27T15:28:06.313+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.dim_date'[0m
 [[34m2024-11-27T15:28:06.314+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_payments'[0m
 [[34m2024-11-27T15:28:06.314+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_orders'[0m
 [[34m2024-11-27T15:28:06.314+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_daily_bars'[0m
 [[34m2024-11-27T15:29:06.358+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.weekly_metrics'[0m
 [[34m2024-11-27T15:29:06.358+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections'[0m
 [[34m2024-11-27T15:29:06.358+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.dim_date'[0m
 [[34m2024-11-27T15:29:06.358+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_payments'[0m
 [[34m2024-11-27T15:29:06.359+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_orders'[0m
 [[34m2024-11-27T15:29:06.359+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_daily_bars'[0m
 [[34m2024-11-27T15:30:06.563+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.weekly_metrics'[0m
 [[34m2024-11-27T15:30:06.568+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections'[0m
 [[34m2024-11-27T15:30:06.568+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.dim_date'[0m
 [[34m2024-11-27T15:30:06.569+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_payments'[0m
 [[34m2024-11-27T15:30:06.569+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_orders'[0m
 [[34m2024-11-27T15:30:06.569+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_daily_bars'[0m
 [[34m2024-11-27T15:31:06.597+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.weekly_metrics'[0m
 [[34m2024-11-27T15:31:06.598+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections'[0m
 [[34m2024-11-27T15:31:06.598+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.dim_date'[0m
 [[34m2024-11-27T15:31:06.598+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_payments'[0m
 [[34m2024-11-27T15:31:06.598+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_orders'[0m
 [[34m2024-11-27T15:31:06.599+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_daily_bars'[0m
 [[34m2024-11-27T15:32:06.633+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.weekly_metrics'[0m
 [[34m2024-11-27T15:32:06.633+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections'[0m
 [[34m2024-11-27T15:32:06.633+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.dim_date'[0m
 [[34m2024-11-27T15:32:06.634+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_payments'[0m
 [[34m2024-11-27T15:32:06.634+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_orders'[0m
 [[34m2024-11-27T15:32:06.634+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_daily_bars'[0m
 [[34m2024-11-27T15:33:03.281+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
 [[34m2024-11-27T15:33:06.675+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.weekly_metrics'[0m
 [[34m2024-11-27T15:33:06.675+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections'[0m
 [[34m2024-11-27T15:33:06.676+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.dim_date'[0m
 [[34m2024-11-27T15:33:06.676+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_payments'[0m
 [[34m2024-11-27T15:33:06.676+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_orders'[0m
 [[34m2024-11-27T15:33:06.676+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_daily_bars'[0m
 [[34m2024-11-27T15:34:06.719+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.weekly_metrics'[0m
 [[34m2024-11-27T15:34:06.719+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections'[0m
 [[34m2024-11-27T15:34:06.720+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.dim_date'[0m
 [[34m2024-11-27T15:34:06.720+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_payments'[0m
 [[34m2024-11-27T15:34:06.720+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_orders'[0m
 [[34m2024-11-27T15:34:06.720+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_daily_bars'[0m
 [[34m2024-11-27T15:35:06.748+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.weekly_metrics'[0m
 [[34m2024-11-27T15:35:06.749+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections'[0m
 [[34m2024-11-27T15:35:06.749+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.dim_date'[0m
 [[34m2024-11-27T15:35:06.749+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_payments'[0m
 [[34m2024-11-27T15:35:06.749+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_orders'[0m
 [[34m2024-11-27T15:35:06.749+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_daily_bars'[0m
 [[34m2024-11-27T15:36:06.786+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.weekly_metrics'[0m
 [[34m2024-11-27T15:36:06.786+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections'[0m
 [[34m2024-11-27T15:36:06.787+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.dim_date'[0m
 [[34m2024-11-27T15:36:06.787+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_payments'[0m
 [[34m2024-11-27T15:36:06.787+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_orders'[0m
 [[34m2024-11-27T15:36:06.787+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_daily_bars'[0m
 [[34m2024-11-27T15:37:06.828+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.weekly_metrics'[0m
 [[34m2024-11-27T15:37:06.828+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections'[0m
 [[34m2024-11-27T15:37:06.829+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.dim_date'[0m
 [[34m2024-11-27T15:37:06.829+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_payments'[0m
 [[34m2024-11-27T15:37:06.829+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_orders'[0m
 [[34m2024-11-27T15:37:06.829+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_daily_bars'[0m
 [[34m2024-11-27T15:38:03.315+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
 [[34m2024-11-27T15:38:06.858+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.weekly_metrics'[0m
 [[34m2024-11-27T15:38:06.858+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections'[0m
 [[34m2024-11-27T15:38:06.858+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.dim_date'[0m
 [[34m2024-11-27T15:38:06.858+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_payments'[0m
 [[34m2024-11-27T15:38:06.859+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_orders'[0m
 [[34m2024-11-27T15:38:06.859+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_daily_bars'[0m
 [[34m2024-11-27T15:39:06.903+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.weekly_metrics'[0m
 [[34m2024-11-27T15:39:06.903+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections'[0m
 [[34m2024-11-27T15:39:06.903+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.dim_date'[0m
 [[34m2024-11-27T15:39:06.904+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_payments'[0m
 [[34m2024-11-27T15:39:06.904+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_orders'[0m
 [[34m2024-11-27T15:39:06.904+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_daily_bars'[0m
 [[34m2024-11-27T15:40:06.953+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.weekly_metrics'[0m
 [[34m2024-11-27T15:40:06.954+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections'[0m
 [[34m2024-11-27T15:40:06.954+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.dim_date'[0m
 [[34m2024-11-27T15:40:06.954+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_payments'[0m
 [[34m2024-11-27T15:40:06.954+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_orders'[0m
 [[34m2024-11-27T15:40:06.954+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_daily_bars'[0m
 [[34m2024-11-27T15:41:07.001+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.weekly_metrics'[0m
 [[34m2024-11-27T15:41:07.005+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections'[0m
 [[34m2024-11-27T15:41:07.005+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.dim_date'[0m
 [[34m2024-11-27T15:41:07.005+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_payments'[0m
 [[34m2024-11-27T15:41:07.005+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_orders'[0m
 [[34m2024-11-27T15:41:07.006+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_daily_bars'[0m
 [[34m2024-11-27T15:42:07.044+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.weekly_metrics'[0m
 [[34m2024-11-27T15:42:07.044+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections'[0m
 [[34m2024-11-27T15:42:07.044+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.dim_date'[0m
 [[34m2024-11-27T15:42:07.044+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_payments'[0m
 [[34m2024-11-27T15:42:07.045+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_orders'[0m
 [[34m2024-11-27T15:42:07.045+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_daily_bars'[0m
 [[34m2024-11-27T15:43:03.426+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
 [[34m2024-11-27T15:43:07.061+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.weekly_metrics'[0m
 [[34m2024-11-27T15:43:07.062+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections'[0m
 [[34m2024-11-27T15:43:07.062+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.dim_date'[0m
 [[34m2024-11-27T15:43:07.063+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_payments'[0m
 [[34m2024-11-27T15:43:07.063+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_orders'[0m
 [[34m2024-11-27T15:43:07.063+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_daily_bars'[0m
 [[34m2024-11-27T15:59:28.266+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.weekly_metrics'[0m
 [[34m2024-11-27T15:59:28.273+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections'[0m
 [[34m2024-11-27T15:59:28.275+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.dim_date'[0m
 [[34m2024-11-27T15:59:28.277+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_payments'[0m
 [[34m2024-11-27T15:59:28.278+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_orders'[0m
 [[34m2024-11-27T15:59:28.279+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_daily_bars'[0m
 [[34m2024-11-27T15:59:28.482+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
 [2024-11-27 15:59:28 +0000] [46] [CRITICAL] WORKER TIMEOUT (pid:48)
 [2024-11-27 15:59:28 +0000] [48] [INFO] Worker exiting (pid: 48)
 [2024-11-27 15:59:29 +0000] [46] [ERROR] Worker (pid:48) exited with code 1
 [2024-11-27 15:59:29 +0000] [46] [ERROR] Worker (pid:48) exited with code 1.
 [2024-11-27 15:59:29 +0000] [20915] [INFO] Booting worker with pid: 20915
 Process DagFileProcessor2887-Process:
 [2024-11-27T16:50:14.735+0000] {manager.py:524} INFO - DAG capstone_andres_dag_ is missing and will be deactivated.
 [2024-11-27T16:50:14.756+0000] {manager.py:536} INFO - Deactivated 1 DAGs which are no longer present in file.
 [2024-11-27T16:50:14.812+0000] {manager.py:540} INFO - Deleted DAG capstone_andres_dag_ in serialized_dag table
 Traceback (most recent call last):
   File "/usr/local/lib/python3.11/site-packages/urllib3/connection.py", line 198, in _new_conn
     sock = connection.create_connection(
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "/usr/local/lib/python3.11/site-packages/urllib3/util/connection.py", line 60, in create_connection
     for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "/usr/local/lib/python3.11/socket.py", line 962, in getaddrinfo
     for res in _socket.getaddrinfo(host, port, family, type, proto, flags):
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
 socket.gaierror: [Errno -2] Name or service not known
 
 The above exception was the direct cause of the following exception:
 
 Traceback (most recent call last):
   File "/usr/local/lib/python3.11/site-packages/botocore/httpsession.py", line 464, in send
     urllib_response = conn.urlopen(
                       ^^^^^^^^^^^^^
   File "/usr/local/lib/python3.11/site-packages/urllib3/connectionpool.py", line 847, in urlopen
     retries = retries.increment(
               ^^^^^^^^^^^^^^^^^^
   File "/usr/local/lib/python3.11/site-packages/urllib3/util/retry.py", line 445, in increment
     raise reraise(type(error), error, _stacktrace)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "/usr/local/lib/python3.11/site-packages/urllib3/util/util.py", line 39, in reraise
     raise value
   File "/usr/local/lib/python3.11/site-packages/urllib3/connectionpool.py", line 793, in urlopen
     response = self._make_request(
                ^^^^^^^^^^^^^^^^^^^
   File "/usr/local/lib/python3.11/site-packages/urllib3/connectionpool.py", line 491, in _make_request
     raise new_e
   File "/usr/local/lib/python3.11/site-packages/urllib3/connectionpool.py", line 467, in _make_request
     self._validate_conn(conn)
   File "/usr/local/lib/python3.11/site-packages/urllib3/connectionpool.py", line 1099, in _validate_conn
     conn.connect()
   File "/usr/local/lib/python3.11/site-packages/urllib3/connection.py", line 616, in connect
     self.sock = sock = self._new_conn()
                        ^^^^^^^^^^^^^^^^
   File "/usr/local/lib/python3.11/site-packages/urllib3/connection.py", line 205, in _new_conn
     raise NameResolutionError(self.host, self, e) from e
 urllib3.exceptions.NameResolutionError: <botocore.awsrequest.AWSHTTPSConnection object at 0x7fbd73ddae90>: Failed to resolve 'sts.amazonaws.com' ([Errno -2] Name or service not known)
 
 During handling of the above exception, another exception occurred:
 
 Traceback (most recent call last):
   File "/usr/local/lib/python3.11/site-packages/airflow/providers/amazon/aws/secrets/secrets_manager.py", line 312, in _get_secret
     response = self.client.get_secret_value(
                ^^^^^^^^^^^
   File "/usr/local/lib/python3.11/functools.py", line 1001, in __get__
     val = self.func(instance)
           ^^^^^^^^^^^^^^^^^^^
   File "/usr/local/lib/python3.11/site-packages/airflow/providers/amazon/aws/secrets/secrets_manager.py", line 198, in client
     session = SessionFactory(conn=conn_config).create_session()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "/usr/local/lib/python3.11/site-packages/airflow/providers/amazon/aws/hooks/base_aws.py", line 192, in create_session
     return self._create_session_with_assume_role(
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "/usr/local/lib/python3.11/site-packages/airflow/providers/amazon/aws/hooks/base_aws.py", line 221, in _create_session_with_assume_role
     "metadata": self._refresh_credentials(),
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "/usr/local/lib/python3.11/site-packages/airflow/providers/amazon/aws/hooks/base_aws.py", line 257, in _refresh_credentials
     sts_response = self._assume_role(sts_client=sts_client)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "/usr/local/lib/python3.11/site-packages/airflow/providers/amazon/aws/hooks/base_aws.py", line 282, in _assume_role
     return sts_client.assume_role(**kw)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "/usr/local/lib/python3.11/site-packages/botocore/client.py", line 553, in _api_call
     return self._make_api_call(operation_name, kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "/usr/local/lib/python3.11/site-packages/botocore/client.py", line 989, in _make_api_call
     http, parsed_response = self._make_request(
                             ^^^^^^^^^^^^^^^^^^^
   File "/usr/local/lib/python3.11/site-packages/botocore/client.py", line 1015, in _make_request
     return self._endpoint.make_request(operation_model, request_dict)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "/usr/local/lib/python3.11/site-packages/botocore/endpoint.py", line 119, in make_request
     return self._send_request(request_dict, operation_model)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "/usr/local/lib/python3.11/site-packages/botocore/endpoint.py", line 202, in _send_request
     while self._needs_retry(
           ^^^^^^^^^^^^^^^^^^
   File "/usr/local/lib/python3.11/site-packages/botocore/endpoint.py", line 354, in _needs_retry
     responses = self._event_emitter.emit(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^
   File "/usr/local/lib/python3.11/site-packages/botocore/hooks.py", line 412, in emit
     return self._emitter.emit(aliased_event_name, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "/usr/local/lib/python3.11/site-packages/botocore/hooks.py", line 256, in emit
     return self._emit(event_name, kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "/usr/local/lib/python3.11/site-packages/botocore/hooks.py", line 239, in _emit
     response = handler(**kwargs)
                ^^^^^^^^^^^^^^^^^
   File "/usr/local/lib/python3.11/site-packages/botocore/retryhandler.py", line 207, in __call__
     if self._checker(**checker_kwargs):
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "/usr/local/lib/python3.11/site-packages/botocore/retryhandler.py", line 284, in __call__
     should_retry = self._should_retry(
                    ^^^^^^^^^^^^^^^^^^^
   File "/usr/local/lib/python3.11/site-packages/botocore/retryhandler.py", line 320, in _should_retry
     return self._checker(attempt_number, response, caught_exception)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "/usr/local/lib/python3.11/site-packages/botocore/retryhandler.py", line 363, in __call__
     checker_response = checker(
                        ^^^^^^^^
   File "/usr/local/lib/python3.11/site-packages/botocore/retryhandler.py", line 247, in __call__
     return self._check_caught_exception(
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "/usr/local/lib/python3.11/site-packages/botocore/retryhandler.py", line 416, in _check_caught_exception
     raise caught_exception
   File "/usr/local/lib/python3.11/site-packages/botocore/endpoint.py", line 281, in _do_get_response
     http_response = self._send(request)
                     ^^^^^^^^^^^^^^^^^^^
   File "/usr/local/lib/python3.11/site-packages/botocore/endpoint.py", line 377, in _send
     return self.http_session.send(request)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "/usr/local/lib/python3.11/site-packages/botocore/httpsession.py", line 493, in send
     raise EndpointConnectionError(endpoint_url=request.url, error=e)
 botocore.exceptions.EndpointConnectionError: Could not connect to the endpoint URL: "https://sts.amazonaws.com/"
 
 During handling of the above exception, another exception occurred:
 
 Traceback (most recent call last):
   File "/usr/local/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
     self.run()
   File "/usr/local/lib/python3.11/multiprocessing/process.py", line 108, in run
     self._target(*self._args, **self._kwargs)
   File "/usr/local/lib/python3.11/site-packages/airflow/dag_processing/processor.py", line 182, in _run_file_processor
     _handle_dag_file_processing()
   File "/usr/local/lib/python3.11/site-packages/airflow/dag_processing/processor.py", line 163, in _handle_dag_file_processing
     result: tuple[int, int] = dag_file_processor.process_file(
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "/usr/local/lib/python3.11/site-packages/airflow/utils/session.py", line 79, in wrapper
     return func(*args, session=session, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "/usr/local/lib/python3.11/site-packages/airflow/dag_processing/processor.py", line 833, in process_file
     dagbag = DagFileProcessor._get_dagbag(file_path)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "/usr/local/lib/python3.11/site-packages/airflow/dag_processing/processor.py", line 797, in _get_dagbag
     return DagBag(file_path, include_examples=False)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "/usr/local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 152, in __init__
     self.collect_dags(
   File "/usr/local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 559, in collect_dags
     found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "/usr/local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 318, in process_file
     mods = self._load_modules_from_file(filepath, safe_mode)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "/usr/local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 381, in _load_modules_from_file
     return parse(mod_name, filepath)
            ^^^^^^^^^^^^^^^^^^^^^^^^^
   File "/usr/local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
     loader.exec_module(new_module)
   File "<frozen importlib._bootstrap_external>", line 940, in exec_module
   File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
   File "/usr/local/airflow/dags/capstone/airflow_dags/dbt_dag_weekly_metrics.py", line 28, in <module>
     dbt_dag_cosmos_DbtDag = DbtDag(
                             ^^^^^^^
   File "/usr/local/lib/python3.11/site-packages/cosmos/airflow/dag.py", line 26, in __init__
     DbtToAirflowConverter.__init__(self, *args, **specific_kwargs(**kwargs))
   File "/usr/local/lib/python3.11/site-packages/cosmos/converter.py", line 260, in __init__
     self.dbt_graph.load(method=render_config.load_method, execution_mode=execution_config.execution_mode)
   File "/usr/local/lib/python3.11/site-packages/cosmos/dbt/graph.py", line 402, in load
     self.load_via_dbt_ls()
   File "/usr/local/lib/python3.11/site-packages/cosmos/dbt/graph.py", line 460, in load_via_dbt_ls
     if not self.load_via_dbt_ls_cache():
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "/usr/local/lib/python3.11/site-packages/cosmos/dbt/graph.py", line 474, in load_via_dbt_ls_cache
     cache_dict = self.get_dbt_ls_cache()
                  ^^^^^^^^^^^^^^^^^^^^^^^
   File "/usr/local/lib/python3.11/site-packages/cosmos/dbt/graph.py", line 361, in get_dbt_ls_cache
     else Variable.get(self.dbt_ls_cache_key, deserialize_json=True)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "/usr/local/lib/python3.11/site-packages/airflow/models/variable.py", line 138, in get
     var_val = Variable.get_variable_from_secrets(key=key)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "/usr/local/lib/python3.11/site-packages/airflow/models/variable.py", line 283, in get_variable_from_secrets
     var_val = secrets_backend.get_variable(key=key)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "/usr/local/lib/python3.11/site-packages/airflow/providers/amazon/aws/secrets/secrets_manager.py", line 279, in get_variable
     return self._get_secret(self.variables_prefix, key, self.variables_lookup_pattern)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "/usr/local/lib/python3.11/site-packages/airflow/providers/amazon/aws/secrets/secrets_manager.py", line 316, in _get_secret
     except self.client.exceptions.ResourceNotFoundException:
            ^^^^^^^^^^^
   File "/usr/local/lib/python3.11/functools.py", line 1001, in __get__
     val = self.func(instance)
           ^^^^^^^^^^^^^^^^^^^
   File "/usr/local/lib/python3.11/site-packages/airflow/providers/amazon/aws/secrets/secrets_manager.py", line 198, in client
     session = SessionFactory(conn=conn_config).create_session()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "/usr/local/lib/python3.11/site-packages/airflow/providers/amazon/aws/hooks/base_aws.py", line 192, in create_session
     return self._create_session_with_assume_role(
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "/usr/local/lib/python3.11/site-packages/airflow/providers/amazon/aws/hooks/base_aws.py", line 221, in _create_session_with_assume_role
     "metadata": self._refresh_credentials(),
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "/usr/local/lib/python3.11/site-packages/airflow/providers/amazon/aws/hooks/base_aws.py", line 257, in _refresh_credentials
     sts_response = self._assume_role(sts_client=sts_client)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "/usr/local/lib/python3.11/site-packages/airflow/providers/amazon/aws/hooks/base_aws.py", line 282, in _assume_role
     return sts_client.assume_role(**kw)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "/usr/local/lib/python3.11/site-packages/botocore/client.py", line 553, in _api_call
     return self._make_api_call(operation_name, kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "/usr/local/lib/python3.11/site-packages/botocore/client.py", line 989, in _make_api_call
     http, parsed_response = self._make_request(
                             ^^^^^^^^^^^^^^^^^^^
   File "/usr/local/lib/python3.11/site-packages/botocore/client.py", line 1015, in _make_request
     return self._endpoint.make_request(operation_model, request_dict)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "/usr/local/lib/python3.11/site-packages/botocore/endpoint.py", line 119, in make_request
     return self._send_request(request_dict, operation_model)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "/usr/local/lib/python3.11/site-packages/botocore/endpoint.py", line 202, in _send_request
     while self._needs_retry(
           ^^^^^^^^^^^^^^^^^^
   File "/usr/local/lib/python3.11/site-packages/botocore/endpoint.py", line 373, in _needs_retry
     time.sleep(handler_response)
   File "/usr/local/lib/python3.11/site-packages/airflow/utils/timeout.py", line 69, in handle_timeout
     raise AirflowTaskTimeout(self.error_message)
 airflow.exceptions.AirflowTaskTimeout: DagBag import timeout for /usr/local/airflow/dags/capstone/airflow_dags/dbt_dag_weekly_metrics.py after 30.0s.
 Please take a look at these docs to improve your DAG import time:
 * http://apache-airflow-docs.s3-website.eu-central-1.amazonaws.com/docs/apache-airflow/stable/best-practices.html#top-level-python-code
 * http://apache-airflow-docs.s3-website.eu-central-1.amazonaws.com/docs/apache-airflow/stable/best-practices.html#reducing-dag-complexity, PID: 20919
 [[34m2024-11-27T16:50:14.895+0000[0m] {[34mmanager.py:[0m285} ERROR[0m - DagFileProcessorManager (PID=179) last sent a heartbeat 3019.20 seconds ago! Restarting it[0m
 [[34m2024-11-27T16:50:14.945+0000[0m] {[34mprocess_utils.py:[0m132} INFO[0m - Sending 15 to group 179. PIDs of all processes in the group: [20919, 20925, 179][0m
 [[34m2024-11-27T16:50:14.945+0000[0m] {[34mprocess_utils.py:[0m87} INFO[0m - Sending the signal 15 to group 179[0m
 [[34m2024-11-27T16:50:16.829+0000[0m] {[34mprocess_utils.py:[0m80} INFO[0m - Process psutil.Process(pid=20919, status='terminated', started='15:59:28') (20919) terminated with exit code None[0m
 [2024-11-27T16:50:17.117+0000] {process_utils.py:263} INFO - Waiting up to 5 seconds for processes to exit...
 [[34m2024-11-27T16:50:17.189+0000[0m] {[34mprocess_utils.py:[0m80} INFO[0m - Process psutil.Process(pid=179, status='terminated', exitcode=0, started='14:12:58') (179) terminated with exit code 0[0m
 [[34m2024-11-27T16:50:17.197+0000[0m] {[34mprocess_utils.py:[0m80} INFO[0m - Process psutil.Process(pid=20925, status='terminated', started='15:59:42') (20925) terminated with exit code None[0m
 [[34m2024-11-27T16:50:17.312+0000[0m] {[34mmanager.py:[0m170} INFO[0m - Launched DagFileProcessorManager with pid: 20934[0m
 [[34m2024-11-27T16:50:17.715+0000[0m] {[34msettings.py:[0m60} INFO[0m - Configured default timezone UTC[0m
 [[34m2024-11-27T16:50:17.783+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.weekly_metrics'[0m
 [[34m2024-11-27T16:50:17.785+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections'[0m
 [[34m2024-11-27T16:50:17.786+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.dim_date'[0m
 [[34m2024-11-27T16:50:17.786+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_payments'[0m
 [[34m2024-11-27T16:50:17.787+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_orders'[0m
 [[34m2024-11-27T16:50:17.788+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_daily_bars'[0m
 [[34m2024-11-27T16:50:17.835+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
 [[34m2024-11-27T16:51:17.821+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.weekly_metrics'[0m
 [[34m2024-11-27T16:51:17.849+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections'[0m
 [[34m2024-11-27T16:51:17.850+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.dim_date'[0m
 [[34m2024-11-27T16:51:17.850+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_payments'[0m
 [[34m2024-11-27T16:51:17.850+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_orders'[0m
 [[34m2024-11-27T16:51:17.850+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_daily_bars'[0m
 [[34m2024-11-27T16:52:17.907+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.weekly_metrics'[0m
 [[34m2024-11-27T16:52:17.907+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections'[0m
 [[34m2024-11-27T16:52:17.907+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.dim_date'[0m
 [[34m2024-11-27T16:52:17.908+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_payments'[0m
 [[34m2024-11-27T16:52:17.908+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_orders'[0m
 [[34m2024-11-27T16:52:17.908+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_daily_bars'[0m
 [[34m2024-11-27T16:53:17.950+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.weekly_metrics'[0m
 [[34m2024-11-27T16:53:17.950+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections'[0m
 [[34m2024-11-27T16:53:17.950+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.dim_date'[0m
 [[34m2024-11-27T16:53:17.950+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_payments'[0m
 [[34m2024-11-27T16:53:17.951+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_orders'[0m
 [[34m2024-11-27T16:53:17.951+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_daily_bars'[0m
 [[34m2024-11-27T16:54:17.998+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.weekly_metrics'[0m
 [[34m2024-11-27T16:54:17.998+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections'[0m
 [[34m2024-11-27T16:54:17.998+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.dim_date'[0m
 [[34m2024-11-27T16:54:17.999+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_payments'[0m
 [[34m2024-11-27T16:54:17.999+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_orders'[0m
 [[34m2024-11-27T16:54:17.999+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_daily_bars'[0m
 [[34m2024-11-27T16:55:17.837+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
 [[34m2024-11-27T16:55:18.044+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.weekly_metrics'[0m
 [[34m2024-11-27T16:55:18.044+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections'[0m
 [[34m2024-11-27T16:55:18.045+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.dim_date'[0m
 [[34m2024-11-27T16:55:18.045+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_payments'[0m
 [[34m2024-11-27T16:55:18.045+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_orders'[0m
 [[34m2024-11-27T16:55:18.045+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_daily_bars'[0m
 [[34m2024-11-27T16:56:18.087+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.weekly_metrics'[0m
 [[34m2024-11-27T16:56:18.088+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections'[0m
 [[34m2024-11-27T16:56:18.088+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.dim_date'[0m
 [[34m2024-11-27T16:56:18.088+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_payments'[0m
 [[34m2024-11-27T16:56:18.088+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_orders'[0m
 [[34m2024-11-27T16:56:18.088+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_daily_bars'[0m
 [[34m2024-11-27T16:57:18.173+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.weekly_metrics'[0m
 [[34m2024-11-27T16:57:18.175+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections'[0m
 [[34m2024-11-27T16:57:18.176+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.dim_date'[0m
 [[34m2024-11-27T16:57:18.177+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_payments'[0m
 [[34m2024-11-27T16:57:18.178+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_orders'[0m
 [[34m2024-11-27T16:57:18.178+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_daily_bars'[0m
 [[34m2024-11-27T16:58:18.232+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.weekly_metrics'[0m
 [[34m2024-11-27T16:58:18.233+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections'[0m
 [[34m2024-11-27T16:58:18.233+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.dim_date'[0m
 [[34m2024-11-27T16:58:18.234+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_payments'[0m
 [[34m2024-11-27T16:58:18.234+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_orders'[0m
 [[34m2024-11-27T16:58:18.234+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_daily_bars'[0m
 [[34m2024-11-27T16:59:18.280+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.weekly_metrics'[0m
 [[34m2024-11-27T16:59:18.281+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections'[0m
 [[34m2024-11-27T16:59:18.281+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.dim_date'[0m
 [[34m2024-11-27T16:59:18.281+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_payments'[0m
 [[34m2024-11-27T16:59:18.281+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_orders'[0m
 [[34m2024-11-27T16:59:18.281+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_daily_bars'[0m
 [[34m2024-11-27T17:00:17.888+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
 [[34m2024-11-27T17:00:18.330+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.weekly_metrics'[0m
 [[34m2024-11-27T17:00:18.330+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections'[0m
 [[34m2024-11-27T17:00:18.330+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.dim_date'[0m
 [[34m2024-11-27T17:00:18.331+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_payments'[0m
 [[34m2024-11-27T17:00:18.331+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_orders'[0m
 [[34m2024-11-27T17:00:18.331+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_daily_bars'[0m
 [[34m2024-11-27T17:01:18.370+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.weekly_metrics'[0m
 [[34m2024-11-27T17:01:18.370+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections'[0m
 [[34m2024-11-27T17:01:18.370+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.dim_date'[0m
 [[34m2024-11-27T17:01:18.371+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_payments'[0m
 [[34m2024-11-27T17:01:18.371+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_orders'[0m
 [[34m2024-11-27T17:01:18.371+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_daily_bars'[0m
 [[34m2024-11-27T17:02:18.413+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.weekly_metrics'[0m
 [[34m2024-11-27T17:02:18.414+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections'[0m
 [[34m2024-11-27T17:02:18.414+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.dim_date'[0m
 [[34m2024-11-27T17:02:18.414+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_payments'[0m
 [[34m2024-11-27T17:02:18.414+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_orders'[0m
 [[34m2024-11-27T17:02:18.415+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_daily_bars'[0m
 [[34m2024-11-27T17:03:18.459+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.weekly_metrics'[0m
 [[34m2024-11-27T17:03:18.459+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections'[0m
 [[34m2024-11-27T17:03:18.459+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.dim_date'[0m
 [[34m2024-11-27T17:03:18.460+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_payments'[0m
 [[34m2024-11-27T17:03:18.460+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_orders'[0m
 [[34m2024-11-27T17:03:18.460+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_daily_bars'[0m
 [[34m2024-11-27T17:04:18.577+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.weekly_metrics'[0m
 [[34m2024-11-27T17:04:18.579+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections'[0m
 [[34m2024-11-27T17:04:18.579+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.dim_date'[0m
 [[34m2024-11-27T17:04:18.579+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_payments'[0m
 [[34m2024-11-27T17:04:18.580+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_orders'[0m
 [[34m2024-11-27T17:04:18.580+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_daily_bars'[0m
 [[34m2024-11-27T17:05:17.914+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
 [[34m2024-11-27T17:05:18.636+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.weekly_metrics'[0m
 [[34m2024-11-27T17:05:18.636+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections'[0m
 [[34m2024-11-27T17:05:18.636+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.dim_date'[0m
 [[34m2024-11-27T17:05:18.637+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_payments'[0m
 [[34m2024-11-27T17:05:18.637+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_orders'[0m
 [[34m2024-11-27T17:05:18.637+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_daily_bars'[0m
 [[34m2024-11-27T17:06:18.679+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.weekly_metrics'[0m
 [[34m2024-11-27T17:06:18.680+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections'[0m
 [[34m2024-11-27T17:06:18.680+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.dim_date'[0m
 [[34m2024-11-27T17:06:18.680+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_payments'[0m
 [[34m2024-11-27T17:06:18.681+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_orders'[0m
 [[34m2024-11-27T17:06:18.682+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_daily_bars'[0m
 [[34m2024-11-27T17:07:18.717+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.weekly_metrics'[0m
 [[34m2024-11-27T17:07:18.717+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections'[0m
 [[34m2024-11-27T17:07:18.717+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.dim_date'[0m
 [[34m2024-11-27T17:07:18.717+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_payments'[0m
 [[34m2024-11-27T17:07:18.718+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_orders'[0m
 [[34m2024-11-27T17:07:18.718+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_daily_bars'[0m
 [[34m2024-11-27T17:08:18.766+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.weekly_metrics'[0m
 [[34m2024-11-27T17:08:18.767+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections'[0m
 [[34m2024-11-27T17:08:18.767+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.dim_date'[0m
 [[34m2024-11-27T17:08:18.767+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_payments'[0m
 [[34m2024-11-27T17:08:18.767+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_orders'[0m
 [[34m2024-11-27T17:08:18.768+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_daily_bars'[0m
 [[34m2024-11-27T17:09:18.803+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.weekly_metrics'[0m
 [[34m2024-11-27T17:09:18.803+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections'[0m
 [[34m2024-11-27T17:09:18.804+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.dim_date'[0m
 [[34m2024-11-27T17:09:18.804+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_payments'[0m
 [[34m2024-11-27T17:09:18.804+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_orders'[0m
 [[34m2024-11-27T17:09:18.804+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_daily_bars'[0m
 [[34m2024-11-27T17:10:17.946+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
 [[34m2024-11-27T17:10:18.850+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.weekly_metrics'[0m
 [[34m2024-11-27T17:10:18.851+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections'[0m
 [[34m2024-11-27T17:10:18.851+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.dim_date'[0m
 [[34m2024-11-27T17:10:18.851+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_payments'[0m
 [[34m2024-11-27T17:10:18.851+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_orders'[0m
 [[34m2024-11-27T17:10:18.852+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_daily_bars'[0m
 [[34m2024-11-27T17:11:18.893+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.weekly_metrics'[0m
 [[34m2024-11-27T17:11:18.893+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections'[0m
 [[34m2024-11-27T17:11:18.894+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.dim_date'[0m
 [[34m2024-11-27T17:11:18.894+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_payments'[0m
 [[34m2024-11-27T17:11:18.894+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_orders'[0m
 [[34m2024-11-27T17:11:18.894+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_daily_bars'[0m
 [[34m2024-11-27T17:12:18.938+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.weekly_metrics'[0m
 [[34m2024-11-27T17:12:18.938+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections'[0m
 [[34m2024-11-27T17:12:18.938+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.dim_date'[0m
 [[34m2024-11-27T17:12:18.939+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_payments'[0m
 [[34m2024-11-27T17:12:18.939+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_orders'[0m
 [[34m2024-11-27T17:12:18.939+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_daily_bars'[0m
 [[34m2024-11-27T17:13:18.983+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.weekly_metrics'[0m
 [[34m2024-11-27T17:13:18.984+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections'[0m
 [[34m2024-11-27T17:13:18.984+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.dim_date'[0m
 [[34m2024-11-27T17:13:18.984+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_payments'[0m
 [[34m2024-11-27T17:13:18.984+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_orders'[0m
 [[34m2024-11-27T17:13:18.984+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_daily_bars'[0m
 [[34m2024-11-27T17:14:19.027+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.weekly_metrics'[0m
 [[34m2024-11-27T17:14:19.028+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections'[0m
 [[34m2024-11-27T17:14:19.028+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.dim_date'[0m
 [[34m2024-11-27T17:14:19.028+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_payments'[0m
 [[34m2024-11-27T17:14:19.028+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_orders'[0m
 [[34m2024-11-27T17:14:19.028+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_daily_bars'[0m
 [[34m2024-11-27T17:15:17.970+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
 [[34m2024-11-27T17:15:19.079+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.weekly_metrics'[0m
 [[34m2024-11-27T17:15:19.080+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections'[0m
 [[34m2024-11-27T17:15:19.080+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.dim_date'[0m
 [[34m2024-11-27T17:15:19.080+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_payments'[0m
 [[34m2024-11-27T17:15:19.080+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_orders'[0m
 [[34m2024-11-27T17:15:19.080+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_daily_bars'[0m
 [[34m2024-11-27T17:16:19.144+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.weekly_metrics'[0m
 [[34m2024-11-27T17:16:19.144+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections'[0m
 [[34m2024-11-27T17:16:19.145+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.dim_date'[0m
 [[34m2024-11-27T17:16:19.145+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_payments'[0m
 [[34m2024-11-27T17:16:19.145+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_orders'[0m
 [[34m2024-11-27T17:16:19.145+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_daily_bars'[0m
 [[34m2024-11-27T17:17:19.194+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.weekly_metrics'[0m
 [[34m2024-11-27T17:17:19.194+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections'[0m
 [[34m2024-11-27T17:17:19.194+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.dim_date'[0m
 [[34m2024-11-27T17:17:19.195+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_payments'[0m
 [[34m2024-11-27T17:17:19.195+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_orders'[0m
 [[34m2024-11-27T17:17:19.195+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_daily_bars'[0m
 [[34m2024-11-27T17:18:19.239+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.weekly_metrics'[0m
 [[34m2024-11-27T17:18:19.239+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections'[0m
 [[34m2024-11-27T17:18:19.240+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.dim_date'[0m
 [[34m2024-11-27T17:18:19.240+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_payments'[0m
 [[34m2024-11-27T17:18:19.240+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_orders'[0m
 [[34m2024-11-27T17:18:19.240+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_daily_bars'[0m
 [[34m2024-11-27T17:19:19.302+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.weekly_metrics'[0m
 [[34m2024-11-27T17:19:19.303+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections'[0m
 [[34m2024-11-27T17:19:19.303+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.dim_date'[0m
 [[34m2024-11-27T17:19:19.304+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_payments'[0m
 [[34m2024-11-27T17:19:19.304+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_orders'[0m
 [[34m2024-11-27T17:19:19.305+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_daily_bars'[0m
 [[34m2024-11-27T17:20:18.004+0000[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
 [[34m2024-11-27T17:20:19.353+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.weekly_metrics'[0m
 [[34m2024-11-27T17:20:19.353+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections'[0m
 [[34m2024-11-27T17:20:19.353+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.dim_date'[0m
 [[34m2024-11-27T17:20:19.353+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_payments'[0m
 [[34m2024-11-27T17:20:19.354+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_orders'[0m
 [[34m2024-11-27T17:20:19.354+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_daily_bars'[0m
 [[34m2024-11-27T17:21:19.399+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.weekly_metrics'[0m
 [[34m2024-11-27T17:21:19.399+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_treasury_revenue_collections'[0m
 [[34m2024-11-27T17:21:19.399+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.dim_date'[0m
 [[34m2024-11-27T17:21:19.400+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_payments'[0m
 [[34m2024-11-27T17:21:19.400+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_orders'[0m
 [[34m2024-11-27T17:21:19.400+0000[0m] {[34mscheduler_job_runner.py:[0m1786} INFO[0m - Orphaning unreferenced dataset 'snowflake://aab46027.us-west-2.aws/DATAEXPERT_STUDENT.andres.stg_daily_bars'[0m
 [[34m2024-11-27T17:22:18.707+0000[0m] {[34mscheduler_job_runner.py:[0m256} INFO[0m - Exiting gracefully upon receiving signal 15[0m
 [[34m2024-11-27T17:22:19.767+0000[0m] {[34mprocess_utils.py:[0m132} INFO[0m - Sending 15 to group 20934. PIDs of all processes in the group: [26703, 26731, 20934][0m
 [[34m2024-11-27T17:22:19.769+0000[0m] {[34mprocess_utils.py:[0m87} INFO[0m - Sending the signal 15 to group 20934[0m
 [[34m2024-11-27T17:22:22.239+0000[0m] {[34mprocess_utils.py:[0m80} INFO[0m - Process psutil.Process(pid=26703, status='terminated', started='17:22:04') (26703) terminated with exit code None[0m
 [2024-11-27T17:22:23.802+0000] {process_utils.py:263} INFO - Waiting up to 5 seconds for processes to exit...
 [[34m2024-11-27T17:22:24.019+0000[0m] {[34mprocess_utils.py:[0m80} INFO[0m - Process psutil.Process(pid=20934, status='terminated', exitcode=0, started='16:50:16') (20934) terminated with exit code 0[0m
 [[34m2024-11-27T17:22:24.021+0000[0m] {[34mprocess_utils.py:[0m80} INFO[0m - Process psutil.Process(pid=26731, status='terminated', started='17:22:14') (26731) terminated with exit code None[0m
 [[34m2024-11-27T17:22:24.022+0000[0m] {[34mlocal_executor.py:[0m403} INFO[0m - Shutting down LocalExecutor; waiting for running tasks to finish.  Signal again if you don't want to wait.[0m
 [[34m2024-11-27T17:22:26.076+0000[0m] {[34mprocess_utils.py:[0m132} INFO[0m - Sending 15 to group 20934. PIDs of all processes in the group: [][0m
 [[34m2024-11-27T17:22:26.077+0000[0m] {[34mprocess_utils.py:[0m87} INFO[0m - Sending the signal 15 to group 20934[0m
 [[34m2024-11-27T17:22:26.077+0000[0m] {[34mprocess_utils.py:[0m101} INFO[0m - Sending the signal 15 to process 20934 as process group is missing.[0m
 [[34m2024-11-27T17:22:26.078+0000[0m] {[34mscheduler_job_runner.py:[0m872} INFO[0m - Exited execute loop[0m
 [2024-11-27 17:22:26 +0000] [46] [INFO] Handling signal: term
 [2024-11-27 17:22:26 +0000] [20915] [INFO] Worker exiting (pid: 20915)
 [2024-11-27 17:22:26 +0000] [57] [INFO] Worker exiting (pid: 57)
 [2024-11-27 17:22:26 +0000] [46] [INFO] Shutting down: Master
